[
    {
        "user": "U02RN7YDRQ9",
        "type": "message",
        "ts": "1659858350.485549",
        "client_msg_id": "1c94febb-0185-4ede-818b-40a401935d06",
        "text": "There's a nice multiproc example in the flumine performance docs but if you want to get in deep then <https:\/\/docs.python.org\/3\/library\/multiprocessing.html>",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "ge46f1f8b708",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/e46f1f8b708a630e3191de7b2c42b1d1.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0023-72.png",
            "first_name": "edblock",
            "real_name": "edblock",
            "display_name": "foxwood",
            "team": "T4G9NBD2M",
            "name": "eanb",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1659818116.353469",
        "parent_user_id": "U03N4QBJ0TV",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "=44Rv",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "There's a nice multiproc example in the flumine performance docs but if you want to get in deep then "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/docs.python.org\/3\/library\/multiprocessing.html"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U016TGY3676",
        "type": "message",
        "ts": "1659858576.649519",
        "client_msg_id": "dbd9556f-92bb-4c66-ac17-3da3794389b7",
        "text": "Also, make sure your files are local rather than on S3 as pulling from S3 will add network latency.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g6a681220e11",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/6a681220e11c1a2ed3685375d658dadb.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0001-72.png",
            "first_name": "",
            "real_name": "birchy",
            "display_name": "birchy",
            "team": "T4G9NBD2M",
            "name": "birchy",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1659818116.353469",
        "parent_user_id": "U03N4QBJ0TV",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "80PDd",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Also, make sure your files are local rather than on S3 as pulling from S3 will add network latency."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UQL0QDEKA",
        "type": "message",
        "ts": "1659870897.687819",
        "client_msg_id": "ecb164e6-da08-4ff6-b7ac-fa2efe9ddd3e",
        "text": "I'm using a 16 core Ryzen (os.cpu count = 32) and :\n```markets_per_worker = 8```\nAs a gauge, Runs around 7000 markets in 6 mins or so.\nI've tried timing it by changing the 8 up and down but seems to slow down when I increase beyond 8.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g444d10128c0",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/444d10128c0aaf094041a52a76796602.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0005-72.png",
            "first_name": "Peter",
            "real_name": "Peter Lenagh",
            "display_name": "PeterLe",
            "team": "T4G9NBD2M",
            "name": "lenagh",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1659818116.353469",
        "parent_user_id": "U03N4QBJ0TV",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "EiD",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I'm using a 16 core Ryzen (os.cpu count = 32) and :\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_preformatted",
                        "elements": [
                            {
                                "type": "text",
                                "text": "markets_per_worker = 8"
                            }
                        ],
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "As a gauge, Runs around 7000 markets in 6 mins or so.\nI've tried timing it by changing the 8 up and down but seems to slow down when I increase beyond 8."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U01C12ZEADQ",
        "type": "message",
        "ts": "1659871525.887829",
        "client_msg_id": "baa6b94a-ca2f-4ec4-bb9b-a05050abd48a",
        "text": "GIL? Or slow SSD?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "e3b7687b0ef8",
            "image_72": "https:\/\/avatars.slack-edge.com\/2020-10-06\/1432945617648_e3b7687b0ef86e4f28b8_72.png",
            "first_name": "Alessio",
            "real_name": "Alessio",
            "display_name": "Alessio",
            "team": "T4G9NBD2M",
            "name": "alessio",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1659818116.353469",
        "parent_user_id": "U03N4QBJ0TV",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "V5C+",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "GIL? Or slow SSD?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UQL0QDEKA",
        "type": "message",
        "ts": "1659874033.753209",
        "edited": {
            "user": "UQL0QDEKA",
            "ts": "1659874991.000000"
        },
        "client_msg_id": "3e4d4f12-b72d-4b6f-bce2-59f21dd5da85",
        "text": "Im using an MP600 PCIe, eitherway Im happy with the backtest speed, 6 mins to test that many is superfast , In the time I can make a brew, i can discover another losing strategy :grinning:",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g444d10128c0",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/444d10128c0aaf094041a52a76796602.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0005-72.png",
            "first_name": "Peter",
            "real_name": "Peter Lenagh",
            "display_name": "PeterLe",
            "team": "T4G9NBD2M",
            "name": "lenagh",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1659818116.353469",
        "parent_user_id": "U03N4QBJ0TV",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "fO8u",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Im using an MP600 PCIe, eitherway Im happy with the backtest speed, 6 mins to test that many is superfast , In the time I can make a brew, i can discover another losing strategy "
                            },
                            {
                                "type": "emoji",
                                "name": "grinning",
                                "unicode": "1f600"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "joy",
                "users": [
                    "U016TGY3676",
                    "U01C12ZEADQ",
                    "U02RN7YDRQ9"
                ],
                "count": 3
            },
            {
                "name": "rocket",
                "users": [
                    "U4H19D1D2"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "U9JHLMZB4",
        "type": "message",
        "ts": "1659901353.150699",
        "client_msg_id": "7704ab1b-d81e-4248-9e7e-2ccb54f8c9c1",
        "text": "Question for users of <@UBS7QANF3>â€™s betfairviz. Is there a way of getting runner names into the a dashboard when the file being examined is gzipped file generated by the marketrecorder. I have the corresponding market catalogue, but can't see a way to inject that data into betfairviz.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g951ddcb43e7",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3951ddcb43e788a387d6daf330dad5ca.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0018-72.png",
            "first_name": "",
            "real_name": "Peter Coles",
            "display_name": "Peter",
            "team": "T4G9NBD2M",
            "name": "peter",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1659901353.150699",
        "reply_count": 3,
        "reply_users_count": 2,
        "latest_reply": "1659957176.675589",
        "reply_users": [
            "UBS7QANF3",
            "U9JHLMZB4"
        ],
        "replies": [
            {
                "user": "UBS7QANF3",
                "ts": "1659903545.797559"
            },
            {
                "user": "U9JHLMZB4",
                "ts": "1659950345.358539"
            },
            {
                "user": "UBS7QANF3",
                "ts": "1659957176.675589"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "7rq8G",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Question for users of "
                            },
                            {
                                "type": "user",
                                "user_id": "UBS7QANF3"
                            },
                            {
                                "type": "text",
                                "text": "â€™s betfairviz. Is there a way of getting runner names into the a dashboard when the file being examined is gzipped file generated by the marketrecorder. I have the corresponding market catalogue, but can't see a way to inject that data into betfairviz."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1659903545.797559",
        "client_msg_id": "da0868a9-9a71-4456-bdb1-6930095e45e5",
        "text": "1. Read the market books using read_prices_file in betfairutil and pass it the market catalogue (the argument is market_catalogues so wrap it in a list)\n2. Pass the market books to create_dashboard instead of just the path to the prices file",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1659901353.150699",
        "parent_user_id": "U9JHLMZB4",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "OL4Vu",
                "elements": [
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Read the market books using read_prices_file in betfairutil and pass it the market catalogue (the argument is market_catalogues so wrap it in a list)"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Pass the market books to create_dashboard instead of just the path to the prices file"
                                    }
                                ]
                            }
                        ],
                        "style": "ordered",
                        "indent": 0,
                        "border": 0
                    }
                ]
            }
        ]
    },
    {
        "subtype": "thread_broadcast",
        "user": "U03N4QBJ0TV",
        "thread_ts": "1659818116.353469",
        "root": {
            "user": "U03N4QBJ0TV",
            "type": "message",
            "ts": "1659818116.353469",
            "edited": {
                "user": "U03N4QBJ0TV",
                "ts": "1659819201.000000"
            },
            "client_msg_id": "db065343-3873-42c9-944d-601681d636d5",
            "text": "AWS\/computing question if any one has a sec - If I am data mining  using 3000 files on a Volume attached to my EC2 does anyone know why switching from an 8 cpu instance to a 32 cpu instance doesn't result in an decrease in the time taken to process the files? Its the same time per file. I am a bit perplexed, thought increased CPUs would result in faster processing time.",
            "team": "T4G9NBD2M",
            "thread_ts": "1659818116.353469",
            "reply_count": 21,
            "reply_users_count": 10,
            "latest_reply": "1659990496.023559",
            "reply_users": [
                "U016TGY3676",
                "UBS7QANF3",
                "U03N4QBJ0TV",
                "UPMUFSGCR",
                "U4H19D1D2",
                "U02RN7YDRQ9",
                "UQL0QDEKA",
                "U01C12ZEADQ",
                "U9JHLMZB4",
                "U0128E7BEHW"
            ],
            "replies": [
                {
                    "user": "U016TGY3676",
                    "ts": "1659819747.532499"
                },
                {
                    "user": "UBS7QANF3",
                    "ts": "1659820060.213719"
                },
                {
                    "user": "U03N4QBJ0TV",
                    "ts": "1659821124.950659"
                },
                {
                    "user": "UPMUFSGCR",
                    "ts": "1659821674.148049"
                },
                {
                    "user": "U03N4QBJ0TV",
                    "ts": "1659824462.780539"
                },
                {
                    "user": "U03N4QBJ0TV",
                    "ts": "1659824487.492939"
                },
                {
                    "user": "U4H19D1D2",
                    "ts": "1659850806.461669"
                },
                {
                    "user": "U4H19D1D2",
                    "ts": "1659850819.044289"
                },
                {
                    "user": "UBS7QANF3",
                    "ts": "1659853517.053049"
                },
                {
                    "user": "U02RN7YDRQ9",
                    "ts": "1659858350.485549"
                },
                {
                    "user": "U016TGY3676",
                    "ts": "1659858576.649519"
                },
                {
                    "user": "UQL0QDEKA",
                    "ts": "1659870897.687819"
                },
                {
                    "user": "U01C12ZEADQ",
                    "ts": "1659871525.887829"
                },
                {
                    "user": "UQL0QDEKA",
                    "ts": "1659874033.753209"
                },
                {
                    "user": "U03N4QBJ0TV",
                    "ts": "1659914573.635739"
                },
                {
                    "user": "U9JHLMZB4",
                    "ts": "1659951595.405659"
                },
                {
                    "user": "U01C12ZEADQ",
                    "ts": "1659954084.832769"
                },
                {
                    "user": "UPMUFSGCR",
                    "ts": "1659955211.184829"
                },
                {
                    "user": "U01C12ZEADQ",
                    "ts": "1659955330.037289"
                },
                {
                    "user": "U01C12ZEADQ",
                    "ts": "1659955968.861269"
                },
                {
                    "user": "U0128E7BEHW",
                    "ts": "1659990496.023559"
                }
            ],
            "is_locked": false,
            "subscribed": true,
            "last_read": "1659990496.023559",
            "blocks": [
                {
                    "type": "rich_text",
                    "block_id": "FrCvS",
                    "elements": [
                        {
                            "type": "rich_text_section",
                            "elements": [
                                {
                                    "type": "text",
                                    "text": "AWS\/computing question if any one has a sec - If I am data mining  using 3000 files on a Volume attached to my EC2 does anyone know why switching from an 8 cpu instance to a 32 cpu instance doesn't result in an decrease in the time taken to process the files? Its the same time per file. I am a bit perplexed, thought increased CPUs would result in faster processing time."
                                }
                            ]
                        }
                    ]
                }
            ]
        },
        "type": "message",
        "ts": "1659914573.635739",
        "edited": {
            "user": "U03N4QBJ0TV",
            "ts": "1659914608.000000"
        },
        "client_msg_id": "f0dcb67c-2707-45b0-bd18-d11db70eb373",
        "text": "Well I got it down to 25mins for 3000 files on an 8 vcpu instance. Did it with some code edits and use of the multiprocessing package which, as I understand it, is making use of all the cores by processing each file asynchronously. Couldn't reduce the time using the threading package. All in all a good learning experience! For anyone else new to multiprocessing using python I would recommend <https:\/\/www.youtube.com\/watch?v=fKl2JW_qrso&amp;t=1215s|this video>.",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "f6+E",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Well I got it down to 25mins for 3000 files on an 8 vcpu instance. Did it with some code edits and use of the multiprocessing package which, as I understand it, is making use of all the cores by processing each file asynchronously. Couldn't reduce the time using the threading package. All in all a good learning experience! For anyone else new to multiprocessing using python I would recommend "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/www.youtube.com\/watch?v=fKl2JW_qrso&t=1215s",
                                "text": "this video"
                            },
                            {
                                "type": "text",
                                "text": "."
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U4H19D1D2",
                    "U02RN7YDRQ9"
                ],
                "count": 2
            }
        ]
    }
]
[
    {
        "user": "U0160E9HS2G",
        "type": "message",
        "ts": "1679573289.218149",
        "client_msg_id": "8425f17d-9b05-4142-85ff-2d115e7819da",
        "text": "Hi, when streaming market data using flumine to S3, is there a recommended way to collect and store metadata and marketcatalogue data into a lookup table in S3 or some database? Cheers",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g824e1cc27e2",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/824e1cc27e2d6ad6290dfb21ea43f1df.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0013-72.png",
            "first_name": "",
            "real_name": "JC",
            "display_name": "JC",
            "team": "T4G9NBD2M",
            "name": "joecussen96",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679573289.218149",
        "reply_count": 6,
        "reply_users_count": 3,
        "latest_reply": "1679591069.307039",
        "reply_users": [
            "U9JHLMZB4",
            "U0160E9HS2G",
            "U4H19D1D2"
        ],
        "replies": [
            {
                "user": "U9JHLMZB4",
                "ts": "1679574043.417719"
            },
            {
                "user": "U0160E9HS2G",
                "ts": "1679574289.132259"
            },
            {
                "user": "U9JHLMZB4",
                "ts": "1679575003.828139"
            },
            {
                "user": "U4H19D1D2",
                "ts": "1679575506.172639"
            },
            {
                "user": "U0160E9HS2G",
                "ts": "1679576827.771019"
            },
            {
                "user": "U4H19D1D2",
                "ts": "1679591069.307039"
            }
        ],
        "is_locked": false,
        "subscribed": true,
        "last_read": "1679591069.307039",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "mOq2L",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Hi, when streaming market data using flumine to S3, is there a recommended way to collect and store metadata and marketcatalogue data into a lookup table in S3 or some database? Cheers"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U9JHLMZB4",
        "type": "message",
        "ts": "1679574043.417719",
        "edited": {
            "user": "U9JHLMZB4",
            "ts": "1679574091.000000"
        },
        "client_msg_id": "7235f30e-2d3a-47d3-9fbb-4eea69f1d2ff",
        "text": "Not explicitly, but the catalogue is just a json file so it's easy to read, filter and persist the data you may want to look up afterwards. Personally I write key data from each to a database table together with a list of the winners (obtained by reading the last few lines of the associated stream file). That table is then used to compile lists of candidate markets for backtesting.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g951ddcb43e7",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3951ddcb43e788a387d6daf330dad5ca.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0018-72.png",
            "first_name": "",
            "real_name": "Peter Coles",
            "display_name": "Peter",
            "team": "T4G9NBD2M",
            "name": "peter",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679573289.218149",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "fu4",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Not explicitly, but the catalogue is just a json file so it's easy to read, filter and persist the data you may want to look up afterwards. Personally I write key data from each to a database table together with a list of the winners (obtained by reading the last few lines of the associated stream file). That table is then used to compile lists of candidate markets for backtesting."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U0160E9HS2G",
        "type": "message",
        "ts": "1679574289.132259",
        "client_msg_id": "57ce9979-721e-4705-832d-80b225910b8f",
        "text": "Thanks <@U9JHLMZB4>. Do you update your database on the fly with `on_market_close` or have a process that crawls through recorded streaming files to update your database periodically e.g. daily? I used to do the former but thinking of moving to the latter as I’m moving all storage to s3.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g824e1cc27e2",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/824e1cc27e2d6ad6290dfb21ea43f1df.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0013-72.png",
            "first_name": "",
            "real_name": "JC",
            "display_name": "JC",
            "team": "T4G9NBD2M",
            "name": "joecussen96",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679573289.218149",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "LzATD",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Thanks "
                            },
                            {
                                "type": "user",
                                "user_id": "U9JHLMZB4"
                            },
                            {
                                "type": "text",
                                "text": ". Do you update your database on the fly with "
                            },
                            {
                                "type": "text",
                                "text": "on_market_close",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " or have a process that crawls through recorded streaming files to update your database periodically e.g. daily? I used to do the former but thinking of moving to the latter as I’m moving all storage to s3."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U9JHLMZB4",
        "type": "message",
        "ts": "1679575003.828139",
        "client_msg_id": "eb85fafc-9204-4470-832c-a3aaef29c2ba",
        "text": "It's a process that runs independently on an EC2 instance. But that's mainly because of how I constructed it originally. It could just as well have been contained by a logging control as the market was closed, with pros and cons each way. E.g. logging control would avoid the need to re-visit the files, but re-visiting the files confirms that they made it to S3 and helps handle those with closing wrinkles (e.g, multiple closures or none recorded).",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g951ddcb43e7",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3951ddcb43e788a387d6daf330dad5ca.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0018-72.png",
            "first_name": "",
            "real_name": "Peter Coles",
            "display_name": "Peter",
            "team": "T4G9NBD2M",
            "name": "peter",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679573289.218149",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "8ruxb",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "It's a process that runs independently on an EC2 instance. But that's mainly because of how I constructed it originally. It could just as well have been contained by a logging control as the market was closed, with pros and cons each way. E.g. logging control would avoid the need to re-visit the files, but re-visiting the files confirms that they made it to S3 and helps handle those with closing wrinkles (e.g, multiple closures or none recorded)."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1679575506.172639",
        "client_msg_id": "dfe96178-b36d-4ae4-a1c0-057b1b040fee",
        "text": "The example is what I do, everything in s3 and a market table in a db which can be a fast lookup",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679573289.218149",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "qnn",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "The example is what I do, everything in s3 and a market table in a db which can be a fast lookup"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U0160E9HS2G",
        "type": "message",
        "ts": "1679576827.771019",
        "client_msg_id": "2d910b3a-d7ec-4c9f-ac36-cbda6902d968",
        "text": "Cheers <@U9JHLMZB4>, makes sense. <@U4H19D1D2> do you add to the market table in the db within flumine or with another process?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g824e1cc27e2",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/824e1cc27e2d6ad6290dfb21ea43f1df.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0013-72.png",
            "first_name": "",
            "real_name": "JC",
            "display_name": "JC",
            "team": "T4G9NBD2M",
            "name": "joecussen96",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679573289.218149",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "ZP2UO",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Cheers "
                            },
                            {
                                "type": "user",
                                "user_id": "U9JHLMZB4"
                            },
                            {
                                "type": "text",
                                "text": ", makes sense. "
                            },
                            {
                                "type": "user",
                                "user_id": "U4H19D1D2"
                            },
                            {
                                "type": "text",
                                "text": " do you add to the market table in the db within flumine or with another process?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1679591069.307039",
        "client_msg_id": "EC5CF891-5C74-4084-B57E-361B03C2A511",
        "text": "Flumine with a custom loggingcontrol that communicates via API to my db ",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679573289.218149",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Cwpg",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Flumine with a custom loggingcontrol that communicates via API to my db "
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U04V95S1G4E",
        "type": "message",
        "ts": "1679609506.297699",
        "edited": {
            "user": "U04V95S1G4E",
            "ts": "1679609574.000000"
        },
        "client_msg_id": "e80a37a5-5c1f-48d2-8454-3ac4a5d5041b",
        "text": "Hey guys, so glad I found this library, very cool!\n\nJust trying to figure out what would be best practice for my use (I read the official betfair stuff but doesn't really answer the question).\nMy understanding is that there is a limit to how many login requests the API can receive before soft-banning. The 'best practices' page states that a single login session can carry out multiple API calls as long as it's alive. I'm trying to make an app that gets some information from the API, hopefully in the future extend it to allow betting through the app as well. The app will make some requests to find out if a particular markets exists (by searching for a team name), and what the odds are at the time of request. The app will be using non-interactive logon as it is hands-off\n\nWould best practice be as follows? Logon --&gt; Get Data --&gt; Logout?\nOr should I just keep the connection open for the next API call? - I'd say the time in-between calls can be variable, sometimes in quick succession (seconds to minutes), but most of the time it would be in the minutes to hours range.\n\nIf the best thing to do is to keep the connection open, does the API send an exception code or some other response if a session is already open?\nI'm just trying to figure out how to code the logic: If already logged-in, send call, otherwise, attempt login and get a session id",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "1a09652c26a9",
            "image_72": "https:\/\/avatars.slack-edge.com\/2023-03-23\/5001808379042_1a09652c26a9d9556541_72.jpg",
            "first_name": "Jesus",
            "real_name": "Jesus Perdomo",
            "display_name": "Jesus Perdomo",
            "team": "T4G9NBD2M",
            "name": "jesusaperdomo",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "8Tly",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Hey guys, so glad I found this library, very cool!\n\nJust trying to figure out what would be best practice for my use (I read the official betfair stuff but doesn't really answer the question).\nMy understanding is that there is a limit to how many login requests the API can receive before soft-banning. The 'best practices' page states that a single login session can carry out multiple API calls as long as it's alive. I'm trying to make an app that gets some information from the API, hopefully in the future extend it to allow betting through the app as well. The app will make some requests to find out if a particular markets exists (by searching for a team name), and what the odds are at the time of request. The app will be using non-interactive logon as it is hands-off\n\nWould best practice be as follows? Logon --> Get Data --> Logout?\nOr should I just keep the connection open for the next API call? - I'd say the time in-between calls can be variable, sometimes in quick succession (seconds to minutes), but most of the time it would be in the minutes to hours range.\n\nIf the best thing to do is to keep the connection open, does the API send an exception code or some other response if a session is already open?\nI'm just trying to figure out how to code the logic: If already logged-in, send call, otherwise, attempt login and get a session id"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U042NB80A1X",
        "type": "message",
        "ts": "1679621030.481029",
        "client_msg_id": "81983fc7-869e-4d87-a722-140097d7a165",
        "text": "I'm still waiting on Betfair to allow my API to stream Delayed Data; it looks like they might have even completely restricted my connection to the Betfair API to make calls.\n\nI am testing code with it, although had throttled it to a single (horseracing) event and the markets within it; but was calling odds every minute to get market percentage - which I presume is why because it's streaming type behaviour.\n\nI've emailed them 3 times about the delayed app key, so it's a bit tricky to get the strategy going; I'm working on other bits and pieces in the meantime but it's tricky to see how flumine works without the streaming.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g3ab37102618",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3ab37102618df1ba57a0e6b5c8538c66.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0021-72.png",
            "first_name": "Tom",
            "real_name": "Tom",
            "display_name": "Tom",
            "team": "T4G9NBD2M",
            "name": "tkuhnau",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1679393463.612649",
        "parent_user_id": "U042NB80A1X",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "jOYzL",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I'm still waiting on Betfair to allow my API to stream Delayed Data; it looks like they might have even completely restricted my connection to the Betfair API to make calls.\n\nI am testing code with it, although had throttled it to a single (horseracing) event and the markets within it; but was calling odds every minute to get market percentage - which I presume is why because it's streaming type behaviour.\n\nI've emailed them 3 times about the delayed app key, so it's a bit tricky to get the strategy going; I'm working on other bits and pieces in the meantime but it's tricky to see how flumine works without the streaming."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]
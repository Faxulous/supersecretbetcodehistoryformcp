[
    {
        "user": "U034TBZ5H6F",
        "type": "message",
        "ts": "1702458470.817909",
        "edited": {
            "user": "U034TBZ5H6F",
            "ts": "1702459286.000000"
        },
        "client_msg_id": "0c82df6d-2149-43d5-b269-59e73c002197",
        "text": "hello i saw this on the betangel forum\n`Tennis API scores feed`\n `Due to a transfer of ATP tennis data ownership, we will no longer be able to offer the ATP and Challenger tennis feed to API customers from 1 January.`\n\n `We hope to resume the service by May 2024, pending ongoing negotiations and integrations with the new data rights owner.`\n\n `This will not impact the WTA events.`\n\n `We apologise for any disruption to your trading. We are working hard to restore service.`\nDoes that mean you  can’t trade ATP via betfair until this is sorted? or is it just there will be no scores info from the API?. Apologies if this is the wrong channel.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "22449732efe3",
            "image_72": "https:\/\/avatars.slack-edge.com\/2022-02-26\/3163398650530_22449732efe359b352bc_72.png",
            "first_name": "Tony",
            "real_name": "Tony",
            "display_name": "Tony",
            "team": "T4G9NBD2M",
            "name": "tonysound101",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1702458470.817909",
        "reply_count": 2,
        "reply_users_count": 2,
        "latest_reply": "1702465097.395449",
        "reply_users": [
            "UBS7QANF3",
            "U034TBZ5H6F"
        ],
        "replies": [
            {
                "user": "UBS7QANF3",
                "ts": "1702461127.897289"
            },
            {
                "user": "U034TBZ5H6F",
                "ts": "1702465097.395449"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "jv1CL",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "hello i saw this on the betangel forum\n"
                            },
                            {
                                "type": "text",
                                "text": "Tennis API scores feed",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\n"
                            },
                            {
                                "type": "text",
                                "text": " Due to a transfer of ATP tennis data ownership, we will no longer be able to offer the ATP and Challenger tennis feed to API customers from 1 January.",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\n\n"
                            },
                            {
                                "type": "text",
                                "text": " We hope to resume the service by May 2024, pending ongoing negotiations and integrations with the new data rights owner.",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\n\n"
                            },
                            {
                                "type": "text",
                                "text": " This will not impact the WTA events.",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\n\n"
                            },
                            {
                                "type": "text",
                                "text": " We apologise for any disruption to your trading. We are working hard to restore service.",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "\nDoes that mean you  can’t trade ATP via betfair until this is sorted? or is it just there will be no scores info from the API?. Apologies if this is the wrong channel."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1702461127.897289",
        "client_msg_id": "9D446C7D-EBCC-4804-8F75-C4AF1DEAAFFB",
        "text": "The latter",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1702458470.817909",
        "parent_user_id": "U034TBZ5H6F",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "dEQeu",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "The latter"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U034TBZ5H6F",
        "type": "message",
        "ts": "1702465097.395449",
        "client_msg_id": "dcb6046f-47ff-476e-acfe-a1ad02d0f9b4",
        "text": "thanks <@UBS7QANF3>",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "22449732efe3",
            "image_72": "https:\/\/avatars.slack-edge.com\/2022-02-26\/3163398650530_22449732efe359b352bc_72.png",
            "first_name": "Tony",
            "real_name": "Tony",
            "display_name": "Tony",
            "team": "T4G9NBD2M",
            "name": "tonysound101",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1702458470.817909",
        "parent_user_id": "U034TBZ5H6F",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "3aM8w",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "thanks "
                            },
                            {
                                "type": "user",
                                "user_id": "UBS7QANF3"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U069YFL03PC",
        "type": "message",
        "ts": "1702468058.333259",
        "client_msg_id": "1f9d240e-e95a-44b2-96d4-1dd5ae12dd39",
        "text": "Hi everyone, does anyone know of a good API for horse and dogs in Australia?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g7aa04499f3f",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/7aa04499f3f563d0098ede94c59a0589.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0014-72.png",
            "first_name": "jnik",
            "real_name": "jnik",
            "display_name": "jnik",
            "team": "T4G9NBD2M",
            "name": "cihilt",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "7lV0H",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Hi everyone, does anyone know of a good API for horse and dogs in Australia?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U01DCR5PXDY",
        "type": "message",
        "ts": "1702485569.261119",
        "client_msg_id": "9c3aa05d-e80a-4e37-900b-d7a57be167a9",
        "text": "What are people using for post-backtest analysis? I'm currently using jupyter-lab + sqlite + matplotlib, but hitting some scaling limits. Still works but with 20+ million rows, gets tedious creating the db and waiting for queries. Any suggestions?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "903bc20cd9db",
            "image_72": "https:\/\/avatars.slack-edge.com\/2020-10-26\/1448127317798_903bc20cd9dba9a758e0_72.png",
            "first_name": "thambie1",
            "real_name": "thambie1",
            "display_name": "thambie1",
            "team": "T4G9NBD2M",
            "name": "thambie1",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1702485569.261119",
        "reply_count": 1,
        "reply_users_count": 1,
        "latest_reply": "1702546843.407949",
        "reply_users": [
            "U05N9773A23"
        ],
        "replies": [
            {
                "user": "U05N9773A23",
                "ts": "1702546843.407949"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "sk1gi",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "What are people using for post-backtest analysis? I'm currently using jupyter-lab + sqlite + matplotlib, but hitting some scaling limits. Still works but with 20+ million rows, gets tedious creating the db and waiting for queries. Any suggestions?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UPMUFSGCR",
        "type": "message",
        "ts": "1702494412.195479",
        "client_msg_id": "e0d85534-d69a-4356-8b1e-d6fb0cf66a94",
        "text": "How much RAM does that use?\n\nI store my data in .feather files, but it all just about fits in RAM. I have 32GB.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "31c0bb5a442c",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-10-28\/812386967189_31c0bb5a442c5b8d2c61_72.png",
            "first_name": "Jon",
            "real_name": "Jon Jon Jon Jon Jon Jon Jon Jon",
            "display_name": "Jonjonjon",
            "team": "T4G9NBD2M",
            "name": "fcmisc",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "fk1vV",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "How much RAM does that use?\n\nI store my data in .feather files, but it all just about fits in RAM. I have 32GB."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UPMUFSGCR",
        "type": "message",
        "ts": "1702494471.800349",
        "client_msg_id": "2f35704e-4b24-4149-8782-d84b651ca7e4",
        "text": "Dask can be used to operate on multiple dataframe files in parallel, treating them as a single frame. But I don't really enjoy using it.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "31c0bb5a442c",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-10-28\/812386967189_31c0bb5a442c5b8d2c61_72.png",
            "first_name": "Jon",
            "real_name": "Jon Jon Jon Jon Jon Jon Jon Jon",
            "display_name": "Jonjonjon",
            "team": "T4G9NBD2M",
            "name": "fcmisc",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1702494471.800349",
        "reply_count": 2,
        "reply_users_count": 2,
        "latest_reply": "1702503807.966929",
        "reply_users": [
            "U01DCR5PXDY",
            "UHV5QTKDZ"
        ],
        "replies": [
            {
                "user": "U01DCR5PXDY",
                "ts": "1702495528.554729"
            },
            {
                "user": "UHV5QTKDZ",
                "ts": "1702503807.966929"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "GVY\/5",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Dask can be used to operate on multiple dataframe files in parallel, treating them as a single frame. But I don't really enjoy using it."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U01DCR5PXDY",
        "type": "message",
        "ts": "1702495528.554729",
        "client_msg_id": "8e4b6292-a661-4aa5-bdc9-16bd86a78eef",
        "text": "If fully loaded into memory about 10GB usually. But I want to have the capacity to run at around 3X that, so 30GB. Happy to buy a larger machine, it's more so the query\/visualization time being an issue. Creating a graph over 20 million rows, requires sqlite reading all rows, then sending 20 million data points to matplotlib, and then matplotlib graphing the data points. Ideally, this would be parallelized across multiple cpu cores, and would only send the necessary number of data points to the visualization engine. Pointless sending 20 million data points, when practically you can't see that many.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "903bc20cd9db",
            "image_72": "https:\/\/avatars.slack-edge.com\/2020-10-26\/1448127317798_903bc20cd9dba9a758e0_72.png",
            "first_name": "thambie1",
            "real_name": "thambie1",
            "display_name": "thambie1",
            "team": "T4G9NBD2M",
            "name": "thambie1",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1702494471.800349",
        "parent_user_id": "UPMUFSGCR",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "C4jpP",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "If fully loaded into memory about 10GB usually. But I want to have the capacity to run at around 3X that, so 30GB. Happy to buy a larger machine, it's more so the query\/visualization time being an issue. Creating a graph over 20 million rows, requires sqlite reading all rows, then sending 20 million data points to matplotlib, and then matplotlib graphing the data points. Ideally, this would be parallelized across multiple cpu cores, and would only send the necessary number of data points to the visualization engine. Pointless sending 20 million data points, when practically you can't see that many."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UHV5QTKDZ",
        "type": "message",
        "ts": "1702503807.966929",
        "client_msg_id": "7FA92416-9FAC-4096-8FE0-74D3E8570C57",
        "text": "SQLite is rock solid. Whatever I’ve chucked at it, it handles. Matplotlib will be your bottleneck , assuming you’ve written sensible sql queries, set a good index on the table etc. Look for a plotting library with rasterisation, or better handling of large data sets. Highcharts had something nice last time I used it. ",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gf2918ffe986",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/cf2918ffe986dfa700c443b86a5a4f9f.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0008-72.png",
            "first_name": "Clive",
            "real_name": "Clive",
            "display_name": "Clive",
            "team": "T4G9NBD2M",
            "name": "dewinn",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1702494471.800349",
        "parent_user_id": "UPMUFSGCR",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "nmXR7",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "SQLite is rock solid. Whatever I’ve chucked at it, it handles. Matplotlib will be your bottleneck , assuming you’ve written sensible sql queries, set a good index on the table etc"
                            },
                            {
                                "type": "text",
                                "text": "."
                            },
                            {
                                "type": "text",
                                "text": " Look for a plotting library with rasterisation, or better handling of large data sets"
                            },
                            {
                                "type": "text",
                                "text": "."
                            },
                            {
                                "type": "text",
                                "text": " Highcharts had something nice last time I used it"
                            },
                            {
                                "type": "text",
                                "text": "."
                            },
                            {
                                "type": "text",
                                "text": " "
                            }
                        ]
                    }
                ]
            }
        ]
    }
]
[
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1632815936.087000",
        "client_msg_id": "edd5b40a-8de7-4b25-84a0-12940713b126",
        "text": "My understanding is that conflation will occur if you are not reading off the socket quick enough, therefore it is bound by your network or your CPU speed",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632778160.086400",
        "parent_user_id": "U01DCR5PXDY",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "CU\/O",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "My understanding is that conflation will occur if you are not reading off the socket quick enough, therefore it is bound by your network or your CPU speed"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U01DCR5PXDY",
        "type": "message",
        "ts": "1632816114.087200",
        "edited": {
            "user": "U01DCR5PXDY",
            "ts": "1632816143.000000"
        },
        "client_msg_id": "c6c7e665-4f6b-4eb9-93e5-d563b34a929a",
        "text": "I checked at the OS level, the socket buffer was never even close to full, which is why I'm thinking the issue is on Betfair's end",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "903bc20cd9db",
            "image_72": "https:\/\/avatars.slack-edge.com\/2020-10-26\/1448127317798_903bc20cd9dba9a758e0_72.png",
            "first_name": "thambie1",
            "real_name": "thambie1",
            "display_name": "thambie1",
            "team": "T4G9NBD2M",
            "name": "thambie1",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632778160.086400",
        "parent_user_id": "U01DCR5PXDY",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "vH3Kp",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I checked at the OS level, the socket buffer was never even close to full, which is why I'm thinking the issue is on Betfair's end"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1632816192.087500",
        "client_msg_id": "c19ac357-deb6-405a-9b2a-f8d376d1a18f",
        "text": "Have you experimented with buffer size and segmentation?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632778160.086400",
        "parent_user_id": "U01DCR5PXDY",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "M0QO",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Have you experimented with buffer size and segmentation?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U01DCR5PXDY",
        "type": "message",
        "ts": "1632816727.087700",
        "client_msg_id": "eef87486-a6a6-4a1f-8e10-6e91c314bf5d",
        "text": "Tried buffer size, I'll look into segmentation. Have you been able to get &gt; 300 updates per second with zero conflation?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "903bc20cd9db",
            "image_72": "https:\/\/avatars.slack-edge.com\/2020-10-26\/1448127317798_903bc20cd9dba9a758e0_72.png",
            "first_name": "thambie1",
            "real_name": "thambie1",
            "display_name": "thambie1",
            "team": "T4G9NBD2M",
            "name": "thambie1",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632778160.086400",
        "parent_user_id": "U01DCR5PXDY",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "QtihI",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Tried buffer size, I'll look into segmentation. Have you been able to get > 300 updates per second with zero conflation?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1632816949.087900",
        "client_msg_id": "d96e77cd-dba5-4af6-9287-21166dd08187",
        "text": "No idea tbh, probably not",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632778160.086400",
        "parent_user_id": "U01DCR5PXDY",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "T1ay",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "No idea tbh, probably not"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U0160E9HS2G",
        "type": "message",
        "ts": "1632831452.090200",
        "client_msg_id": "f1db1a91-1c77-49ea-8955-f3638a9a6744",
        "text": "Hi everyone, I've been running an old version of the S3 Market Recorder and am trying to convert all of my old recorded zip files to gzip on S3. EC2 bash script seems like potentially the best option, but will need a larger instance as inflating and zipping takes ages. Anyone had any experience with this or got an easy\/serverless way to do it? Cheers, Joe",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g824e1cc27e2",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/824e1cc27e2d6ad6290dfb21ea43f1df.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0013-72.png",
            "first_name": "",
            "real_name": "JC",
            "display_name": "JC",
            "team": "T4G9NBD2M",
            "name": "joecussen96",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632831452.090200",
        "reply_count": 3,
        "reply_users_count": 2,
        "latest_reply": "1632832251.092700",
        "reply_users": [
            "U4H19D1D2",
            "U0160E9HS2G"
        ],
        "replies": [
            {
                "user": "U4H19D1D2",
                "ts": "1632831777.092300"
            },
            {
                "user": "U0160E9HS2G",
                "ts": "1632832165.092500"
            },
            {
                "user": "U4H19D1D2",
                "ts": "1632832251.092700"
            }
        ],
        "is_locked": false,
        "subscribed": true,
        "last_read": "1632832251.092700",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Rjp",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Hi everyone, I've been running an old version of the S3 Market Recorder and am trying to convert all of my old recorded zip files to gzip on S3. EC2 bash script seems like potentially the best option, but will need a larger instance as inflating and zipping takes ages. Anyone had any experience with this or got an easy\/serverless way to do it? Cheers, Joe"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1632831777.092300",
        "client_msg_id": "b59a2406-9bec-4a00-91ae-d22972a37b7e",
        "text": "```import os\nimport sys\nimport zipfile\nimport gzip\nimport boto3\nfrom io import BytesIO\nfrom concurrent import futures\nfrom tqdm import tqdm\n\n\"\"\"\nMove s3 to s3\n\"\"\"\n\ns3_client = boto3.client(\"s3\")\n\nBUCKET = \"flumine\"\nNEW_PREFIX = \"marketdata\/marketBook\"\nOLD_PREFIX = \"marketdata\/streaming\/{0}\"\n# threading\nWORKERS = 4\nCHUNKS = 10\n\n\ndef chunks(l: list, n: int) -&gt; list:\n    for i in range(0, len(l), n):\n        yield l[i : i + n]\n\n\ndef upload_object(object, key, metadata):\n    return s3_client.put_object(\n        Body=object,\n        Bucket=BUCKET,\n        Key=key,\n        Metadata=metadata,\n    )\n\n\ndef create_new_compressed_file(zf, contained_file):\n    with zf.open(contained_file, \"r\") as f:\n        file_compressed = gzip.compress(f.read())\n    return file_compressed\n\n\ndef process_files(files):\n    for file in files:\n        # download file\n        response = s3_client.get_object(Bucket=BUCKET, Key=file[\"Key\"])\n        _zip_data = response[\"Body\"].read()\n        try:\n            with zipfile.ZipFile(BytesIO(_zip_data)) as zf:\n                for contained_file in zf.namelist():\n                    market_id = contained_file\n                    gz_object = create_new_compressed_file(zf, contained_file)\n                    key = \"{0}\/{1}.gz\".format(NEW_PREFIX, market_id)\n                    upload_object(gz_object, key, response[\"Metadata\"])\n        except Exception as e:\n            print(\"Error in market {0}\".format(market_id), e)\n\n\ndef process(event_type_id):\n    print(\"Starting process on eventTypeId: {0}\".format(event_type_id))\n    old_prefix = OLD_PREFIX.format(event_type_id)\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    page_iterator = paginator.paginate(\n        Bucket=BUCKET, Prefix=NEW_PREFIX\n    )\n    processed = []\n    for page in page_iterator:\n        for object in page[\"Contents\"]:\n            file_path = os.path.basename(object[\"Key\"])\n            processed.append(file_path[:-3])\n\n    print(\"{0} files already processed\".format(len(processed)))\n\n    event_type_ids = [7]\n    for event_type_id in event_type_ids:\n        print(event_type_id)\n        pcs, skp = 0, 0\n        page_iterator = paginator.paginate(\n            Bucket=BUCKET, Prefix=OLD_PREFIX.format(event_type_id)\n        )\n        for page in page_iterator:\n            for object in page[\"Contents\"]:\n                file_path = os.path.basename(object[\"Key\"])\n                if file_path[:-4] not in processed:\n                    pcs += 1\n                else:\n                    skp += 1\n        print(\"{0} files to process ({1} skipped)\".format(pcs, skp))\n    return\n\n    page_iterator = paginator.paginate(\n        Bucket=BUCKET, Prefix=old_prefix\n    )\n    skp = 0\n    files_to_process = []\n    for page in page_iterator:\n        for object in page[\"Contents\"]:\n            file_path = os.path.basename(object[\"Key\"])\n            if file_path[:-4] not in processed:\n                files_to_process.append(object)\n            else:\n                skp += 1\n    print(\"{0} files to process ({1} skipped)\".format(len(files_to_process), skp))\n\n    with futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n        _process_jobs = []\n        for files in chunks(files_to_process, CHUNKS):\n            _process_jobs.append(executor.submit(process_files, files=files))\n        for job in tqdm(futures.as_completed(_process_jobs), total=int(len(files_to_process)\/CHUNKS)):\n            job.result()  # wait for result\n    print(\"{0} files processed\".format(len(files_to_process)))\n\n\nif __name__ == \"__main__\":\n    event_type_id = sys.argv[1]\n    process(event_type_id)```",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632831452.090200",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "BrC",
                "elements": [
                    {
                        "type": "rich_text_preformatted",
                        "elements": [
                            {
                                "type": "text",
                                "text": "import os\nimport sys\nimport zipfile\nimport gzip\nimport boto3\nfrom io import BytesIO\nfrom concurrent import futures\nfrom tqdm import tqdm\n\n\"\"\"\nMove s3 to s3\n\"\"\"\n\ns3_client = boto3.client(\"s3\")\n\nBUCKET = \"flumine\"\nNEW_PREFIX = \"marketdata\/marketBook\"\nOLD_PREFIX = \"marketdata\/streaming\/{0}\"\n# threading\nWORKERS = 4\nCHUNKS = 10\n\n\ndef chunks(l: list, n: int) -> list:\n    for i in range(0, len(l), n):\n        yield l[i : i + n]\n\n\ndef upload_object(object, key, metadata):\n    return s3_client.put_object(\n        Body=object,\n        Bucket=BUCKET,\n        Key=key,\n        Metadata=metadata,\n    )\n\n\ndef create_new_compressed_file(zf, contained_file):\n    with zf.open(contained_file, \"r\") as f:\n        file_compressed = gzip.compress(f.read())\n    return file_compressed\n\n\ndef process_files(files):\n    for file in files:\n        # download file\n        response = s3_client.get_object(Bucket=BUCKET, Key=file[\"Key\"])\n        _zip_data = response[\"Body\"].read()\n        try:\n            with zipfile.ZipFile(BytesIO(_zip_data)) as zf:\n                for contained_file in zf.namelist():\n                    market_id = contained_file\n                    gz_object = create_new_compressed_file(zf, contained_file)\n                    key = \"{0}\/{1}.gz\".format(NEW_PREFIX, market_id)\n                    upload_object(gz_object, key, response[\"Metadata\"])\n        except Exception as e:\n            print(\"Error in market {0}\".format(market_id), e)\n\n\ndef process(event_type_id):\n    print(\"Starting process on eventTypeId: {0}\".format(event_type_id))\n    old_prefix = OLD_PREFIX.format(event_type_id)\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    page_iterator = paginator.paginate(\n        Bucket=BUCKET, Prefix=NEW_PREFIX\n    )\n    processed = []\n    for page in page_iterator:\n        for object in page[\"Contents\"]:\n            file_path = os.path.basename(object[\"Key\"])\n            processed.append(file_path[:-3])\n\n    print(\"{0} files already processed\".format(len(processed)))\n\n    event_type_ids = [7]\n    for event_type_id in event_type_ids:\n        print(event_type_id)\n        pcs, skp = 0, 0\n        page_iterator = paginator.paginate(\n            Bucket=BUCKET, Prefix=OLD_PREFIX.format(event_type_id)\n        )\n        for page in page_iterator:\n            for object in page[\"Contents\"]:\n                file_path = os.path.basename(object[\"Key\"])\n                if file_path[:-4] not in processed:\n                    pcs += 1\n                else:\n                    skp += 1\n        print(\"{0} files to process ({1} skipped)\".format(pcs, skp))\n    return\n\n    page_iterator = paginator.paginate(\n        Bucket=BUCKET, Prefix=old_prefix\n    )\n    skp = 0\n    files_to_process = []\n    for page in page_iterator:\n        for object in page[\"Contents\"]:\n            file_path = os.path.basename(object[\"Key\"])\n            if file_path[:-4] not in processed:\n                files_to_process.append(object)\n            else:\n                skp += 1\n    print(\"{0} files to process ({1} skipped)\".format(len(files_to_process), skp))\n\n    with futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:\n        _process_jobs = []\n        for files in chunks(files_to_process, CHUNKS):\n            _process_jobs.append(executor.submit(process_files, files=files))\n        for job in tqdm(futures.as_completed(_process_jobs), total=int(len(files_to_process)\/CHUNKS)):\n            job.result()  # wait for result\n    print(\"{0} files processed\".format(len(files_to_process)))\n\n\nif __name__ == \"__main__\":\n    event_type_id = sys.argv[1]\n    process(event_type_id)"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U0160E9HS2G",
        "type": "message",
        "ts": "1632832165.092500",
        "client_msg_id": "f064325a-2538-43e9-bdfb-00190128bb76",
        "text": "Thanks so much, did you run this on a decent EC2 instance for speed?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g824e1cc27e2",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/824e1cc27e2d6ad6290dfb21ea43f1df.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0013-72.png",
            "first_name": "",
            "real_name": "JC",
            "display_name": "JC",
            "team": "T4G9NBD2M",
            "name": "joecussen96",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632831452.090200",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "8xp3q",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Thanks so much, did you run this on a decent EC2 instance for speed?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1632832251.092700",
        "client_msg_id": "0e500b3d-2f48-4896-af1d-5d8c7f07b224",
        "text": "I was lazy and ran it locally",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1632831452.090200",
        "parent_user_id": "U0160E9HS2G",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "cL0k+",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I was lazy and ran it locally"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "subtype": "channel_join",
        "user": "U02GA5B816D",
        "text": "<@U02GA5B816D> has joined the channel",
        "type": "message",
        "ts": "1632839950.093100"
    }
]
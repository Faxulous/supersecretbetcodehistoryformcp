[
    {
        "user": "U06C58H2274",
        "type": "message",
        "ts": "1704210609.122569",
        "client_msg_id": "90916ded-aa16-4e85-850d-6998ac8825ad",
        "text": "Hi everyone! I'm fairly new to the automated betting world and would like to start by getting my hands on some of the historical data that betfair supplies at <https:\/\/historicdata.betfair.com\/#\/home>. Problem is that I live in Sweden and this service is not accessible for us swedes... Does anyone know an alternate way to get access to historical data? Thanks in advance! :raised_hands:",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "e438e3601d8f",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-01-02\/6436067707120_e438e3601d8f654a851a_72.jpg",
            "first_name": "Ricky",
            "real_name": "Ricky Klasson",
            "display_name": "Ricky",
            "team": "T4G9NBD2M",
            "name": "mrklasson",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1704210609.122569",
        "reply_count": 10,
        "reply_users_count": 4,
        "latest_reply": "1706901251.234799",
        "reply_users": [
            "U04NWADNCFR",
            "U06C58H2274",
            "U065R0MLLBB",
            "U9JHLMZB4"
        ],
        "replies": [
            {
                "user": "U04NWADNCFR",
                "ts": "1704211231.551379"
            },
            {
                "user": "U06C58H2274",
                "ts": "1704211413.177779"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1704211586.738199"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1704211649.578909"
            },
            {
                "user": "U06C58H2274",
                "ts": "1704211728.029979"
            },
            {
                "user": "U06C58H2274",
                "ts": "1704289514.933639"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1704297106.785039"
            },
            {
                "user": "U065R0MLLBB",
                "ts": "1706889604.238799"
            },
            {
                "user": "U9JHLMZB4",
                "ts": "1706892217.427659"
            },
            {
                "user": "U065R0MLLBB",
                "ts": "1706901251.234799"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "nri5X",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Hi everyone! I'm fairly new to the automated betting world and would like to start by getting my hands on some of the historical data that betfair supplies at "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/historicdata.betfair.com\/#\/home"
                            },
                            {
                                "type": "text",
                                "text": ". Problem is that I live in Sweden and this service is not accessible for us swedes... Does anyone know an alternate way to get access to historical data? Thanks in advance! "
                            },
                            {
                                "type": "emoji",
                                "name": "raised_hands",
                                "unicode": "1f64c"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U04NWADNCFR",
        "type": "message",
        "ts": "1704211231.551379",
        "client_msg_id": "cf5b755c-682b-41ac-9cf7-e8039838f1d2",
        "text": "I can't answer your question, but a lot of us are collecting our own data: <https:\/\/github.com\/betcode-org\/flumine\/blob\/master\/examples\/strategies\/marketrecorder.py>",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g3c4e33a28db",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3c4e33a28dbfc1fe70799f8e243bf305.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "Derek",
            "real_name": "Derek C",
            "display_name": "Derek C",
            "team": "T4G9NBD2M",
            "name": "dataexmachinazzzzz",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1704210609.122569",
        "parent_user_id": "U06C58H2274",
        "attachments": [
            {
                "id": 1,
                "footer_icon": "https:\/\/slack.github.com\/static\/img\/favicon-neutral.png",
                "color": "24292f",
                "bot_id": "B021ZJYSBMW",
                "app_unfurl_url": "https:\/\/github.com\/betcode-org\/flumine\/blob\/master\/examples\/strategies\/marketrecorder.py",
                "is_app_unfurl": true,
                "app_id": "A01BP7R4KNY",
                "fallback": "<https:\/\/github.com\/betcode-org\/flumine\/blob\/master\/examples\/strategies\/marketrecorder.py | marketrecorder.py>",
                "text": "```\nimport os\nimport json\nimport time\nimport logging\nimport gzip\nimport boto3\nimport queue\nimport threading\nfrom boto3.s3.transfer import S3Transfer, TransferConfig\nfrom botocore.exceptions import BotoCoreError\n\nfrom flumine import BaseStrategy\nfrom flumine.utils import create_short_uuid, file_line_count\n\nlogger = logging.getLogger(__name__)\n\n\nclass MarketRecorder(BaseStrategy):\n\n    \"\"\"\n    Simple raw streaming market recorder, context:\n\n        market_expiration: int, Seconds to wait after market closure before removing files\n        remove_file: bool, Remove txt file during cleanup\n        remove_gz_file: bool, Remove gz file during cleanup\n        force_update: bool, Update zip\/closure if update received after closure\n        load_market_catalogue: bool, Store marketCatalogue as {marketId}.json\n        local_dir: str, Dir to store data\n        recorder_id: str, Directory name (defaults to random uuid)\n    \"\"\"\n\n    MARKET_ID_LOOKUP = \"id\"\n\n    def __init__(self, *args, **kwargs):\n        BaseStrategy.__init__(self, *args, **kwargs)\n        self._market_expiration = self.context.get(\"market_expiration\", 3600)  # seconds\n        self._remove_file = self.context.get(\"remove_file\", False)\n        self._remove_gz_file = self.context.get(\"remove_gz_file\", False)\n        self._force_update = self.context.get(\"force_update\", True)\n        self._load_market_catalogue = self.context.get(\"load_market_catalogue\", True)\n        self.local_dir = self.context.get(\"local_dir\", \"\/tmp\")\n        self.recorder_id = self.context.get(\"recorder_id\", create_short_uuid())\n        self._loaded_markets = []  # list of marketIds\n        self._queue = queue.Queue()\n\n    def add(self, flumine) -> None:\n        <http:\/\/logger.info|logger.info>(\"Adding strategy %s with id %s\", self.name, self.recorder_id)\n        # check local dir\n        if not os.path.isdir(self.local_dir):\n            raise OSError(\"File dir %s does not exist\" % self.local_dir)\n        # create sub dir\n        directory = os.path.join(self.local_dir, self.recorder_id)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n    def start(self, flumine) -> None:\n        # start load processor thread\n        threading.Thread(\n            name=\"{0}_load_processor\".format(self.name),\n            target=self._load_processor,\n            daemon=True,\n        ).start()\n\n    def process_raw_data(self, clk: str, publish_time: int, data: dict):\n        market_id = data.get(self.MARKET_ID_LOOKUP)\n        file_directory = os.path.join(self.local_dir, self.recorder_id, market_id)\n        with open(file_directory, \"a\") as f:\n            f.write(\n                json.dumps(\n                    {\"op\": \"mcm\", \"clk\": clk, \"pt\": publish_time, \"mc\": [data]},\n                    separators=(\",\", \":\"),\n                )\n                + \"\\n\"\n            )\n\n    def process_closed_market(self, market, data: dict) -> None:\n        market_id = data.get(self.MARKET_ID_LOOKUP)\n        if market_id in self._loaded_markets:\n            if self._force_update:\n                logger.warning(\n                    \"File: \/{0}\/{1}\/{2} has already been loaded, updating..\".format(\n                        self.local_dir, self.recorder_id, market_id\n                    )\n                )\n            else:\n                return\n        else:\n            self._loaded_markets.append(market_id)\n        <http:\/\/logger.info|logger.info>(\"Closing market %s\" % market_id)\n\n        file_dir = os.path.join(self.local_dir, self.recorder_id, market_id)\n        market_definition = data.get(\"marketDefinition\")\n\n        # check that file actually exists\n        if not os.path.isfile(file_dir):\n            logger.error(\n                \"File: %s does not exist in \/%s\/%s\/\",\n                self.local_dir,\n                market_id,\n                self.recorder_id,\n            )\n            return\n\n        # check that file is not empty \/ 1 line (i.e. the market had already closed on startup)\n        line_count = file_line_count(file_dir)\n        if line_count == 1:\n            logger.warning(\n                \"File: %s contains one line only and will not be loaded (already closed on startup)\"\n                % file_dir\n            )\n            return\n\n        self._queue.put((market, file_dir, market_definition))\n\n    def _load_processor(self):\n        # process compression\/load in thread\n        while True:\n            market, file_dir, market_definition = self._queue.get(block=True)\n            # check file still exists (potential race condition)\n            if not os.path.isfile(file_dir):\n                logger.warning(\n                    \"File: %s does not exist in %s\", market.market_id, file_dir\n                )\n                continue\n            # compress file\n            compress_file_dir = self._compress_file(file_dir)\n            # core load code\n            self._load(market, compress_file_dir, market_definition)\n            # clean up\n            self._clean_up()\n\n    def _compress_file(self, file_dir: str) -> str:\n        \"\"\"compresses txt file into filename.gz\"\"\"\n        compressed_file_dir = \"{0}.gz\".format(file_dir)\n        with open(file_dir, \"rb\") as f:\n            with gzip.open(compressed_file_dir, \"wb\") as compressed_file:\n                compressed_file.writelines(f)\n        return compressed_file_dir\n\n    def _load(self, market, compress_file_dir: str, market_definition: dict) -> None:\n        # store marketCatalogue data `{marketId}.json.gz`\n        if market and self._load_market_catalogue:\n            if market.market_catalogue is None:\n                logger.warning(\n                    \"No marketCatalogue data available for %s\" % market.market_id\n                )\n                return\n            market_catalogue_compressed = self._compress_catalogue(\n                market.market_catalogue\n            )\n            # save to file\n            file_dir = os.path.join(\n                self.local_dir, self.recorder_id, \"{0}.json.gz\".format(market.market_id)\n            )\n            with open(file_dir, \"wb\") as f:\n                f.write(market_catalogue_compressed)\n\n    @staticmethod\n    def _compress_catalogue(market_catalogue) -> bytes:\n        market_catalogue_dumped = market_catalogue.json()\n        if isinstance(market_catalogue_dumped, str):\n            market_catalogue_dumped = market_catalogue_dumped.encode(\"utf-8\")\n        return gzip.compress(market_catalogue_dumped)\n\n    def _clean_up(self) -> None:\n        \"\"\"If gz > market_expiration old remove\n        gz and txt file\n        \"\"\"\n        directory = os.path.join(self.local_dir, self.recorder_id)\n        for file in os.listdir(directory):\n            if file.endswith(\".gz\"):\n                gz_path = os.path.join(directory, file)\n                file_stats = os.stat(gz_path)\n                seconds_since = time.time() - file_stats.st_mtime\n                if seconds_since > self._market_expiration:\n                    if self._remove_gz_file:\n                        <http:\/\/logger.info|logger.info>(\n                            \"Removing: %s, age: %ss\", gz_path, round(seconds_since, 2)\n                        )\n                        os.remove(gz_path)\n                    txt_path = os.path.join(directory, file.split(\".gz\")[0])\n                    if os.path.exists(txt_path) and self._remove_file:\n                        file_stats = os.stat(txt_path)\n                        seconds_since = time.time() - file_stats.st_mtime\n                        if seconds_since > self._market_expiration:\n                            <http:\/\/logger.info|logger.info>(\n                                \"Removing: %s, age: %ss\",\n                                txt_path,\n                                round(seconds_since, 2),\n                            )\n                            os.remove(txt_path)\n\n    @staticmethod\n    def _create_metadata(market_definition: dict) -> dict:\n        try:\n            del market_definition[\"runners\"]\n        except KeyError:\n     â€¦",
                "title": "<https:\/\/github.com\/betcode-org\/flumine\/blob\/master\/examples\/strategies\/marketrecorder.py | marketrecorder.py>",
                "footer": "<https:\/\/github.com\/betcode-org\/flumine|betcode-org\/flumine>",
                "mrkdwn_in": [
                    "text"
                ]
            }
        ],
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Jt2jI",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I can't answer your question, but a lot of us are collecting our own data: "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/github.com\/betcode-org\/flumine\/blob\/master\/examples\/strategies\/marketrecorder.py"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U06C58H2274",
        "type": "message",
        "ts": "1704211413.177779",
        "client_msg_id": "87a75f8b-26f3-44ea-a58f-54dc16d2f7bd",
        "text": "Thanks for the script! Is there a particular reason for collecting your own data?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "e438e3601d8f",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-01-02\/6436067707120_e438e3601d8f654a851a_72.jpg",
            "first_name": "Ricky",
            "real_name": "Ricky Klasson",
            "display_name": "Ricky",
            "team": "T4G9NBD2M",
            "name": "mrklasson",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1704210609.122569",
        "parent_user_id": "U06C58H2274",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "4YrJn",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Thanks for the script! Is there a particular reason for collecting your own data?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U04NWADNCFR",
        "type": "message",
        "ts": "1704211586.738199",
        "client_msg_id": "914e135b-9ef0-4a66-9155-7bc22c484522",
        "text": "cost is one good reason :slightly_smiling_face:",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g3c4e33a28db",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3c4e33a28dbfc1fe70799f8e243bf305.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "Derek",
            "real_name": "Derek C",
            "display_name": "Derek C",
            "team": "T4G9NBD2M",
            "name": "dataexmachinazzzzz",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1704210609.122569",
        "parent_user_id": "U06C58H2274",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "mylFE",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "cost is one good reason "
                            },
                            {
                                "type": "emoji",
                                "name": "slightly_smiling_face",
                                "unicode": "1f642"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U04NWADNCFR",
        "type": "message",
        "ts": "1704211649.578909",
        "client_msg_id": "e64324d3-60c2-4c8f-8c33-b1764baba00a",
        "text": "best\/highest level of granularity is another",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g3c4e33a28db",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3c4e33a28dbfc1fe70799f8e243bf305.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "Derek",
            "real_name": "Derek C",
            "display_name": "Derek C",
            "team": "T4G9NBD2M",
            "name": "dataexmachinazzzzz",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1704210609.122569",
        "parent_user_id": "U06C58H2274",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "GSb44",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "best\/highest level of granularity is another"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U06C58H2274",
        "type": "message",
        "ts": "1704211728.029979",
        "client_msg_id": "97c9e84f-b64d-4094-9f22-56b2d133aa61",
        "text": "I see. Thanks for the rapid replies :slightly_smiling_face:",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "e438e3601d8f",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-01-02\/6436067707120_e438e3601d8f654a851a_72.jpg",
            "first_name": "Ricky",
            "real_name": "Ricky Klasson",
            "display_name": "Ricky",
            "team": "T4G9NBD2M",
            "name": "mrklasson",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1704210609.122569",
        "parent_user_id": "U06C58H2274",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "VD1BW",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I see. Thanks for the rapid replies "
                            },
                            {
                                "type": "emoji",
                                "name": "slightly_smiling_face",
                                "unicode": "1f642"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U0128E7BEHW",
        "type": "message",
        "ts": "1704226590.477899",
        "client_msg_id": "8c5532b1-85cf-40a3-b707-063c06b04a6e",
        "text": "On the 2024 aims topic: 2023 was the year of making racing work for me. The (very good) advice on here suggests to first make non-ML methods work before moving onto ML - though for me the opposite has been true. I started with ML in football which has been operational for the last few years and continues to operate at similar yields. I could never crack racing in the same way though, but once I put ML aside and used more basic methods I was able to break through - my racing volume is about 2x my football volume, and the pnl ratio is similar. Biggest lesson for me is that diversification is useful; you never know when some exogenous change leads to your edge eroding (e.g. data feeds disappearing, impact of market-structure changes etc). My aim for 2024 will be to continue diversifying into new sports, and maybe make ML work for racing now that non-ML strategies are ticking along.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gb57a2bdd15a",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/b57a2bdd15acccdb845ce257f38940cc.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0024-72.png",
            "first_name": "Dave",
            "real_name": "Dave R",
            "display_name": "Dave",
            "team": "T4G9NBD2M",
            "name": "d7m",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "uadCn",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "On the 2024 aims topic: 2023 was the year of making racing work for me. The (very good) advice on here suggests to first make non-ML methods work before moving onto ML - though for me the opposite has been true. I started with ML in football which has been operational for the last few years and continues to operate at similar yields. I could never crack racing in the same way though, but once I put ML aside and used more basic methods I was able to break through - my racing volume is about 2x my football volume, and the pnl ratio is similar. Biggest lesson for me is that diversification is useful; you never know when some exogenous change leads to your edge eroding (e.g. data feeds disappearing, impact of market-structure changes etc). My aim for 2024 will be to continue diversifying into new sports, and maybe make ML work for racing now that non-ML strategies are ticking along."
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "rocket",
                "users": [
                    "U4H19D1D2",
                    "U03N4QBJ0TV"
                ],
                "count": 2
            }
        ]
    }
]
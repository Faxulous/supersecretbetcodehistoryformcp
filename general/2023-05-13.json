[
    {
        "user": "U013K4VNB6D",
        "type": "message",
        "ts": "1683963791.175779",
        "client_msg_id": "39892eb9-9cba-46f1-b77d-3fc498343ce1",
        "text": "If I'm doing back testing and the same runner features in two races, will two different RunnerBook objects be created?\n\nThanks",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gf188478b89f",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/f188478b89f4a5e2c5af9eafa0433712.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0004-72.png",
            "first_name": "",
            "real_name": "Jeff Waters",
            "display_name": "Jeff Waters",
            "team": "T4G9NBD2M",
            "name": "watersjg",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1683963791.175779",
        "reply_count": 2,
        "reply_users_count": 2,
        "latest_reply": "1683963834.572049",
        "reply_users": [
            "U4H19D1D2",
            "U013K4VNB6D"
        ],
        "replies": [
            {
                "user": "U4H19D1D2",
                "ts": "1683963814.860769"
            },
            {
                "user": "U013K4VNB6D",
                "ts": "1683963834.572049"
            }
        ],
        "is_locked": false,
        "subscribed": true,
        "last_read": "1683963834.572049",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "4FC",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "If I'm doing back testing and the same runner features in two races, will two different RunnerBook objects be created?\n\nThanks"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1683963814.860769",
        "client_msg_id": "91E5B88A-00C4-4785-A680-A081440F3FE1",
        "text": "Yes",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1683963791.175779",
        "parent_user_id": "U013K4VNB6D",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "9Ps",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Yes"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U013K4VNB6D",
        "type": "message",
        "ts": "1683963834.572049",
        "client_msg_id": "018337c0-ef14-464c-b886-f7256493e696",
        "text": "Thanks Liam.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gf188478b89f",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/f188478b89f4a5e2c5af9eafa0433712.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0004-72.png",
            "first_name": "",
            "real_name": "Jeff Waters",
            "display_name": "Jeff Waters",
            "team": "T4G9NBD2M",
            "name": "watersjg",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1683963791.175779",
        "parent_user_id": "U013K4VNB6D",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "H7F6g",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Thanks Liam."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U027SDUHU3T",
        "type": "message",
        "ts": "1683967968.130179",
        "client_msg_id": "857f1470-e256-4b40-835c-716aa80e1e96",
        "text": "did something change in the list_cleared_orders endpoint? I have some code that was working fine for ages but then fell over a few months back, now it just times out with an error\n```cleared_orders = trading.betting.list_cleared_orders(\n                bet_status=\"SETTLED\",\n                record_count=10,\n                from_record=0,\n                settled_date_range=range_dates\n            )```\nand I get an 'UNEXPECTED_ERROR' after a long time",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "6c9fde9d0a4e",
            "image_72": "https:\/\/avatars.slack-edge.com\/2021-07-07\/2251814660098_6c9fde9d0a4e91db5714_72.png",
            "first_name": "Dennis",
            "real_name": "Dennis",
            "display_name": "Dennis",
            "team": "T4G9NBD2M",
            "name": "dennis.john.conway",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1683967968.130179",
        "reply_count": 1,
        "reply_users_count": 1,
        "latest_reply": "1683975775.748889",
        "reply_users": [
            "UBS7QANF3"
        ],
        "replies": [
            {
                "user": "UBS7QANF3",
                "ts": "1683975775.748889"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "vP4",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "did something change in the list_cleared_orders endpoint? I have some code that was working fine for ages but then fell over a few months back, now it just times out with an error\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_preformatted",
                        "elements": [
                            {
                                "type": "text",
                                "text": "cleared_orders = trading.betting.list_cleared_orders(\n                bet_status=\"SETTLED\",\n                record_count=10,\n                from_record=0,\n                settled_date_range=range_dates\n            )"
                            }
                        ],
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "and I get an 'UNEXPECTED_ERROR' after a long time"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U03N4QBJ0TV",
        "type": "message",
        "ts": "1683971059.005489",
        "client_msg_id": "78e6f80a-8235-4e1b-8d7f-960c64ce1c84",
        "text": "Morning all. Does anyone have a good resource where you can learn how to work with big data files (4-40GB) in Python. Struggling to work with this Data using AWS Jupyter instances at the moment because of its size (and my lack of knowledge).",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g2c8538b47ad",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/2c8538b47ad4bed6facbb148134bb486.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0020-72.png",
            "first_name": "Trex44",
            "real_name": "Trex44",
            "display_name": "Trex44",
            "team": "T4G9NBD2M",
            "name": "c.s.mpharm",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1683971059.005489",
        "reply_count": 16,
        "reply_users_count": 2,
        "latest_reply": "1706651613.873789",
        "reply_users": [
            "U04NWADNCFR",
            "U03N4QBJ0TV"
        ],
        "replies": [
            {
                "user": "U04NWADNCFR",
                "ts": "1684041949.303929"
            },
            {
                "user": "U03N4QBJ0TV",
                "ts": "1685612837.743359"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1685632421.350699"
            },
            {
                "user": "U03N4QBJ0TV",
                "ts": "1685646610.183809"
            },
            {
                "user": "U03N4QBJ0TV",
                "ts": "1685696412.354759"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1685711541.820359"
            },
            {
                "user": "U03N4QBJ0TV",
                "ts": "1685893250.162209"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1686040218.016709"
            },
            {
                "user": "U03N4QBJ0TV",
                "ts": "1693942637.969669"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1693947201.651119"
            },
            {
                "user": "U03N4QBJ0TV",
                "ts": "1704895247.486379"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1704899317.661109"
            },
            {
                "user": "U03N4QBJ0TV",
                "ts": "1706624707.210429"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1706632992.212849"
            },
            {
                "user": "U03N4QBJ0TV",
                "ts": "1706635796.894449"
            },
            {
                "user": "U04NWADNCFR",
                "ts": "1706651613.873789"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "5JR",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Morning all. Does anyone have a good resource where you can learn how to work with big data files (4-40GB) in Python. Struggling to work with this Data using AWS Jupyter instances at the moment because of its size (and my lack of knowledge)."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1683975775.748889",
        "client_msg_id": "27B14DC3-A027-4585-9E63-9B93251A9FA1",
        "text": "What’s range_dates?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1683967968.130179",
        "parent_user_id": "U027SDUHU3T",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "C59\/O",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "What’s"
                            },
                            {
                                "type": "text",
                                "text": " range_dates?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U04NWADNCFR",
        "type": "message",
        "ts": "1684041949.303929",
        "edited": {
            "user": "U04NWADNCFR",
            "ts": "1684042276.000000"
        },
        "client_msg_id": "ccaf98dd-9cd5-4f13-91fd-6ebbe4f63f29",
        "text": "<@U03N4QBJ0TV> that's a tough question because there are so many options and it depends on how your data is structured and what you're preferences are in terms of analysis tools.\n\nIf your issue is just disk space, then using S3 instead of local storage is a good way to go.\n\nIf the issue is analysing large datasets then something I do quite a lot of, is to keep my data files in Amazon S3 and use Athena SQL to query them. It's a little tricky to set up but once you have defined your 'database' as this set of files (e.g. CSV files) you can then run SQL queries against it like any other DB. Of course this relies on you being comfortable with AWS\/S3\/SQL in the first place.\n\nAnother SQL option would be to get a trial Snowflake account and load the data into Snowflake - more user-friendly than Athena but more expensive once the trial expires.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g3c4e33a28db",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/3c4e33a28dbfc1fe70799f8e243bf305.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "Derek",
            "real_name": "Derek C",
            "display_name": "Derek C",
            "team": "T4G9NBD2M",
            "name": "dataexmachinazzzzz",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1683971059.005489",
        "parent_user_id": "U03N4QBJ0TV",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "ufiR",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "U03N4QBJ0TV"
                            },
                            {
                                "type": "text",
                                "text": " that's a tough question because there are so many options and it depends on how your data is structured and what you're preferences are in terms of analysis tools.\n\nIf your issue is just disk space, then using S3 instead of local storage is a good way to go.\n\nIf the issue is analysing large datasets then something I do quite a lot of, is to keep my data files in Amazon S3 and use Athena SQL to query them. It's a little tricky to set up but once you have defined your 'database' as this set of files (e.g. CSV files) you can then run SQL queries against it like any other DB. Of course this relies on you being comfortable with AWS\/S3\/SQL in the first place.\n\nAnother SQL option would be to get a trial Snowflake account and load the data into Snowflake - more user-friendly than Athena but more expensive once the trial expires."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]
# Strategies Knowledge Base

*Generated from Slack chat history - 3201 technical conversations*

---

## 2020-02-16

**Mo** - *17:31:56*

Something like tennis or football you can model the scoring process - much easier to get at the fundamentals

*Tags: Strategies*

---

## 2020-02-17

**Jonjonjon** - *10:36:25*

(Apologies for having 2 acounts called Jonjonjon, one is my phone, the other is my PC. I don't know how to join them up)

*Tags: General Technical*

---

**Unknown** - *10:48:04*

It's not an amazing strategy, so I'm willing to share results here:

*Tags: Strategies*

---

**James T** - *10:55:53*

I’ve researched / tried something similar with decent (low latency) bookmaker feeds for horse racing and didn’t find a massive edge, and in the end wasn’t worth the high fees for the feeds. TxOdds is like £6k a month. 

*Tags: Performance*

---

**Jonjonjon** - *11:02:53*

[@USYQKE5HN](@USYQKE5HN) By keeping a high comm rate, do you mean you can use fundamentals to help avoid the super premium charge?

*Tags: General Technical*

---

**Mo** - *11:18:27*

Yeah, TXODDS (and similar) price is a very high hurdle to overcome for a strategy, especially as you can't offset it against PC

*Tags: Strategies*

---

**Jonjonjon** - *11:31:07*

Ah, bdp is an email address! Neil just confirmed that my problem on the other thread is due to the geographical thing.:expressionless:

*Tags: General Technical*

---

**James T** - *11:38:08*

They used to have a support contact form on the developer’s site but I can’t seem to find it anymore. You can email them at [mailto:bdp@betfair.com|bdp@betfair.com](mailto:bdp@betfair.com|bdp@betfair.com), but they don’t seem to advertise that email either so maybe don’t give that out willy nilly. Neil is the main API support guy and he’s super helpful. Don’t know if you know him already or not. 

*Tags: General Technical*

---

## 2020-02-19

**James T** - *13:54:11*

I’ve just checked that there is a getAccountStatement call on the API which you could automate an iteration through. There’s an ItemClass field which might help you identify transaction charges, but without having one already, you might have trouble being able to identify it on the API. 

*Tags: General Technical*

---

## 2020-02-22

**Jonjonjon** - *21:02:31*

The Pandas package can be used to do analysis and can calculate standard indicators.

*Tags: Feature Engineering*

---

## 2020-02-24

**D C** - *16:08:22*

You notice any odd errors in the positions? I'm pretty sure my rendering code is as accurate but I get some very strange offset errors at times. Obviously I can't rule out bugs (who can) - do you see these things too?

*Tags: Errors Debugging*

---

**D C** - *16:10:44*

I'm adjusting for latency as best I can to cater for asynchronous packet transmission from the sensors themselves but I still seem to get constant offset vectors. Even when in the stalls they don't always line up in what I would call a "line"

*Tags: Performance*

---

**D C** - *16:13:36*

Well yeah I thought that they would average out but the errors I am getting often look like what you would see on a calibration error. I am (rightly or wrongly) expecting any error to be bivariate guassian with mean as the "true" position

*Tags: Errors Debugging*

---

**liam** - *16:18:37*

i have a lot of checks that have to pass before i execute, the market can also be used to validate the feed but its never going to be perfect. Simply rounding up and playing the numbers game helps

*Tags: General Technical*

---

**James T** - *16:20:43*

I think with any kind of data source there’s always a risk of errors, so basically you need to manage your risk around that, and sometimes there’s not a huge amount you can do about it, but having the data feed is still better than not. 

*Tags: Errors Debugging*

---

**D C** - *16:22:38*

Oh totally agree. I am not being critical just wish I could understand why. It's very useful indeed and I have more ideas of how to use it than hours to implement them. Really just getting started out but I like to understand the data first - hence rendering the positions.

*Tags: Getting Started*

---

**D C** - *16:30:21*

Ive sent a few videos back asking the question. Im sure TPD must have some accurate playback rendering software. I don't know much about GPS tech at all really so don't know what kind of environmental factors can affect the readings.

*Tags: General Technical*

---

**D C** - *16:30:55*

Yes indeed. If it helps the IP horse markets in other contries pick up that would be great

*Tags: General Technical*

---

**liam** - *16:31:40*

they have a similar gui to yourself, positions overlayed onto google maps (i think), they monitor latency and accuracy, replacing any trackers which aren't up to scratch

*Tags: Performance*

---

**D C** - *16:58:06*

I did not know what slack was until this afternoon. Ive been botting for years - been aware of the github project too despite python not being "my thing" yet. Been a member of a few forums over the years (remember [http://fred77.co.uk|fred77.co.uk](http://fred77.co.uk|fred77.co.uk) ??) but this seems the best dev focused place. Even started my own telegram room to try and get some savvy people on board and reduce the twitter noise. This looks a better fit all round.

*Tags: General Technical*

---

## 2020-02-26

**Mo** - *11:27:21*

• You could back test with the assumption everything will get fully filled which is obviously unrealistic but should help you compare strategies

• You could entirely randomise the amount matched

• You could model the amount matched, for example by taking account the size of your edge and real trading data (e.g. when your edge is high then your probability of getting matched is low because you've got a lot of competition for the price)

You also need to worry about double counting liquidity if you're testing something where you might bet on the same market multiple times

*Tags: Strategies*

---

**Lee** - *11:29:05*

I guess i need some real trading data to help

*Tags: Strategies*

---

**liam** - *12:04:50*

Looking to add backtesting capabilities to flumine eventually 

*Tags: General Technical*

---

**Lee** - *12:07:26*

also there was a small typo in the readme `flumine.add_strategy(strategy)` should be `framework.add_strategy(strategy)`

*Tags: Strategies*

---

## 2020-02-29

**Lee** - *17:21:04*

I’ve been testing my first (simple) strategy on live data and seem to be able to make ~5pence per a back+lay trade from £2 stakes.

I know I can potentially optimise the strategy and increase these stakes but is ~1% profit on my stake (£2 x 2 trades) a reasonable strategy? I know I should aim for as much as possible but just trying to understand whats realistic vs risk and reward.

*Tags: Deployment, Strategies*

---

## 2020-03-02

**Mo** - *07:04:35*

e.g. there is £10 available to back at 2.0 which you take. If you let your strategy bet again without restriction and it places the same bet without someone else having posted more at that price then you have double counted the available liquidity leading to an incorrect view of how much volume your strategy can get down and its profitability.

*Tags: Strategies*

---

## 2020-03-03

**Lee** - *16:31:23*

Sorry to jump on this question, but i'm curious what makes it work for horse racing and not other sports?

*Tags: General Technical*

---

**Mo** - *16:41:46*

Only considering pre-event:



I don't really know much about horse racing markets but from what I've heard people say, there is often a lot of price action with nothing fundamental behind it. Under these circumstances, it makes sense that you could take some kind of typical financial modelling approach of treating the price evolution as a random walk and trading a mean reversion strategy by continually backing above and laying below a central price.



It feels like other sports are much more fundamental driven - e.g. a star player is injured in training or an updated weather forecast comes in that produces a step change in the price. I would imagine you'd get killed when this happens as you'd be completely taken out at the wrong price, wiping out any profits that you'd pick up from the mean reversion.

*Tags: Strategies*

---

**Lee** - *16:58:56*

Makes me think you guys must have some very clever / complex strategy's. Most of mine tend to be reasonably straight forward.

*Tags: Strategies*

---

**D C** - *21:18:45*

Yes there are some insane moves on the pre-off horses for no apparent reason at all. Really not unusual to see a runner start at 3.0 drift out to 5.0 then back again in the 10 mins prior to off. Of course sometimes there are things that make it move like a runner playing up when handlers are trying to get it in the stalls etc but a lot of the time there is no obvious reason for moves. [@UKD8R5P9N](@UKD8R5P9N) when you say probability based tick movement what kind of scale are you talking about? You looking to model probability of price moving +/- X ticks from current based on prior time series?

*Tags: Strategies*

---

**AP** - *21:34:40*

I'll add with racing, adding in form/fundamental data certainly helps. You can identify profiles of horses that the market likes to oppose/support.

*Tags: General Technical*

---

**D C** - *21:40:21*

If you had a half decent pricing model for the horses, asssuming that final BFSP is indeed efficient you could make a killing value betting alone. The price fluctuations would give you a decent window of opportunity if you had something moderately accurate. I tried myself years ago scraping RP website to get a database going with the view to doing a multinomial logistic regression on it but I found too many errors in the data. Might be easier these days with something like proform but I suspect that it would still be very very hard as there is a lot of "inside" info which probably has far more impact than the publicly available data.

*Tags: Errors Debugging, Strategies*

---

## 2020-03-04

**liam** - *08:22:04*

The strategy was initially to try and reduce PC but eventually morphed into something profitable 

*Tags: Strategies*

---

**D C** - *09:17:24*

It is usally the crowd that have a big hard on for Pinnacle that promote this narrative. I have asked many times that if their analysts are the best and if analytics have improved over the years we should see an improvement in the efficiency of opening prices over the last X years but this seems to never be addressed. Also always seem to just cover US sports and football all of which are much easier to build a statistical model for than horse racing. I really think pre-off horse racing is an a tier of its own in that regard, as well as price action before the start.

*Tags: Strategies*

---

**Unknown** - *09:42:18*

I posted this on BA forum a while ago but it is basically showing the profit of a strategy based on the time of order placement where -100 is BSP. As you can see there is more value at BSP then there is at 0 (scheduled start time) 

*Tags: Strategies*

---

## 2020-03-08

**Jack Kaminski** - *11:48:19*

Building a book that's over or under 100% is much better than a pure TA indicator based strategy right

*Tags: Strategies*

---

**D C** - *16:12:46*

Could be a bug in my GUI/parsers - hence the question

*Tags: Errors Debugging*

---

**D C** - *16:17:23*

If you were getting prg=0 when the race was runing that at least confirms it came that way from source and its not a problem my end

*Tags: General Technical*

---

**Newbie99** - *16:27:08*

Possibly a dumb question, but are you using a different source for GPS data, or is there a way to get it via the API that I've been completely oblivious to?

*Tags: General Technical*

---

**Mo** - *16:35:40*

Not a dumb question, it’s not widely advertised

*Tags: General Technical*

---

## 2020-03-15

**Peter** - *06:16:49*

[@UUCD6P13J](@UUCD6P13J), I suspect you’re not getting much of a response because the question is way bigger than it looks. But I’ll bite … though I’m not sure you’ll find the answer very helpful!



I can only answer for myself, but I doubt I’m unusual, in saying that it’s both of the above and with way more nuance.



Pre-match predictions, basic or otherwise, aka fundamental analysis, can be useful for spotting where the market is getting it wrong. Most markets will start off a bit away from “fair odds” and work their way towards them as the market comes together - though as Liam has pointed out elsewhere, even then they’re not perfect then and money can still be made at starting prices.



Oftentimes the aim is not to use fundamental analysis to guess the event’s outcome, but simply to anticipate how the market is likely to move and then exit before the event starts, e.g. can you spot a set of circumstances where the favourite is habitually overbet.



Other strategies may ignore the fundamentals and simply look at what’s happening in the market, aka technical analysis or charting. So for example from the history of matched prices I might look for upper and lower bounds, aka resistance and support prices and trade pre-event between these extremes.



Yet another class of strategies uses the prices in one market, often WIN or MATCH_ODDS, to make inferences about related markets and trade on inconsistencies.



Other strategies will use fundamental and technical analysis, for example in-play soccer strategies will take note of how the market is moving with time decay and balance that against the probability of certain events (primarily goals) occurring based on information from matches with similar features.



So overall, it’s a big it depends and is one of the factors that makes this goal so interesting and creates so much space for wildly different approaches.



Given that, in the search for your first profitable strategy, I’d recommend that you dig where you feel most comfortable. Whichever of those two high level approaches calls loudest to you, there’s a profitable strategy waiting for you to discover it.

*Tags: Feature Engineering, Strategies*

---

**Jonjonjon** - *13:48:48*

Good question. But I don't have a good answer.:disappointed:

*Tags: General Technical*

---

## 2020-03-16

**James T** - *20:24:53*

Sorry for the slow reply. How to calculate a favourable price (but one which will get matched) is obviously a trade secret for anyone that’s profitable. My only tip with offering is to say it’s just as important to remove your offer if you deem it not to be of good value anymore. A favourable price you calculate now might not be the same favourable price in X seconds. So just leaving an offer up indefinitely is generally a route to ruin. 

*Tags: Performance*

---

## 2020-03-19

**Mo** - *17:43:10*

```from bisect import bisect_left, bisect_right

from decimal import Decimal

from typing import Optional, Tuple, Union



BETFAIR_TICKS = [

    1.01, 1.02, 1.03, 1.04, 1.05, 1.06, 1.07, 1.08, 1.09,  1.1, 1.11, 1.12, 1.13, 1.14, 1.15, 1.16, 1.17, 1.18, 1.19,

    1.2,  1.21, 1.22, 1.23, 1.24, 1.25, 1.26, 1.27, 1.28, 1.29,  1.3, 1.31, 1.32, 1.33, 1.34, 1.35, 1.36, 1.37, 1.38,

    1.39, 1.4,  1.41, 1.42, 1.43, 1.44, 1.45, 1.46, 1.47, 1.48, 1.49, 1.5,  1.51, 1.52, 1.53, 1.54, 1.55, 1.56, 1.57,

    1.58, 1.59, 1.6,  1.61, 1.62, 1.63, 1.64, 1.65, 1.66, 1.67, 1.68, 1.69, 1.7,  1.71, 1.72, 1.73, 1.74, 1.75, 1.76,

    1.77, 1.78, 1.79, 1.8,  1.81, 1.82, 1.83, 1.84, 1.85, 1.86, 1.87, 1.88, 1.89, 1.9,  1.91, 1.92, 1.93, 1.94, 1.95,

    1.96, 1.97, 1.98, 1.99, 2,    2.02, 2.04, 2.06, 2.08, 2.1,  2.12, 2.14, 2.16, 2.18, 2.2,  2.22, 2.24, 2.26, 2.28,

    2.3,  2.32, 2.34, 2.36, 2.38, 2.4,  2.42, 2.44, 2.46, 2.48, 2.5,  2.52, 2.54, 2.56, 2.58, 2.6,  2.62, 2.64, 2.66,

    2.68, 2.7,  2.72, 2.74, 2.76, 2.78, 2.8,  2.82, 2.84, 2.86, 2.88, 2.9,  2.92, 2.94, 2.96, 2.98, 3,    3.05, 3.1,

    3.15, 3.2,  3.25, 3.3,  3.35, 3.4,  3.45, 3.5,  3.55, 3.6,  3.65, 3.7,  3.75, 3.8,  3.85, 3.9,  3.95, 4,    4.1,

    4.2,  4.3,  4.4,  4.5,  4.6,  4.7,  4.8,  4.9,  5,    5.1,  5.2,  5.3,  5.4,  5.5,  5.6,  5.7,  5.8,  5.9,  6,

    6.2,  6.4,  6.6,  6.8,  7,    7.2,  7.4,  7.6,  7.8,  8,    8.2,  8.4,  8.6,  8.8,  9,    9.2,  9.4,  9.6,  9.8,

    10,   10.5, 11,   11.5, 12,   12.5, 13,   13.5, 14,   14.5, 15,   15.5, 16,   16.5, 17,   17.5, 18,   18.5, 19,

    19.5, 20,   21,   22,   23,   24,   25,   26,   27,   28,   29,   30,   32,   34,   36,   38,   40,   42,   44,

    46,   48,   50,   55,   60,   65,   70,   75,   80,   85,   90,   95,   100,  110,  120,  130,  140,  150,  160,

    170,  180,  190,  200,  210,  220,  230,  240,  250,  260,  270,  280,  290,  300,  310,  320,  330,  340,  350,

    360,  370,  380,  390,  400,  410,  420,  430,  440,  450,  460,  470,  480,  490,  500,  510,  520,  530,  540,

    550,  560,  570,  580,  590,  600,  610,  620,  630,  640,  650,  660,  670,  680,  690,  700,  710,  720,  730,

    740,  750,  760,  770,  780,  790,  800,  810,  820,  830,  840,  850,  860,  870,  880,  890,  900,  910,  920,

    930,  940,  950,  960,  970,  980,  990,  1000

]

BETFAIR_DECIMAL_TICKS = [round(Decimal(t), 2) for t in BETFAIR_TICKS]



class Side(Enum):

    BACK = 'back'

    LAY = 'lay'



    @property

    def other_side(self):

        other_side = Side.LAY if self is Side.BACK else Side.BACK



        return other_side



def make_price_betfair_valid(price: Union[Decimal, float, int], side: Side) -&gt; Optional[Union[Decimal, float, int]]:

    if isinstance(price, Decimal):

        ticks = BETFAIR_DECIMAL_TICKS

    elif isinstance(price, float) or isinstance(price, int):

        ticks = BETFAIR_TICKS

    else:

        raise TypeError('price must be of type Decimal or float or int.')



    if side == Side.BACK:

        fun = bisect_left

        offset = 0

    elif side == Side.LAY:

        fun = bisect_right

        offset = -1

    else:

        raise TypeError('side must be of type Side')



    index = fun(ticks, price) + offset



    if index &lt; 0 or index &gt; len(ticks) - 1:

        return None



    return ticks[index]```



*Tags: Errors Debugging*

---

**Fab** - *18:10:05*

I recently read the book Kelong Kings, it’s about football match fixing orchestrated by Singaporean/Chinese syndicates.



It’s quite possible in Asia (if you have the right contacts) to bet huge sums of money even on lowly matches. So those figures you mention are not surprising.



A far cry from the European bookies we’re used to dealing with :grinning:

*Tags: Errors Debugging*

---

**Fab** - *18:11:55*

[https://www.amazon.co.uk/Kelong-Kings-Confessions-prolific-match-fixer/dp/9630891220/ref=sr_1_1?keywords=kelong+kings&amp;qid=1584641507&amp;sr=8-1](https://www.amazon.co.uk/Kelong-Kings-Confessions-prolific-match-fixer/dp/9630891220/ref=sr_1_1?keywords=kelong+kings&amp;qid=1584641507&amp;sr=8-1)

*Tags: Errors Debugging*

---

**hugo** - *18:23:02*

slightly more functional approach to the same problem

*Tags: General Technical*

---

## 2020-03-24

**D C** - *15:01:20*

Specific to sports exchanges? You mean from a time series market type perspective or from a outcome prediction type approach with statistical models ?

*Tags: Strategies*

---

**Unknown** - *16:49:12*

Here is something recent that typifies the problem with academic research into betting applications: academics are quite naive about how the betting industry works. This paper advertises a profitable betting strategy but it's only profitable when you bet at the best odds available across a range of bookmakers and substantially unprofitable betting at the average odds.



Most likely, the model is utterly pointless and you could have shown you'd have been profitable taking the average odds as true and betting at the maximum odds.

*Tags: Strategies*

---

**brightcake** - *16:50:45*

I guess it suffers from the same problems as academic papers in finance: great in theory but useless in practise

*Tags: General Technical*

---

**Peter** - *17:21:34*

[@U01093Z1KF0](@U01093Z1KF0) There's a ton of papers out there findable by a simple Google search but no easy answers, though there are some clues as to where value might lie in some of the more fundamental papers, i.e. those that seek to understand the markets rather than simple build a model that's better than them. Tip: learn to spot and avoid the plethera of masters dissertations in this area - almost all a waste of time.

*Tags: Strategies*

---

## 2020-03-28

**brightcake** - *14:08:04*

What area in particular are you interested in? Unlikely that there will be anything interesting for trading strategy wise as most academics tend not to focus on this too much.

*Tags: Strategies*

---

**brightcake** - *15:11:07*

If you're able to trade rates products in any way I've heard there is some value in using the same models as central banks to forecast interest rates - might be something to look into. If you're interested, you should check out [https://www.sas.upenn.edu/~fdiebold/papers/paper49/Diebold-Li.pdf|Forecasting the term structure of government bond yields](https://www.sas.upenn.edu/~fdiebold/papers/paper49/Diebold-Li.pdf|Forecasting the term structure of government bond yields), central banks use a modified version of this model (I believe its the nelson siegal svensson model or something)

*Tags: Strategies*

---

## 2020-04-10

**Jonatan (skyw)** - *17:59:12*

Checkout Flumine, then you can collect live data yourself without paying :slightly_smiling_face:

*Tags: Deployment*

---

**Jonatan (skyw)** - *18:04:21*

Just read live data... I do not know about the other sorry ..

*Tags: Deployment*

---

**Christian Tox** - *18:58:08*

Could do stuff like that or scrape other sites, at cost of delay and not error proof

*Tags: Errors Debugging*

---

## 2020-04-11

**Lee** - *21:03:42*

I’d recommend the book How to own the world by Andrew Craig

*Tags: General Technical*

---

## 2020-04-12

**ash** - *11:16:23*

[@U4H19D1D2](@U4H19D1D2) “dollar cost averaging mixed with diversifying”

100% agree. Any idea of being able to predict the short term due to the number of variables that contribute to the price moving is to risky, in my opinion, to be attempted with any size that would equate to meaningful gains. I am not an expert by any means, but i think doing so, proficiently, is a full time job. I am looking at long term / large cap / dividend etc. Keeping a smaller position in more risky pursuit  .. like betfairlightweight ..

*Tags: General Technical*

---

**liam** - *11:25:33*

Nothing risky about bflw! :smile:

*Tags: General Technical*

---

**Jonjonjon** - *13:41:30*

I consider bflw lower risk than the stockmarket.



Suppose you make £xx,xxx from bflw. The chances are that you won't be able to reinvest that money in Betfair. So you will withdraw it and have less money at risk (hence my initial question). However, stocks probably have a higher chance of silly exponential growth than bflw. Bflw is more like a job with (relative to the stock market) steady returns.

*Tags: General Technical*

---

**Jonjonjon** - *13:42:49*

If bflw stops working for you, it is like getting fired

And searching for the next strategy is just like a continuous job interview process.

*Tags: Strategies*

---

## 2020-04-13

**Jonjonjon** - *18:44:33*

Probably dependent on person and strategy. I don't need more than 5 figures in my account. I could probably manage just as well on 4 figures as I place lots of wide orders that rarely get filled. If I stopped that I could manage on low 4 figures.

*Tags: Strategies*

---

## 2020-04-30

**Paul** - *09:43:46*

"The application of recurrent layers in this approach is investigated. The experiments were performed on real anonymized data. The best architecture demonstrated a trading strategy for the RTS Index futures (MOEX:RTSI) with a profitability of 66% per annum accounting for commission." :eyes:

*Tags: Strategies*

---

**AP** - *10:16:40*

This may not relate at all but I did an internship at an options market making firm and I found that they barely used ML models at all, most of their systems and strategies were designed from good old fashioned empirical research and trial and error

*Tags: Errors Debugging, Strategies*

---

**Dave** - *19:31:37*

Had a read through the paper - sure, maybe they have decent signal, but that is about it...execution is an equally difficult problem (and one that is much more difficult to backtest). Likely larger players will get to the desirable orders faster, or MMs will cancel before you can take.  You'll end up only being able to take the worst positions.

*Tags: General Technical*

---

## 2020-05-07

**Mo** - *15:16:26*

Did some modelling for it in the past but not trading it now

*Tags: Strategies*

---

## 2020-05-17

**Unknown** - *15:09:57*

I don't currently have more data to backtest more but would this be considered a good strategy or are the days with large losses a concern? (ignore the currency)

*Tags: Strategies*

---

**Mo** - *15:18:40*

What are the average odds you're betting at? i.e. is the profitability of the strategy dependent on a few long shot bets coming in? Then periods of sustained losses and big jumps in profit are to be expected

*Tags: Strategies*

---

**Lee** - *15:20:48*

I don't think it's relying on long shot bets coming in. I'm just thinking i might have just tweaked my strategy to work with a small dataset instead of what the real market will return. Can't wait for more data quick enough :slightly_smiling_face:

*Tags: Strategies*

---

**James T** - *15:34:46*

I mean you could do something similar in Python or whatever over 1000 bets, given edge, 1 million simulations and see what your expected profit would be. Again, assuming bets are independent. 

*Tags: General Technical*

---

**Mo** - *15:44:17*

Of course, a strategy could also be considered very successful if it produces consistent low volatility returns

*Tags: Strategies*

---

**Mo** - *15:49:19*

This was a good talk at MathSport 2019 "Establishing a performance edge in P2P betting": [https://rstudio-pubs-static.s3.amazonaws.com/509874_7a4aef970d924450bb246cc4b5128d52.html](https://rstudio-pubs-static.s3.amazonaws.com/509874_7a4aef970d924450bb246cc4b5128d52.html)

*Tags: Performance, Deployment, Strategies*

---

**Lee** - *16:10:47*

Not yet, i'm just starting with simple strategies by venue and then try to tweak and improve them by looking for certain signals. I just tested by venue to begin with as I thought it's easy way to decide on a subset of the data which had volume.

Thats another reason why my sample size is low. I can add in more races but then i'd be choosing venues which show a profit with the strategy, which also makes me think i'm "forcing" a result rather than getting the "true" result.

*Tags: Strategies*

---

**James T** - *16:14:07*

Yeah, definitely sounds like overfitting by choosing historically profitable venues without good reason. In fact if it doesn’t work for some venues then you need to question why - or it could just be your sample is too small. 



I supposed volume could be an issue if that’s somehow related to your strategy. 

*Tags: Strategies*

---

**James T** - *16:24:04*

Well without knowing what the strategy edge is supposed to be (and I guess it might not always be that clear anyway), yes, I’d include everything. 



Are you backtesting or betting live? I would personally backtest so I can try lots of variations in a short space of time, not risk money, etc... But keep a set of out-of-sample data which I can verify the strategy on afterwards. 

*Tags: Deployment, Strategies*

---

**James T** - *16:27:02*

From what I’ve read, flumine isn’t quite ready yet? Or just about ready for testing? That’s basically what you want to use if you don’t have a backtesting system of your own. Then you just need the data to backtest with. And hope the backtest is a good representation of live betting. Which it mostly is at low stakes and when taking volume. 

*Tags: Deployment, Strategies*

---

**Lee** - *16:30:10*

Thanks for all your help James. Yeah i'm backtesting using flumine and also collecting data using it too.

*Tags: General Technical*

---

**Lee** - *19:50:22*

I can’t get this out of my head. Are people really doing this amount from a single strategy? Are some of them in this group?

*Tags: Strategies*

---

## 2020-05-18

**liam** - *08:21:22*

You guys mind if I take some of the above and put it in an area of the docs? Probably flumine rather than bflw, 'strategy development'

*Tags: Strategies*

---

**Lee** - *08:23:52*

I know i'd love something like that. I think this was one of the most helpful conversations i've had pointing me in the right direction. Thanks once again guys!

*Tags: General Technical*

---

**James T** - *11:02:34*

:+1: I think some of it might be considered just an opinion of how to approach things and I don’t want to sway people the wrong way, but it’s worked for me which is the reason I advocate it. There is certainly always more than one way of doing things though. So anything I personally say shouldn’t be taken as gospel. I’m still figuring things out like everyone else!



*Disclaimer: I take no financial responsibility for your gambling investments and bets can both win and lose :stuck_out_tongue_winking_eye::joy:

*Tags: General Technical*

---

**SrFabio** - *22:54:26*

I've been here for a day or so and I'm already fascinated with the discussions I've been silently witnessing in here :muscle: I'm building a mobile app with the betfair API-NG ( nothing too smart) and once I finish this I'll deep dive into automation. I mostly trade on football, NBA and been having a dig at some E-Sports too. To be fair I've traded just about everything, not very successful tho lol.

*Tags: General Technical*

---

**SrFabio** - *23:09:18*

Thanks! I'd like to ask a question as I'm a bit confused: when building a trading model do you guys mostly seek statistical arbitrage opportunities based on historical data or is there something better to look for?

*Tags: Data Quality, Strategies*

---

## 2020-05-19

**James T** - *11:09:10*

Horses and dogs are good because there’s a lot of them and a lot of liquidity every single day of the year so a good basis for building a staple. At least that’s why I initially chose it. 



It also has mostly smooth continuous movement in odds rather than being governed by discrete events like goals or sets which helps to make strategy returns less volatile. There’s obviously a lot more pre-event movement than in the other sports too. 



But in terms of how much you can win, I expect it’s of the same magnitude as any of the other big sports. 

*Tags: Strategies*

---

**James T** - *11:39:38*

In terms of strategy sensitivity to latency it really depends on the strategy. I would expect something fundamental based to be less sensitive to arbitrage for example. I would class what I do as statistical arbitrage, and I’m sensitive to around 100ms when betting pre-race. ie. I can notice lower returns with an extra delay of 100ms. 1ms not. 

*Tags: Performance, Strategies*

---

**James T** - *11:41:00*

This is all down to backtesting though. Not live split testing. 



So that’s another good use for a backtesting system like flumine. You can adjust your simulated latency and see what effect that has on returns. 

*Tags: Performance, Deployment*

---

**James T** - *11:45:41*

Backtesting does have its caveats though, as much as I advocate it. You can’t account for market impact or how the market will react to your bets. And that can be enough to kill a live strategy compared to its backtest. 



eg. If you offer, you can’t know if someone will undercut you. If you take, you don’t know how the book will react (potentially further down the line). 



I know [@U4H19D1D2](@U4H19D1D2) had some great ideas on measuring market impact. I’m hoping he’ll include a page on that too when he has time!

*Tags: Deployment, Strategies*

---

**SrFabio** - *11:57:14*

Do you guys normally try different money management approaches when testing the strategies? Seeing the results based on a percentage staking vs kelly criterion approach. I recently finished reading a book that simulated this and the results were very interesting/surprising

*Tags: Strategies*

---

**SrFabio** - *11:58:42*

I still feel a bit confused as to when exactly we should factor in the staking approach on a strategy, do we define the strategy and use montecarlo simulations to try the money management approaches or should that be integral part of the strategy itself?

*Tags: Strategies*

---

**MacrcoPolo** - *11:59:19*

Money management only matters if your strategy will work with a flat stake

*Tags: Strategies*

---

**SrFabio** - *12:03:54*

Would you not be interested to see if different staking would considerably alter the profitability of your strategy? From what this book has shown (despite being a simple Montecarlo simulation) using a "better" staking approach allowed for a much sustainable bankroll growth rate  vs traditional flat stake of fixed percent stakes

*Tags: Errors Debugging, Strategies*

---

**SrFabio** - *12:08:40*

Have you guys tried the Kelly Criterion staking by any chance?

*Tags: Strategies*

---

**liam** - *12:09:18*

I use a variation of it, full kelly just doesn’t work due to market capacity limits 

*Tags: Strategies*

---

**Mo** - *12:09:51*

Yes, fractional Kelly is a very good starting point

*Tags: Strategies*

---

**SrFabio** - *12:15:41*

Nice...errr I have a lot of reading to do before I can do something interesting (plus learning Python, don't think I can run away from it much longer lol). In terms of books, what would you guys recommend? I'm a .Net / Javascript developer, my knowledge of statistics is rusty and I've not done a machine learning / AI project just yet (although I'm somewhat familiar with it based on books I've read in the past)

*Tags: General Technical*

---

**SrFabio** - *12:17:09*

More recently I finished reading both Betfair programming books from James Butler but can't find a lot of books doing technical stuff with the Betfair API

*Tags: General Technical*

---

**SrFabio** - *12:19:48*

[@U4H19D1D2](@U4H19D1D2) I've been building a mobile app for the past couple of years using betfair API (part part part time xD) and it's nothing to do with trading automation, just a fancy cashout plus some other bits. Once I finish that (soon, I hope) I'll dive into more advanced stuff. So far it's given me a good understanding of the API-NG at least

*Tags: Strategies*

---

**SrFabio** - *12:22:51*

I didn't know about flumine and it's fantastic that's something like that is already built, big thanks for that :+1::+1:

*Tags: General Technical*

---

**Mo** - *12:27:08*

Don't feel obligated to take a ride on the machine learning hype train

*Tags: General Technical*

---

**James T** - *12:31:38*

Yeah, I’d echo the same thing about machine learning. 



It’s a big hit or miss. Probably not the best place to start I reckon.

*Tags: General Technical*

---

**James T** - *12:34:19*

I think you have two choices re: getting started and setting your future path in terms of implementation. 



You can either learn Python and use the libraries provided by the great community here. 



Or you stick with a language you’re already experienced in and use existing code or Betfair sample applications. But then build a data collection and backtesting platform yourself. 

*Tags: Getting Started*

---

**brightcake** - *12:44:59*

Machine learning doesn't really seem to add that much of an edge at the moment (mostly speaking from the finance side but I assume it's pretty much the same in sports betting, if not even more so). You'd probably be able to get most information you need just using simple linear regressions.



If you really want to learn some ML, I would suggest taking one of those intro courses on Udemy (when there's a sale on ofc). They can give you a decent introduction to a topic in relatively small amounts of time, at least in my experience.

*Tags: Strategies*

---

**liam** - *13:06:52*

[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1589726588178500](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1589726588178500)

*Tags: General Technical*

---

**SrFabio** - *13:16:22*

Thanks for the advice guys! I have a genuine interest in ML, but I agree it would probably be overwhelming with all I need to learn. Perhaps building something like flumine on .net would be a good exercise to do, I'd like to learn it inside out I'm sure it would be massively beneficial to my career as well.

*Tags: General Technical*

---

**SrFabio** - *14:06:19*

[@USYQKE5HN](@USYQKE5HN) that's the dream xD I've been jobless now for about 3 months, first month was because of parental leave (unpaid as I'm a contractor) and then came the pandemic to screw me over  lol so now I'm working very hard to finish this project to put on the market soon whilst having to help taking care of my 3 month old baby, going late to bed and not sleeping much but still making good progress...it has to pay off cause the goddamn mortgage won't pay itself by itself :joy:

*Tags: General Technical*

---

**SrFabio** - *14:11:53*

[@U010GM77S4W](@U010GM77S4W) Thanks :slightly_smiling_face: it's a huge joy especially during this time but it's incredibly hard to work decently. I really want to get into trading automation (even have an idea to try on the exchange games ahah) but I also like manual trading on a ladder too despite having mixed results (mainly bad xD) due to my incompetence on bankroll management (that's where my app is going to "help")

*Tags: Strategies*

---

**mandelbot** - *14:21:00*

No I'm currently transitioning to bflw quite slowly as my background in coding is limited. I operate mainly on Bet Angel, which has some very good functionality but ultimately want to get to grips with bflw.

*Tags: Performance*

---

**mandelbot** - *19:37:56*

[@U4H19D1D2](@U4H19D1D2) cant say for certain but the strategies that I employ on AUS don't work on UK/IRE, and AUS is feeling a lot more like UK/IRE since the lockdowns. Same with greyounds, previously very profitable strategy now in the red.

*Tags: Strategies*

---

## 2020-05-20

**PeterLe** - *07:20:50*

It will be interesting to see just what effect lockdown has had on the markets and how it may change after lockdown. Ive found the US inplay markets particularly good using one strategy (last night was excellent).  The same strategy doesnt work as well on the Aus in-play unfortunately

*Tags: Strategies*

---

## 2020-05-22

**Jonjonjon** - *11:07:21*

I did many years ago. But don't bother now as it'll cause problems with my day job (I'm an IT contractor at a bank). Also, it costs a lot more to do algo trading in financials. You need thousands of pounds in margin if you want to just trade single futures contracts. Plus expensive data feeds. It might costs upwards of £2k a month in connectivity and data fees. And one flash crash can wipe out a year or more of profits.



With Betfair, you can realistically start with under £100. You can rent a server to run your bot for under £10 a month. (In my experience) it's easier to find an edge. You get better diversification benefits, due to the hundreds of uncorrelated markets that trade every day. A flash crash wont' wipe out a years worth of profits.



A negative point of doing Betfair vs Financials is that Betfair is probably (in general) less scalable than financials. But that isn't to say that you can't make more than enough to feed your family using Betfair.

*Tags: Deployment, Strategies*

---

**SrFabio** - *11:34:31*

There's probably other routes for automated financial trading without having to delve into Level 2 and other high cost services. I've done quite well trading CFDs manually and quite frankly would have scaled massively if I had taken the time to automate the strategy

*Tags: Strategies*

---

**mandelbot** - *11:36:09*

[@U013Z24LD0U](@U013Z24LD0U) How did you arrive at your strategy? via backtesting or via developing trading skills?

*Tags: Strategies*

---

**Mo** - *11:37:32*

I think he means bankroll

*Tags: General Technical*

---

**SrFabio** - *11:45:47*

For CFDs I used 2 different brokers, one would set a fixed fee for opening/closing orders which would make it much more expensive than the other for smaller orders, but had better spreads and a better service overall, the other was much better in terms of promotions and leverage - was also dodgy AF lol. Nowadays the rules have changed by some EU regularions and leverage is capped ar 1:30...I used to have 1:500 and 1:1000 xD

*Tags: Errors Debugging*

---

**brightcake** - *13:59:31*

I heard of a guy who made some NLP model that scanned trumps tweets and traded based on that

*Tags: Strategies*

---

**SrFabio** - *14:23:00*

That's the problem really...at least from my experience sometimes the market moves against the news, so many times great news for a company and the market is on the red. You've got to have access to more up to date news

*Tags: General Technical*

---

**SrFabio** - *15:13:29*

I was in the process of purchasing a house so I cashed out for the deposit and couldn't afford to invest for a few months (mortgage + all expenses on me), the stock price of the companies I trade was not good enough for me to open positions and I also didn't have enough spare cash (apart from the balance...need an emergency fund for the trading account lol) to trade confidently, during the pandemic the brokers reduced the leverage even more so right now is not a good time for me, no job not much help from gov and the way I trade CFDs is risky and stressful. I currently only have a few small positions on FX but it's money I'm not counting on

*Tags: Strategies*

---

## 2020-05-23

**Unknown** - *18:23:42*

thanks for the tips yesterday, now positive on UK racing (the screenshot) and also US over the course of 18k races from the same strategy

*Tags: Strategies*

---

**liam** - *18:35:00*

Depends on what the strategy is doing / odds taken but I would want that line a bit straighter, is this backing and laying? Preplay? 

*Tags: Strategies*

---

**Lee** - *18:40:44*

laying inplay, the graphs you helped me with yesterday are showing a few areas for improvement. i'm aiming for a graph that looks similar to the one you shared recently

*Tags: General Technical*

---

**VT** - *22:49:13*

Guys, I'm working in the football databases and I have noticed the market every year more efficient and with fewer opportunities, you who have been here the longest, is it possible to profit from statistics and bots in football games? I have encountered so many difficulties that sometimes I think it can be a waste of time.

*Tags: General Technical*

---

## 2020-05-24

**SrFabio** - *09:50:26*

[@U013ZS16QJZ](@U013ZS16QJZ) It's never a waste of time, the more experience you have (even by implementing strategies that end up not being profitable) the better you will become at tweaking your strategies. I firmly believe it's possible, there are so many things to try it's a matter figuring out up the best information (historical stats, ingame stats, etc etc) for the particular games you run your strategy on

*Tags: Strategies*

---

**VT** - *13:26:50*

[@UPMUFSGCR](@UPMUFSGCR) I work as a trader at Betfair for 3 years, full time, I see the market changing and improving every season, correcting the value of the odds according to the time and what happens on the field, without a doubt this is more difficult today than 3 years for those who trade live, I notice repetitive behaviors and this is what I try to automate with bots, but when I start investigating the past data (2 - 4 seasons) the market errors are small and the exorbitant 6% rate of Betfair consumes most of the "errors and gaps" in the market.

*Tags: Errors Debugging, Deployment*

---

**SrFabio** - *16:28:46*

Would you guys mind sharing which metrics do you normally consider when assessing the performance of your strategies? I use the list below but I'm curious if you guys use a simpler or more complex set of metrics

• Win / Strike Rate

• Volume Weighted Win Rate

• ROI

• Streaks

• Profit Factor

• Maximum Drawdown



*Tags: Performance*

---

**VT** - *19:19:15*

Guys, I'm a Betfair worker, almost all day (before the virus) I was trading on Betfair, my favorite markets are match odds, correct score and goals after 80 minutes, I like to lay at low odds (&lt;1 , 09) in some situations. In the last 3 years there are an average of 8 hours of football a day. Working in the main leagues in Europe and Brazil, I don't have much preference, I think that German football is difficult for match odds but it is easy for goals, and so each league usually has its own characteristics and our work method needs to be adjusted. I have also been looking for value in Asian leagues, I had a good ROI in Vietnam before the stop, but games cannot happen at the same time as the major leagues. The capital is mine, personal, built with years of work.



I like computers, mathematics in general, so with the stop I went back to focusing on bots, machine learning etc ... I have some bets on goals recorded on my old blogabet [https://somatipsbr.blogabet.com/](https://somatipsbr.blogabet.com/) and my initial studies on this forum [https://forum.apostaganha.com/index.php/topic,110507.0.html](https://forum.apostaganha.com/index.php/topic,110507.0.html)

*Tags: Strategies*

---

**Unknown** - *19:34:51*

I have difficulty correlating data with the target (bet market). When I put everything on a dataframe (3 seasons) the number of goals scored, the recent performance, and other data like shots in the goal, corners etc ... has almost insignificant correlation, many goals seem just random.



I am doing a combination of all this and also with the SofaScore chart and there is improved a little, nothing very safe but enough to invest and try to profit.

*Tags: Feature Engineering, Performance*

---

## 2020-05-25

**liam** - *09:03:22*

Big fan of quantopian, zipline is a good read, inspiration for the design of flumine. Need to think more about how a library like that could be integrated / used by flumine when backtesting 

*Tags: General Technical*

---

**Dave** - *10:56:50*

Bit of a vague question, but curious about responses. Specifically RE racing markets, who here is trading based of their own predictive modelling of actual event outcomes, and who here is trading order flow? My approach for football has always been the former, but have been looking at expanding my strategies recently to racing and was curious what people's approaches were. For the former a clean, consistent data source is required (which I spent a lot of time building for football), considering doing the same for racing but obviously requires a fair bit of time investment. Given how volatile racing markets are (compared to the standard exponential decay you see in footy), maybe it lends itself better to order flow trading. I know Liam has already mentioned people having success with timeform data for the predictive modelling approach.

*Tags: Strategies*

---

**James T** - *12:30:12*

I trade order flow on racing, but am actively trying to move to a more fundamental based approach. Order flow techniques tend to be lower margin and less scalable but can be very consistent, but will soon have issues with transaction limits (number of bets you can place and cancel as you move with the price). 



And on fundamentals it’s sort of flipped. You’d aim for bigger margins, fewer bets, but end up with bigger volatility in returns. 



I suppose it’s looking like I might end up somewhere in between that actually. But I won’t know until I finish my project and go live with it. 



Trouble with fundamentals, especially in this current situation, is that it needs historical performance data for the runners. And if there’s been a two month break for the sport, it’ll be hard to know how that translates when it restarts. It’s not something that could be backtested because it’s never happened before. So I’d be cautious on any fundamental approach until there’s at least a couple months of new data. I might even wait until next year now. 

*Tags: Performance, Deployment*

---

**James T** - *12:54:13*

I suppose the question was more specifically about whether people are predicting the outright winner or not. With either fundamentals or order flow I suppose you can still predict either outright or change in price. But fundamentals lends itself more towards predicting outright winner and order flow towards price movement. I use order flow and am essentially predicting a bit of both. ie. I will close out some positions and some I will let run if I think it’s a value bet. 

*Tags: General Technical*

---

**Amanda** - *13:37:56*

No this is genuine, I got this info from my "Premium Account Manager", a know a lot of you guys don't have one because you're not successful enough, but that's not my problem.

*Tags: General Technical*

---

**Mo** - *17:52:57*

There’s also nothing forcing you to trade maiden races. A strategy doesn’t have to work on everything

*Tags: Strategies*

---

## 2020-05-27

**Troy Edwards** - *12:47:31*

Hi guys just joined this channel and have read the entire thread.  I sometimes jump out on to Betfair forums ([https://forum.developer.betfair.com/](https://forum.developer.betfair.com/)).  Brizzie boy who trades gallops/trotts/dogs.  Have an ok bot i have developed with [http://VB.net|VB.net](http://VB.net|VB.net) and i guess like everyone still struggling to find a successful system.  I note betfair have racing models and their models (for instance the horse racing model) calculates a market price for the runner which can be compared against the current market to determine whether the bet is good value or poor value.  Does anyone know how this is calculated/determined?

*Tags: Strategies*

---

**JonJonJon** - *12:50:39*

Interesting. Do you know if anyone bets using Betfair's models as a signal? I'd be wary of using something that I did not understand, or which could easily disappear if they decided to stop it.

*Tags: Strategies*

---

**Mo** - *12:50:41*

What model are you referring to?

*Tags: Strategies*

---

**Remi** - *13:40:52*

Or one day they ~change~ upgrade their model.

*Tags: Strategies*

---

## 2020-05-28

**Sandy Caskie** - *22:43:57*

Yes, I can tell you a little but I feel a wee bit precious about how I implement it to my strategy. So it basically adapts my strategy based on data collected by myself. I really just wanted to show to what extent ML can be used to improve return. Has anyone else implemented a ML process and how effective have they found it? Please share...

*Tags: Strategies*

---

## 2020-05-29

**Zbanga** - *07:27:43*

Hardest part is edge. Similarly to sports betting edge is hard to come by. Definitely doable but like most people when it comes to betting they gamble. Financial data is also different and relatively hard to model. I used to work for a prop firm and the things they were doing isn’t ground breaking, but they had cheaper access to capital and way better technology and they had edge. 

*Tags: Strategies*

---

**Lee** - *15:43:57*

yes, there was a screenshot showing how ML helped Sandy improve/optimize their strategy

*Tags: Strategies*

---

## 2020-05-30

**Stefan** - *08:57:53*

ML gives probability of prediction for some models, so you can narrow your betting for this values as well.

*Tags: Strategies*

---

## 2020-06-02

**Troy Edwards** - *00:43:51*

Currently I am testing strategies to determine final outcome, either runner wins (backable) or runner loses (layable) and this seems an endless process of dream up strategy/test strategy/pass-fail/implement strategy.



I am also looking at pre-race green up strategies to trade the market in and out before the race has even run?  I have done this plenty of times however not because of strategy but more luck, ie I place a bet, I noted the odds moved in my direction so I greened up.  I note when charting runners prices (especially higher odds) the lay-to- back spread is wide and narrows up only in the final 10 minutes as money comes in (even on Australian Saturday races) however I don't believe I can apply any TA to these charts?   OR can I ????   If I was should I be using the Back price, OR average of Back and Lay price?



Oh and I would love to be able to work out if a runner is a true favourite or a false favourite.  Currently I can work this out once a race is complete which is too late :laughing::laughing::laughing:

*Tags: Strategies*

---

**liam** - *06:29:49*

[@U0143UVG294](@U0143UVG294) regarding your first paragraph, I think you are thinking about things wrong. It doesn’t matter if the runner wins or loses it’s about finding value ([https://www.pinnacle.com/en/betting-resources|pinnacle](https://www.pinnacle.com/en/betting-resources|pinnacle) have some good blog posts on this)



It’s entirely possible to back more winners than losers and still lose money and visa versa. In order to beat the market you need to take value, the trick is 



a) calculating what is and isn’t value 

b) getting matched 



A can be helped by analysing data and/or getting money through the markets and then analysing what does and doesn’t work, [https://www.betfairtradingblog.com/|Peter Webb](https://www.betfairtradingblog.com/|Peter Webb) has made a few blog posts on this.

*Tags: Strategies*

---

**Troy Edwards** - *11:52:57*

With "it doesn't matter if the runner wins or loses", I think I understand what you are saying.   When I test my systems I just apply the system which returns the highest amount of dollars.  I also check win loss ratio and number of bets to make that that dollar amount but its the dollar amount that concerns me most.  With finding value to me that is more about optimising my strategy to maximise wins and minimises losses. I'll do some reading on those links you posted which I assume are about value etc.     Thanks [@U4H19D1D2](@U4H19D1D2)

*Tags: Strategies*

---

## 2020-06-03

**Dave** - *19:12:25*

&gt;  Things got even better for us in Korea. Now that I had the bits streaming off one by one, or at least byte by byte, you could see that the packets took a long time on the wire. Milliseconds. A common technique in network monitoring is slicing, where you just take the first part of the packet. I did this with the HDLC bypass code and we were now a long way ahead of the game as the bid and ask fields were at the front of the quote packets. Save 100 bytes at 128 kbps and you’re 6.1 milliseconds ahead. As specifications changed and KRX feeds sped up, our advantage reduced but life remained good. Now we were sending out orders before the data packet arrived.

*Tags: General Technical*

---

## 2020-06-08

**Remi** - *10:30:42*

So I finally got a live key activated a few days ago (thank you [@U4H19D1D2](@U4H19D1D2)!) and the first results look good though this is just a single weekend. Mean is around 7 cents per trade (a single trade can be multiple orders/markets for this strategie), value at risk on that 7ct is low but sometimes I do have small negative returns, and I calculated the probability that the mean is higher than 0 with a one-sided T-test and that came out very close to 1. I feel pretty confident this thing is working (for now). 



I have a few questions though. 



1. Do you optimize order size for rounding+commission? Sometimes you can get one extra cent for free because of rounding. Is this a micro optimization or worth some effort?

2. I don’t mean to be impolite but, do you see markets being manipulated? E.g. in more regulated markets you are not allowed to match your own orders but on Betfair you can (I don’t know if it is allowed on bf). I don’t plan on doing this but I’d like to know better what I am dealing with.

3. What are some strategies to lower the commission? Right now I have 0% commission discount and my Betfair Points isn’t growing.



*Tags: Performance, Deployment, Strategies*

---

**Remi** - *10:58:19*

I guess that comes to my question 3: what do you do / can you do to lower commission ?

*Tags: General Technical*

---

## 2020-06-10

**James T** - *22:27:20*

Have you ever tried to quantify the effectiveness of iceberg-ing? Like do you know whether it’s improved your average matched price, or do you just do it because you think it will help?

*Tags: General Technical*

---

**James T** - *22:32:10*

I’m kind of in the middle of deciding whether it’s worth doing. If we consider taking volume only - on the one hand smaller orders might not make as much impact, but on the other splitting orders when the volume you want is already there gives the offerer time to remove their offers. 



So if the volume is there, would it be best just to take all the volume at the risk of the price contracting more than if you took it in little pieces?



I guess it needs testing, but I’m not even sure how to test it effectively. 



Market impact is definitely an area I need to improve my knowledge on!

*Tags: General Technical*

---

**JonJonJon** - *22:32:18*

I've never quantified it. I'm just a part-timer. And to be honest have not thought about how to analyse it properly.



I mainly do it as a risk management exercise, as I structure my individual bets into baskets/spreads. And the execution risk is larger for larger bets. If my execution goes wrong on a smaller bet, then it's not such a big deal.

*Tags: General Technical*

---

**James T** - *22:34:31*

Perhaps I need to evolve my strategy onto offering rather than taking. But it seems to work as take so I thought I’d try that first. 

*Tags: Strategies*

---

**JonJonJon** - *22:45:48*

Surely lots of automated betting has an element of iceberging to it anyway? For example, I'm currently working on a strategy that quotes £10 of bets either side of the best price. It gets filled in clips under £10. So in a way, that is being iceberged too.

*Tags: Strategies*

---

## 2020-06-11

**James T** - *11:29:27*

Thanks for the tip, [@U4H19D1D2](@U4H19D1D2). I’m initially talking about Take for now, but eventually Offer as well. I do already measure amounts matched and performance against SP. I find that days are too variable to be comparable though. Even if I don’t change anything figures vary a lot between days. I do trade from early morning. So to get a feel I’d need a sample over a long period. But then over a long period lots of other things change - weather, competitors, race types... 



Then split testing doesn’t really work because the algos will just compete?

*Tags: Performance*

---

**liam** - *14:32:41*

tbh not something I can expand on openly but as you know markets have a tendency to overreact especially to certain signals/events so delaying can result in a better price, however very strategy dependant

*Tags: Strategies*

---

## 2020-06-12

**Lee** - *09:37:51*

Yeah i'm starting out, every time i ask a question on here it makes me realise how much more I still have to learn. :slightly_smiling_face: Definitely something for me to look into this weekend. Thanks

*Tags: General Technical*

---

**Dave** - *11:04:29*

To offer some contrast, my strategies work primarily by using models to generate prices based on in play activity, and just take if the the diff between market prices and mine is &gt;=x% (without paying attention to market dynamics)

*Tags: Strategies*

---

**Dave** - *11:13:42*

[@UUCD6P13J](@UUCD6P13J) - correct, my logic is as follows: take current state of the event (could be football match, race) -&gt; encode to machine understandable values (aka features) -&gt; get the model to forecast an outcome -&gt; convert the forecast probability to a price -&gt; take in the market if there is value. And no I never green up, greening up doesn't make sense in 99% of situations IMO.

*Tags: Feature Engineering, Strategies*

---

**Dave** - *11:38:16*

[@U01093Z1KF0](@U01093Z1KF0) - if I were to quantify I'd say 50% of my bets only see value of around 10% of the forecast probability, i.e. market says an event is 60% likely and my models say 66%.

*Tags: Strategies*

---

**Lee** - *11:38:26*

Is the model purely based on the current state of events? (no historic data)

*Tags: Strategies*

---

**Dave** - *11:39:38*

[@UGV299K6H](@UGV299K6H) - indeed, you pretty much hit the nail on the head there. If you want to be putting through decent volume then you most definitely need to look at market impact. The only optimization I've made is to bet over a longer time period rather than go in with a large swipe.

*Tags: Performance, Strategies*

---

**Dave** - *11:40:33*

[@UUCD6P13J](@UUCD6P13J) - it takes historical data into account, I spend a lot of time gathering clean, consistent data and transforming it and getting rid of anomalies and etc.

*Tags: Data Quality*

---

**Dave** - *12:14:16*

[@UUCD6P13J](@UUCD6P13J) I won't lie, it takes a lot of effort to make a model that makes any reasonable predictions (reasonable being anything better than random). I suggest arming yourself with various statistical test that can give you confidence that your model actually makes sense (stuff like precision-recall, Area under the ROC curve, etc etc.). Good luck!

*Tags: Strategies*

---

**Dave** - *13:56:04*

It is effectively the same thing [@UGV299K6H](@UGV299K6H) , at least in my case where I use fixed stake. My returns / amount invested is the same as (my strike rate - implied strike rate)/implied strike rate

*Tags: Errors Debugging*

---

**Michael** - *14:09:47*

[@U4H19D1D2](@U4H19D1D2) So now I'm curious - what term do you use for the difference between implied and modelled probabilities?

*Tags: Strategies*

---

**Michael** - *14:19:18*

[@U4H19D1D2](@U4H19D1D2) What I mean is: You model a horse as having 10% chance of winning but you match it at 50. Under your definitions the 'edge' in your bet is 400% (which I would call your expected value) - so what's your name for the 8% that I would call 'edge'?

*Tags: Strategies*

---

## 2020-06-14

**Peter** - *16:32:36*

Not in the published Betfair API.

*Tags: General Technical*

---

**Mo** - *16:33:36*

[https://betfairlightweight.slack.com/archives/C4H05KKMY/p1591007035136500](https://betfairlightweight.slack.com/archives/C4H05KKMY/p1591007035136500)

*Tags: General Technical*

---

**Jorge** - *20:02:37*

Looks like quoteValue is a Mandatory Parameter, which sucks because you have to know it beforehand... Any idea how to get this number through the API?

*Tags: General Technical*

---

## 2020-06-15

**Remi** - *09:08:18*

[@U0155J92A7Q](@U0155J92A7Q), you could create a small, separate lib which takes the bflw trading object and does the cash out, if it doesn’t become part of bflw. Or some other code separate from bflw

*Tags: Strategies*

---

**liam** - *09:23:47*

Depends on the strategy, I can normally tell what the commission will be depending on the strategy type and it can get complicated when you factor in PC

*Tags: Strategies*

---

**Stefan** - *10:24:13*

That is really strange, so we are all here programmers but some have problems to calculate bet position and then fire up hedge bet? When betfair introduced web api, I won project for use case scenario, it run just one year and was called cash out all, and yes betfair requirements was to check if profit/loss did not change after user pressed button, only when P/L of cash out was the same web app executed hedging procedure. Web app was able to monitor all markets and as All in the name, unlike what betfair cash-out offers user could close bet position on all, or selected markets, so not just on one market like in betfair.

*Tags: General Technical*

---

**Jorge** - *11:54:39*

Hi [@U013MLED3V1](@U013MLED3V1) I'm curious, how do you cashout in more than 2 runners markets without using the API cashout call? Even if you have the bet position, the number of equations is big to be solved for 3+ runners [2 runners is very easy, I know]

*Tags: General Technical*

---

## 2020-06-17

**SrFabio** - *11:10:33*

The other "problem" is that the cashout might be composed by multiple orders (one per traded selection) and some of those orders might not get matched, so we need some type of "slippage" to prevent this...there's a lot of things to consider on a cashout

*Tags: General Technical*

---

**Jorge** - *11:49:54*

[@U013MLED3V1](@U013MLED3V1) well I meant for 2 runners it is possible to solve all the possible complications in a simple way (you'll only have to place 1 bet), whereas 2+ runners makes it very complicated to be solved

*Tags: General Technical*

---

## 2020-06-18

**Stefan** - *17:14:21*

[@U0155J92A7Q](@U0155J92A7Q) Really it does not matter for how many selections you are closing bet position. Have a look at here on app I use for manual trading: [https://bit.ly/3fzrNm6](https://bit.ly/3fzrNm6) and of course you can use dutching as well. You can take it as inspiration, or just reverse engineer app's algorithms. [@U013Z24LD0U](@U013Z24LD0U), yes when you trade in-play where odds moves very rapidly, there is need to recalculate closing bets and update them again, till all bets are matched, or level profit is gained on all selections, therefor we call such short programs bots, they do simple work, and can be reused in more complicated strategy. For instance in football correct score market it makes no sense to close bet position on when opened on already unrealistic scores, so you need live score for the match and feed close market bet position just by selections with possible score lines...

*Tags: Deployment, Strategies*

---

**Julio** - *18:49:54*

do you know if we can get some historical data from timeform?

*Tags: Data Quality*

---

## 2020-06-19

**SrFabio** - *08:08:11*

[@U013MLED3V1](@U013MLED3V1) That's a very good point, I don't think even the betfair cashout does that optimization (for the correct score example) and it's certainly way too much work for me to do it at this point and feels beyond the scope of the app. What I'll do is simply remove the individual selection cashouts that would make the overall market PL worst, and still thinking about the option of not using the best price, but the 3rd best price for the calculations as a way to avoid orders not being matched (since they will be matched at the best price available anyway)

*Tags: Performance, Strategies*

---

**Stefan** - *08:15:32*

@[https://app.slack.com/team/U013Z24LD0U|SrFabio](https://app.slack.com/team/U013Z24LD0U|SrFabio) of course betfair will not optimize, actually think clearly, why would they avoid additional fees on such bets, it is betfair business model to earn on winning bet fees, really just use common sense.

*Tags: Strategies*

---

**Stefan** - *18:07:45*

It was not so couple years back when I first heard about timeform api. If you think there is some value in forecast prices then just setup simple bot strategy and run it for couple days to see some results.

*Tags: Getting Started, Strategies*

---

## 2020-06-22

**Lee** - *08:34:47*

Is there a way with flumine to process two markets at the same time? i.e. win and place

*Tags: General Technical*

---

## 2020-06-25

**Stefan** - *18:40:08*

[@U7E6NE1DM](@U7E6NE1DM), any progress in testing forecast strategy in any form for horse racing?

*Tags: Strategies*

---

## 2020-06-29

**liam** - *13:24:50*

[@UUCD6P13J](@UUCD6P13J) this is now available in [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#180-2020-06-29|1.8.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#180-2020-06-29|1.8.0), currently very [https://github.com/liampauling/flumine/blob/master/flumine/markets/market.py#L66|simple](https://github.com/liampauling/flumine/blob/master/flumine/markets/market.py#L66|simple)



```place_markets = market.event.get("PLACE")  # [&lt;Market&gt;, ..]



event_markets = markets.event  # {"PLACE": [&lt;Market&gt;], "WIN": [&lt;Market&gt;], "EACH_WAY": [&lt;Market&gt;]}```

*Tags: General Technical*

---

## 2020-07-05

**Sandy Caskie** - *12:01:01*

So I have streamed horse racing data saved this and used this to build a predictive price model. My streamed data is not of a super high latency though. I will have to look in more detail the reason for this though it seems pretty accurate on a first glance. My question relates to historical data. I also built a predictive model using the free package but this was pretty useless with a frequency of per minute. For anyone else who has done something similar was the pro data particularly useful for building a predictive model?

*Tags: Data Quality, Performance, Strategies*

---

**liam** - *12:22:53*

Forgot to actually answer your question, but things have now settled back down, still better than before but during lockdown the markets where different (less lower hanging fruit) 

*Tags: General Technical*

---

**Lee** - *17:12:54*

Did your strategies require any changes or optimisations because of the market volume or the strategy had the fundamentals that would make profit regardless of volume

*Tags: Strategies*

---

**D C** - *18:08:49*

I had not formally analysed it to that depth. It was something that was working nicely for me on the UK jumps racing earlier in the year before the racing suspension. I decided to give it a try on the US markets after looking at some inplay charts as it looked like it might work. It did but as soon as the UK racing was back it dropped like a stone. The only difference I noticed was lower volume and liquidity although I can't say for sure why. Could have been pure luck in the first place - a month is not a long time for testing a strategy.

*Tags: Strategies*

---

## 2020-07-14

**Stefan** - *09:06:22*

[@U0143UVG294](@U0143UVG294) yes, I used this type of data when implementing chart visualization tool to my app. that second chart which appears on right side in my video. You can try to make simple trigger for that and test your strategy. For me it was just use case scenario to show any type of data in time series:



[http://bfexplorer.net/Community/BlogContent/530#Signal%20Value%20Trend%20Line%20-%20PriceTradedHistory.MyValues](http://bfexplorer.net/Community/BlogContent/530#Signal%20Value%20Trend%20Line%20-%20PriceTradedHistory.MyValues)

*Tags: Strategies*

---

## 2020-07-15

**Lee** - *10:24:34*

Just been reading an old blog post from Peter Webb where he posted the following profit/loss graph. [https://www.betangel.com/blog_wp/wp-content/uploads/2013/07/26-07-2013-What-is-value.png](https://www.betangel.com/blog_wp/wp-content/uploads/2013/07/26-07-2013-What-is-value.png)

With the massive drawdown in the middle would this strategy seem to be profitable based on luck or can losses like this be normal?

I mainly ask because i've had graphs that look similar to this but i've canned them on the basis that the strategy might be based on luck and wouldn't work in the long term.

*Tags: Strategies*

---

**Mo** - *10:30:20*

There's no way I'd exclude a strategy just because of that graph.



If you want to work out the impact of luck on a strategy's returns use one of the Monte Carlo methods that have previously been discussed.



I do like to look at these plots when assessing a potential new strategy as it reveals aspects that are hidden by just looking at e.g. total return/ROI, such as how long and how bad are drawdown periods, and dependence on big wins.

*Tags: Strategies*

---

**mandelbot** - *10:30:21*

im dealing with the same lately, found a strategy that would do 1000x stake in a year but with a drawdown of up to -250x stake

*Tags: Strategies*

---

**Unknown** - *11:09:38*

Here is a similar example of one of my own, note the massive difference in number of bets, not sure you can base anything on [@USDCJS1CK](@USDCJS1CK) strategy without more info on the odds/ev. All relative on stake size/odds but my example probably has a larger percentage drawdown but over a much larger sample, not sure I would base any decisions on PW's graph.

*Tags: Strategies*

---

**Lee** - *12:34:08*

I’m guessing the first question is for me as the graphs are Liams.

I can wait I’m just getting impatient :grin:

*Tags: General Technical*

---

**Michael** - *12:35:49*

Ah sorry I didn't notice that - even so the questions above are all the type of thing you should be considering. That graph might be terrific in one situation and no good at all in another.

*Tags: General Technical*

---

## 2020-07-17

**Jono** - *14:30:03*

Hey everyone got pointed to here after problems with keepAlive functionality. I am wondering the exact request I should have in this function (taken from examples in the bf developer library) in order for my session to never time out:





------------------------------------------

def callApi(jsonrpc_req, appKey, sessionToken, initialHeaders):



url = "[https://api.betfair.com/exchange/betting/json-rpc/v1](https://api.betfair.com/exchange/betting/json-rpc/v1)"



initialHeaders = {'X-Application': appKey, 'X-Authentication': sessionToken, 'content-type': 'application/json'}



request = urllib.request.Request(url, jsonrpc_req. encode('utf-8'), initialHeaders)



response = urllib.request.urlopen(request)

jsonResponse = response.read()



return jsonResponse.decode('utf-8')



------------------------------------------



I have read the keep live documentation but unfortunately am very inexperienced with other programming languages other than python and was therefore wondering if there is a simple change I can make to the code above in order to keep my session live, or if there is a much more straightforward function that is applicable to my situation. Thank you for any help.

*Tags: Deployment, Strategies*

---

**Jono** - *14:33:54*

Thanks but is that not betfairlightweight specific? i delved in (perhaps foolishly) and created my own scripts and process for placing bets and reviewing current orders about three months ago and got quite deep in to my workflow before finding out about betfairlightweight. Is there anyway to ensure a session is kept live using the basic api call above?

*Tags: Deployment*

---

**Mo** - *14:34:18*

Switch to betfairlightweight

*Tags: General Technical*

---

**Jono** - *14:38:53*

sweet lord ok. the more i get into this task the more i realise that I do need to do that :sweat_smile: Is it such a complicated undertaking that trying to send a "keep live" request using my current method just isnt even worth trying? also apologies for the "Thanks but" above it sounded snappy but im extremely greatful for any help

*Tags: Deployment*

---

**Mo** - *14:43:36*

I'm not trying to be an asshole but your code is not great in my opinion. I mean you're not even using requests instead of urllib. This makes it hard for me to engage with your code when I really genuinely believe the correct answer is for you to switch to betfairlightweight. I can see how it might seem like you're taking two steps back to do it but once you do then you have a great community and ecosystem to support you

*Tags: General Technical*

---

**Jono** - *14:54:57*

Thats fair man, I did crap my pants when I saw the functionality of the repo straight out of the box. No point delaying the inevitable necessary. I guess one additional thing is: Are there any problems or legal implications for using betfairlightweight for large scale commercial gain? I work for a company that sells predictions to clients drawing traffic into betfair, betfred etc and we thought initially that building our own system from scratch would prevent intellectual ownership problems from arising. Would using betfairlightweight for our bf api uses be violating any rules/laws in this context?

*Tags: Deployment*

---

**Mo** - *15:07:30*

Good luck, here to help

*Tags: General Technical*

---

**liam** - *15:28:23*

Obviously a biased opinion but integrating bflw should just involve deleting a shit ton of code

*Tags: General Technical*

---

## 2020-07-22

**birchy** - *19:17:24*

Quick backtesting question....if your market data is stored on S3, are you downloading it to the local machine first to avoid latency issues?

*Tags: Performance*

---

**birchy** - *19:18:52*

Or running the bot/backtest code from AWS?

*Tags: Deployment*

---

**liam** - *19:21:44*

I download as I process, run everything on ec2 or lambda so latency is low anyway 

*Tags: Performance, Deployment*

---

**birchy** - *19:35:20*

So yes...and no. lol

[@U4H19D1D2](@U4H19D1D2) are you running your bots on EC2? I've been using Tagadab (now SimplyHosting) VPS for years as it had low latency to the bf servers.

*Tags: Performance, Deployment*

---

**liam** - *19:38:49*

Yeah I am 100% aws, found tagadab to be useless 

*Tags: Deployment*

---

**birchy** - *19:45:14*

What problems did you have with tagadab?

*Tags: General Technical*

---

**birchy** - *20:08:28*

oh... I must of been lucky then, as I've not had any noticeable issues other than regular connection errors, but I've always assumed that was a betfair issue rather than Tagadab? What's the latency like on EC2?

*Tags: Errors Debugging, Performance, Deployment*

---

**birchy** - *20:10:58*

I've not used AWS before. Setup S3 last night and that wasn't as straight forward as I was expecting due to the plethora of options available.

*Tags: Getting Started, Deployment*

---

**birchy** - *20:15:50*

Just to clarify, do you run your bots in lambda or EC2?

*Tags: Deployment*

---

**liam** - *20:17:10*

EC2, lambda for backtesting and cron like jobs 

*Tags: Deployment*

---

**L** - *23:27:53*

lightsail is just an ec2 instance under the hood with less bells and whistles

*Tags: Deployment*

---

## 2020-07-24

**Lee** - *13:49:24*

Finally got my first strategy live which seems to be going well so far. My betfair rewards is set to Basic (2% commission), is this something i should change to increase my commission / reduce premium charge if/when it comes or not something to worry about yet?

*Tags: Deployment, Strategies*

---

**Amanda** - *17:35:40*

Good one Lee, “first strategy live” and your worrying about premium charges, that’s optimism for you, I like it.

*Tags: Deployment, Strategies*

---

## 2020-07-25

**Michael** - *09:23:43*

I really think a lot of people become irrational when it comes to PC. Minimising PC is not a desirable end in itself, what's desirable is maximising your income. You know what's worse than paying PC? NOT paying PC. Whatever your situation the prescription is the same: Maximise your betting profits. If you have a high variability strategy that generates a lot of comm that's fine, but you'll still take more money home if you can reduce that variability. Variability is losing, losing isn't winning, it's that simple.

*Tags: Strategies*

---

**liam** - *09:49:22*

My problem is that I lock in that profit with a nice live dashboard and then it drops 5 figures 

*Tags: Deployment*

---

**Aaron** - *10:17:00*

do you count as supplementary if your strategy is losing YTD :stuck_out_tongue:

*Tags: Strategies*

---

**Mathias Tejs** - *16:09:12*

Hi all, thanks for letting me join the group :) I want to get into betting and I'm wondering if there are any resources (e.g. books, blogs, etc.) that you guys can recommend? I'm an "expert" in algorithms, software development, machine learning and probability theory but I know very little about the world of betting (and also very little about trading) - I'm only saying that to say that I'm not too interested in general programming books and so on :)



Another related newbie question: I'm especially interested in football (soccer) betting. My guess is that my biggest chance for betting profitably there would be through live betting (I want to bet 100% programmatically). Are there any live feed sources that you can recommend? I have saved up around 30k euro that I plan to risk on getting started - is that enough, or do you need a higher volume to avoid all of your winnings going to paying monthly live feed APIs? (I can see from the slack history that some of you guys are winning considerably more than that)

*Tags: Getting Started, Deployment, Strategies*

---

**Twatter** - *16:23:14*

Newbie question - I'm assuming basic market strategies like any inter market arbs or dutching whole field when book &lt; 100% is pretty much non existent these days?

*Tags: General Technical*

---

**Mo** - *17:01:25*

There are academic papers worth reading to give you ideas for how to model specific sports. For example, [https://tolstoy.newcastle.edu.au/R/e8/help/att-6544/dixoncoles97.pdf|Dixon and Coles](https://tolstoy.newcastle.edu.au/R/e8/help/att-6544/dixoncoles97.pdf|Dixon and Coles) is the seminal paper for modelling football pre-event and [https://wwwf.imperial.ac.uk/~ejm/M3S4/Problems/football.pdf|Dixon and Robinson](https://wwwf.imperial.ac.uk/~ejm/M3S4/Problems/football.pdf|Dixon and Robinson) extends the idea to in-running.



For tennis, a good but increasingly dated starting point is [http://www.strategicgames.com.au/PhD.pdf|Tristan Barnett's PhD thesis](http://www.strategicgames.com.au/PhD.pdf|Tristan Barnett's PhD thesis). The various papers coming out of [https://www.imperial.ac.uk/people/w.knottenbelt/publications.html|Will Knottenbelt](https://www.imperial.ac.uk/people/w.knottenbelt/publications.html|Will Knottenbelt)'s group at Imperial College are more recent additions.



But thinking about how to take these models and build automated trading platforms that utilise them is a whole other ball game. There aren't really any books out there to recommend. Personally, I think you are already utilising the best available resource by being part of this Slack channel.

*Tags: Strategies*

---

**Mathias Tejs** - *17:03:26*

Wow. Cool. That surprises me quite a bit :) Are you saying that no additional data is needed because it's possible to win small amounts without additional data or because you don't think that additional data is worth much to a strategy?

*Tags: Strategies*

---

**Mo** - *18:32:10*

Also worth mentioning that fundamental modelling is one angle but understanding market behaviour is another

*Tags: Strategies*

---

## 2020-07-26

**SrFabio** - *08:37:28*

How do you guys manage multiple live strategies running in parallel (in terms of bankroll management)? Are you allowed to create sub-accounts and run a strategy per account or do you create some "virtual balance" and assign a portion to each bot

*Tags: Deployment, Strategies*

---

**Michael** - *09:03:14*

Depending on your account status and KYC you might be allowed be allowed MSA but you don't need it just to differentiate strategy liabilities. Just keep some kind of track and handle for it internally.

*Tags: Strategies*

---

**Dave** - *09:06:30*

For tracking the "customerStrategyRef" argument when placing an order is totally sufficient. Though it's limited to 16 chars i think so you will want to keep it in mind when generating these references (e.g I would encode my strategy parameters into a hash and keep the mapping somewhere so I can easily reverse the hash and figure out which params are responsible for the order. )

*Tags: Strategies*

---

**Michael** - *09:09:47*

I think [@U013Z24LD0U](@U013Z24LD0U) is thinking about how to track the absolute performance of a strategy over time rather than the individual bets so that he can scale automatically according to performance (or something like that). For that he needs to keep some sort of overall record and write it out to a file.

*Tags: Performance, Strategies*

---

**Mo** - *09:11:14*

In principle MSA structure is very useful for having a per strategy absolute exposure limit but in practice I rarely use more than one of my subaccounts

*Tags: Strategies, Multi Client*

---

**Dave** - *09:15:16*

Yeah sorry I wasn't clear - if you have all the underlying bets grouped by strategy then you can do whatever aggregation you want (e.g. performance over time). Given Betfair's pnl tracking at account level is pretty basic you might as well do it yourself anyway, for which customerStrategyRef is totally sufficient to look at performance by strategy at whatever granularity/aggregation level IMO. Besides, betfairs orders only persist for 90 days so you ought to write em to a file anyway, at which point self-aggr is the way to go. Same advantages of the field apply to bankroll management, e.g. computing your exposure and whatnot.

*Tags: Performance, Strategies*

---

**Mo** - *09:16:03*

Yeah, important to mention that MSA predated the customerStrategyRef field and that nowadays Betfair want you to use that field if all you need to do is differentiate strategies. MSA has other use cases such as allowing multiple users e.g. traders

*Tags: Strategies*

---

**SrFabio** - *09:29:00*

Cool that makes sense! Thanks for the insight that's really useful. I think sub account is probably overkill for now as the customerStrategyRef should cater for it + some internal logic to build performance metrics which I already have

*Tags: Performance, Strategies*

---

**Michael** - *09:39:31*

The GC is not keen on MSA because it can be abused to facilitate problem gambling and money laundering so BF are not that keen to give them out. It helps if you're well known to them and UK based. Bu yeah - for most people they're not of any function and just complicate your setup with a bunch of different user names and app keys.

*Tags: Getting Started*

---

**Misha** - *09:40:17*

As noted, there isn't anything you can't do with the strategy ref and order ref properties and logging all bets to a table periodically

*Tags: Strategies*

---

**Mo** - *11:25:28*

Well, there's this which prevents a new rogue strategy from wiping out your entire account if it blows up

*Tags: Strategies*

---

**birchy** - *11:30:54*

Just wondering how you're all implementing tracking for each strategy? I am currently saving the bet IDs by strategyRef and combining with getAccountStatement settled bet calls to identify the gross P&amp;L for each strategy and then pickling that as a dict for future analysis. Downside is that it's difficult to match up commission rates for net profit calcs, particularly where bets have been placed on Aus markets which have variable rates rather than my usual 2%.

*Tags: Strategies*

---

**birchy** - *11:37:36*

Regarding MSA, I've been using 2 normal accounts for years. One is my test account which has limited funds to avoid any catastrophic "oops" moments and the other is used for production bots. Now that I'm adopting streaming, I've discovered that my test account is not enabled for streaming but my main one is. Any ideas why?

*Tags: Deployment*

---

**PeterLe** - *12:13:20*

[@U016TGY3676](@U016TGY3676) Just send the API key to Neil and ask (him nicely) for it to be made active for streaming

*Tags: General Technical*

---

**Misha** - *13:18:54*

They seem to run some sort of software to ensure the ping time doesn't get below about 10 ms, which you can get either from London or Dublin. And 1 ms is within the margin of error

*Tags: Errors Debugging*

---

**Misha** - *13:24:00*

I would also suspect that streaming updates aren't within a 1 ms margin of error either, so trying to shave a ms or two would be pointless

*Tags: Errors Debugging*

---

**birchy** - *13:56:11*

Ah ok. I'm seeing 12ms for London and 11ms for Ireland. Haven't checked the actual streaming response times yet as I only created the Ireland instance for testing. Is it even worth me comparing the two?

*Tags: General Technical*

---

**birchy** - *15:00:10*

Very true. I like to optimise my find bet -&gt; place bet turnaround time, but 12ms is the same as my current Tagadab VPS so I've not lost anything performance-wise moving to AWS.

*Tags: Performance, Deployment*

---

**richard** - *15:35:06*

[@U013Z24LD0U](@U013Z24LD0U) thank you! sorry if the next question is a bit dumb, but how would you calc backReturn and layReturn? Would they be if they win. eg. backReturn = stake*odds and lay = stake

*Tags: General Technical*

---

**birchy** - *17:23:28*

Thanks [@UBS7QANF3](@UBS7QANF3). It's not something I obsess about, more of a "nice to have". I was only testing the water as I'm new to the AWS setup and was wondering how it compared to Tagadab.

*Tags: Getting Started, Deployment*

---

**SrFabio** - *17:30:34*

Not a dumb question don't worry...I've been dabbling myself in these fields for a couple of months too, getting the equation is only half of the problems to solve xD

*Tags: General Technical*

---

**richard** - *17:52:55*

haha indeed. I’m using it for scalping on dogs. Problem 1 - work out which way the price is going. 2. trade out for a sure profit.

*Tags: General Technical*

---

**SrFabio** - *18:36:09*

For me the cashout started to be problematic if I had more than one runner traded because of how the price moves dramatically against my position (on one of the runners) and it's hard to match the hedge stake on that runner -- only tried with delay API key but I anticipate the same sort of issue happen with prod api key too

*Tags: General Technical*

---

**mlpanda** - *23:41:46*

Totally agree Dave, but if it’s not illegal it’s quite an important thing to be aware of when I’m creating new models/strategies as I doubt those who do it will care about the spirits of the markets :sunglasses:

*Tags: Strategies*

---

## 2020-07-27

**richard** - *08:38:54*

I plan to, but at the moment it’s just one at a time.

I have a neural net doing price predications and scalp based on that.

Working quite well at the moment but need to cash out when things go wrong which is why I need this.

I tried this equation out, am I right in saying it calculates a ‘win / no bet’ result eg. no loss but only one result gives a profit?

I’m hoping for something like: layAmount = (backOdds / layOdds) * backAmount Or backAmount = (layOdds / backOdds) * layAmount

Which gives a arbitrage bet where you generate a profit on either side.

How’s your strategy working?

*Tags: Strategies*

---

**D C** - *11:32:16*

With all this talk of milliseconds and latency, has anyone done a study of this in detail? I mean you will surely get different results due to thread/process scheduling times unless you are running things such that you have a process single connection using a blocking socket connection configured such that the process runs on a specific processor? I mean I know that people have multiple processors and cores etc but everything is subject to OS scheduling. I mean yesterday I was getting over 100ms delays (close to 200ms at one point) on the GPS feed as calculated from the FEED time (not the 'pt'). I had Firefox running too and once I closed that down the delay dropped to a steady 70ms. I imagine that some of the people here making larger profits are running setups that have minimal lag. Anyone looked at this kind of thing in depth, or can point out any flaw in my thinking on this matter?

*Tags: Getting Started, Performance*

---

**liam** - *11:33:07*

you run production code on your normal computer?

*Tags: Deployment*

---

**D C** - *11:36:04*

But my crap approach aside, my question stands

*Tags: General Technical*

---

**Mo** - *11:36:52*

To answer your question, yes I've studied latency a lot

*Tags: Performance*

---

**D C** - *11:39:35*

And to get the lowest latency do you need to assign applications to specific cores/processors or can you get the same results using max (min) nice level or similar?

*Tags: Performance*

---

**Misha** - *11:39:50*

I can't speak for Python or Linux, but I can run my entire system inside a VM on my laptop, with over 50 threads running across all applications, but because most are idle at any time, it still takes less than a millisecond to pass from one thread to another. I see no reason why this wouldn't be the case for Linux, and Python (to a point). The key is not to do anything time intensive in the chain from price to calculation to betting

*Tags: Strategies*

---

**D C** - *11:45:25*

I would like to avoid busy loops that is for sure. My background is mostly C++ stuff in comms for hardware (stuff like CAN and serial port connections) so when it comes to socket coding I try to stick to blocking socket stuff within a designated thread and thread safe queues to read/write. I am sure there are far better ways, but for HTTPS stuff and python users I imagine you are using higher level packages rather than some low level stuff using OpenSSL libraries. I was just wondering what kind of analysis you guys had done on latency. I am moving to this headless deployment on designated server (not sure yet probably AWS) but these things take time when you are essentially migrating to a new language.

*Tags: Performance, Deployment*

---

**Mo** - *11:46:45*

I don't use Python for latency sensitive strategies. And yes, I know my way around the OpenSSL source code :wink:

*Tags: Performance*

---

**AP** - *12:22:11*

Terraform looks very good, definitely going to try and learn how to use it

*Tags: General Technical*

---

**Lee** - *20:25:26*

For tracking which strategy places which orders in flumine, should I add my own LoggingControl and post to an API on CLEARED_ORDERS event?

*Tags: Strategies*

---

**liam** - *20:29:18*

ORDER on a new order, CLEARED_ORDERS when the market is closed and CLEARED_ORDERS_META for the flumine order object on cleared status if you want to record extra data e.g. seconds to match 

*Tags: General Technical*

---

## 2020-08-02

**Lee** - *12:07:42*

When trying to optimise a strategy on low stakes, i've duplicated and deployed both the original side by side with the new variant to have something to compare again. Basically an AB test. Does this seem a reasonable approach?

*Tags: Deployment, Strategies*

---

**Michael** - *12:29:40*

I think it depends how you do it. Your bets have an effect on the market and if your two strategies are similar then they'll be competing with each-other, so if you're effectively running two bots simultaneously then you'll confuse things (at best). Depending on how you're operating it'd be better to make sure that only one version is executing at a time. If you're trying to optimise prices with a fixed trigger then that's easy and you can do a very simple a v b, if you're trying to optimise a trigger then you can't. In that case I'd either run them on separate markets or (more often) just measure the relative performance of each strategy looking deeply at the various metrics and comparing both data sets to the hypothetical performance of a fixed strategy that shows how promising the markets were but doesn't place any bets at all.

*Tags: Errors Debugging, Performance, Strategies*

---

**Michael** - *12:44:38*

Personally I would make a hypothesis (and write it down), something along the lines of: By reducing threshold X from 0.05 to 0.04 I anticipate that the strategy will generate 10% more bets at an earlier stage in the race with and increase in transactions and matching rates offsetting a decrease in expected value estimated at 2% (all made up - just whatever's relevant). Then you let it go, check it's not doing anything nuts at about 200 markets in, then leave it 'till you've got 1000 markets, then analyse.

*Tags: Strategies*

---

**Lee** - *12:57:34*

Yeah, it's frustrating but at least i'm enjoying the process and love watching a strategy trade

*Tags: Strategies*

---

**Dave** - *13:21:20*

It would suggest the strategy is only profitable if I added a markup of ~0.1-0.2+ to guarantee I only take when price improvement would be of that size or higher. So you can then implement that in your strategy. It's applicable to other metrics that are treated as thresholds.

*Tags: Strategies*

---

**PeterLe** - *17:26:28*

[@U0128E7BEHW](@U0128E7BEHW) Just to add to your point and worth consideration for anyone new to strategies; I think that as you try and work towards the optimal and efficient bet to maximise value, you eventually reach a tipping point at which the p&amp;L starts to reverse (ie similar to a diminishing returns situation). So this is something to be aware of also. I also try and think about who's taking my bet on the other side and why they would do so. Very early on in my betfair career, I had a simple system that placed a back bet on certain triggers, and it lost money. I thought at the time, ok instead of backing Ill lay at the reverse price. We all know now that if only it was this simple :grinning: The problem is that when the lay would have been successful, you dont always get matched, but when the lay wasn't successful you almost always get matched.

*Tags: Getting Started*

---

## 2020-08-06

**D C** - *14:08:43*

Hence the question so I can get the bot to just block placement if it happens again. I know its early days for Ripon but the sensors are the same as anywhere else you would hope.

*Tags: General Technical*

---

**D C** - *14:10:57*

To me looked like a constant offset error like calibration problem. Not compared with the race replay yet though

*Tags: Errors Debugging*

---

**D C** - *14:17:47*

That response you got was interesting though. Watching the render replays back over hundreds of races now I have seen that kind of vector offset type error. Interesting that they associate it with an old device. I would love to know more about their hardware and how it works internally.

*Tags: Errors Debugging*

---

**Twatter** - *14:18:52*

So I'm guessing that if the sensor was off at the start of the race in the stalls, I'm guessing could you not add a check to say if the gps readings don't map up within a certain percentage, and if your strategy is to bet in-play, then best just void the whole race just in case?

*Tags: Strategies*

---

**Ruben** - *19:37:40*

I would appreciate your input on something...I'm testing a strategy that uses a ML model to calculate the underlying probabilities of events; the system decides to bet when it detects value. However, I am finding that almost all bets (&gt; 99%) are lay bets. On a typical market, I would expect inefficiencies to appear equally likely on the back/lay side; this makes me think that my model is heavily biased towards lay bets

*Tags: Strategies*

---

**Ruben** - *22:01:26*

yes, most likely my model is just terrible :man-facepalming:

*Tags: Strategies*

---

**Mo** - *22:02:52*

This sounds more like a bug than a poor model

*Tags: Errors Debugging, Strategies*

---

**Ruben** - *22:05:55*

yes, that also, thanks for your tips, i'll start the debugger

*Tags: Errors Debugging*

---

## 2020-08-07

**Mo** - *11:08:08*

[@UQL0QDEKA](@UQL0QDEKA) it took me 3 months to go from having access to the data to running a strategy at test stakes and you know what my level of experience and time commitment is like.



But I was coming from a cold start with no existing horse racing strategies - either in running or pre-event - so that may help you get going faster. 



A very important point that anyone starting now should consider is that the historically available GPS data undergoes some significant transformations from what you would have received live which seriously limits its usefulness for back testing. Guys like [@U4H19D1D2](@U4H19D1D2) who have been doing this from the start and collected their own data are in a much better position in this regard. 



You definitely have to account for trackers on the wrong horse, horses running without trackers, seriously miscalibrated trackers (horse running outside the track, starts the race 10m behind the stalls, appears to be leading when is actually in second place, etc)

*Tags: Deployment, Strategies*

---

**D C** - *11:53:12*

The data is useful but as others have mentioned, there are sometimes some hideous inaccuracies in it. I do think that the data provides many possible angles of attack but there are many issues to be aware of. I have come across many banana skins and dodgy happenings, but this slack group has been so useful in that regard with the more experienced users basically explaining that these things are "the norm". Biggest problem I have found is identifying anyone who actually DOES use it. Again this slack has been excellent in that regard when something goes wrong at least you can confirm that others are seeing the same rather than it being your own code that is at fault. I would say that if you are expecting highly accurate positional data you will be disappointed BUT it is still very useful. From a profit point of view it has helped me deal with no income after I lost my job at the end of May. It is safe to say that I would be very close to the bottom of the food chain in terms of the GPS players here but even I am making it pay.

*Tags: General Technical*

---

**PeterLe** - *12:00:02*

[@UUE6E1LA1](@UUE6E1LA1) Sorry to hear about the job, but there is some great help on here and hopefully you will look back in the future and think it a godsend! Thanks for pointers too.

*Tags: General Technical*

---

**D C** - *12:01:18*

I should probably add [@UQL0QDEKA](@UQL0QDEKA) that I am most likey using the least advanced setup and least optimal execution. My stuff is currently running inside a Qt GUI application running on Linux on my home desktop with CPU time wasted on such frivolous things as a real time GPS render tool and other unecessary things. Once I make the switch to headless running on AWS or similar I would be hoping that the profitability increases as there are SO many optimisations to potentially make.

*Tags: Getting Started, Deployment*

---

**PeterLe** - *12:04:35*

Sorry just another question..do you pay for the subscription for the feed on a meeting by meeting basis from Toal Performance data, or is the feed direct from Betfair? ([@UUE6E1LA1](@UUE6E1LA1) I like less complex least advanced :grinning:

*Tags: Performance*

---

**Mo** - *18:25:14*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1595421580229700](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1595421580229700) :sunglasses:

*Tags: General Technical*

---

## 2020-08-08

**Michael** - *08:54:29*

That said: Experience tells me that if you're finding almost 100% lay bets from a model that attempts to re-price a whole market then you're doing something seriously wrong.

*Tags: Strategies*

---

**Mo** - *10:22:58*

Thanks [@UGV299K6H](@UGV299K6H), now I have the missing piece for my GPS strategy!

*Tags: Strategies*

---

**Michael** - *10:23:44*

Happy to help!

*Tags: General Technical*

---

**Unknown** - *12:42:42*

Hi All, this is my first post, Does anyone know how to get *All* traded volume using betfairlightweight or Streaming API as shown in below figure?

*Tags: General Technical*

---

**Mo** - *12:44:10*

Use streaming including EX_TRADED in your market_data_filter

*Tags: General Technical*

---

**ricky** - *23:07:52*

I have question about snap listener, by definition "Instead of waiting for an update you can snap the listener to get an up to date version of the data."

However, i didn't see any difference with or without snap listener via streaming,  the question is what is the expect behaviour for snap? if the streaming update automatically, why and when to use snap? I put my example code in below.

*Tags: General Technical*

---

## 2020-08-09

**ricky** - *00:09:14*

# create queue

output_queue = None



market_filter = streaming_market_filter(

    event_type_ids=["1"], market_types=["MATCH_ODDS"], market_ids=["1.171324077"]

)

market_data_filter = streaming_market_data_filter(

    fields=["EX_BEST_OFFERS", "EX_MARKET_DEF", "EX_TRADED"], ladder_levels=10

)



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener)



market_books = listener.snap(

    market_ids=["1.171324077"]

)



# subscribe

streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=1000,  # send update every 1000ms

)



# create streaming object

streaming = Streaming(trading, market_filter, market_data_filter)



# start streaming (runs in new thread and handles any errors)

streaming.start()



...

while True:

    market_books = streaming.output_queue.get()

    print(market_books)

*Tags: Errors Debugging, Strategies*

---

## 2020-08-12

**Troy Edwards** - *12:07:46*

Hi all in relation to the 15 character "customerStrategyRef" used in placeOrders can we use any characters ie &gt;&lt;() ????

*Tags: Strategies*

---

**Mo** - *13:20:04*

It's unclear whether those restrictions also apply to the `customerStrategyRef` field

*Tags: Strategies*

---

**Misha** - *13:24:00*

As a general rule I use customerStrategyRef as a broad category, and customerOrderRef to uniquely identify the bets. Between the two you can be very creative without resorting to special characters. I use the customerOrderRef to uniquely identify every bet over the lifetime of any of my systems, no matter where and when the bet was placed

*Tags: Strategies*

---

**Troy Edwards** - *22:00:07*

Oh customerOrderRef did not see this in Betfair documentation.   I guess I will test in coming days that I can have duplicate customerStrategyRefs as well as characters allowed.    I do store my bets placed in a bet table with my unique rule index (pID) however i would like to use the CSR so I can review Betfair profit and loss or 3 month bet report.   Will also check out COR.  

*Tags: Strategies*

---

## 2020-08-15

**Michael** - *17:30:13*

Liam's strategy locks in a return but you lose on the spread.

*Tags: Strategies*

---

**Remi** - *17:34:08*

So I wonder what should the strategy be to make the best use of these free bets with 50 stakes

*Tags: Strategies*

---

## 2020-08-19

**JC** - *12:59:59*

Just doing some backtesting on a strategy using flumine and seem to have run into an order violation error related to MAX_ORDER_COUNT, as the total number of orders for this hour has exceeded 1016. I can't imagine my strategy would reach this number in an hour in real time, just wondering if this limit should be disabled for backtesting, and what the safest way to disable it would be? Cheers

*Tags: Errors Debugging, Strategies*

---

## 2020-08-26

**liam** - *16:09:19*

FYI fixed in this [https://github.com/liampauling/flumine/commit/11f67d2b3cebc927835252451306a5c53a89b3c9|commit](https://github.com/liampauling/flumine/commit/11f67d2b3cebc927835252451306a5c53a89b3c9|commit)

*Tags: Errors Debugging*

---

## 2020-09-02

**JC** - *19:41:57*

Hi guys I know that this is something that has been mentioned before but just wondering if there are plans to add hedging/take profit/stop loss functionality to flumine? I see that from the value perspective it doesn’t make much sense, but for trading order flow and also for reducing drawdowns it might be useful. What I am looking at implementing is an adjustable take profit and stop loss price for each successful order. If it’s not on the cards for an official commit, for reasons previously mentioned, any advice on the cleanest way for me to add it in to my code? Cheers 

*Tags: Strategies*

---

## 2020-09-03

**JC** - *11:15:04*

[@U0128E7BEHW](@U0128E7BEHW) that sounds like a good way of doing it, my code is pretty messy atm doing it all in one. Is there a function that gives your overall aggregated position on a selection in a market across all orders, or do you just loop over orders in `process_orders` in the second strategy?

*Tags: Strategies*

---

## 2020-09-09

**Remi** - *08:30:12*

I have a question about generating commission. Am I generating (implied) commission if return in a market is exactly 0.00 ?

E.g. I lay a runner at 2.00 and then back the runner at 2.00

OR I back runnerA at 2.00 and then I back runnerB at 2.00 and one of them wins

Both cases same stake size for each bet.

*Tags: General Technical*

---

## 2020-09-10

**Gary** - *09:28:51*

How do you decide if your strategy is outdated/stopped working as compared with a statistical slump? Is it as simple as waiting for a statistically meaningful number of outcomes or something else?

*Tags: Strategies*

---

## 2020-09-11

**DH** - *09:27:40*

Yeah fair point. I can see the argument for restricting order types (especially given the way this has played out in US equities microstructure) albeit that I do prefer your definition of FOK to theirs, but the latency point seems little odd given that an API user by default will have the ability to interact at much lower latencies. Nonetheless, I guess applying a 10ms 'speed bump' equally across all users seems fair! Thanks for the extra context you provided elsewhere in the thread around exactly where the speed bump is applied, I hadn't been aware of that.

*Tags: Performance*

---

## 2020-09-16

**JC** - *13:40:58*

Where there is an in-play delay, is it possible to cancel/ reduce size or in any way change/update a bet during the delay before it is offered to the market? Also (this is a noob question) when would it be useful to use replace_orders() or update_orders() and do the same in-play delay rules apply to their usage as well?

*Tags: General Technical*

---

## 2020-09-24

**Newbie99** - *19:38:32*

I know this question gets asked a lot, but its impossible to go back and search back far enough!!



I'm trying to decide which provider to get a one off download of historical result data (just results with basic meta stuff, nothing too crazy), going forward I can get what I need from recording etc, but want a one off data dump. There's plenty of places, but anywhere to avoid/any stand out provider?

*Tags: General Technical*

---

**Newbie99** - *21:45:46*

(that said, if there is an all in one that has football and/or tennis as well for a fixed price, pretty happy with that too, but not essential)

*Tags: Errors Debugging*

---

**D C** - *21:50:49*

[@UFTBRB3F1](@UFTBRB3F1) I might be sending you up the garden path here, but the full platinum subscription to proform might allow you to export the data. I have used it in the past although never tried the export so I don't even know if it is possible. Downside is I think it is only installable on windows but that may not be a problem for you. I know a lot of people rate the data very highly but it might be overkill if all you want is basic result data.

*Tags: Getting Started*

---

## 2020-09-25

**Mathias Tejs** - *10:18:45*

I'm interested in the same question for football/soccer and so far the best I've found [http://whoscored.com|whoscored.com](http://whoscored.com|whoscored.com) which has a lot of event-data on the shots, passes etc. during the game and [http://www.football-data.co.uk|www.football-data.co.uk](http://www.football-data.co.uk|www.football-data.co.uk) has historically provided odds for a lot of games - cup games are missing unfortunately. Would love to hear what other people use :slightly_smiling_face:

*Tags: General Technical*

---

## 2020-09-26

**birchy** - *14:22:53*

What are your thoughts on the Betfair API `ScoresAPING/v1.0/listRaceDetails`? I've been experimenting with it for getting race status but I'm not yet convinced it can be relied on.

*Tags: General Technical*

---

**birchy** - *17:07:27*

Based on the no response, I guess I may be the latter. :thinking_face: 

[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1601129635034400|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1601129635034400](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1601129635034400|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1601129635034400)

*Tags: General Technical*

---

**Michael** - *17:15:50*

It's problematic though because the data is inconsistent.

*Tags: General Technical*

---

**PeterLe** - *19:47:15*

[@U016TGY3676](@U016TGY3676) if you can’t get via Betfair, you could drop George Swindells (TPD) an email and see if they would give you a trial at a reduced rate. He’s a good guy and very helpful. Just a note on the GPS feeds in general. I found it a strep learning curve and it’s def not a silver bullet

*Tags: General Technical*

---

**PeterLe** - *21:50:25*

I meant using the data once fully implemented. Ive only ever done in-play (horses), Id prefer not to say why, but I actually think that this is the easiest area to master. My initial intention by adding the GPS feeds were to use that data as the primary criteria for new in play systems, but Ive not found that yet. If anything Ive found Ive been using the data to bolster existing in-play strategies.  There must be many people reading this who dont have the GPS feeds thinking they are missing out (I did too). I may be wrong (as Ive only had this a couple of weeks), but my truly honest opinion is that if a person cant make money off the standard API data in play, then this feed isn't going to turn a losing trader into a profitable one. If you are a profitable trader already, then it like getting a new set of tools, but you still need to learn how to use them

*Tags: General Technical*

---

**PeterLe** - *22:03:54*

hi [@UGV299K6H](@UGV299K6H) I think the 'problem' i have is that because ive been using a successful system all these years, i find myself gravitating back to that method. I think if anything i've muddied the water by adding new inputs (remember Occams razor?). I was hoping to use the data solely to discover new ideas and methods. So still very much exploring I love the challenge, thats what keeps most of us doing it day in day out! Edit: I was reading around this last night and it falls into "the Law of the instrument" category (See wiki). Ie if you only have a hammer, then every problem to solve looks like a nail..cognitive bias. I guess just being able to recognise this is a step forward to growth :grinning:

*Tags: General Technical*

---

## 2020-10-04

**birchy** - *15:14:59*

For backtesting, what are you guys using hardware-wise? I'm currently using my rather old desktop PC, but had to put a sleep(0.001) in the code due to multiprocessing all 4 cores at 100% which pushed the CPU temperature to 105C and caused a hard shutdown. Obviously the cooling fans (4x80mm &amp; 1x120mm) are not sufficient, but I'm a little reluctant to start upgrading cooling on a machine that is mainly a workstation and is otherwise reliable. I'm wondering if AWS offer anything reasonably priced? Not too bothered about 'fast' backtesting, but would like to be able to run multiple backtest strategies concurrently. Would an AWS micro or lightsail instance be sufficient?

*Tags: Deployment*

---

**Jonjonjon** - *15:56:37*

Would reapplying thermal paste on the cpu help? Or upgrading the cooler? Also, if it is dusty and clogged, that will make it hotter. I reduced temps on my gpu from 90c to 85c by using an air duster on it.

*Tags: General Technical*

---

**Mo** - *15:57:07*

Yeah it sounds like a thermal paste problem

*Tags: General Technical*

---

**Jonjonjon** - *15:58:03*

When I reapplied thermal paste on my laptop it did not help that much. So it's probably a job left to the competent.:disappointed:

*Tags: General Technical*

---

**birchy** - *16:14:12*

I don't want to use my main workstation for backtesting as I need it for other development work. Under normal use, it runs at around 50c and I've never had any issues with it before. My main aim is to find a way to run multiple backtests on multiple machines. Do AWS offer anything that is designed for high CPU use? They have so many services available, that I'm really struggling to identify what I need. Just read that there's a T2/T3 "unlimited" option but I don't fully understand how the burst credits work.

*Tags: Deployment*

---

**Mo** - *16:32:32*

The service you want is the same: EC2. You just want a different instance type. It depends how much you want to pay and how quickly you want the backtests to run. Personally I would not scale with multiple instances but just use a bigger instance type i.e. one with more vCPUs. Look at the M5 family. 

*Tags: Deployment*

---

**Misha** - *22:37:19*

Dust - has caused all my overheating problems

*Tags: General Technical*

---

## 2020-10-11

**Misha** - *21:40:49*

Rule of thumb for me: minimum 5% profit / size pre-commission, would always aim for 10% profit / size pre-commission, ideally 10% and large volume is the "gold standard". But that is betting on a statistical model (on results) - not "trading"

*Tags: Strategies*

---

**Misha** - *22:38:32*

Basically I would withdraw funds as often as the variance in my model allows (although no point doing so more often than once every month or two)

*Tags: Strategies*

---

**Ruben** - *22:39:33*

any considerations on the trade-off between withdrawing vs increasing the stakes of your model?

*Tags: Strategies*

---

**birchy** - *22:44:25*

[@U011VL3CA2Y](@U011VL3CA2Y) the idea is to increase stakes to maximize return but market capacity defines the point at which increasing stakes is no longer +EV. Unfortunately, betfair doesn't offer infinite volume and quite often, the capacity changes because the punter(s) who are feeding your strategy reduce their stakes or change their strategy to something else.

*Tags: Strategies*

---

## 2020-10-12

**Remi** - *10:13:05*

Related to the ROI question, how many bets are you able to get matched per day on average?

*Tags: General Technical*

---

**liam** - *10:26:19*

and to answer [@U016TGY3676](@U016TGY3676) question, average of 4.49% ROI (profit/size_matched) across all strategies/markets this month

*Tags: General Technical*

---

**Michael** - *10:33:31*

Yeah I just wonder why ROI gets talked about so much. If you're doing some sort of Kelly type scaling then yeah - but are many people doing that?

*Tags: Strategies*

---

**birchy** - *10:36:16*

Ironically, I only calculate ROI every few months as more of a sanity check than anything else. As long as my balance goes up steadily, I don't really check ANYTHING, although I should probably do more analysis and optimization.

*Tags: Performance, Strategies*

---

**Michael** - *10:52:48*

Essentially once a strategy is well developed for me it's not dependent on 'bank' at all because I have more appetite to bet than the market will stand.

*Tags: Strategies*

---

**Michael** - *11:10:23*

I look at a whole bunch of things separately: Profit on each transaction if fully matched at flat stakes, proportion of that realised and so on. Optimising stakes for similar transactions is just profit. I also run a fixed 'benchmark' simulation on every market so that I can compare groups of markets in terms of their overall capacity for getting value bets in and try to control for the background performance of markets in that way (to factor out things like the post lockdown surge). It get quite complex though.

*Tags: Errors Debugging, Performance*

---

**birchy** - *17:52:43*

I have found a couple of inplay strategies via Flumine backtesting that are showing a positive return over 500 markets/~4000 bets. Going to test against a bigger sample, but am just wondering how backtesting compares to live betting? I've not (yet) run any live betting with Flumine as I'm taking cautious steps due to the sheer speed of streaming, which means a small mistake can quickly become expensive. What would you consider as a "goer"?

*Tags: Performance, Deployment, Strategies*

---

**Lee** - *19:54:35*

You do have `max_selection_exposure` to help you too, it really helps me feel more comfortable

*Tags: General Technical*

---

**birchy** - *20:32:56*

The Mrs is pretty good at helping to spend it as well. If she knew what I was doing, she'd be writing a list of things we "need". :flushed:

*Tags: General Technical*

---

## 2020-10-13

**Mo** - *23:04:58*

No because you need to take an opinion on the relationship between the probabilities and how to infer them from each market

*Tags: General Technical*

---

## 2020-10-14

**Mo** - *09:06:01*

Which is a shame because it's a very sound strategy

*Tags: Strategies*

---

**ricky** - *12:31:27*

Did any one have problem withdraw money from bookmarker? ( like Novibet )



My Novibet account suspended 12 weeks ago, because i request to withdraw money, even after i sucessful verfitication my account.

Every time when I asked Novibet customer service to update the status of my account. They will reply as "the review of your account by the responsible department is still in progress we do not have any further information to share at the moment. As customer service they cannot provide any time frame for this review. And ask me to wait."

It already 12 weeks now without any lucks.



There are two things I probably can do now

1. wait for Novibet update

2. complaint to AskGamblers or the operator's ADR service IBAS.



Any suggestions on this issue?

*Tags: General Technical*

---

**D C** - *13:04:33*

No idea then [@U0160KZB6QP](@U0160KZB6QP). I thought multiple accounts at same address were a problem (even with different person) but I have not used a bookie in years so that might be nonsense. No idea why responsible gambling department would get involved unless you or your partner were registered SE at some point in the past. It is very frustrating getting no response like that. Have you tried contacting by phone? Not sure what else you can do really. You could try casinomeister site - novibet is accredited there and that used to mean they had a rep from the casino as a forum member. You could try that  to get some answers but its a long shot.

*Tags: Multi Client*

---

**Twatter** - *13:13:40*

[@U0160KZB6QP](@U0160KZB6QP) I'd first go down the route of getting in touch with LiveChat and threaten an IBAS complaint since there's no reason why a withdrawal should not have completed in that time and anything beyond 10 working days is not on. The only time a really long delay has happened to me is when I had a card expired and I issued a withdraw to expired card and it was stuck in limbo - but was still resolved after contacting LC

*Tags: Deployment*

---

**ricky** - *13:29:19*

[@U017A7TRF6E](@U017A7TRF6E) I tried the live chat,  they alway told me "i can not give you an exact time frame as it is conducted by another department". The problem was i dont know where to complaint. i will give a try again today.

*Tags: Deployment*

---

## 2020-10-22

**birchy** - *13:21:35*

In Flumine backtesting, does the matching code replicate the queue system? i.e. if I place a bet at a price with £100 in front of me, does it match the £100 before my bet is matched?

*Tags: General Technical*

---

**river_shah** - *14:00:04*

```    def _check_pending_packages(self):

        for order_package in self._pending_packages:

            _client = order_package.client

            if order_package.package_type == OrderPackageType.PLACE:

                if order_package.elapsed_seconds &gt; (

                    _client.execution.PLACE_LATENCY + order_package.bet_delay

                ):

                    self._process_order_package(order_package)

            elif order_package.package_type == OrderPackageType.CANCEL:

                if order_package.elapsed_seconds &gt; _client.execution.CANCEL_LATENCY:

                    self._process_order_package(order_package)

            elif order_package.package_type == OrderPackageType.UPDATE:

                if order_package.elapsed_seconds &gt; _client.execution.UPDATE_LATENCY:

                    self._process_order_package(order_package)

            elif order_package.package_type == OrderPackageType.REPLACE:

                if order_package.elapsed_seconds &gt; (

                    _client.execution.REPLACE_LATENCY + order_package.bet_delay

                ):

                    self._process_order_package(order_package)```



*Tags: Performance*

---

## 2020-10-31

**Alessio** - *16:41:25*

i have a question i couldn't easily find an answer and it's a bit difficult to test live ..

*Tags: Deployment*

---

**liam** - *17:02:00*

It will error, can’t remember off the top of my head what the error code is though 

*Tags: Errors Debugging*

---

## 2020-11-06

**RichL** - *06:53:14*

It’s possible, but certainly not reliable, to even do it on the historic free data / lagged live data.

*Tags: Deployment*

---

## 2020-11-07

**birchy** - *13:10:02*

FYI, it's a £2 BACK strategy using nothing but price data for triggers

*Tags: Strategies*

---

**Mo** - *13:18:43*

Is this backtesting with flumine? Does it remove matched bets from the order book to avoid double counting liquidity?

*Tags: General Technical*

---

**birchy** - *14:06:41*

I agree with all of the above, which is why I wanted confirmation. It's a Flumine backtest, so no commission. I have about 1200 UK horse racing markets at present, so the graph represents that full dataset as I currently have nothing else to test against.

[@UBS7QANF3](@UBS7QANF3) not sure what you mean regarding "remove matched bets to avoid double counting liquidity"? It's a Flumine backtest, so I'm just relying on the simulated matching, assuming it was already catered for?

The strategy is still very raw, i.e. unrefined as I threw it together just to get a ballpark figure, so it may be fine tuneable.

*Tags: Strategies*

---

**Mo** - *14:12:43*

I don’t use flumine so I don’t know how things are implemented but I mean ensuring that if you take £2 of liquidity that it’s not there for you to take it again

*Tags: General Technical*

---

**birchy** - *14:19:13*

Ohhh...no... I've not been doing that as I assumed the Flumine matching engine would do that. Maybe [@U4H19D1D2](@U4H19D1D2) will confirm?

*Tags: General Technical*

---

**birchy** - *14:44:59*

[@UGV299K6H](@UGV299K6H) I've generally found the same thing, but I've only recently started using Flumine because it has a simulated matching engine and I recall [@U4H19D1D2](@U4H19D1D2) saying it tends to underplay the matching.

*Tags: General Technical*

---

**birchy** - *14:59:23*

Yeah, overfitting is a problem. I need to get a lot more data anyway, as 1200 markets is not really enough.

*Tags: General Technical*

---

**liam** - *15:09:40*

That looks like a tiny number of triggers for 1200 markets, obviously strategy dependant

*Tags: Strategies*

---

**birchy** - *15:27:27*

[@U4H19D1D2](@U4H19D1D2) I'm used to placing only a handful of bets per market, so thought I'd already "upgraded". :grinning: 

How many bets per market would be normal? The strategy is for inplay GB horse racing, an area that is totally new to me at present.

*Tags: Getting Started, Strategies*

---

## 2020-11-08

**Jonjonjon** - *10:47:07*

Same market. Basically, a time series built up from multiple market book updates from the streaming api.

*Tags: General Technical*

---

**birchy** - *11:16:38*

Morning everyone. Since I've been asking "dumb" questions regarding strategies/markets/etc, I've received a number of PM's from members on here encouraging me to ask more similar questions because they are also learning. It's fairly obvious that there are a small number of professionals on here and a significant number of wannabes. I'm definitely one of the latter.

Based on this, I'm wondering if any of you pros would be willing to put together a list of suitable reading/learning materials for noobs? Understanding the fundamentals of how the markets operate, how to find edges, how to calculate value, what tools you consider essential (nn's, ai, Bayes, indicators, etc) would be a great starting point.

*Tags: General Technical*

---

**Mo** - *11:22:05*

In the past we have discussed having some kind of "recipes" documentation/GitHub repository although I think that was more focused on the software development side of things

*Tags: General Technical*

---

**Mo** - *11:24:09*

I hope I've made it clear that I'm happy to try to help anyone no matter how basic their questions are and that any scathing criticism is intended tongue in cheek

*Tags: General Technical*

---

**Mo** - *11:25:19*

In other words, if people have specific questions they should ask away

*Tags: General Technical*

---

**liam** - *11:39:47*

The limit is frustrating, I have copied a few of the discussions with the plan to create a strategy development / faq on the flumine docs but haven’t found the time to put it all together 

*Tags: Strategies*

---

**Jonjonjon** - *11:41:08*

To be honest I've found reading material to be pretty useless. Running a spray and pray strategy, and then analysing the results, has been more useful for me.

*Tags: Strategies*

---

**Mo** - *11:42:33*

Generally I'd agree with that [@UPMUFSGCR](@UPMUFSGCR) but I think in this case the problem is that the right reading material doesn't yet exist. And [@U016TGY3676](@U016TGY3676) wants to address that

*Tags: General Technical*

---

**mandelbot** - *11:43:23*

I personally would benefit a lot from a few more sample code examples. Takes me too much time to figure out things like how to place an order x minutes before the off :joy:

*Tags: General Technical*

---

**Newbie99** - *11:48:37*

tbh scathing is good (IMO), I'd rather ask a ton of dumb questions and get scathing answers that actually help, rather than ask a load of questions and get fluffy nice answers that don't!

*Tags: General Technical*

---

**birchy** - *13:37:18*

Not sure about anyone else, but for me, the fundamentals are things like what moves prices, when is the market most/least efficient, trading Vs betting, how to evaluate if a strategy is +EV, etc. Most likely the things that some of you take for granted. And probably more importantly, what NOT to read, i.e. trading "professionals", Betfair "education", etc.

*Tags: Strategies*

---

**PeterLe** - *17:41:14*

Basics: Just out of interest, I wonder how many of the most successful folk on here would own up to having a simple strategy(s). The difference between profitable and non profitable can be tiny. For my own part; I know Im not one of the top earners on here but Ive only ever used simple ideas and churn as much as I can through the markets.(After reading some of the posts on here today I know I need to push a bit harder though :grinning:). Ive always found that where possible try and keep things as simple as possible and then create variations of a theme.

Is there any other successful traders who would care to own up to simple strategies/ideas :grinning:

*Tags: Strategies*

---

**PeterLe** - *18:00:17*

Good question, well normally Id say greater than 0, but for the sake of this particular question lets say &gt; 6 figures

*Tags: General Technical*

---

## 2020-11-10

**JC** - *22:23:28*

Is there any python code in betfairlightweight/ flumine for working with the betfair in-running GPS API?

*Tags: General Technical*

---

## 2020-11-12

**Mo** - *18:56:34*

I’m still available to write any C code if it helps

*Tags: General Technical*

---

**liam** - *19:05:41*

Got any resources for the C API seems to be very limited info online other than the docs and a few stackoverflow questions 

*Tags: General Technical*

---

**klozovin** - *19:20:51*

maybe some alternative Python implementation, PyPy possibly...?

*Tags: General Technical*

---

**klozovin** - *19:24:34*

don't think I've ever seen any other Python impelmentation except the CPython one

*Tags: General Technical*

---

**liam** - *19:37:55*

My current process is try and write some C, build, then run and then get a cryptic error, is there a better process?

*Tags: Errors Debugging*

---

**Mo** - *19:40:38*

I’m sure you could sort out debugging with gdb but I have no idea how to do that with a Python C extension

*Tags: Errors Debugging*

---

**Alex B** - *19:40:39*

Yep, multithreading in python is only really useful if your threads are waiting on network, disk, etc.  They won’t help with compute-heavy parallelism.  Traditionally in python you’d do this by vectorising your code (e.g. numpy), but backtesting doesn’t really fit into that paradigm all too well.

*Tags: General Technical*

---

**Alex A** - *19:44:31*

Although in hft people tend to care a lot more about latency than throughput.

*Tags: Performance*

---

**liam** - *19:45:53*

Hmm seems like some secret club where no one needs help as everyone is already an expert 

*Tags: General Technical*

---

**Josh** - *19:54:52*

Cython backs a lot of scientific/numerical python code (e.g. scikit-learn, pandas, etc) and is pretty easy if you're familiar with a bit of C/C++. You typically don't get a big boost unless you go through and tell the compiler the equivalent C type of variables; It's type inference is pretty weak. But for certain algorithms using the proper data structures, you can get orders of magnitude speed-up and if you're not dealing with actual python objects, you can explicitly release the GIL to get scalable cpu-bound multithreading. Numba is JIT compiles python to efficient machine code, but is primarily for numerical work, so not general purpose, for things like json parsing. As with anything, profile, profile, profile using any combination of cProfile, line_profiler, py-spy, austin, vmprof, etc to find out where the actual hot spots are in the code.

*Tags: Feature Engineering, Performance*

---

**Mo** - *20:05:16*

No, Python extension written in C

*Tags: General Technical*

---

**liam** - *20:11:00*

This is the [https://github.com/liampauling/betfair/blob/1af4115d5f7493a0b0b4dfb674987830b556464f/betfairlightweight/streaming/cache.py#L14|problem](https://github.com/liampauling/betfair/blob/1af4115d5f7493a0b0b4dfb674987830b556464f/betfairlightweight/streaming/cache.py#L14|problem) it’s the sorting / list comp that is just so slow when backtesting because it is called a *lot* 

*Tags: Performance*

---

**Michael** - *23:01:05*

[@U4H19D1D2](@U4H19D1D2) - Maybe I'm missing something or not understanding the problem (probably) but why not just buy a computer with more cores? Even I back-test across a bunch of cores and I'm way behind the pack in terms of programming. I just use multiprocessing and a pool which I'm sure isn't the best approach but it's a whole lot quicker than learning C.

*Tags: General Technical*

---

## 2020-11-13

**Michael** - *07:15:06*

Fully behind your perfectionism though... that's why we have BFLW.

*Tags: General Technical*

---

**liam** - *07:26:08*

I hate strategy development, it’s so boring

*Tags: Strategies*

---

**liam** - *07:27:56*

I think with a bit of C we can make bflw fly when it comes to streaming/back testing, I am aiming for 5x speed improvement 

*Tags: Performance*

---

**Michael** - *07:55:44*

Takes all sorts huh. Strategy development is the part I sort of enjoy. Very happy for someone else to sort the code. :pray:

*Tags: Strategies*

---

**John T** - *08:38:14*

A couple of books that I found helped a bit were python cookbook 3rd edition (chapter 15) and Programming Python, 4th Edition Chapter 20 on Python/C integration

*Tags: General Technical*

---

**John T** - *08:38:26*

Happy to help out if I can

*Tags: General Technical*

---

**Alessio** - *10:54:39*

Yeah i see your point. It's easy to hack up something and do some data science, but to really put the money on it you need extensive platform done spending the necessary time and optimizations.

*Tags: Performance, Strategies*

---

**Julio** - *11:52:41*

Regarding strategy development. Assuming you have a winning strategy, how do you guys (try to) assess the capacity of a strategy (how much you can increase your bet size). Especially in an illiquid market. My fear is that if I am betting too big, I'll end up destroying the edge or worse being spotted by some algo taking advantage of me. In the first case, it is not that bad, as if i start losing i could just re-decrease the size. But the second case could be catastrophic, as I'd have to go back to the drawing board....

*Tags: Strategies*

---

**birchy** - *13:29:22*

I played with it about 7 or 8 years ago but Betfair seed the markets, so it's pretty much a casino. "Value" prices are rarely taken because Betfairs offers are in front of you. Did occasionally get 1 or 2 matched but that was only when some mug with more money than sense had gone deep into their martingale strategy. I only tried the exchange poker because I had a lookup table to quickly calculate the probabilities but could never get to the front of the queue. It's ok if you're happy with matching a couple of bets per day, but it wasn't for me.

*Tags: Strategies*

---

**birchy** - *14:23:52*

[@U4H19D1D2](@U4H19D1D2) regarding code optimization, have you abandoned gofair? GO is not quite as fast as C, but certainly a big improvement over python.

*Tags: Performance, Strategies*

---

**Twatter** - *15:44:28*

Error handling will be much easier though... just add my favourite line of code to it and you're set... "ON ERROR RESUME NEXT"

*Tags: Errors Debugging*

---

**liam** - *15:45:26*

quite enjoying learning about C (can see why python is what it is) and with a lot of the CPython code having c modules I like how pure it is

*Tags: General Technical*

---

**river_shah** - *15:53:31*

[@U4H19D1D2](@U4H19D1D2) is C bflw actively under development? I want become a better C dev and a concrete project such as porting strategies from python to C will definitely be good impetus

*Tags: General Technical*

---

**Michael** - *17:36:17*

I've always had bad code and good strategy and I've done fine. As long as your code isn't terrible the return on the bottom line of better optimised code diminishes quickly for my type of betting. At first I was worried about my code being crumby but then I decided to see how much difference slow code makes by deliberately slowing my code down. I stopped worrying after that. I can see that for someone like [@UBS7QANF3](@UBS7QANF3) it can be important to be well optimised but it does depend on how you're playing. In the end you can make money with good strategy and bad code but if your strategy is poor you will lose - or at least not win much. There's no point in posting bad bets very quickly.

*Tags: Performance, Strategies*

---

**Alessio** - *18:07:23*

But if your code doesn't handle rare events and edge cases right, you will end up in crappy situations. Also, faster backseating really gives you the ability to do better and you can't always just throw more AWS credits at the problem

*Tags: Deployment*

---

**Michael** - *18:18:05*

Obviously it's better to have good code than bad code, but strategy is Batman, code is Robin.

*Tags: Strategies*

---

**liam** - *18:21:46*

Infrastructure / CI / deployment is the bat mobile then

*Tags: Deployment*

---

**Julio** - *19:33:34*

Let's hope it is not going to be a cliff. Thank you guys for your advice. I'll try to slightly increase and monitor ROI (hopefully variance wont get to much in the way).

Moreover, I slightly amended my code, to allow for several instances of the same strategy with much smaller stakes. Hopefully it will smooth a bit execution and I could stay a bit more under the radar... Thanks to flumine it is so easy to do. Thanks again. liam !!!!!!!!

*Tags: Strategies*

---

## 2020-11-16

**liam** - *13:40:26*

Starting looking into optimising bflw and noticed that `create_resource` (the most expensive call in the entire library) was being called twice when using the generator :face_palm:



[https://github.com/liampauling/betfair/compare/task/historical-gen-double-call|Removing](https://github.com/liampauling/betfair/compare/task/historical-gen-double-call|Removing) this has almost halved the processing time for a market so should make a massive difference when used for backtesting

*Tags: General Technical*

---

## 2020-11-17

**Mo** - *08:16:57*

From my perspective, I don't think that simply porting the Available class to C is a particularly promising optimisation. I think a big problem is the number of objects that are created and the port won't change that. I started working on it more because it's somewhere to start and I've never written an entire Python class in C before despite using the Python C API to write plenty of functions

*Tags: General Technical*

---

**D C** - *09:42:35*

[@UBS7QANF3](@UBS7QANF3) is C++ an option for you? People who need very high performance code seem to have a huge hard-on for the newer features like move semantics ? Or does C++ not play well with Python?

*Tags: Feature Engineering, Performance*

---

**D C** - *11:45:46*

I saw you referring to reference counting and performance issues caused by object creation and immediately thought of smart pointers and move semantics. Looks like I have put 2 and 2 together and made 5.

*Tags: Performance*

---

**Mo** - *11:46:43*

By reference counting I'm talking about how you need to manage the reference count of Python objects when using the C API so it can do garbage collection

*Tags: General Technical*

---

**Mo** - *11:46:57*

By object creation I'm talking about creating Python objects

*Tags: General Technical*

---

**Mo** - *11:50:59*

Just to be precise, I am rewriting things in C but as a Python extension using the Python C API rather than a standalone application

*Tags: General Technical*

---

**D C** - *11:53:06*

OK. I will educate myself in this regard especially now I am having to learn Python myself.

*Tags: General Technical*

---

**Mo** - *11:56:24*

As recently discussed in this channel, it's hard to find good resources on the Python C API but the concept itself is great if you know C/C++ because it allows you to write performance critical parts of your application in C and the rest in Python. It also works the other way if you want to run Python code from C

*Tags: Performance*

---

**D C** - *12:03:34*

Just wondering really if I could reuse any C++ code within a Python project in future  basically

*Tags: General Technical*

---

**liam** - *18:54:12*

Not sure you would get the performance increase without C or something really clever 

*Tags: Performance*

---

**Mo** - *18:57:25*

Not sure if you mean some of the functions in the Available class or just some functions more generally; regarding the former, I couldn't easily work out if it's possible to attach a method written in C to a class defined in Python. Regarding the latter: possibly but [@U4H19D1D2](@U4H19D1D2) directed me to the Available class being the bottleneck so that's where I started

*Tags: General Technical*

---

**liam** - *19:07:00*

Interesting, so you have a dictionary of all the prices/size and then track the top, when profiling it’s the serializing into lists that just kills performance 

*Tags: Performance*

---

**brightcake** - *19:12:12*

How do you guys manage closing out of positions in play within the Flumine framework?

The way I am currently looking at it seems a bit messy... Creating a dictionary with placed orders and then using this dictionary to place the opposite order once some condition is met.

*Tags: General Technical*

---

**MacrcoPolo** - *19:15:21*

Just as an aside, in high-frequency trading this is a really big important problem (get the market update and re-arrange the data into a useful form very quickly). Typical approaches use things like b-trees for their log(n) complexity on a variety of opps. The key difference here is that on betfair we know _all_ possible prices in advance, and curcially there isn't many of them. If there's anything you can do to exploit this, you can speed things up

*Tags: Performance, Strategies*

---

**Mo** - *19:25:35*

Fixed the memory leak but I think there's another bug

*Tags: Errors Debugging, Performance*

---

**Alessio** - *21:34:32*

given the implementatino of python sort it may be they already do insertion sort on small sized lists, so indeed not sure how much you can get there, if that's the case

*Tags: General Technical*

---

**birchy** - *23:26:08*

[@U0135SLHQBW](@U0135SLHQBW)'s idea is the way to go. It takes me back to about 15 years ago when I was enumerating the ~134 million possible 7-card poker hands in a couple of seconds on a P4 with 1GB or 2GB of ram.

There is often a speed/memory tradeoff, but for this case the memory required for a lookup table is minimal. When designing, bear in mind that sequential lookups are significantly faster than random lookups due to CPU L1/L2 cache.

I don't know how fast the cpython implementation is, but I would imagine that porting the `sort()` and `update()` functions to C would give a decent improvement, particularly the nested `for` loops? I'd also be looking at handling the price/size/etc as integers rather than floats/decimals as they _generally_ (but not always) run faster, depending on the CPU instruction set.



I'd also like to suggest using some bitwise lookups, but it's far too late to be thinking in binary. :grinning:

*Tags: Performance*

---

## 2020-11-18

**Jonjonjon** - *08:09:22*

Thanks Mo. Does it give a performance benefit?

*Tags: Performance*

---

**Mo** - *08:10:11*

I think it takes roughly 83% of the time as the original Python

*Tags: General Technical*

---

**Stefan** - *09:41:47*

I think you really made big mistake when choosing programming language for your betfair framework. Betfair api is about json data interchange, and have a look how python treats json de/serialization, you take a lot of performance degradation just in such trivial part of code.

*Tags: Performance*

---

**birchy** - *09:42:37*

Nice job [@UBS7QANF3](@UBS7QANF3). I didn't realise that you could call CPython directly from C, I'd always assumed that extensions were meant to be native C code that added something that didn't exist already.

*Tags: General Technical*

---

**MacrcoPolo** - *18:16:08*

What about adding an optional number of levels to serialize? The serialization will be far slower when we have to serialize the entire book, but not every strategy needs that. If we only serialize the top n levels (where n is enough for your strat) that can probably shave a decent amount off

*Tags: Performance, Strategies*

---

**Alessio** - *18:22:56*

tbh (and here i'm just speaking out of cheap words instead of hard data, but ..) maybe the real fix would be to change the scheme we do these things: now we are having an asynchronous system where the listener updates the book and the market book reader is another thread, that way you have a contention point. another way would be to have a pipeline style where the strategies run in the thread that updates the book, however you may introduce latency if you do any other shit there between one update and the next one (albeit that should be measurable, not sure how much computation there is and one could move to a  lossy view where you batch all the latest updates that weren't read and are in the queue)

*Tags: Errors Debugging, Performance*

---

**MacrcoPolo** - *18:26:56*

Pipeline style works, each strategy maintains it's own book and consumes updates as required I think. The fundamental bottleneck will still be that serialization, or book construction though

*Tags: Strategies*

---

## 2020-11-19

**Stefan** - *19:13:29*

I am not so smart, so is here anyone able to show short video of strategy back-testing/data analyzing and bot testing? Just to see advantage of any technology used.

*Tags: Strategies*

---

## 2020-11-20

**Stefan** - *09:50:03*

[@U01EYDA8345](@U01EYDA8345) That is not what I meant, actually on general thread  [@U01FSSHULHE](@U01FSSHULHE) asked as well similar question. Someone mentioned of using Tableau app to visual data, and yes back in 2015 I started to use it as well, just google "Betfair Tennis Trading and Data Visualization" and you find maybe my blog post about. So my question was how you use this python library in back-testing/data analyzing and bot testing? Can you show short video of workflow from beck-tesing to bot execution. My point is that as all of you maybe, I am open to any new suggestions and can learn something new. What I am for these days is find simple way to data visualization.

*Tags: Strategies*

---

**Artur Gräfenstein** - *10:43:02*

Im not using this python library. I have wirtten my own libs to fit my needs. As a result I get an ecxel file with graphs and all data I need to understand what is going on. Maybe I can show it later to you if that is what you want to see.

*Tags: General Technical*

---

**Stefan** - *13:17:31*

[@U013R0E7SUV](@U013R0E7SUV) Andrej yes I am looking for any tool helping to visualize or analyze, but when you end with last phase, so implementing bot data analyzing algorithm, you need  to use the same algorithm for data processing like your used tool is using, and that can be sometimes a problem. This way the code preprocessing data for visualization or analyzing can be used directly in the bot trigger, you can actually see that in my video where back-testing strategy/so bot, generates data to be shown on the charts, so helping me manually looking for some data patterns.

*Tags: Strategies*

---

## 2020-11-21

**birchy** - *14:11:30*

So with all this chitter chatter about modelling data and probability prediction, I'm wondering where simpletons like me would get started on these approaches? What tools would you consider are essential? I'm not a complete dunce (some parts are missing!) but Google searches seem to muddy the waters even more and all these Greek symbols are...all Greek to me.

I've been experimenting with various strategies using nothing but market data, but am really struggling to find anything worth punting some real money. Truth is, I seem to be able to construct strategies very easily but _winning_ strategies are much harder to come by. And with you all saying that the last few months have been golden, I really do feel quite disheartened with the whole thing. :flushed:

*Tags: Strategies*

---

**Dave** - *14:46:36*

Linear models is where I got started. Logistic regression for classification is a decent starter if you're looking at binary problems ([http://e.g.as|e.g.](http://e.g.as|e.g.) win / no win) as it's a pretty transparent model (unlike something more black box like neural networks/trees etc). Linear regression for regression tasks (there are a variety of methods out there to implement such a model). Scikit learn has all the basic tools you need for all the stages of modelling; data cleaning, standardisation, model fitting and evaluation

*Tags: Strategies*

---

**Dave** - *14:47:00*

Even RenTech started with plain ol' linear models :)

*Tags: Strategies*

---

**Dave** - *14:47:57*

Important thing is to understand the fundamental relationship between your inputs and the thin you're forecasting - i.e. no point fitting a linear model to predict y from X if fundamentally X doesn't have a linear relationship to y. And to evaluate on out-of-sample data to understand if your model can actually generalise well.

*Tags: Strategies*

---

**Dave** - *14:49:39*

As many ppl will tell you, build your model with context and understanding of the relationship between your input and target - just shoving some data into an algorithm without any context will most likely lead you to a nonsensical model

*Tags: Strategies*

---

**D C** - *15:14:31*

Totally agree with this. While ML is great for certain things, learning some techniques concerned with statistical inference is a good start. Trouble is that the entry level of mathematics is quite high to get a handle on even logistic regression (personally never came across GLMs until postgraduate levels of study). True, you don't need to know the theory to bang together a model in R/Python/Matlab/whatever, but having some appreciation of what you are modelling and the underlying assumptions of the techniques you are using is very important. Yes you can use a multitude of ML/stats tools blindly in the black box sense, but knowing how to interpret what comes out of them. It is not always about p-values. Trouble is the bar for entry is quite high.

*Tags: Strategies*

---

**D C** - *15:16:53*

But again, depends on what you are trying to predict - if you want to try and model a time series of market data, it is a very different beast to trying to estimate probability of a sporting outcome.

*Tags: Strategies*

---

**Oliver Varney** - *15:31:56*

personally [@U016TGY3676](@U016TGY3676) if you dont have an idea for a strategy (learning task) before using AI/ML its unlikely to magic something up especially as the guys have mentioned if you dont understand what its doing. I think [@UGV299K6H](@UGV299K6H) gave some great advice about understanding market fundamentals. In my opinion the algorithm is always an after thought after you feel you have a solid idea (for which people wont hand to you as it will lead to them losing profit, after years of work).

*Tags: Strategies*

---

**Oliver Varney** - *15:32:58*

watch the markets, understand the fundamentals, come up with a hypothesis, test vs historical data

*Tags: Data Quality*

---

**D C** - *15:48:19*

The choice of tool depends on what you want to do. As some of the big guns say, "simple" things make them profit. But for example, if you wanted to try and build a probability model for each runner in greyhound races, you are going to need identify the right tools for the job. Part of the problem is knowing what you want to analyse, then digging around and finding the right tool. Trouble with ML (and statistics for that matter) is that there are so many methods and techniques and it can be bewildering at times. And as has been mentioned before, even if you FIND the right tool, chances are that there are others out there with access to bespoke data that puts them way ahead of you.

*Tags: Strategies*

---

**D C** - *15:57:27*

Yes I agree but I think for some sports, such as football, the analytics available and the bespoke data around pretty much put the average "one man and his computer" way behind. Maybe for obscure leagues and markets like bookings/corners etc there is an edge to be obtained. I reckon even the man with the most sophisticated model in the world is going to be behind the guy with advance news of the team squad when it comes to profit making. Don't get me wrong, I am not downplaying ML/stats at all (I am/was a mathematician after all), I am just saying that to beat market prices in the bigger sports as a one man band going down the analytical route is a tough gig.

*Tags: Strategies*

---

**Oliver Varney** - *16:00:42*

I think partially the reason why you may feel discourage is those that have had success are positive about the experience on here but dont tell you how much work it took and people maybe get the wrong expectation (from my experience at least). I always come back to the idea that if its easy then everyone would be doing it and it would be worthless. probably not helpful but it is reality.

*Tags: General Technical*

---

**Michael** - *16:03:25*

He also wrote a whole article on his website about how streaming was a terrible idea because it would INCREASE the load on BF's servers.

*Tags: Deployment*

---

**Alessio** - *16:07:29*

:smile: no prob i was just trying to help but i am also very beginner

*Tags: Getting Started*

---

**PeterLe** - *16:13:14*

...and if your only tool is a hammer, then every problem looks like a nail

*Tags: General Technical*

---

**Michael** - *16:21:30*

The thing is it's hard to formulate a nice way of saying _"I've done well at this and it's been great but I don't think you should waste too much time on it because the reality is that most people fail and it's far more likely that you'll be on that team than this one."_ Even though obviously that's the truth. So instead you answer the questions and try to help even though you know it's probably for nothing at best.

*Tags: General Technical*

---

**Michael** - *16:26:04*

Oh I don't think that's fair.. Of course no one gives away their special sauce recipe but I do try to help with methodology, the basics of market dynamics and so on. Some people are fully across all that stuff already but plenty aren't.

*Tags: General Technical*

---

**Michael** - *16:49:25*

Sometimes you see a question and think _"yeah actually I don't think I'll give that away"_ and there are a few things that some people know about but don't talk about but then why should they? I've built my knowledge over more than a decade I'm not going to post it all up for the world to see for free. Even so there's still more valuable, honest and well founded advice given here than I think you can find anywhere else.

*Tags: General Technical*

---

**Michael** - *17:07:39*

I don't use any of those things. In market based operations I just look for instability, then I have some special sauce that takes a stab at what a stable book ought to look like and bet on the differences. For GPS I have a GPS model for the race and I compare it to the book.

*Tags: Strategies*

---

**birchy** - *17:38:02*

Problem is, I have hundreds of ideas that lead to other offshoots and before I know it I'm looking at something else. :grinning:

*Tags: General Technical*

---

**birchy** - *17:50:23*

Good point, I've been using _some_ previous prices but only a handful as I wanted to avoid using stale info, particularly with the assumption that the GPS players will be correction any errors?

*Tags: Errors Debugging*

---

**birchy** - *19:24:28*

Thank you all. Certainly given me a bit of direction.  So to summarise, using betfair prices as the only source of data IS possible, however modelling your own probabilities is far more lucrative IF you have a higher level understanding of maths and can build a decent model.

*Tags: Strategies*

---

**Misha** - *23:22:13*

[@U016TGY3676](@U016TGY3676) - modelling your own probabilities is as hard as it gets, but it's also the path to the biggest profits, basically because it is so hard to do well

*Tags: Strategies*

---

## 2020-11-22

**birchy** - *00:26:20*

Going to persist with the live data for now as I'm not a massive fan of using 3rd party feeds as they can be unreliable and high maintenance. Even the GPS feed is hit and miss with false information. I've always believed that profits can be made from mug punters and will concentrate on that until it makes or breaks.

*Tags: Deployment*

---

**Dave** - *00:53:32*

The way I see it - not all participants in the market are looking to make money from every bet they offer or take, or consider EV in the same way as you might do. People skew from what would be considered your "true price" for all sorts of reasons: hedging existing risk (reducing exposure to a more palatable level, maybe having operational issues etc), or doing silly "offset greening by fixed X ticks"-type things. Computation of the true price also depends on your strategy - if you are predicting the outcome of the event then you'll be applying different techniques to predicting price change over the next X seconds. This also means that not every bet has a "winner" and a "loser" - the person you were matched with may just have a different target they are wagering on, and therefore the bet can have positive expected value to both sides as the target of their forecast is diffeeent. So you don't even need "mugs" to profit on - you just need to be able to pick a reasonable target to forecast (price movement, event outcomes etc), and figure out what data can provide you with decent signals to make your forecast.

*Tags: Errors Debugging, Strategies*

---

**Michael** - *08:08:27*

It seems to be being taken as read that all the biggest operators use external data sources to model probabilities but this just isn't true.

*Tags: Strategies*

---

**liam** - *09:21:05*

Boring is good but I guess it doesn’t sell, [@UGV299K6H](@UGV299K6H) are you still up for contributing to some sort of strategy development resource either on the bflw or flumine docs? 

*Tags: Strategies*

---

**liam** - *09:24:37*

Nope I have just been copying some of the chats from you and [@UBS7QANF3](@UBS7QANF3) over the past few months, any idea on how to lay it out?

*Tags: General Technical*

---

**Mo** - *09:46:36*

I'm happy to contribute material on sport specific probability models (rather than general purpose machine learning models)

*Tags: Strategies*

---

**liam** - *10:11:39*

I would like it to have a python focus but obviously applicable to all languages, brings us back to creating a community page / site or similar, I really like what [https://pysport.org/|pysport](https://pysport.org/|pysport) have done

*Tags: General Technical*

---

**birchy** - *10:13:35*

Can creating a resource be automated? :joy:

Seriously though, lots of people could benefit from understanding the fundamentals, myself included. I guess the only thing to be wary of is if/when some asshole rips it off and starts selling on eBay. I speak from experience as I once published online, a free strategy called "lay the draw" in 2002. I may have been the twat who invented it as I've not found any evidence of it existing before then.

*Tags: Strategies*

---

**Artur Gräfenstein** - *19:32:47*

I can offer my help. I am currently working full-time on bots for strategy development etc. but in nodejs.

*Tags: Strategies*

---

## 2020-11-23

**liam** - *14:22:37*

Agreed one of these a day please, can already see this being a quote to start a section in the strategy docs

*Tags: Strategies*

---

**AP** - *14:44:14*

It’s easy to do with BSP because all of the data is downloadable in CSV format and flumine has made it easy to do with in-play bets too 

*Tags: General Technical*

---

**AP** - *14:46:43*

I’m not as experienced as the other people on here but I just feel like I wasted a lot of time trying to create my own price whereas I found success a lot more easily just by analysing factors/variables and building some simple models that way 

*Tags: Strategies*

---

**MacrcoPolo** - *14:51:28*

One of the standard techniques in modelling low-sample-size sports (NFL comes to mind...) is to benchmark your predictions based on the odds moves towards the start to measure how good your predictions are. You can use this to get an idea of whether your predictions are strong without needing to actually 'realize' the pnl on the outcome.

*Tags: Strategies*

---

**Michael** - *16:22:55*

It's not a trick question.

*Tags: General Technical*

---

**D C** - *16:46:25*

Yeah but you can't predict things like a fat finger error any more than you can predict a runner will fall at the next fence. But as Michael says, the price should always come back to 0EV.

*Tags: Errors Debugging*

---

**liam** - *17:04:34*

A while ago we were arguing about trading and finding profitable EV being different, [@UGV299K6H](@UGV299K6H) helped explain it best by saying if you are a profitable ‘trader’ you have to by definition be getting +EV or you wouldn’t be in profit as that is just simple addition 

*Tags: Strategies*

---

**Alessio** - *17:04:49*

No no they are totally acceptable, but you are saying "price movement implies change in ev". Sounds sane, and on paper it may work if you define price movement by the rules I/you/we think drive the market (it goes to 0EV within a certain time, all information gets incorporated into the market etc..). Question is whether we are right at making those assumptions or not.

*Tags: General Technical*

---

**D C** - *17:07:28*

Alessio there is a lot of talk about modelling recently. If you accept that savvy market participants in the market have a good estimate of a true probability of something happening, when those participants see a price that is value they will get their money in the game. If a price is too large, it is only logical that it will get taken advantage of and so the price corrects back to 0EV ?

*Tags: Strategies*

---

**Dave** - *17:12:18*

But the original posts suggests that the two tasks (forecasting pricing and forecasting outcome) can be unified. But it's be unreasonable to use a model that's very good at predicting a price 5 ticks ahead that takes in order-book/trade features to forecast the outright outcome of event...

*Tags: Feature Engineering, Strategies*

---

**Michael** - *17:15:58*

Your model needn't forecast the outcome of an event to spot +EV. By successfully predicting that a price is about to move you're essentially saying 'I may not know what a 0EV price is but that's too high'.

*Tags: Strategies*

---

**Dave** - *17:16:48*

Just sounded like you were saying the two tasks are the same which I disagree with :sweat_smile:  one is modelling market microstructure (specifically if you do it over a small timeframe, e.g. 5 market ticks) whereas the other is modelling the actual event. The difference I suppose is that if you're betting on price changes 5 ticks ahead, you don't care _which_ of your trades are + EV /  -EV relative to the outcome of your event and believe that your model in general will lead to more +EV than -EV. As opposed to outright value betting where you aim for _each_ bet to be +EV.

*Tags: Strategies*

---

**Michael** - *17:40:57*

Or to put it another-other way: If you understand that to make a profit your bets must show +EV when evaluated against final outcomes (and if you don't I can't help you) and you have a model of any kind that produces profitable results then your model must be identifying +EV bets whether you designed it to do that or not because how else did you get them? My opening comment was intended simply to reveal how this comes about. Hopefully I've put it enough ways now.

*Tags: Strategies*

---

**Dave** - *18:05:42*

I think it's slightly more nuanced than that. Say I forecast the price 5s ahead to be higher than what it is now. I lay at the current price. 5s later the price is indeed higher. I redo my forecast and now my model says "no the price will now drop". So I back. In both cases I assess my positions as +EV relative to price movement 5s ahead. I could not careless about the event outcome. The techniques and skills applied are totally different to looking at value relative to event outcome. You might say that your fair price for a horse is 5.0 and won't back it any price below that. Someone forecasting prices might say I'm happy to back at 4.9 because I expect the price to be lower than 4.9 5s ahead. And while this would be -ev position for you and you'd never take it, it's still a value bet for this person because they assess +EV relative to price movement rather than event outcome, and totally reasonable for them to take it. So I don't think it's fair to unify the tasks if they end up in scenarios like this. (Edit: corrected trade directions)

*Tags: Strategies*

---

**Michael** - *18:31:45*

You're agreeing that your bets are +EV when evaluated against the outcome (and you're right of course) - how do you account for that coming about unless your models identify +EV bets? You might not have conceptualised that as being it's purpose but that's what it does.

*Tags: Strategies*

---

**Michael** - *18:48:03*

I think your assumption that looking for +EV means forecasting an outcome might be the problem.

*Tags: General Technical*

---

**Michael** - *18:56:40*

Ja, das ist kein Problem

*Tags: General Technical*

---

**Artur Gräfenstein** - *19:01:25*

A general question: would it ever be possible, that bookmakers (for examle we forget about the excange etc.) predict the price with all data awailable on planet so it would not be posible to make profit from betting?

*Tags: Strategies*

---

**Artur Gräfenstein** - *19:14:15*

My question is about: when the bookmaker would use 1000 parameters for price calculation, is it possible with lets say 5 parameters of the 1000 make profit? Its more a mathematical question that I am thinking about since some months. It’s also about whether betting has a future.

*Tags: Strategies*

---

**birchy** - *19:14:37*

This is why I've never been 100% convinced that modelling probabilities yourself is worthwhile.

*Tags: Strategies*

---

**thambie1** - *19:20:54*

Also, per Dave's earlier comment; bookamakers just as any trader my not have the same goals or strategy as you. A bookmaker may want a balanced book to reduce risk, more than they want to offer the 'accurate' odds.

*Tags: Strategies*

---

**Matthieu Labour** - *19:23:00*

The way I understand [@U01EYDA8345](@U01EYDA8345)’s question is, will there be a point where there won’t be an edge anymore like a game that has been resolved.

*Tags: General Technical*

---

**Misha** - *20:05:29*

The corporates aren't that great on the modelling side these days - they look at each other, and particularly Betfair, look at their exposure, and adjust accordingly. Of course the UK model is pretty much to ban winners, so they have very few on their books who have good predictions

*Tags: Strategies*

---

**Misha** - *20:07:17*

For stats models, 90% of the profit comes from a few "features". Adding lots of features might fine tune a good model, and make it a great model, but if it isn't profitable just looking at a few variables, you aren't on the right track

*Tags: Feature Engineering, Strategies*

---

**Misha** - *20:08:18*

I have seen a profitable model on Aussie Rules and Rugby League using basically one main feature. Of course, you have to identify that feature :wink:

*Tags: Feature Engineering, Strategies*

---

**Michael** - *20:11:57*

Haha - glad to help. You won't find anything comparable on the BF forum it's mostly me trying to convince half wits that yes probability does exist and yes it does matter.

*Tags: General Technical*

---

**Artur Gräfenstein** - *20:57:16*

If I understand this correctly, you will make profit from value odds. Because you are using data (for example yellow cards) that the bookmaker does not include in their price. Or you have a better Model than the bookmaker. If the bookmaker would now include yellow cards and start using your model too, you will stay at +-0. Now you could say: “Ok, forget about the yellow cards, I will look only at the red cards(wich the bookmaker also does not include in their price for now)“. If the bookmaker includes the red cards too, you will not be profitable at both of these strategies anymore, right? Or the odds will move in an direction that the yellow card strategie will be now profitable at some point? Is it right to assume that the more data the bookmaker use, the less value can be found in the price? (Ignore the fact that bookmakers make mistakes) So any strategy will only work until the day your bookmaker takes over your calculations?(theoretical question) And maybe in 50 years there will no longer be a profitable strategy?

I don’t know if I’m understanding the price calculation right, so...! Also sorry for the bad explanation of translation.

*Tags: Strategies*

---

**Misha** - *20:59:32*

Even if two models use the same inputs, they won't use them the same way, or weight them the same way, or really do anything the same

*Tags: Strategies*

---

**Misha** - *21:00:29*

Introduction of a new feature, unless very carefully considered, will often make the modelling worse

*Tags: Feature Engineering, Strategies*

---

**Misha** - *21:04:44*

So if you are good at modelling the sport, you will always be able to make money, and that will never change. Not to mention that on Betfair you aren't going up against a bookmaker, you are up against 1000s of others with their own ideas of who will win

*Tags: Strategies*

---

**D C** - *21:55:46*

[@U01EYDA8345](@U01EYDA8345) You will never get to the point where everything is modelled to perfection but this could be a philosophical argument as much as a tech argument. Perhaps if we can model things on a quantum level it could happen but in practical terms that is intractible. Even it if were possible, could quantum mechanics predict human behaviour such as corruption for thigs like match or spot fixing?? Maybe one day but we will all be in the ground by then!! As for using 1000 parameters, modelling favours less parameters for their greater generality. Consider a fairly simple mathematical example where you want to approximate a set of data using a polynomial curve - if you choose the degree of polynomial equal to the number of points, you get an interpolation (where the curve passes through every point) but this is seldom any good at predicting the general shape of the data points - better to choose a lower degree curve that is able to be acceptably "close" to the data. The first case is termed overfitting and building linear statistical models favours choosing SMALLER amount of parameters but ones that help "explain" the data wherever possible. I cannot speak for machine learning methods as I am a novice in that area. As for bookies they don't need to be that accurate in pricing (well soft books) because they will just ban winners. Pinnacle use profitable/savvy customers to adjust their prices accordingly but I believe that they have pretty good analytics too (just from what I hear).

*Tags: Errors Debugging, Strategies*

---

## 2020-11-25

**JC** - *19:32:06*

Hi guys, got a question about evaluating the performance of a model. What is the best way, mathematically, to evaluate how accurate a model is at spitting out probabilities in-play. I guess you could for example group together all occasions where the output is 0.6 implied probability and if close to 60% of these predictions are correct then it is a good model? Is there a better way of doing this?

*Tags: Performance, Strategies*

---

**D C** - *19:44:24*

Hosmer–Lemeshow might work here too - its a test that shows how closely probability estimates of a logistic regression model match observed outcomes so is closely related to what you want. I used this years ago and it may not be applicable to this case for the reasons that [@UBS7QANF3](@UBS7QANF3) highlights but it might be an additional topic to explore.

*Tags: Strategies*

---

**Dave** - *20:36:25*

ROC/prec-recall AUC is usually a good indicator of how sensible your model is

*Tags: Strategies*

---

## 2020-11-29

**Charlie 303** - *22:57:03*

Could anyone explain when/what sort of strategy it would be beneficial to turn off Best Price Execution as per [@U4H19D1D2](@U4H19D1D2) s comment in this thread in the bf forums [https://forum.developer.betfair.com/forum/sports-exchange-api/exchange-api/32967-issue-on-betting-in-ordinary-odd-ie-3-4-and-have-it-matched-with-odd-1000?p=32973#post32973](https://forum.developer.betfair.com/forum/sports-exchange-api/exchange-api/32967-issue-on-betting-in-ordinary-odd-ie-3-4-and-have-it-matched-with-odd-1000?p=32973#post32973) ?

*Tags: Strategies*

---

## 2020-11-30

**birchy** - *10:27:31*

So I have a preplay strategy that I'm currently live testing on UK horse racing WIN markets. Long term, I want to extend this strategy to cover _every_ horse racing market, followed by implementation on other sports and ultimately coverage for ALL betfair markets.

Last week, during initial testing, I attempted to start Flumine with:

```market_filter=bflw.filters.streaming_market_filter(

    event_type_ids=['7'],

),```

...and managed to crash an AWS Lightsail instance when CPU &amp; RAM (2GB) hit 100% during startup. This is only a small % of where I actually want to be. I know Flumine creates a lot of objects, but is it actually THAT resource hungry?

Regarding implementing ALL markets, what's the recommended way to achieve this? Currently thinking that I may have to redesign as a polling bot instead. The strategy is not speed sensitive.

*Tags: Performance, Deployment, Strategies*

---

**liam** - *10:33:13*

That. Create a dummy strategy with the same filter and see how that performs to confirm if it is flumine or your strategy code 

*Tags: Strategies*

---

**Unknown** - *10:42:32*

[@U4H19D1D2](@U4H19D1D2) 100% CPU for 5+ minutes. RAM usage was > 80% before the machine started lagging badly until the point where I had to force a restart from AWS admin.

Current live bot is running OK with:

``` market_filter=bflw.filters.streaming_market_filter(

    event_type_ids=['7'],

    country_codes=['GB', 'IE'],

    market_types=['WIN'],

),```



*Tags: Deployment*

---

**liam** - *10:59:12*

Interesting, how many markets did that subscribe to?  I would create a dummy empty strategy to isolate the issue and profile if necessary. I have similar subscriptions on even smaller instances 

*Tags: Strategies*

---

**Unknown** - *13:28:46*

Tested with dummy strategy and then re-implemented my strategy one step at a time. Now running full strategy on 453 markets with:

```market_filter=bflw.filters.streaming_market_filter(

    event_type_ids=['7'],

),```

*Tags: Strategies*

---

**birchy** - *16:45:24*

Did test with conflate = 1000 last week but it seemed to be taking forever to startup, i.e. I pulled the plug after ~20 minutes. That was with no filters specified. Does the conflate also affect the polling workers, or just the stream?

From a personal design point, is it OK to run one strategy on a few thousand markets using one framework instance, or is it preferential to use smaller chunks?

*Tags: Strategies*

---

## 2020-12-01

**birchy** - *12:20:46*

Could be my strategy code but it's not doing anything intense. Just a couple of loops through the runners and some simple calculations. Will retry loading all markets with the dummy strategy when I get time.

*Tags: Strategies*

---

**liam** - *12:23:59*

Cool, the streaming_timeout will also have an impact as this will snap the stream and send through the queue 

*Tags: General Technical*

---

## 2020-12-03

**birchy** - *12:18:50*

[@U4H19D1D2](@U4H19D1D2) Update:

The following runs for about 15 minutes with CPU at 100% and memory gradually increases until 80%+ and then starts using swap. Lots of 'High Latency' warnings in the logger ~700 (not sure if this is seconds or milliseconds?). This is on a Lightsail instance with 2GB RAM. Are you able to replicate?

```class Dummy(BaseStrategy):

    """dummy strategy"""

    def __init__(self, *args, **kwargs):

        BaseStrategy.__init__(self, *args, **kwargs)



    def check_market_book(self, market, market_book):

        return True



    def process_market_book(self, market, market_book):

        """only called if check_market_book() returns True"""

        return



    def process_orders(self, market, orders):

        return```

```strategy = Strat(

    conflate_ms=3000,

    market_filter=bflw.filters.streaming_market_filter(

        event_type_ids=['7', '1', '4339'],

    ),

)

framework.add_strategy(strategy)

framework.run()```



*Tags: Performance, Strategies*

---

**liam** - *12:57:52*

So two issues I can see, the first one is the logging module is actually spiralling out of control using more and more CPU cycles as the latency increases the second is the `_add_market` function is slow, its slow because `.info` property is really really slow however not sure why it is soo slow

*Tags: Performance*

---

**Dave** - *15:57:14*

Not much use forecasting over a horizon shorter than your latency (where latency is anything slowing you down including the enforced delay). Remember it's asymmetric though, so your total latency for cancels doesn't include this enforced delay ;o

*Tags: Performance*

---

**Michael** - *16:06:57*

No. You're either under-thinking it or thinking it wrong. Excess of thinking in not the problem.

*Tags: General Technical*

---

**Dave** - *17:27:53*

It's useful for market making - if you're maintaining a spread then you can avoid getting swept where you anticipate the price to move sharply. Obvious and hypothetical example - if you have a realtime football feed you might want to cancel _fast_ whenever a player with the ball reaches a certain area of the pitch. I think it's just a feature you might want to take advantage of if your strategy suits (and use as protection), but not necessarily a source for inspiration.

*Tags: Feature Engineering, Strategies*

---

**Dave** - *17:30:05*

But yeah, I've never looked at building a strategy focused on the asymmetry

*Tags: Strategies*

---

**birchy** - *18:23:34*

[@U4H19D1D2](@U4H19D1D2) Not directly related to this issue, but am I using Flumine incorrectly by putting strategy functions in the strategy object? e.g.

```class SomeStrategy(BaseStrategy):

    """strategy"""

    def __init__(self, *args, **kwargs):

        BaseStrategy.__init__(self, *args, **kwargs)

        self.bet_age = 2



    def calc_lay_price(self, runner):

        # do some calcs....

        return



    def check_triggers(self, market_book, market):

        """returns list of bets"""

        bets = []

        # do some magical wizardy stuff...

        return



    def check_market_book(self, market, market_book):

        if not market_book.inplay: return

        if market_book.status != 'OPEN': return

        return True



    def process_market_book(self, market, market_book):

        bets = self.check_triggers(market_book, market)

        if not bets: return

        for bet in bets:

            trade = Trade(

                market_book.market_id, bet['id'], bet['hcp'], self,

            )

            order = trade.create_order(

                side=bet['side'],

                order_type=LimitOrder(

                    price=bet['price'],

                    size=bet['stake']

                ),

            )

            self.place_order(market, order)

        return



    def process_orders(self, market, orders):

        for order in orders:

            if (order.status == OrderStatus.EXECUTABLE

                and order.elapsed_seconds

                and order.elapsed_seconds &gt;= self.bet_age

            ):

                self.cancel_order(market, order)```

That was how I'd started off when I first used Flumine and I just carried it forward, but in hindsight, my functions should probably static?

*Tags: Strategies*

---

**PeterLe** - *18:32:13*

[@U016TGY3676](@U016TGY3676) once you have a concept of a new strategy (assuming it is well thought out and reasonable) Theres no substitute for placing real bets and gathering the data. Ive done exactly that today on something that is completely different to what Ive done before (in an attempt to increase matching rates). Whilst I have shown a slight loss I can already see how i might modify it tomorrow (and Ill be modifying it in such a way that is also counter-intuitive). Im not expecting to win much, but Im not expecting to lose much either. What I will have tomorrow is more data that I can look how to hone and refine. In the back ground Ill keep all my other bots churning away

*Tags: Strategies*

---

**birchy** - *18:48:25*

Thanks fellas. I've got no issues with laying runners at any price as the £10 payout target is a decent protection that allows sub-£2 bets.

Bearing in mind that I'm fairly new to this Flumine wizardry, what's the best way to save 'real' market data? I have a MarketRecorder running but that's a strategy of it's own, taken from the Flumine examples.

*Tags: Getting Started, Strategies*

---

**birchy** - *19:43:51*

[@U4H19D1D2](@U4H19D1D2) great features, and will definitely start using them. Probably missing something here, but I presume the MarketRecorder won't pick up any bets I've got placed as it's a different strategy, so what's the recommended way to save data from a live bot?

*Tags: Feature Engineering, Deployment, Strategies*

---

**liam** - *19:50:56*

Logging control, see example [https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py|here](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py|here), if you look at the class you can see all the events it receives. I and a few others send the data to an API but you can just log to a csv / db 

*Tags: General Technical*

---

## 2020-12-04

**Michael** - *13:54:01*

That's not a great tip. Best use of BPE is strategy dependent.

*Tags: Strategies*

---

**Amanda** - *14:04:15*

Excellent tip if you want to avoid backing horses that have already lost after the delay. There are more than one type of strategy.

*Tags: Strategies*

---

## 2020-12-06

**Ben** - *08:46:05*

Question for those who use Flumine. I've got a strategy I'd like to backtest that relies on placing a back bet, then hedging the stake with a lay order if the back bet is executed. I've managed to code up the back bet portion of the strategy just fine, but can't seem to figure out how to apply the logic to execute the lay order. Anyone able to share some tips or relevant code snippets (old strategies)

*Tags: Strategies*

---

**liam** - *08:48:35*

Some discussion [https://github.com/liampauling/flumine/issues/267|here](https://github.com/liampauling/flumine/issues/267|here)

*Tags: General Technical*

---

**Ben** - *09:42:46*

[@U4H19D1D2](@U4H19D1D2) better way to ask my question, can I place two opposing orders simultaneously? And instead of hedge, say I wanted to implement a green up strategy?

*Tags: Strategies*

---

**liam** - *09:51:21*

Yes but you need to change the strategy.[https://github.com/liampauling/flumine/blob/dabcaab5853ed71e95eed1c076706a41ded334e9/flumine/strategy/strategy.py#L44|max_live_trade_count](https://github.com/liampauling/flumine/blob/dabcaab5853ed71e95eed1c076706a41ded334e9/flumine/strategy/strategy.py#L44|max_live_trade_count) 

*Tags: Deployment, Strategies*

---

**JC** - *10:47:25*

When I was considering making a hedging strategy, I think [@U0128E7BEHW](@U0128E7BEHW) mentioned that one way to do it would be to have two flumine strategies running, one that backs and one that lays (hedges). After reading the +EV related discussions on here I decided to forget about hedging though

*Tags: Strategies*

---

**Alessio** - *15:12:17*

Random Q, did anybody try to reverse engineer some of the odds model that are around, i.e. try to figure out which are the features that mostly approximate the odds providers give you?

*Tags: Feature Engineering, Strategies*

---

**Oliver Varney** - *16:19:56*

I think that's a question no one will answer if they have

*Tags: General Technical*

---

**D C** - *16:46:11*

What do you mean [@U01C12ZEADQ](@U01C12ZEADQ)? Surely this would just be modelling rather than reverse engineering?

*Tags: Strategies*

---

**Oliver Varney** - *16:50:19*

May only be my opinion but anyone willing to give up their model details is either a fool or trying to mislead you

*Tags: Strategies*

---

**Mo** - *16:53:09*

[@UUE6E1LA1](@UUE6E1LA1) it’s modelling but you model the odds not the outcomes. For example you do a regression of various factors where the target is the odds, then you can see which factors are most highly weighted

*Tags: Strategies*

---

**D C** - *16:59:46*

Yes obviously the available techniques would be different. But from a regression perspective, what target distribution would odds even come from? Assuming decimal odds, you have a response that is a member of the set (1,infinity) from what I can see. Are we talking about ML or nonparametric approaches? I would not know where to start in trying to model someone else's model odds without trying to map the problem to one of implied probabilities.

*Tags: Strategies*

---

**Mo** - *17:01:23*

Not just the techniques; you can do a lot more with less data if you are modelling probabilities rather than outcomes: there's more information in them

*Tags: Strategies*

---

**Alessio** - *17:07:46*

I asked them and they said they use odds as well. It's a bit interesting they come up with a model which is worse than their input.. so I wonder if the structure is instead the real thing that is different (i.e. how you model the sport itself)

*Tags: Strategies*

---

**Mo** - *17:08:23*

Possibly inappropriate model structure as [@UUE6E1LA1](@UUE6E1LA1) alludes to

*Tags: Strategies*

---

**D C** - *17:09:42*

I swear I must be getting thicker as I get older. For those who have done this, what model would you even use to do this? I can't think of anything in the GLM family?

*Tags: Strategies*

---

**Oliver Varney** - *17:13:54*

[@U01C12ZEADQ](@U01C12ZEADQ) why is using odds as an input to a model that predicts odds a bad thing?

*Tags: Strategies*

---

**Alessio** - *17:15:09*

it's not a bad thing, but in theory your model should be better than what you feed into it, no?

*Tags: Strategies*

---

**Alessio** - *17:15:35*

(assuming the model tries to do the same thing the odds do, i.e. predicting win/loss)

*Tags: Strategies*

---

**Oliver Varney** - *17:16:06*

i.e. if you creating a regression model to predict the value of your house, surely a good feature is the price the neighbours house sold at? Or even better the prediction from the estate agent?

*Tags: Feature Engineering, Strategies*

---

**bb** - *17:17:03*

[@U01C12ZEADQ](@U01C12ZEADQ) How are you defining the model as being good or bad? Accuracy, profitability or using the loss function the model was trained with

*Tags: Strategies*

---

**bb** - *17:18:47*

As the impact of odds on a model's performance will differ according to each

*Tags: Performance, Strategies*

---

**Oliver Varney** - *17:20:06*

if you have a set of features that is prices from various sources + additional features,with each odds provider being slightly better in certain race types (or other splits), the model will learn this and should be better overall

*Tags: Feature Engineering, Strategies*

---

**bb** - *17:25:18*

A few transformations can turn decimal odds into a target variable that is much closer to a normal distribution and easier to model

*Tags: Strategies*

---

**Alessio** - *17:26:27*

[@ULDAVFDRP](@ULDAVFDRP) depends on model. If you are building something that fits to a specific distribution, not sure they have such a non-linear power

*Tags: Strategies*

---

**Oliver Varney** - *17:28:26*

surely your model would at least use the current exchange price? Isnt the bsp just the price parity of many thousands of participants (using models)

*Tags: Strategies*

---

**Alessio** - *17:33:29*

ah sorry, (1) i didn't build the model (2) this is not for live bets, it's for pre-match prediction

*Tags: Deployment, Strategies*

---

**Alessio** - *17:37:36*

curiosity, these guys had a model, wanted to check if their accuracy was inline, better or worse than pinnacle closing odds

*Tags: Strategies*

---

**Oliver Varney** - *17:40:27*

so really if its based on incoming bets, surely its just like the exchange which is basically a price party from many participants with all separate models ?

*Tags: Strategies*

---

**Oliver Varney** - *17:41:15*

apologies if these are stupid questions I dont use any bookies/ pinnacle

*Tags: General Technical*

---

**bb** - *17:43:04*

If the odds are far more predictive of the outcome variable than any other feature a model will weight them very heavily. At the extreme the model could just be a proxy for backing the favourite and accuracy/profitability will be in line with this

*Tags: Feature Engineering, Strategies*

---

**Oliver Varney** - *17:56:25*

if you have a model that trys to predict a winner with odds as an input

*Tags: Strategies*

---

**Oliver Varney** - *17:57:35*

but the implied probablity maybe lower then the accuracy and you have a model with a high accuracy  but is not profitable

*Tags: Strategies*

---

**D C** - *17:59:32*

AFAIK, Pinnacle have good modelling but they change their lines based on laying bets from sharp customers. Not exactly sure how that gets weighted but whatever they do, the betting community seem to consider them as the gold standard. People like Joseph Buchdahl have a massive hard-on for Pinnacle closing prices.

*Tags: Strategies*

---

**Oliver Varney** - *18:01:44*

so if all of your records have 2 as an input your model should be 50% accurate

*Tags: Strategies*

---

**Alessio** - *18:03:43*

It depends how you build the model, but you could have odds most_of_the_time and in certain cases other features could be overpowered. At least I'd expect decision-tree style models to be able to recognize that in certain situations you can refine the odds

*Tags: Feature Engineering, Strategies*

---

**Oliver Varney** - *18:04:05*

forget the model

*Tags: Strategies*

---

**Oliver Varney** - *18:06:26*

yep now imagine think one of your features is the price the bookmaker produced the night before. if your model goal is to predict the winner with the most accuracy, surely it will pick the bookie odd that is the lowest ?

*Tags: Feature Engineering, Strategies*

---

**Alessio** - *18:06:55*

yes, the model will definitely go on the favorite, it's the best "prior" it has got

*Tags: Strategies*

---

**Oliver Varney** - *18:09:20*

you may have a model with a 50% accuracy, but the average odds is 1.75

*Tags: Strategies*

---

**Oliver Varney** - *18:12:15*

yep, whereas if you have a model with only 20% accuracy but average odds of 6, your in the money even though your accuracy is much lower then the 50% model

*Tags: Strategies*

---

**Oliver Varney** - *18:13:31*

A profitable model is retaliative to the implied probability

*Tags: Strategies*

---

**Oliver Varney** - *18:14:09*

value in my mind is when you model finds a bet that beats the implied probability of the benchmark( bsp)

*Tags: Strategies*

---

**Alessio** - *18:15:49*

this means on average, there's  (a subset of )events where your model outputs a probability that is more precise than the baseline odds themselves, though..

*Tags: Strategies*

---

**Oliver Varney** - *18:16:39*

as the model will learn when they are wrong ..

*Tags: Strategies*

---

**Alessio** - *18:17:01*

ok i think we agree. it's a matter of how to benchmark it

*Tags: General Technical*

---

**liam** - *18:37:21*

[https://www.pinnacle.com/en/betting-articles/betting-strategy/pinnacle-sports-margins/lah22jal3xrpkq42|Pinnacle](https://www.pinnacle.com/en/betting-articles/betting-strategy/pinnacle-sports-margins/lah22jal3xrpkq42|Pinnacle) have some good resources although I don’t agree with Buchdahl most of the time 

*Tags: Strategies*

---

**Oliver Varney** - *18:47:22*

just out of interest, for those that build there own odds, how do you track how accurate they are. I assume you use some form of kelly which assume you know your edge

*Tags: Strategies*

---

**thambie1** - *20:57:39*

I use brier score, logarithmic score, and profit/loss to gauge my models.

*Tags: Strategies*

---

**thambie1** - *20:58:22*

I don't think kelly would be useful for accuracy measurements. But rather to optimize profitability after you already have a model.

*Tags: Strategies*

---

**Oliver Varney** - *21:05:12*

I guess what I mean is that doesn't Kelly stake sizes depend on knowing an accurate edge on each bet? Or is an approximation good enough?

*Tags: Strategies*

---

**MacrcoPolo** - *21:25:49*

yes it does - this is one of the reasons people use fractional kelly, you should under stake rather than over stake in the presence of edge uncertainty

*Tags: Strategies*

---

**Oliver Varney** - *21:49:12*

so if you said, with this typical prediction (and its associated strength however you define that), on average my edge is X. so in essence your not fully sure on the edge but you know across an average on backtesting the edge is X, or is this still too flimsy to use fractional kelly ?

*Tags: Strategies*

---

**thambie1** - *22:38:34*

In my case, I just backtest using kelly. If profit goes up, and varience goes down, I must have got it reasonably right.

*Tags: Strategies*

---

## 2020-12-07

**MacrcoPolo** - *01:18:58*

Short answer is this: with kelly there's 2 highly undesirable things happening, despite it being the optimal way to grow _log_ wealth. Firstly, the setting for kelly is that you're not constrained by the amount you can wager, and that you know the edge on the bet. If you have uncertainty in your edge, then wagering slightly smaller is optimal I believe (I don't have the proof at hand - can try and throw something together tomorrow if you want). Second, even if you are wagering at kelly where the relevant odds and proabilities are known, you still put yourself in a spot where your bankroll halfs before doubling 1/3rd (again, this is from memory, might be a little higher, a little less) of the time. That's terrifyingly large swings for most mortals, and not good for the mind. Cut to half kelly, and you dramatically reduce the chances this happens (I have no hard numbers and it's 1 am!).

*Tags: Performance, Strategies*

---

**MacrcoPolo** - *01:20:01*

reading blackjack theory is one place where kelly is discussed a huge amount with all the caveats and I recommend people see if they can find some old posts on old blackjack forums about this. Blackjack pros typically advocate 1/4th to 1/3rd kelly from what I remember for the reasons above (and others - such as the fact that if you're drawing life expenses from the bankroll, your 'effective' bankroll is much smaller)

*Tags: Strategies*

---

**MacrcoPolo** - *01:21:56*

Also I think on betfair, you'll probably be constrained by scale much more than you need to worry about optimal kelly staking!

*Tags: Strategies*

---

**liam** - *08:09:06*

From what I have found the majority use fixed Kelly (I do) at a certain percentage 

*Tags: Errors Debugging, Strategies*

---

**Oliver Varney** - *08:21:57*

cool cheers, I havent used it thus far due to the style of prediction my model comes up with, which means I wouldn't for each individual bet know exactly my edge but I would have an expectation across a number of bets with a certain prediction strength. Im wondering what is the effect of using kelly if your edge is not fully accurate (but your bets +EV), id be interested to read up on that if you can find something [@U0135SLHQBW](@U0135SLHQBW)

*Tags: Strategies*

---

**jp** - *08:23:41*

Remember that Kelly is optimal only if you consider a fixed bank size. I you are in a position where you can adjust your bank size to the profitability of your strategy, Kelly may not be the right tool to decide bet size.

*Tags: Errors Debugging, Strategies*

---

**liam** - *08:27:07*

Yeah for me it’s about even out the pnl across the odds range rather than optimising profit as market capacity makes Kelly pointless 

*Tags: Strategies*

---

**Oliver Varney** - *12:42:16*

for me at least, the more experienced may disagree but, BSP is on average a fairly okay representation of the implied probability of a horse winning the race (has cases where massively off and value can be found, overbacking / underbacking etc). So if you can come up with a model that gets you into positions that consistently achieves a better price (back side ) then the BSP your onto a profitable strategy in the long run. the reverse is true also for laying. The inplay guys might be able to shed more light on how they benchmark their inplay strategies

*Tags: Strategies*

---

**Oliver Varney** - *13:15:17*

talking pre off [@U0160E9HS2G](@U0160E9HS2G) , if I was doing an inplay model I would likely capture a price very close to off and use it as a feature

*Tags: Feature Engineering, Strategies*

---

**Oliver Varney** - *14:07:14*

agreed, was more thinking in terms of model building. For new strategies you will get feedback alot faster from BSP then the realising of the horse winning if your backing for example.

*Tags: Strategies*

---

## 2020-12-08

**liam** - *08:33:55*

Just rereading this, [@ULDAVFDRP](@ULDAVFDRP) does this mean you use fixed stakes across the odds range? If so do you not hit market capacity issues? / large draw downs?

*Tags: Errors Debugging*

---

**Oliver Varney** - *08:42:25*

in the old days I used a single fixed stake across all ranges but that was when I could match against the exchanges money, meaning huge wins on long odds

*Tags: Errors Debugging*

---

**user34** - *12:05:56*

For anyone interested, this paper ([https://www.researchgate.net/publication/262425087_Optimal_Betting_Under_Parameter_Uncertainty_Improving_the_Kelly_Criterion](https://www.researchgate.net/publication/262425087_Optimal_Betting_Under_Parameter_Uncertainty_Improving_the_Kelly_Criterion)) looks at how to reduce optimal stakes to account for uncertainty in the edge.

*Tags: Strategies*

---

## 2020-12-09

**mandelbot** - *13:32:46*

What's the best way to hang on to some historical info within a strategy? Create a global variable and modify within `process_market_book`?

*Tags: Strategies*

---

**liam** - *13:33:44*

`strategy.context` or `market.context` if you want it for all strategies for a particular market

*Tags: Strategies*

---

## 2020-12-11

**Newbie99** - *13:19:12*

The recent chat on here about BPE has been quite interesting, since I've started looking at in-play I've definitely noticed some ridiculous fills where the odds have clearly drifted, so I can completely see the logic as to why people get it switched off, however I did have two questions, that hopefully someone could clarify?



Lets say the market is 40/50 you back @ 48, presumably if someone tries to lay @ 60 they match you @ 48, but what would happen if you both had BPE turned off?



Second question, lets say as you place your back order @ 48 during the 1 second delay the price moves, so the market is now 50/60, your order doesn't get executed, but what happens to it, does it behave like a standard cancelled order, or does it just sit there in some sort of Betfair limbo unless you explicitly cancel it?

*Tags: General Technical*

---

**liam** - *13:41:31*

It is cancelled and you get an error, check out the docs.

*Tags: Errors Debugging*

---

**Jonjonjon** - *23:20:18*

Suppose I have a trading strategy, where the profit-per-race is inversely correlated to the race-length. The profit-per-race is not normally distributed. Tending to be either -£1x or +£10x. What's the best optimisation algorithmn for choosing the maximum race length at which I should run the strategy?

*Tags: Strategies*

---

**Misha** - *23:22:50*

I won't answer that question directly, but you absolutely have to take care to not overfit here

*Tags: General Technical*

---

**Misha** - *23:44:08*

[@UPMUFSGCR](@UPMUFSGCR) - but if you choose your optimisation based on profitability, you are, by the very definition, over fitting. It's a hard question with no easy answers, and something that I ask myself every day with my model

*Tags: Strategies*

---

## 2020-12-12

**Misha** - *01:30:18*

It's why with standard modelling you have a training set and an evaluation set that don't overlap. Use the training set to create the model, but only evaluate profitability on data that doesn't go into any model choices

*Tags: Strategies*

---

**Oliver Varney** - *06:12:43*

yes and no, its likely that if your learning task is solely focused around profitability, it will learn that the longer the odds the higher the edge is (typically for me at least) and may give you a funny model that bets on 1000.0 horses, because when they eventually come in the have the largest edge (not factoring in volume available and market impact).

*Tags: Strategies*

---

**Michael** - *09:29:04*

I don't think that's right. You can make anything the target of your optimisation. Overfitting describes the quality of your model.

*Tags: Strategies*

---

**Stefan** - *09:39:07*

Therefor I do not build my models on odds. This is actually quite funny because I posted here something about ML, in screenshots or videos, I think I posted this article before as well: [http://bfexplorer.net/Articles/Content/502](http://bfexplorer.net/Articles/Content/502)

*Tags: Strategies*

---

**Michael** - *09:40:10*

I do exactly what [@U016TGY3676](@U016TGY3676) describes as part of my analysis. You can't fully separate profit from underlying edge so by analysing profit you are analysing probability discrepancies indirectly but with market factors on top. The answer to all of these questions is that you have to apply human intelligence and understanding to the problem and not just expect a black box model to make it all go away. Typically my approach would be to focus on areas of high or low profitability first, then drill down into those to see whether they are caused by the bets inherently having or not having the right amount of value or something to do with the matching. I analyse a lot of things separately, then I look at them together and how they interact. When I get to a general model I assume it's overfitted, dial it back a bit for real world use and randomise it slightly to generate data for the future.

*Tags: Strategies*

---

**Michael** - *09:45:01*

.....actually I can think of some exceptions but broadly I think that's right.

*Tags: Errors Debugging*

---

**Michael** - *09:49:36*

As I understand it over fitting is simply when your model too closely matches the noise in your data. All model types are prone to it and in s sense all models are over fitted it's just a matter of degree.

*Tags: Strategies*

---

**Michael** - *09:52:43*

No wait that's wrong. Models can be under fitted.

*Tags: Strategies*

---

**Misha** - *09:54:20*

Not really - over fitting is the process of modelling temporary trends and mistaking them for long-term trends

*Tags: Strategies*

---

**Misha** - *09:59:27*

In my view overfitting is the number one reason why models fail over time

*Tags: Strategies*

---

**Artur Gräfenstein** - *10:00:26*

I think you are both right. But that hits it pretty well: “Over fitting is simply when your model too closely matches the noise in your data!” Pretty easy to understand for a beginner!

*Tags: Getting Started, Strategies*

---

**Misha** - *10:05:20*

I only work with sporting results models, but we don't use any odds in our model at all. We test against odds where we have them, but prices are kept separate from modelling

*Tags: Strategies*

---

**Misha** - *10:05:59*

But if you are modelling market movements, then that is a different type of model

*Tags: Strategies*

---

**Misha** - *10:07:59*

So my views are only relevant to statistics models

*Tags: Strategies*

---

**D C** - *10:38:09*

It is popular for everyone to talk about this in the context of ML stuff now, but it is most easy to explain in the context of polynomial approximation versus interpolation of a set of (x,y) pairs of data. It is also context dependent - there are times where interpolation is what you want even though this is overfitting (every residual is zero by definition). In the sense of approximation though it is usually crap. Fitting requires more thought than that. For example, you can sling a least-squares linear regression at data but model assumptions are that the error is normally distributed - you can still fit it but it breaks model assumptions. Just because you can doesn't mean you should. You have a couple of outliers in your data set and that totally screws a least-squares approximation in the sense of how useful it is. In such cases fitting a l1 approximation gives you a much better way to go. It is basically a trade off between too many parameters in your model (huge flexibility) amd too few (comparatively rigid form). Both ends of the spectrum give a shite representation of the underlying functional relationship between response and explanatory variables. But how you get at that functional relationship should be governed by the error distribution of your measurements.

*Tags: Errors Debugging, Strategies*

---

## 2020-12-14

**PeterLe** - *21:36:38*

Evening, I was recently fortunate enough to have the TPD GPS Switched on for me by my account manager.

A number of fellow users have proclaimed how useful this is, but hand on heart, I haven’t been able to capitalise on it. (Either as a standalone strategy or to enhance existing strategies)

So for the time being, I have switched back to my long term non TPD profitable stuff.

I don't how many licences Betfair have allocated in total but I expect that a number of those granted access will be on here, so a good place to ask.

So how have you found it? Appreciate any thoughts, reflections please.

Either-way I will be looking at this again over the holidays, just looking for a gauge rather than the secret sauce thanks :grinning:

*Tags: Strategies*

---

**Jonjonjon** - *21:50:30*

I can't even get an in-running strategy to work without TPD. So adding TPD would just complicate matters anyway

*Tags: Strategies*

---

**Misha** - *22:39:41*

My knowledge of horse racing, while fairly good, is not good enough (I reckon) to start from scratch in terms of modelling

*Tags: Strategies*

---

**Misha** - *22:41:27*

My view, and this is my view only, is that I find it hard to model if I am not a real expert in the field. I know that others can overcome this, but I would rather work on "solid ground" :wink:

*Tags: Strategies*

---

**Misha** - *23:05:11*

Big swings, lot of stats, standard playing rules now (with a few minor exceptions)

*Tags: Errors Debugging*

---

**Michael** - *23:20:36*

It's not a coincidence that most of the successful TPD players were established IR horses players before it came along. To me using TPD does feel like printing money, but then I was already doing well on those markets before it existed. Having been betting blind on HR markets for years suddenly having a notion of what's actually happening in the race was miraculous. I wasn't troubled by lack of data because I could use my existing market analysis techniques to make sure I wasn't doing anything ludicrous and I already had a good idea of how to pitch the betting in terms of staking, pricing and so on. Absent all of that I can see it being much harder and I've heard similar experiences to [@UQL0QDEKA](@UQL0QDEKA) from a few people.

*Tags: Strategies*

---

## 2020-12-15

**PeterLe** - *11:44:41*

Thanks [@UGV299K6H](@UGV299K6H) I think the ink must have run out of my printer, although the kids still think it is free flowing! :grinning: (The original question went off at a tangent a little, surprised there weren't more comments overall. Would be interested to know how many user licences were issued for TPD )

*Tags: General Technical*

---

**liam** - *12:29:13*

Like Mike I have been using it since March 2018, I think execution is more important than getting a model working, surprised you haven’t found some success Peter 

*Tags: Strategies*

---

**birchy** - *14:31:01*

When backtesting, at what point do you decide to pull the plug on a losing strategy? I'm never really sure whether to just let it run for 1000 markets/5000 bets/1000 matched bets/etc. Obviously the more data, the better but I guess the real question is: at what point do you decide that the strategy isn't going to work as expected?

This is for straight `check triggers -&gt; place simulated bets -&gt; calc P&amp;L's` backtests.

*Tags: Strategies*

---

**thambie1** - *14:38:28*

Completely depends on your strategy and the associated variance. High variance strategies need longer backtesting periods than lower variance strategies. In my case, I backtested over my full dataset, to get an idea of how long potential downswings may be. And then used that to determine the length of backtest I use for testing

*Tags: Strategies*

---

## 2020-12-16

**Kai** - *10:30:25*

Statistical tests or Monte Carlo simulations for complicated setups help, but in the end it comes down to judgement.

You can also plot your pl and see how smooth it looks

*Tags: Getting Started*

---

**birchy** - *10:54:28*

[@UPCNDMPSN](@UPCNDMPSN) yes, I do all of those things where the ROI is &gt; 0. Which is not very often. :grinning:

[@UQL0QDEKA](@UQL0QDEKA) Up until a ~6 months ago, when I was using my own polling library, I used nothing but real money as I didn't have a reliable backtesting method and would rather spend the time working on "real" strategies. Then I started using Flumine for it's streaming capability and realised that it was possible to lose money 100x faster than I did previously, so chickened out in favour of backtesting. :joy:

*Tags: General Technical*

---

## 2020-12-24

**Ruben** - *11:58:39*

Good morning everyone, I'm trying to test my first strategy with fixed stakes, and I would like to add an exposure control so that the maximum amount I bet into any selection is, for example, 5 EUR.



For back bets, I believe this would be equivalent to limiting the max exposure of the selection to 5, but for lay bets its a bit trickier since it the exposure depends on the odds.



Since I would like this control to happen in the same place for both back/lay bets, is the best way to go about this to just implement an instance of TradingControl that specifically handles this "max bet stakes"? This could be as simple as just allowing to bet once per selection_id, with fixed stakes.

*Tags: Errors Debugging, Strategies*

---

**birchy** - *17:07:18*

Flumine validation checks should do that for you?

[https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/strategy/strategy.py#L30|https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/strategy/strategy.py#L30](https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/strategy/strategy.py#L30|https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/strategy/strategy.py#L30)

*Tags: Strategies*

---

## 2020-12-25

**Oliver Varney** - *09:27:36*

I think its quite hard to use that param to control stuff if you have both backs and lays under the same strategy or if you have variable stake sizes / exposures in my opinion. The selection exposure function might be worth checking if you havent found it already [https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/markets/blotter.py#L110](https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/markets/blotter.py#L110)

*Tags: Strategies*

---

## 2020-12-27

**Unknown** - *13:02:07*

Working on a three line addition when using flumine to get a jupyter notebook to start on backtest completion :sunglasses:



Very much a work in progress but could form the start of an interactive strategy development tutorial or tear sheet:



```control = JupyterLoggingControl()

framework.add_logging_control(control)

...

control.launch()```

Welcome any thoughts?

*Tags: Strategies*

---

**liam** - *13:03:31*

ha this is just the LowestLayer strategy on 30ish markets

*Tags: Strategies*

---

**liam** - *13:05:30*

this is the [https://github.com/liampauling/flumine/pull/353|branch](https://github.com/liampauling/flumine/pull/353|branch) focus was mostly on a generic json format which had all the metadata but can be easily added to

*Tags: General Technical*

---

**liam** - *13:06:23*

Yeah was going to add a dataframe which contains some stats such as profit/sharpe ratio/roi/match rate per strategy etc and then a series of graphs with text on what to look for etc.

*Tags: Feature Engineering, Strategies*

---

**liam** - *13:08:49*

yeah this is doing the heavy lifting [https://github.com/liampauling/flumine/blob/ef7ace33a73cc8265201f372a0071e4c761640b2/examples/controls/jupyterloggingcontrol.py](https://github.com/liampauling/flumine/blob/ef7ace33a73cc8265201f372a0071e4c761640b2/examples/controls/jupyterloggingcontrol.py)

*Tags: General Technical*

---

**AP** - *13:10:13*

Ok that’s very cool, I’ve been outputting dataframes from the flumine backtests and working on those. But this seems a lot more in depth. 

*Tags: Feature Engineering*

---

**mandelbot** - *13:13:42*

literally just starting thinking about this, how do you output such data frames? from from the strategy or the backtest itself? do you write them to a file or something?

*Tags: Strategies*

---

**AP** - *13:15:29*

I actually leveraged this example.



[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1605132356439000|https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1605132356439000](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1605132356439000|https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1605132356439000)

*Tags: General Technical*

---

**liam** - *13:22:24*

thinking of edge cases where users have stored python objects in the context fields but tbh json is always going to be easier

*Tags: General Technical*

---

**Oliver Varney** - *13:26:14*

can see advantages for both. pickling would be nice if you could see the state of the market (last market book)/ strategy  when trades were created, but that seems alot more work I guess

*Tags: Strategies*

---

## 2020-12-28

**birchy** - *19:51:12*

Has the betfair XM algorithm been published or is it a closely guarded secret? Looking to replicate in python...

*Tags: General Technical*

---

**PeterLe** - *20:43:07*

[@U016TGY3676](@U016TGY3676) not sure if you’ve seen this; its not the code, but it gives an explanation that may help [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Additional+Information](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Additional+Information) (half way down)

*Tags: General Technical*

---

## 2020-12-29

**Misha** - *00:16:17*

Just checked my database for tennis - cross matching is not used for tennis

*Tags: General Technical*

---

**Mo** - *07:30:01*

Your database is wrong

*Tags: General Technical*

---

**Misha** - *09:52:30*

I use the same prices from the streaming API (as reflected by the web site)

*Tags: General Technical*

---

**Mo** - *09:53:21*

Well you can request either the virtual prices or the non virtual prices using streaming 

*Tags: General Technical*

---

**Misha** - *09:54:32*

So does the historical data (provided by Betfair) use the non-virtual prices, or the virtual prices?

*Tags: Data Quality*

---

**Oliver Varney** - *09:58:14*

[@UBS7QANF3](@UBS7QANF3) so if you model trained on non virtual (with volume features), do you essentially have two market streams (virtual &amp; Non virtual)? If im understanding this correctly

*Tags: Feature Engineering, Strategies*

---

**mandelbot** - *09:59:41*

Is there an option on BFLW to turn virtual streams on/off?

*Tags: General Technical*

---

**Oliver Varney** - *10:08:06*

just thinking as the view of the market will be different in terms of perceived volumes by the model vs live trading. I assume virtual prices will be show much larger volume due to crossmatching?

*Tags: Deployment, Strategies*

---

**Oliver Varney** - *10:12:04*

cheers mate, always learning new stuff, appreciate the help.

*Tags: General Technical*

---

**Oliver Varney** - *10:14:04*

I guess this also raises the question if people can calculate virtual prices faster than the 150 milliseconds ?

*Tags: General Technical*

---

**Oliver Varney** - *10:15:21*

quite an interesting AI problem maybe

*Tags: General Technical*

---

**Mo** - *10:17:40*

My view is that you would be crazy to unnecessarily introduce 150ms of latency when betting in-play

*Tags: Performance, Strategies*

---

**PeterLe** - *11:07:37*

The difference between having XM on and off is very subtle in stable markets, often it is just the amount at the price that is different. This can have a slightly -ve effect (ie if you see a price at 10.5 and lay £10 at 11 you may only get £2 matched at 10.5 and £8 at 11 etc, here is a video i did 10 years or so ago which shows a single market with XM on and adjacent XM off. If you pause the video at points you can see the differences. [https://www.youtube.com/watch?v=3njy483sL-w](https://www.youtube.com/watch?v=3njy483sL-w) it helped me fall asleep many times over the years thinking how to get the best out of it :grinning:

*Tags: General Technical*

---

**Misha** - *11:19:02*

There is a limit where volume will come into play, but that will be a very "nice" problem to have

*Tags: General Technical*

---

**Mo** - *11:56:53*

In terms of Betfair streaming you can request both but my understanding of the code is that betfairlightweight will prioritise the non-virtual price stream in terms of what it puts into the runner books

*Tags: General Technical*

---

**liam** - *12:15:50*

Correct, logic here [https://github.com/liampauling/betfair/blob/52b5732c0704294ee162f11853612cb5100352b1/betfairlightweight/streaming/cache.py#L108|https://github.com/liampauling/betfair/blob/52b5732c0704294ee162f11853612cb5100352b1/betfairlightweight/streaming/cache.py#L108](https://github.com/liampauling/betfair/blob/52b5732c0704294ee162f11853612cb5100352b1/betfairlightweight/streaming/cache.py#L108|https://github.com/liampauling/betfair/blob/52b5732c0704294ee162f11853612cb5100352b1/betfairlightweight/streaming/cache.py#L108)

*Tags: General Technical*

---

**MacrcoPolo** - *13:40:49*

I'd suggest just literally getting a pen and paper out, writing down the formulas you need and then taking a snapshot of a market from betfair and playing in excel to get it working correctly before moving it to python

*Tags: General Technical*

---

**liam** - *17:48:38*

Would be a cool addition to flumine.. :sunglasses:

*Tags: General Technical*

---

**jp** - *20:51:50*

If EX_ALL_OFFERS_DISP is enabled when streaming, will also non-virtual prices be delayed? I thought they were not, but not sure about this...

*Tags: General Technical*

---

## 2020-12-30

**Ruben** - *17:22:36*

With Flumine, when backtesting several markets, are limits such as the transaction_count reset to 0 after every market?

*Tags: General Technical*

---

**liam** - *17:25:32*

It’s based on the backtest time, flumine patches datetime so it will work as though it was ‘live’ however you could just remove 

*Tags: Deployment*

---

## 2020-12-31

**Oliver Varney** - *07:03:58*

populate the CSV's into a database and sum + rank and it should give you an idea of most traded I believe. Obviously there are variables quality / class of the race will effect the amount traded. Like in the UK/IRE a grade 1 / class 1 will trade alot more then a cold winters midweek chase at Plumpton

*Tags: General Technical*

---

## 2021-01-02

**Mo** - *15:43:06*

It's not a documented feature but you can pretty easily work out how to use the website endpoint

*Tags: Feature Engineering*

---

**birchy** - *20:34:32*

I'm sure this has been asked before but I can't find it in the slack history...

Say you have a strategy that you backtest using Flumine. After ~6k matched bets (match rate ~ 15%, so ~40k bets submitted), it shows -0.1% ROI calculated from `total_pnl / total_risk`, where total risk is the sum of each bet risk rather than the net risk per market. No commission is deducted and each matched bet is deducted from the available volume.

Would you push this strategy to real money? I'm thinking no, but have not (yet) compared a Flumine backtest vs live, so have no idea how matching, P&amp;L, etc compares.

*Tags: Deployment, Strategies*

---

**Mo** - *20:38:35*

But as a profitable strategy by itself? No

*Tags: Strategies*

---

**birchy** - *21:21:14*

Thanks guys, you've confirmed my own thoughts. In particular, I'd prefer to see &gt;2% in order to cover commission. PC is not an issue yet, and unlikely to be due to the churning nature of some other bots I have running (they're +EV, but only just) and have been ticking along for many years, so my lifetime commission is colossal. Currently enumerating through trigger combinations... should have an answer by 2022. :joy: 

Am curious what [@U4H19D1D2](@U4H19D1D2) thinks, as he has previously mentioned that Flumine generally underplays the match rates?

*Tags: General Technical*

---

## 2021-01-03

**Michael** - *00:42:27*

I agree with [@U0128E7BEHW](@U0128E7BEHW)’s advice but I'd add that 15% matched seems very low to me. People's operations differ but personally I look to get a much higher proportion on, like maybe 70% for a purely market based strategy.  Might you be asking for too much value? Have you tried asking for less? Also at just 15% matched I'd bet that the orders that do match do so quite a long time after being posted (unless you're cancelling most of your transactions quite quickly). If your bets match too slowly then the signals you're using are rendered ineffective because they've likely expired by the time your money is taken. Any profit that's left probably either comes from the spread or from skimming a bit of noise - either way it won't be much and won't scale well.

*Tags: Performance, Strategies*

---

**birchy** - *01:53:46*

[@UGV299K6H](@UGV299K6H) agree on the 15% match rate being low, I generally prefer 50%+ and that's something I'm working towards. Bets are cancelled after 100ms as this is a geegees inplay strategy... perhaps not long enough?

*Tags: Strategies*

---

**liam** - *05:20:25*

Yeah bump to around 3s and then plot profit against seconds to match, in flumine this is `order.elapsed_seconds_executable` however this will be dependant on if you are taking prices / how far from the centre of the book etc.



It will underplay matching rates (mainly due to xm) but you still need something profitable

*Tags: General Technical*

---

**liam** - *09:54:35*

info properties on the order and trade are the best, it's what is used for the jupyter poc [https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/order/order.py#L251](https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/order/order.py#L251)

*Tags: General Technical*

---

**JC** - *15:46:01*

I'm testing an in-play strategy and in backtesting I am often getting matched at much better prices than I post due to BPE and the 1 second delay (I imagine). Has anyone found flumine's matching to be more generous than the odds matched when using real money? These favourable prices are contributing massively to the profitability of the strategy in backtesting

*Tags: Strategies*

---

**liam** - *15:47:59*

market delay is simulated in flumine, BPE is on by default when backtesting, do you have it on your account? What market types? Stake size?

*Tags: General Technical*

---

**JC** - *16:18:00*

Thanks, just found it on mobile and it is switched on. Couldn't find a page that looks like that on desktop. R.e. example - I will run the strategy against some of todays markets and compare with real matching and let you know. In backtesting there are some crazy differences like backs getting average price matched at 7.6 when posting at 1.01. The strategy offers backs at best available lay price and shouldn't cross the spread, so it seems weird

*Tags: Strategies*

---

**JC** - *16:25:59*

Yes BPE is on and in backtesting these matches are actually favourable and seem to have positive EV. So I want BPE to be on. I'm just hoping that the real matching is the same as in simulation. If so, the strategy must be placing bets at times of extreme volatility in order to get these kind of results.

*Tags: Strategies*

---

**Ruben** - *17:06:03*

Good evening everyone, about Flumine: to analyze when the orders one posts are getting matched, is there a better way than looking at order.date_time_execution_complete?

*Tags: General Technical*

---

**liam** - *17:22:43*

have a look at `order.simulated.matched` for a list of indiv matches [https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/backtest/simulated.py#L28](https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/backtest/simulated.py#L28)

*Tags: General Technical*

---

**Mo** - *17:51:35*

I'm not sure how flumine handles disconnections when scraping but I've seen something similar on files I've scraped myself when there's a gap in the stream due to a disconnection

*Tags: General Technical*

---

**JC** - *17:51:41*

Yeah I would agree, maybe there is a problem with my market recorder

*Tags: Data Quality*

---

**mandelbot** - *17:58:11*

Funnily enough I had to restart my recorder that day due to my ec2 acting up so it seems like I also have a dodgy version

*Tags: Deployment*

---

**liam** - *18:13:58*

Yeah so you have a lot of orders all going in at the same time, we can’t really simulate that sort of behaviour accurately.



What you are probably seeing is flumine continually matching against the same liquidity causing more orders to get fired in and then get ‘matched’ when in reality this won’t happen especially Inplay 

*Tags: General Technical*

---

## 2021-01-05

**mandelbot** - *07:45:34*

AUS racing start times often change, does this get updated in flumine when it's updated on betfair?

*Tags: General Technical*

---

**liam** - *07:58:06*

Flumine will update based on the catalogue 

*Tags: General Technical*

---

## 2021-01-09

**birchy** - *15:12:24*

Currently calculate in my own utils add-on in strategy, so maybe not efficient?

*Tags: Strategies*

---

**liam** - *15:15:47*

If used middleware then all your strategies can use it, simulated already has what you want [https://liampauling.github.io/flumine/markets/#middleware|https://liampauling.github.io/flumine/markets/#middleware](https://liampauling.github.io/flumine/markets/#middleware|https://liampauling.github.io/flumine/markets/#middleware)

*Tags: General Technical*

---

**liam** - *15:19:26*

[https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/baseflumine.py#L48|Here](https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/baseflumine.py#L48|Here) you can see how it’s added, if you then debug on a market update and have a look at `market.context` you will see the simulated data which under runners? has what you are after and more 

*Tags: Errors Debugging*

---

## 2021-01-10

**Dave** - *16:04:25*

Modelling question: I have a model, A, that by standard statistical tests performs better than model B (higher ROC, better precision/recall etc). However, on backtest, model B results in higher Sharpe and more total pnl. How would one choose which model to use? My theory is that model A is just better at forecasting situations that are already easy for the market to forecast too, and therefore unlikely to find opportunities where the market gets this wrong (hence an opportunity to take a position in the market). Note: Model B is just fit on a subset of features.

*Tags: Feature Engineering, Strategies*

---

**D C** - *16:19:07*

You guys who do proper modelling based on variables / fundamentals as opposed to pure market / price based backfitting , how do you go about your sport(s) when subject to a new rule change or similar? Or something like covid era where home advantage effect is very likely altered by absence of crowds. Mainly I am thinking about introduction of VAR in football which was a big one, but in other sports there are subtle rule changes introduced at times. Do you just run prior models on the "new" version of the sport and carefully monitor for differences in your expected profits or do you shut up shop until you can rebuild a model from "new" data as it becomes available?

*Tags: Strategies*

---

**Alessio** - *16:28:39*

not really, tbh. depends on the structure of the model, but if it is highly nonlinear it may correlate, and that's enough to screw up things.

*Tags: Strategies*

---

**Michael** - *17:37:56*

I'm far from being a stats expert so maybe this is bollocks - but doesn't it depend on your sample size? Surely the various statistical tests essentially just substitute for a larger sample, so if you have a big sample you just go for the biggest profit and if your sample is limited you lean towards the models that perform best on the statistical tests... or is that wrong?

*Tags: Strategies*

---

**Michael** - *17:39:13*

^ above refers to [@U0128E7BEHW](@U0128E7BEHW)'s original question.

*Tags: General Technical*

---

**Dave** - *18:07:33*

[@US2K70JR0](@US2K70JR0) I guess that raises the q of what is "big sample". In my case I thought my holdout set on which these metrics are computed is big enough, and so I was just surprised a model that shows up as having better predictive ability was yielding worse results backtest-wise (I.e. when you bring the forecasts to the market). Probably just the case that the dataset is not large enough and just seeing some continued variance. Might as well run both models on 50% of events each and see how it goes.

*Tags: Strategies*

---

**Dave** - *18:32:05*

About 3K football matches. On this size I would not expect one model to provide 2x the pnl than the other, but I think I am just underestimating the variance I can expect

*Tags: Strategies*

---

**Mo** - *18:35:13*

I don’t know if you’re targeting specific leagues or trying to model every football match but your _effective sample size_ could be (a lot) smaller if the profits are driven by a small number of large bets you can get down on the higher liquidity events 

*Tags: Strategies*

---

**Dave** - *18:40:01*

Generally fixed stake, and due to the events I'm forecasting the odds are quite short (usually prices &lt;2). And yeah, my backtest definitely factors in spread and volumes so low liquidity events generally get skipped over

*Tags: Errors Debugging*

---

**Michael** - *18:48:39*

Would opting for the model with fewer features allow you to analyse the remaining features against the bets? If I were struggling to make up my mind I'd generally plump for the simpler model. 3000 markets with bets in them backing at &lt; 2 seems like quite a lot to me.

*Tags: Feature Engineering, Strategies*

---

**Dave** - *18:56:36*

Yeah the fact that the model is simpler is what makes it quite attractive over the other. I should stop being lazy and do some tests on the features themselves like you say!

*Tags: Feature Engineering, Strategies*

---

**Michael** - *18:57:18*

Do both models generate similar numbers of bets?

*Tags: Strategies*

---

**Dave** - *19:00:56*

That's a good question, I was just about to comment about it! The more complex model requires a larger history to exist for each team, and naturally it cuts the tradeable number of events down by about 20%. It's possible that the uncertainty in this 20% is larger, therefore more market innefficiency and higher margins opportunities that the simpler model is able to take advantage of. I'll check the margin on these 20% and see if it massively outweighs the rest

*Tags: Strategies*

---

**Michael** - *19:01:49*

...or just test model B against the markets that qualify for model A?

*Tags: Strategies*

---

**Michael** - *19:06:37*

also nice to have the option of using model A where sufficient data is available and model B where it's not.

*Tags: Strategies*

---

**liam** - *19:18:27*

I have this predicament quite a lot but if the market has capacity just hit it with both and just optimise as time goes on, obviously this isn’t that simple if they are working off the same signals and likely to start impacting each other from a feedback / matching point of view. But I assume with football / low odds you won’t have this problem anyway?

*Tags: General Technical*

---

**Michael** - *19:34:46*

Personally I'm not keen on [@U4H19D1D2](@U4H19D1D2)'s approach to this problem for a few reasons, one of which is that you can easily end up with a proliferation of poorly optimised models that might be interacting in all kinds of ways that you'll never know about. I prefer to try to understand the features I'm using as deeply as possible and integrate them into a single unified model wherever possible.

*Tags: Feature Engineering, Strategies*

---

**Dave** - *19:58:35*

In my case I'd probably decommission the model that performs worse over some period of time and double the stakes of the better one, rather than run both perpetually with half the potential stake.

*Tags: Strategies*

---

**Michael** - *20:19:47*

[@U4H19D1D2](@U4H19D1D2) In this case maybe but you're still just kicking the can down the road. What are you going to do when you find another feature? Start a model C and run that too? Just in the general case I don't like to approach. You can make it better by having some structure like a perpetual AvB with a 'winner stays on' structure or something like that but you can see the potential for muddying the waters if you don't. I suppose it is a question of style and how you manage your development though.

*Tags: Feature Engineering, Strategies*

---

**Dave** - *20:26:44*

Additional models also come with maintenance costs, rather have fewer models that have been chosen empirically

*Tags: Strategies*

---

**Michael** - *20:31:19*

[@UQL0QDEKA](@UQL0QDEKA) Like I said to an extent it's a question of style, but if you're running a lot of variations on a theme it would presumably be quite easy to identify the commonalities in them and bring them all into a single strategy with triggers that cover all reasonable variations. Wouldn't that be better? If there is sufficient capacity for overlap why not just explicitly place more or bigger bets at those times?

*Tags: Strategies*

---

**PeterLe** - *20:37:21*

What Ive found is that if you run a single profitable strategy and the stakes scale, you eventually hit a point where you cant increase stakes - diminishing returns, (im talking in play horses now, not football where the volume is much greater). Id rather have 10 systems using smaller stakes than a single one using large stakes. Its just my preference [@UGV299K6H](@UGV299K6H).

*Tags: Strategies*

---

**Michael** - *20:41:00*

I think a lot of this comes down to how you do your analysis. Some people are into analysing the performance of a strategy as a whole and will test strategy A against strategy B and so on. To me this doesn't make much sense as your strategy is only visible or meaningful to you, the exchange only sees your bets and treats them individually. Personally I like to take my bets independently of whatever triggered them and analyse them that way round.

*Tags: Performance, Strategies*

---

**Michael** - *20:42:51*

That way you get to see and understand the variables that determine the value of each bet and you can feed that knowledge back into your strategy.

*Tags: Strategies*

---

## 2021-01-11

**Joseph kravets** - *02:44:33*

So I run trend following strategies on futures, crypto , equities. I don’t know anything about sports. What’s a good resource  to learn about the betting markets? Is there a sample strategy anywhere ? What are some good indicators to look at here?

*Tags: Strategies*

---

**Mo** - *08:39:46*

It's hard to engage with such broad questions but I will say I think you should be cautious about taking ideas that work in financial markets and blindly applying them to betting markets. It's important to appreciate that placing a bet is different to buying (or selling) a financial instrument. Your risk and potential payoff is known in advance and therefore capped. There is some limited drifting of odds but generally temporal trends are dominated by the simple fact that as you get closer to the end of the event, the implied probabilities approach 0 or 1.



Speaking about Betfair specifically, you don't get to see individual orders in the order book, only the aggregate volumes available at each price. Similar for matched bets, you do not see individual trades on each update but aggregate data on all trades that happened in that matching cycle.

*Tags: Strategies*

---

**Byron** - *14:15:22*

[@UPMUFSGCR](@UPMUFSGCR) I would advise strongly that if you are building a model for betting in Australia that you create a variable "days" which is the day of the year (so January 1st, is 1, the 2nd is 2 and so on). You will find that there are two sets of 60-90 day periods that run from about mid Feb-April and then Aug-mid Nov where the model will perform better than at other times (at least mine does).

*Tags: Strategies*

---

**Jonjonjon** - *14:16:39*

Interesting. Thanks Byron. Why do you think that is? My model might be suffering from the same problem

*Tags: Strategies*

---

**Jonjonjon** - *14:46:40*

Do bet volumes tend to vary a lot by season? I don't really have any stats gathered to measure performance properly.:disappointed:

*Tags: Performance*

---

## 2021-01-12

**Oliver Varney** - *11:54:38*

when backtesting is it best to backtest one strategy at a time or do people add a few that are variations of each other, then evaluate which one is best. Do you run into issues if you add multiple strategies?

*Tags: Strategies*

---

**liam** - *12:37:31*

Latter, 90% of the time is processing the market so you should take advantage of this and put in as many strategies as you can. If you have issues then you will probably have issues live and you need to fix 

*Tags: Errors Debugging, Deployment*

---

## 2021-01-13

**birchy** - *19:21:40*

I'm curious...for those of you that have strategies using only betfair API market data and no external data feeds, excluding boilerplate API code, approximately how many SLOC are your strategies? Mine are typically ~20-50 per strategy because I like to keep things as simple as possible, however I don't really have any 'big' earners and am wondering if my obsession with minimal code is holding me back?

*Tags: Strategies*

---

**Dave** - *19:28:50*

Given different coding styles I can't imagine SLOC has much correlation between how much a strategy earns beyond a certain point! Also what counts as a strategy, the actual decision making and execution logic or also the core frameworks code that sorts out plumbing between your components etc?

*Tags: Strategies*

---

**thambie1** - *19:30:51*

Agree with Dave, really hard to draw a clear line on what is strategy code. On top of what he mentioned, there can be model code, model training code, etc.

*Tags: Strategies*

---

**birchy** - *20:44:03*

Well just those 2 responses alone tells me that your setups are far more complicated than mine are. I'm literally taking data as it arrives from betfair, doing a few simple calculations and submitting some bets. I don't have any external frameworks or components beyond a single utilities file that contains a handful of boilerplate functions. As for "modelling", when I have an idea, I create some triggers/parameters in a Flumine strategy, press the go button and wait...modify triggers...rinse and repeat. I don't have 100's of hours available to be able to learn data science and then write a million lines of code to create an AI that tells me I need to back winners and lay losers. Yes, I'm a Luddite.

Anyone got a link to some sort of "modelling for twats" articles/books?

*Tags: Getting Started, Strategies*

---

**John A** - *20:54:02*

Question for Academics “If you’re so smart, why aren’t you rich?” 

*Tags: General Technical*

---

**D C** - *21:05:07*

Most academics are in the game for their passion for the subject matter - not the application to making money. Exceptions of course exist but you won't get far in academia without at least a lot of passion for your subject matter. Brains help too but hard work and deep interest drives it for most academics I have worked with.

*Tags: Errors Debugging*

---

**birchy** - *21:43:12*

I was referring to everything that's not betfair API, i.e. modelling, decision making, execution, analysis, etc. Basically everything that is/was required to get to a point where satisfactory bets are placed. The question was asked as a sanity check to discover if I've totally underestimated how 'simple' a strategy could be.

*Tags: Strategies*

---

## 2021-01-14

**Mo** - *07:15:01*

I'm sure if you asked for help understanding a specific paper you'd get some feedback (even if it's just from me :joy:)

*Tags: General Technical*

---

**birchy** - *08:28:36*

Yeah, I agree that there's no direct correlation between SLOC and £££. Hell, I've coded loads of 10 liners over the years that could empty my account funds in a matter of minutes. :joy: The question was more of a "can it be done".

*Tags: General Technical*

---

**Unknown** - *08:34:20*

HR: [https://betfairlightweight.slack.com/files/UBS7QANF3/FV582N4CE/lessmann2010.pdf](https://betfairlightweight.slack.com/files/UBS7QANF3/FV582N4CE/lessmann2010.pdf)



Tennis: [http://www.strategicgames.com.au/PhD.pdf](http://www.strategicgames.com.au/PhD.pdf)

*Tags: General Technical*

---

**Christian Tox** - *08:47:24*

Maybe check out fastai. I am a big fan.. some emphasis on how to read academic papers too 



[https://www.fast.ai|https://www.fast.ai](https://www.fast.ai|https://www.fast.ai)

*Tags: General Technical*

---

**Alessio** - *09:09:35*

That's the model that you were referring to sometimes ago that it doesn't work anymore?

*Tags: Strategies*

---

**Mo** - *09:11:15*

Big fan of Nate Silver generally speaking. I don't know too much about what he does in tennis specifically. Elo rankings are a great place to start with any pre-event model.

*Tags: Strategies*

---

**Mo** - *09:18:18*

Stephanie has done a lot of interesting work in tennis, I rate her research very highly. I had a profitable pre-event tennis strategy based off Elo but it required a human expert's input. The Elo model was like a filter to highlight matches for the expert to focus on

*Tags: Strategies*

---

**Mo** - *09:46:28*

No doubt that was part of the expert's mental model :wink:

*Tags: Strategies*

---

**Alessio** - *13:18:52*

These are in general linear models where you use a linear combination of variables to generate a probability. So, yes, it normalizes things. It does it in a specific way though, Assume you only have 2 horses just for plotting reasons, it will look like this: [https://en.wikipedia.org/wiki/Sigmoid_function#/media/File:Logistic-curve.svg](https://en.wikipedia.org/wiki/Sigmoid_function#/media/File:Logistic-curve.svg)

*Tags: Strategies*

---

**Alessio** - *13:23:02*

The key point of these things is that they are linear models, i.e. you give one weight for each of the values you put inside, and you don't spend time thinking about things that are intercorrelated. It's very simple, but that's a very good place to start when thinking about probabilities that are function of multiple variables.

*Tags: Strategies*

---

**Crofty** - *16:17:01*

Re the image that Birchy posted above and Mo kindly explained, how can a non-mathematician learn how to interpret that i.e. what courses / articles / videos would explain how to do so for a complete beginner?

*Tags: Getting Started*

---

**Crofty** - *17:23:24*

9 pints of scrimmy!



When I was at school, if you weren't good at a subject, you generally didn't get the attention you needed. Now I'm in a situation where maths would be really helpful, it would be great to be able to have resources that simplify it right down.

*Tags: General Technical*

---

## 2021-01-15

**birchy** - *02:01:27*

Good to hear some confirmation of what I'd concluded myself after trying a few indicators over the years. My own 'indicators' are basically combinations of `IF n &gt; x`  type statements.

Going back to the stats conversation from earlier, I'd like to start off investigating linear regression. What libraries are recommended? NumPy?

*Tags: General Technical*

---

**Mo** - *07:22:35*

For linear regression I like statsmodels 

*Tags: Strategies*

---

**river_shah** - *09:10:50*

[https://scikit-learn.org/stable/modules/linear_model.html](https://scikit-learn.org/stable/modules/linear_model.html) good primers on all the major techniques including the various penalised regressions

*Tags: Strategies*

---

**Alessio** - *10:21:48*

One thing i recently discovered is the 'interpret' package by MS folks. It contains both things like logistic and linear regression + xgboost stile stuff, but the cool stuff is that they are all explainable models, i.e. you can graph stuff very easily. [https://github.com/interpretml/interpret](https://github.com/interpretml/interpret)

*Tags: Strategies*

---

**MacrcoPolo** - *13:12:16*

Assuming you know what a linear regression is and how they work then I'd probably use statsmodels. Otherwise I'd probably use excel - easier to build intuition and play about with toy problems

*Tags: Strategies*

---

**Unknown** - *14:02:47*

Seminal paper on modelling football: [https://betfairlightweight.slack.com/files/UBS7QANF3/F010NTJJK88/dixoncoles.pdf](https://betfairlightweight.slack.com/files/UBS7QANF3/F010NTJJK88/dixoncoles.pdf)

*Tags: Strategies*

---

**Unknown** - *14:12:52*

Not read that one; I was going to suggest this for in-running football: [https://betfairlightweight.slack.com/files/UBS7QANF3/F010A4MK9MY/dixon1998.pdf](https://betfairlightweight.slack.com/files/UBS7QANF3/F010A4MK9MY/dixon1998.pdf) although treating time as discrete and using a Markov chain simplifies the math significantly

*Tags: General Technical*

---

## 2021-01-20

**Dave** - *16:21:11*

Came across this a while ago when looking for more "industrial" market participants, appears to be some kind of service where you give these guys money and their platform identifies value and takes positions. Thought their PNL path was very peculiar: [https://mercurius.io/en/trader-app/performance](https://mercurius.io/en/trader-app/performance)

*Tags: Performance*

---

**Dave** - *16:23:55*

Just curious, background suggests some sophisticated data collection and analysis, though overall pnl suggests almost just break even performance

*Tags: Performance*

---

**D C** - *16:28:06*

I've chatted with these guys on twitter. They are nice chaps and very helpful - sent me a few football modelling papers (not sure if directly used by them). That in itself means nothing but they were not pestering me to try the product. For what it is worth, I can't see how it can work as if they have loads of customers how do they prioritise whose money gets matched ahead of others as they appear to run exclusively on Betfair markets.

*Tags: Strategies*

---

**Chris** - *17:41:18*

[@UUE6E1LA1](@UUE6E1LA1) the larger stake/bankroll customers get matched at the higher prices and then scales downwards to smaller bankrolls/stake amounts. This is what I was told by someone who has trialed it on a modest bankroll that it wasn’t really worth it as the bigger edges are gobbled up by the whales 

*Tags: General Technical*

---

**Jonjonjon** - *21:32:58*

If I wanted to pull orders if my latency was above 0.5 seconds, what would be the recommended way of doing it in Flumine?

*Tags: Performance*

---

## 2021-01-21

**Jonjonjon** - *09:07:40*

No.



I am backtesting a strategy that places orders below/above the best "available to back"/"available to lay" prices. The strategy cancels the orders if a condition is met. However, if the latency is high, the strategy might not cancel them fast enough. So if the latency is too high, I'd prefer not to cancel the orders.

*Tags: Performance, Strategies*

---

**liam** - *09:17:54*

What do you mean latency?

*Tags: Performance*

---

**Jonjonjon** - *09:19:26*

The time between the streaming update being sent from Betfair, and my code in `process_market_book` being able to process it.

*Tags: General Technical*

---

**liam** - *09:20:13*

But if you are backtesting there is no latency 'time' is not updated till the next book comes through

*Tags: Performance*

---

**Jonjonjon** - *09:22:01*

You're right. It won't be there in the backtest. But  I think it will be there if I run the strategy for real.

*Tags: Strategies*

---

**liam** - *09:24:03*

example here of how we raise a logging warning [https://github.com/liampauling/flumine/blob/051d71304dfc9a8fdf6bc309a9e03cb22c6d8494/flumine/baseflumine.py#L119](https://github.com/liampauling/flumine/blob/051d71304dfc9a8fdf6bc309a9e03cb22c6d8494/flumine/baseflumine.py#L119)

*Tags: General Technical*

---

**Jonjonjon** - *09:34:16*

How did Mercurius raise EUR 800k? They've got a well presented website. Shame about the performance statistics.

*Tags: Performance*

---

**liam** - *09:46:03*

its user configurable in flumine with a `streaming_timeout` var which snaps the cache

*Tags: General Technical*

---

**Misha** - *11:03:51*

The best models (in the main) are developed by the people who best understand the sport and how the markets for that sport work

*Tags: Strategies*

---

**Misha** - *11:05:54*

It counts less for more trading based models, but I go for the stats based models

*Tags: Strategies*

---

**river_shah** - *11:07:05*

What I have seen though is that not matter how well documented and tracked your research process was, most staff invariably “found” things that never pushed the boundaries of knowledge. The guys who pushed the boundaries of knowledge were typically with very deep expertise (Phds etc or deep market knowledge in their respective domains) . I am all for hiring research scientists straight from uni but then paying them £35K and expecting to crack big alpha is a dumb business model.

*Tags: Strategies*

---

**river_shah** - *11:17:22*

That being said throwing money at the problem also has constraints. Been in plenty of interviews with candidates from deep mind etc who with a straight face expect a huge sign on bonus having zero experience in finance or any provable commercial success

*Tags: General Technical*

---

**Alessio** - *11:18:31*

(the real problem is taxes :D)

*Tags: General Technical*

---

**Jonjonjon** - *11:50:58*

Where can numerai's performance be seen?

*Tags: Performance*

---

**Misha** - *12:02:19*

Be careful with published performances. It might be "here are our last 2 years' returns", because the year before that they made a big loss

*Tags: Performance*

---

## 2021-01-25

**birchy** - *09:42:29*

[@U4H19D1D2](@U4H19D1D2) what's the thinking behind using the middle price from the previous update, rather than the current middle? [https://github.com/liampauling/flumine/blob/051d71304dfc9a8fdf6bc309a9e03cb22c6d8494/flumine/markets/middleware.py#L170](https://github.com/liampauling/flumine/blob/051d71304dfc9a8fdf6bc309a9e03cb22c6d8494/flumine/markets/middleware.py#L170)

*Tags: General Technical*

---

## 2021-01-28

**Oliver Varney** - *17:01:41*

can you not just load to python /pandas and call sample/shuffle ?

*Tags: Feature Engineering*

---

**Jonjonjon** - *17:01:55*

I just use Python's random number generator, as I don't know if there is any pattern in the final digit

*Tags: General Technical*

---

**Michael** - *17:35:54*

It's sort of un-knowable because everything's shifting around you all the time and you can have no control on any experiment. That's why I think AvB is the best approach to the problem. Actually it's the only thing I use AvB for. Broadly you're tracking how your returns diminish as your betting increases to try to find an optimum, that's easily observable relative to stake but a lot of small bets in a short period can have a similar effect to a larger stake so that's the tricky thing to understand - it can only be tested at market level (hence AvB). You might end up thinking more in terms of 'stake per second' or something like that.

*Tags: Strategies*

---

**Michael** - *17:38:49*

It's the most crazy making problem we face.

*Tags: General Technical*

---

**Lee** - *17:39:03*

okay so it's not really understanding how your bet moved the market in a certain way but more seeing how different approaches effect your return / matching / roi / etc? so you can optimise/maximise a strategy

*Tags: Strategies*

---

**Michael** - *17:43:49*

....but again that's problematic because you only run out of appetite in certain types of market.

*Tags: General Technical*

---

**liam** - *17:55:59*

`The trigger was a guy running a ridiculous bot on the API on one of his unfunded sub accounts.



It was doing a martingale over and over again, going from tiny to huge stakes in a cycle. The max stake on the exchange was some very large number, I don’t remember what, but bigger than 2e31.



Two things happened at the exact same moment: this martingale bot tried to place some huge bet way over the numeric max (and crucially, over that subaccount’s exposure limit) this caused a numeric error inside the trading engine exposure checking code.



This bet was rejected and you never saw it: all good. Unfortunately, the long dormant bug activated and allowed another bet which was not over the numeric max stake but was over the exposure limit for the account (it had nothing like the 20m to bet in it) to skip the exposure check.



Because it was a valid stake and the exposure check was missed, it went into the market and became good value pretty quickly. The nature of the bug meant that both bets had to be placed effectively at the same time, otherwise the tiny window of time that the numeric error opened up for the second bet to skip the exposure check would be closed again.



All involved in all aspects of handling this bug at the time were given the usual spiel of secrecy, which was fair because the bug was exploitable for a little while until fixed.



However, it was fixed long ago (although similar bugs probably existed because the codebase was very difficult to reason over) and now that whole trading system has been rewritten in a more modern language with a safer architecture (and it’s much much faster). I imagine there are all sorts of bugs in it, but they won’t be this type.`

*Tags: Errors Debugging, Strategies, Multi Client*

---

## 2021-01-31

**Aaron Smith** - *10:31:47*

How would you approach the same question if you had 1 strat with high variance? Probably (depending on your risk aversity and your bank account) you wouldnt go all in on a high risk strat. If you have a solution to the problem for 1 strat, you could view your correlated strats as 1 and apply same solution. For example you can share max_runner_exposure between the two

*Tags: General Technical*

---

**Kai** - *10:33:35*

I have some moderate correlation between different strategies and estimate it from pnl. But for me the first question about the strategy is always about value. If strategies are correlated and you want to reduce variance then you can adjust stakes, but in general I am happy about every strategy with value and correlation is a secondary question for me.

*Tags: Strategies*

---

**Misha** - *10:34:16*

We are essentially doing that (one strategy at the moment) and trusting the model. Makes for a roller coaster ride

*Tags: Strategies*

---

**Oliver Varney** - *10:41:21*

they do kind a merge into one strategy, although they operate on different inputs, models, timeframes and order side as the opportunity and value all exists at the time of taking the bet. Bank structure / sizes is separate and im not really concerned about return or overly too much about risk. Several strategies may price up a market very good, i.e. backs get backed in lays get pushed out, but again this leads to a very correlated outcome across all bets. For example I may have backed something, it gets pushed it then runs awful and a horse that hasnt won in years pops up and wins. Its likely that I have layed that horse also, so you get a double hit. is this typically what others see across their bets?

*Tags: Strategies*

---

**D C** - *10:55:42*

[@ULDAVFDRP](@ULDAVFDRP) surely this is just unavoidable if you are working off of your own calculations of each runner's probability of winning? If the market is underpricing one runner, then it has to be overpricing at least one other. If you are +EV with your model overall surely all you need to do is make sure you stake sensibly and have sufficient bankroll? I can't see how you can prevent correlation between the two if you consider laying as a different "strategy" to backing - if the market goes the wrong way and the wrong runner comes home you will clearly take a bigger hit but overall you are compensated by the races where the market goes the right way and the right runner comes home?

*Tags: Strategies*

---

**Misha** - *10:58:07*

Although in some sports it is possible. We are thinking of developing a tiebreaker model for tennis that wouldn't be related to out main model, and could in fact bet against it

*Tags: Strategies*

---

**Oliver Varney** - *10:58:25*

good explanation kind of what I was getting at [@U01DVUAE2G1](@U01DVUAE2G1). [@UUE6E1LA1](@UUE6E1LA1) yes 100% agree, just something ive run into now as a result of flumine providing the facility / framework to easily add more and more strategies. Long term all strategies are +EV.

*Tags: General Technical*

---

**Sam Asin** - *19:56:11*

Hey guys, for my background I've always just used linear regressions and made 'em work. I think it's pretty fun and you can do a lot to get around stuff, but, like I *think* [@UBS7QANF3](@UBS7QANF3) said a month or two ago, it's not 1904. I wanna mess with something a bit more fancy/modern, especially to handle things like non-linearity. I saw Xgboost mentioned here and am wrapping my head around decision trees a bit, they seem cool. Do you guys have other recommendations for things to look at if I'm especially interested in non-linear effects? I hear a lot about neural nets as well, not sure what the pros and cons are. Within decision trees, I'm thinking random forest models seem interesting? Just curious if there's anything else I should be thinking about if I'm looking into non-linear models.

*Tags: Strategies*

---

## 2021-02-03

**jp** - *12:04:07*

From time to time I see the question raised from bettors about how many bets they need to make from a certain strategy before they can conclude that the strategy is +EV. Isn't this a question well suited for statistical hypothesis testing? So, given a history of bets, you define a confidence interval, and this analysis could either conclude that the strategy is +EV within the given confidence interval or not. Has anybody tried such an approach?

*Tags: Strategies*

---

**jp** - *12:25:54*

Yes, I have a 10-12 year history of bets, out of which the last 7-8 years are stored in a database, so a lot of data for statistics in there.

*Tags: General Technical*

---

**jp** - *12:27:36*

So, I am not longer wondering if my strategy is profitable :slightly_smiling_face:

*Tags: Strategies*

---

**Mo** - *12:27:49*

I think that's an important distinction and it should be much easier to say whether this shows you have a systematic edge than try to use 7 year old back test data to justify rolling your strategy out into production

*Tags: Deployment, Strategies*

---

**jp** - *12:29:45*

Good point, that is maybe something I can use here, a strategy that worked a few years ago has no guarantee to work in the future...

*Tags: Strategies*

---

**Michael** - *12:32:03*

[@U01A3B4SBDZ](@U01A3B4SBDZ)is your income being legally questioned in some way? If so what country are you in?

*Tags: General Technical*

---

**Alessio** - *15:07:11*

I guess that's one problem with this whole shebang: there's only one company, and you are a "marginal case" for regulators most of the time

*Tags: General Technical*

---

**thambie1** - *15:54:12*

Since were on the subject, I'm likely going to hire someone someone soon to help me in my trading activities. Anyone have suggestions on the best country for doing this, and best way to minimize taxes? (Currently living in the UK, but would consider moving)

*Tags: Strategies*

---

## 2021-02-04

**liam** - *10:08:02*

i average around 50%, the problem with 1% is that you will really struggle to scale / market capacity is likely to kill you

*Tags: General Technical*

---

**Jonjonjon** - *10:35:01*

Yes, thanks Michael, it's the more relevant number for my question.

*Tags: General Technical*

---

## 2021-02-05

**birchy** - *13:39:52*

[@U4H19D1D2](@U4H19D1D2) is this used internally by Flumine? i.e. can it be changed by users?

[https://github.com/liampauling/flumine/blob/3b819f5370499ccb249edcd3bb21fbaa7f4ba6f6/flumine/order/orderpackage.py#L49](https://github.com/liampauling/flumine/blob/3b819f5370499ccb249edcd3bb21fbaa7f4ba6f6/flumine/order/orderpackage.py#L49)

*Tags: General Technical*

---

**birchy** - *13:47:19*

OK. Am implementing polling listClearedOrders to get history and was going to use that for identifying the strategies. It's fine for my own frameworks, but not the Flumine ones.

*Tags: General Technical*

---

**liam** - *13:52:19*

You then change [https://github.com/liampauling/flumine/blob/3b819f5370499ccb249edcd3bb21fbaa7f4ba6f6/flumine/streams/orderstream.py#L28|this](https://github.com/liampauling/flumine/blob/3b819f5370499ccb249edcd3bb21fbaa7f4ba6f6/flumine/streams/orderstream.py#L28|this) as well?

*Tags: General Technical*

---

**Oliver Varney** - *13:53:24*

nope, should I? I only touch the order ref not the strategy ref

*Tags: Strategies*

---

**Oliver Varney** - *14:00:53*

are you confusing customer_strategy_ref with customer_order_ref or am I being silly?

*Tags: Strategies*

---

**liam** - *14:03:59*

are you confusing customer_strategy_ref with customer_order_ref?

*Tags: Strategies*

---

**birchy** - *15:02:30*

Don't suppose you have some code available for that? Undecided on what format to use (CSV, database, pickle, etc)?

*Tags: General Technical*

---

**liam** - *15:03:29*

My production stuff is closely coupled to my order recording api but the logging control base class / example is pretty much all there you just need to decide what to record

*Tags: Deployment*

---

**birchy** - *15:11:26*

And therein lies the problem, I want to save enough data to be useful but also avoid finding out in 3 months that I've missed something crucial. :grinning:

*Tags: General Technical*

---

**birchy** - *15:15:20*

And presumably you then have some framework to re-run "what-if" type questions against data from MarketRecorder?

*Tags: General Technical*

---

**AP** - *16:29:07*

I should really start using the logging control but for now I map the strategy name hash to a strategy ref when my lambda function that pulls in account statement and cleared orders 

*Tags: Strategies*

---

**AP** - *16:29:54*

Because flumine puts the name hash in the order ref 

*Tags: General Technical*

---

**Oliver Varney** - *16:32:00*

just to clarify [@U016TGY3676](@U016TGY3676) me and [@U4H19D1D2](@U4H19D1D2) were talking about two different things confusing each other, I didnt phrase my initial response very well, apologies. basically I change  the customer_order_ref property on the order class, and encode internal information that can be linked back to IDs in my database. Therefore can rebuild the trade if necessary directly from betfair. I think best practice however is to use logging.

*Tags: General Technical*

---

## 2021-02-06

**RicHep** - *16:46:20*

Oh ok, years ago I found the early moves to be the best, but more recently it seems to have changed, for my strategy anyway

*Tags: Strategies*

---

**Jonjonjon** - *21:07:19*

The y scale is pounds sterling, without commission taken off. x axis is either the date (at the top), or 1 tick per market (bottom). It's my first live bot with Flumine.

*Tags: Deployment*

---

## 2021-02-07

**Jonjonjon** - *16:25:22*

Has anyone had success with using the favourite as an indicator for inplay horse racing?



I have been focussing my analysis on looking at inplay racing, when the odds are very volatile, which I have defined as (sum(1/[low price])&gt;X, where X is some parameter greater than 1.0. But this filter is not really helping. I get great in-sample results but awful out-of-sample results.

*Tags: General Technical*

---

**liam** - *16:33:05*

Can be tricky to model without the fave, which odds are you using when calculating your results?

*Tags: Strategies*

---

## 2021-02-08

**mandelbot** - *01:39:35*

If your strategy has money waiting it wont get taken by such moves when in fact it should

*Tags: Strategies*

---

**Jorge** - *11:36:23*

For the math guys in here... I need to calculate the Variance of my strategy's P&amp;L. I assume the P&amp;L per market are independent random variables so I can sum their Variances up. What statistical test can I use to determine what is a good sample size to estimate the strategy's Variance?

*Tags: Strategies*

---

**Jorge** - *11:51:47*

Yes, I want to know how many observed values (markets) do I need to feel confident about the Strategy's Variance

*Tags: Strategies*

---

**liam** - *12:23:36*

Aka Monte Carlo simulation, [@UBS7QANF3](@UBS7QANF3) do you have something developed in python for this to hand?

*Tags: General Technical*

---

**Jorge** - *12:37:14*

Yes, my Strategy's EV is positive. But my concern is that it is too small compared to the Variance. So I want to estimate the Variance and try to reduce it without damaging the EV

*Tags: Strategies*

---

**Michael** - *13:05:39*

No. You set probability to 1/your matched odds. That's what [@UBS7QANF3](@UBS7QANF3) means by step 1. You don't need to concern yourself with other runners. To be honest; it looks like your understanding of basic probability is very weak. You will need to improve that to make any headway with this kind of thing so I'd suggest you start there rather than worrying about the significance of your results. Knowing that they are or aren't significant probably won't help you with anything anyway.

*Tags: General Technical*

---

## 2021-02-10

**Aaron Smith** - *13:28:25*

all good, thanks for preparing me for the problem then :smile:

*Tags: General Technical*

---

**Unknown** - *14:06:21*

The main problem I find is that (real_start-x) is super important in terms of price efficiency, and being able to actually know this would be useful. Liam's post about the race status API is probably the closest one can get to approximating this I guess, though still possibly not precise enough :(

*Tags: General Technical*

---

**Misha** - *20:34:17*

The simple thing is that large bettors have been after this info for years. But it's either not available or not accurate. So you can't really have a strategy that relies on this information

*Tags: Strategies*

---

## 2021-02-11

**D C** - *09:51:22*

£60 every 15 minutes for 12 hours adds up to quite a lump if their method is junk. Assuming its all the same person and they they don't consolidate their position of course. I've always thought that if you had a good model price, then getting in early if the price is value was a good thing - but a lot of you here tell me that is not what the "big" boys do as it will drag the price down because of their action. I guess it depends on the amounts you want to get matched for at the end of the day.

*Tags: Strategies*

---

**D C** - *10:39:30*

That is an interesting question but you have to assume "yes" but in ways that would be unknown to all but a very few. Imagine the horse travels badly on the way to the venue and has been upset significantly - this could have a huge impact but might only manifest itself in horse body language later. And that would need to be seen by skilled horse readers and only a few of those would notice it prior to parading.

*Tags: General Technical*

---

**D C** - *10:42:36*

[@U01B8031PM1](@U01B8031PM1) That is kind of what I am getting at. If I can lump on a few hundred quid on a donkey to get a price contraction, that could raise the price of the runner I REALLY want to back later for tens of thousands. That contraction has to result in drift on a subset of the other runners (possibly all of them) so if it were that easy, would not everyone be doing it? I talk here of people who obviously have good models and decent probability estimates for horses and that is going to be a very small proportion of the population of people betting on that market (in terms of numbers of participants, not betting volume necessarily).

*Tags: Strategies*

---

**Michael** - *10:44:24*

It's certainly true that your money impacts the market but it's worth adding that it's wrong to assume that the best players get the most money on. There are some good players who match a lot but in general getting tonnes of money on is more a characteristic of lower margin players who do it because they have to. There is an inevitable and unavoidable trade off between margin and turnover. Give me more margin and less turnover any day. Also - let's assume that your £60 guy is a winner; he doesn't have to be the worlds biggest winner to make a lot of money. Maybe he's figured out that he can get really good margin on small amounts of money early in the day (which makes perfect logical sense) - good for him, he'll make a killing and no one will even notice. That's smart. This idea that very successful individual players provide the only or best model is total nonsense. Most people, (including most people on this forum) make losses, a few make a little and a tiny number make a lot. None of them need to concern themselves with what the world's top players do, just how they can make a little bit more than last month. This guy could be making a choice that more value for less stake early in the day is worth it to him despite his market impact. That would put him well ahead of the pack. If he's matching a few hundred quid with good margin on every race that's easily £20 per market which is over £250k a year. That would be adequate for most people's needs and would far exceed most players here. How many of the 800+ players here made &gt; £250k last year? How many posting on this topic?

*Tags: Strategies*

---

**Misha** - *10:49:07*

I have never seen a model that doesn't go up and down

*Tags: Strategies*

---

**Michael** - *10:54:46*

Or maybe your previous client isn't the only or best model for everyone else and the answer to every question.

*Tags: Strategies*

---

**Misha** - *11:28:19*

My model and actuals align pretty closely. I have detailed data going back to August 24, so now closing in on 6 months. So two basic tests (it's a probability model based on stats): Brier score v market, and profitability per market for my betting "patterns"

*Tags: Strategies*

---

**Misha** - *11:29:31*

So modelling by day from August 24 to February 7:

*Tags: Strategies*

---

**Jonjonjon** - *11:53:57*

I think my automated bots started in 2013, mainly on horse and dogs. It took around 2 years for me to become relatively consistent. As you can tell from my stupid questions, I'm still learning a lot.

*Tags: General Technical*

---

**Jonjonjon** - *11:58:32*

I'm pretty sure they have market impact, so plugging them into a flumine backtest would not help.

*Tags: General Technical*

---

**Jonjonjon** - *12:02:28*

I'm currently trying to search for a different source of "alpha" that scales better. My inplay research hasn't really borne any fruit. So I might start staring at horse racing prices a long way before races start. Hence my initial question this morning

*Tags: General Technical*

---

**Michael** - *12:03:02*

[@U0128E7BEHW](@U0128E7BEHW) yeah assumes some small fixed stake. In my case that's about £5 for that type of betting.

*Tags: Errors Debugging, Strategies*

---

**liam** - *14:34:11*

I started in 2012, from memory streaming was a big improvement both from the number of markets now available and my pnl increased. Every time I have met up with someone (normally from this group) I got another kick to improve but it has been steady with profit improving every year since I started (hopefully continues :slightly_smiling_face:)

*Tags: Performance*

---

## 2021-02-13

**Jonjonjon** - *20:18:37*

This might be a bit of a vague question, but does your model have many parameters, and have you optimised on them?

*Tags: Strategies*

---

**user34** - *20:32:30*

Is there something inherently wrong with relying on long shots? I can see that it makes it very hard to evaluate your profits as the variability will be very large, but is there something else to be aware of? My models tend to predict a lot of value at long odds.

*Tags: Strategies*

---

**Misha** - *23:20:55*

My view is if you can average around 5% for around 2 months then you "might" be there (depending on what your model is relying on)

*Tags: Strategies*

---

## 2021-02-14

**MacrcoPolo** - *12:32:01*

To be honest, I'd still run this strategy on small stakes - it won't cost much and you'll collect more data. Worst case you lose a few quid, best case you're able to validate it.

*Tags: Strategies*

---

## 2021-02-15

**Michael** - *08:33:30*

This kind of question comes up a lot but I don't really get why - the answer is always 'just run it with small stakes and find out.' The only scenario where I can see it being important is if you have only a small number of events to bet on (NFL maybe). For something where there are thousands of events stretching off into the future you  can just let time remove doubt and until it's done so there's no reason to scale except for silly human impatience. Rushing to scale is always the wrong call. If you're going to play this game well you need to be calm, level headed and patient. Don't fantasise, don't catastrophise, just wait, and whilst you're waiting; do something else.

*Tags: General Technical*

---

**Michael** - *09:54:31*

Actually I think that might be the best question: _"Is this sensible?"_ You don't normally need a number for that.

*Tags: General Technical*

---

**RicHep** - *11:50:44*

Well overall it beats the SP, it also seems unlikely (although not impossible) that a strategy would  churn out 1100 points to single point stakes in 2.5 months and not have some long term profitability, so I shall run it for now and see what happens

*Tags: Strategies*

---

## 2021-02-19

**birchy** - *19:20:55*

Spawned by [@U4H19D1D2](@U4H19D1D2)'s link to the BA greyhound thread earlier, I was wondering how many of us play the dogs? I had a very successful dogs model many years ago and have revisited it in recent weeks, particularly as the traded volumes are now much higher than they were back then (i.e. £5k per race was average). Fired up a new bot a few days ago and have a few hundred quid more than I did on Tuesday, so looking forward to expanding on this in the future.

*Tags: Strategies*

---

## 2021-02-23

**Lee** - *20:33:25*

What about when comparing two strategies (dummy numbers), if i had the first strategy and tried optimising the signal to remove some poor bets and ended up with strategy 2, would you see that as a good/bad change?

1: size settled £1k and makes roi 5% (£50 profit)

2: size settled £500 and makes roi 8% (£40 profit)

*Tags: Strategies*

---

**Mo** - *20:40:55*

I prefer strategy 3, you find a way to squeeze some extra volume through



3. size settled £1.5k and makes roi 4% (£60 profit)

*Tags: Strategies*

---

## 2021-02-24

**Michael** - *13:25:46*

I think this is one of the knottiest problems of betting.

*Tags: Strategies*

---

**Lee** - *14:09:57*

I think that's also why i've been looking at ROI. Pushing more volume from the market is easier said than done, so went for improving my matching / removing bad bets and then try again when i have more margin to play with. It felt like a logical approach sacrificing a small amount of profit hopefully in the short term for the long term gains but at the same time choosing a less profitable strategy didn't seem logical.

*Tags: Strategies*

---

**river_shah** - *14:56:13*

Driving up margin (to a limit) usually reduces adverse selection. All trading strategies, even well selected ones tend to exhibit out of sample overfit. If margin is high enough that usually means strategy ends up avoiding lots of trades which do nothing but cause noise PL and lastly alert the market to your impending trading direction.

*Tags: Strategies*

---

**river_shah** - *15:06:44*

It is not a one to one mapping to the trading domain but ml techniques such as support vector regression have a built in concept of “margin” to separate points better and give better out of sample performance. maintaining margin for trading strats to me is similar to how SVR creates bands around training examples to help classify points better. too low of a margin and SVR has high variance. Something similar happens to trading strats

*Tags: Performance, Strategies*

---

## 2021-02-25

**liam** - *10:43:35*

[@UUCD6P13J](@UUCD6P13J) bringing your question back up have you looked at testing market capacity / roi / profit by pure back testing? I have done a few tests where I bump stakes up to silly sizes to see how the profit varies and it can make some interesting graphs that do match up with live betting more than you would think 

*Tags: Deployment, Strategies*

---

**Lee** - *10:52:09*

But i don't really know how to scale so been learning through a bit of trial and error

*Tags: Errors Debugging*

---

## 2021-02-28

**liam** - *15:57:16*

Much easier to find value but harder to get it, strategy / scale dependant obviously but pre play tends to be the opposite (in my opinion) 

*Tags: Strategies*

---

**Michael** - *17:37:07*

I think the premise of the question (that you have to 'beat' other successful players) is wrong. It's not like bad players never strike a good bet. You can just take their good bets off them - there's plenty of that to go round. Obviously there's a limit to the total pot but that's not of much concern to a new player. Also a new player will typically delighted with what a more experienced player might consider dregs not worth the bother.

*Tags: General Technical*

---

## 2021-03-01

**birchy** - *19:01:33*

Indeed, I have found some more dregs the last few days and am more than happy to take them. Regarding strategy tuning, let's say you have a fairly simple strategy that uses a handful of `if` statements to check volume traded, last traded, current price, etc and then places bets at N ticks offset. You started off with a rough idea of what these numbers should be, ran a backtest and then went live with it. The strategy shows a decent profit on a reasonable sample size but you don't know if the trigger or execution setpoints are optimal. Given that each of your triggers has a range of possible values that equates to &gt;1000 combinations, how would you approach finding the optimal ranges?

*Tags: Deployment, Strategies*

---

**Atho55** - *20:15:21*

Evening all. This is my 1st post and would like to canvas a few opinions if that`s ok. Do any of you use any available data to project forward potential bets where the race outcome decides if it`s a win or loss? Simple Back or Lay bets to a fixed stake. Like this for the week coming. The Rank is at the top and assumes all meetings/all races follow the script for that day.

*Tags: Errors Debugging*

---

**Oliver Varney** - *20:25:35*

using previous historical data to predict whether you should bet on a horse at a certain price is fairly common if you mean that

*Tags: Data Quality*

---

**Atho55** - *20:34:59*

It`s not betting on a specific horse, but betting on the BSP Rank favourite for every race (UK) for that day regardless of price and obv assumes bets matched at BSP. Have used historical data to give a pointer to what might be a reasonable combination on the day.

*Tags: Data Quality, Strategies*

---

**AndyL** - *20:37:15*

I think my main issue is my "mindset" is just wrong as to what a profitable strategy is, so basically I am creating strategy/method after strategy, but although they are different strategies I am using the same mental approach to find them. I suspect my mental approach is just wrong, so I create losing algo after losing algo..!! I need someone to "push" me onto the right page!

*Tags: Strategies*

---

**Oliver Varney** - *20:38:06*

so you want to bet blind at BSP? If I had to hazard a guess you will be at best flat in the long run doing this. What is your out of training set performance (i.e. test set and or holdout) ?

*Tags: Performance*

---

**Oliver Varney** - *20:45:53*

So the strategy is either to back or lay the favourite (lowest price horse) at BSP? A binary classification task if it is. You would have to bet like 5 seconds out to do this also as you wouldn't know what BSP would be.

*Tags: Strategies*

---

**Atho55** - *21:13:56*

From historical data, hardly going to go into detail

*Tags: Data Quality*

---

**Oliver Varney** - *21:15:00*

I can see many faws focusing on ranking of prices rather than the actual prices but maybe your onto something

*Tags: Deployment*

---

**Oliver Varney** - *21:15:59*

typically people take +EV bets, i.e. price up a horse using your model and if the price is better in the market, back it and worse then your price lay it

*Tags: Strategies*

---

**Oliver Varney** - *21:38:50*

so your lays assuming they were a good lay at BSP, these will most likely have drifted further and you wont match and the reverse is true for backs, they will have shortened. your smart money wont match and the no so smart money will get matched and you will have a -EV strategy

*Tags: Strategies*

---

**birchy** - *22:00:41*

How good are your Flumine updates? :face_with_hand_over_mouth:

Seriously though, I'm currently enumerating a range of parameter combos to find the sweet spot and hopefully not end up overfitting. I'll let you know how it went next month.

*Tags: General Technical*

---

## 2021-03-02

**Unknown** - *10:02:44*

Morning all, just to outline the details. Have a system that looks to suggest Ranks to either Back or Lay. Rank is the order of favouritism as determined by BSP. My monetary results are based up correct Rank being selected and a £25 Back bet or £25 Lay bet achieved at BSP. £25 was chosen when I set up my database as a figure that should be able to be matched around the off time. In reality this may not always be the case but this way I have a consistent approach. I am not at the implementation stage yet. Other caveats, I do not consider the odds when Laying. In reality I would not be placing £25 on 1000 BSP even though this might show up as a win in the results however having some visibility on the impact when a high odds Lay goes on to win I`m hoping will give some pointers on frequency and risk reward at the implementation stage. I have shown visibility on Ranks 11 &amp; 12 although they do not feature in the daily total so I can compare the rate of progress to other systems on test. This is what I believe to be the results for Mon assuming my sums/data is correct. To clarify, getting matched on the correct Rank is more important than any odds changes to the BSP value when getting matched. As a measure of success or failure I count the No of suggestions then the No that have a value &gt;£0.00. New system so thus far Count = 79, CountIF &gt;£0 = 60. I`m hoping that posting here will turn into a bit of a peer review.

*Tags: Feature Engineering*

---

**Oliver Varney** - *10:21:03*

is the green cells profit? if so how can you get £297.16 profit from a £25 lay? Not trying to be negative, just trying to understand and potentially help save time

*Tags: General Technical*

---

**Atho55** - *10:48:05*

"The Rank is at the top and assumes all meetings/all races follow the script for that day". From the 1st Post. If it says Lay Rank 2, you Lay Rank 2 at every race at every meeting. Yes it`s a different approach hence my question in post 1.

*Tags: General Technical*

---

**Oliver Varney** - *11:12:38*

Id suggest downloading the daily BSP files, splitting your model data into 80% and 20% roughly. Build your script/ decision maker off of the 80% of the historical data, do not let it see the  other 20%. Once you have fully come up with the script / decision maker rules, return to the 20% and see if it makes profit.

*Tags: Data Quality, Strategies*

---

**Unknown** - *11:15:17*

I do something similar to goatiwish. It might be based on technical analysis, but it's definitely got legs. Though it definitely fits into the "Dregs" category of strategies. Below is the live equity curve. The recent bumpiness is because I started trading a second parameterisation of the strategy.

*Tags: Deployment, Strategies*

---

**Oliver Varney** - *11:38:05*

Personally Id never run any model off only training data. If so Id be able to predict 100% of winners

*Tags: Strategies*

---

**Unknown** - *11:39:56*

Splitting the data tells you how well your model generalises. This is a recipe for chucking away money

*Tags: Strategies*

---

**Oliver Varney** - *12:05:22*

This is the typical learning approach from machine learning. Take your all the historical data you have and randomly shuffle it. Once shuffled take 20% of you data and put it aside, forget about it for now, almost like it doesn't exist.  With the remaining 80% you will use this data to come up with your model / script / decision maker. Apply the process of learning to this data solely making sure it never sees any of the 20% hidden away. A typical learning approach could be linear regression (gradient descent) but it can be as simple as using excel to filter records although this is sub optimal. Once you think you have constructed your model / script / decision maker, return to that 20% of data and apply the rules / model to it and evaluate performance.

*Tags: Data Quality, Performance, Strategies*

---

**Oliver Varney** - *12:08:36*

The performance in the 80% is somewhat irrelevant and should be mainly disregarded, its the 20% you care about

*Tags: Performance*

---

**Atho55** - *12:11:09*

I am looking at historical data to try to determine what the likely outcome might be going forward based upon what happened in the past. No rocket science behind it unfortunately.

*Tags: Data Quality*

---

**James T** - *12:24:04*

I think like you suggested at the very start you need to change your overall thinking / mentality when coming up with strategy ideas. 



Very first thing is have an idea of where there might be value and possibly why. Imagine a specific point in time and you see odds at 2.1, is backing at that price cheap? Is 2.15 cheap? Is 2.2 cheap? How can you determine if any particular bet is cheap? For me, that’s what a strategy is - a method of determining a fair price so that I can either back or lay with a margin on top of the fair price. 



If you base things on ordering, why would the 3rd favourite necessarily be cheap or expensive based on history? For me the 3rd favourite has just as much probability of being either, so any trend you find in historical data will turn out to be you fitting to the data. 



So I recommend going back to the basics and write down 10 ideas of how you can measure whether a price is cheap or expensive to back or lay. Then test those ideas to see if they are actually a good measure. 

*Tags: Data Quality, Strategies*

---

**Unknown** - *12:26:01*

[@U01PQ9SR9MJ](@U01PQ9SR9MJ) the point is that your conclusion of "what is the likely outcome going forward" is unlikely to be useful unless you evaluate this conclusion on data you have not incorporated when doing your data mining. To put it simply, you can't say that your observation of some pattern is a useful observation unless you find that this observation is profitable on data you have kept seperate from where you identified this pattern in the first place. indeed you are attempting to do something like this with your "forward testing", but you can very easily learn the results of your forward testing by just holding out some of your historical data. (This is of course not to disqualify the usefulness of actually taking your strategy to the market with small stakes).



also to add - if you are relying on real trading to do this "validation" for you, your whole process is going to be slow. By holding out some historical data you can instantly check if the patterns you've identified have some profitability (and  therefore meaning you have not just overfit your historical data set).

*Tags: Data Quality, Performance, Strategies*

---

**Atho55** - *13:00:26*

Tbh I did not realise that I might be guilty of backfitting because the event result I presented has not yet happened rather than present a result that has. Fair point James T which is why I asked the question in my 1st post. I have skimmed the posts and could not see anything that specifically excluded other concepts.

*Tags: General Technical*

---

**James T** - *13:04:54*

Oh, no I don’t think any sort of questions are excluded. Just that there’s more of a specific type of expertise here.

*Tags: General Technical*

---

**James T** - *13:11:30*

Sorry, I also realised I confused your initial question with the one goatiwish posted around the same time. 

*Tags: General Technical*

---

**birchy** - *17:23:28*

[@U01PQ9SR9MJ](@U01PQ9SR9MJ) It can be quite difficult to understand what "value" is and there is no binary answer to the question. The basic idea of making a profit is to place bets at prices greater than the true probability of an event happening. That could be horse winning a race or a football team losing, etc. The fact is that we never _truly_ know if the bet we are placing is good value or not and all we can do is make our best approximation. If our approximation is correct often enough, we make a profit. Sometimes we get it right, sometimes we get it wrong, but as long as we're right more often than the market, we make a profit. Unlike tossing coins, rolling dice or spinning roulette wheels, sports betting does NOT have probabilities that can be calculated exactly, so at best, we're using educated guesses. Whether intentional or not, strategies like "lay the favourite", "back the outsider", etc will only show a profit IF they are finding value.



Regarding backtesting, as others have mentioned, you are trying to predict _future_ patterns, so building a strategy using _all_ of your data only tells you what happened in the past. The problem is overfitting, where, through multiple filter adjustments and fine tuning, you end up picking all the sweet spots from events that have already happened. It's no different to shopping...if the item you want to buy is £10 today but was £8 yesterday, you will have to pay £10 for it because yesterday's price has now gone. Your strategy may tell you to buy at £8, but that's no longer available, so it's not a true value. You _can_ build a strategy using _all_ of your data but then you would have to place real bets and wait a long time to see if it's actually profitable. OR you can split the data into a "training" set and a "validation" set, the validation set representing the "future" data. It's critical to ensure that while developing the strategy that you _never_ use the validation data as it's for...you guessed it...validation only.



After many years at this game (I was botting [http://Flutter.com|Flutter.com](http://Flutter.com|Flutter.com) via browser emulation before Betfair bought them out in ~2002) I had also become very stuck in my thought process. I have a handful of bots that have earnt small amounts for many years but never enough to actually live off. Joining this Slack group was the best thing I've done in the last 20 years. The quality of knowledge collected here is second to none and the members are more than happy to answer stupid questions (and I've asked a lot of them!). I've recently had an epiphany after reading [https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723|The Logic Of Sports Betting](https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723|The Logic Of Sports Betting) and listening to the [https://player.fm/series/business-of-betting-podcast|Business Of Betting](https://player.fm/series/business-of-betting-podcast|Business Of Betting) podcasts, both of which were recommended on this group. Don't be put off by negative comments, they're actually a very truthful lot on here and, believe it or not ARE trying to help.

*Tags: Deployment, Strategies*

---

**Beeblebrox** - *18:56:37*

One question I have is what do you do when you've created a strategy using the test data and then when you come to use the validation data the strategy doesn't perform as well as you hoped?



Because you could go back and tweak some parameters and then rerun the strategy on the validation data, but by doing that your validation data has now effectively become your test data.  So how do you refine/improve a strategy after you've used the validation data?

*Tags: Strategies*

---

**V** - *19:04:45*

Split your data in two: an in sample set and an out of sample. The latter is what you use a final test of “does this strategy generalise to unseen data”. Then you split the in sample further into train/test data sets. You do your tweaking on those. Once you’ve done finished all your tweaking, then you finally run that once on the OOS set.

*Tags: Strategies*

---

**river_shah** - *19:06:14*

[@U01DJ4DBF9R](@U01DJ4DBF9R) is correct. But remember once you have checked on OOS, don’t then go back, tweak your models and then reuse that OOS. That OOS gets effectively “burned” even though your model never saw it in training or validation

*Tags: Strategies*

---

**Beeblebrox** - *19:12:25*

That's great, very helpful.  I'd not thought of splitting up the in sample set into train/test data sets too.

*Tags: General Technical*

---

**James T** - *21:37:20*

Yeah, there will certainly be parties that price things using fundamentals, and some of these people take home some of the biggest sums. However don’t under estimate the amount of data required to go into modelling who wins a horse race for example. I’d say fundamental modelling is one of the more advanced methods and shouldn’t be tackled unless you’re already confident you have a chance of success.

*Tags: Strategies*

---

**Aaron Smith** - *21:40:34*

I dont think a huge amount of data is needed for modeling, but the model has to be good. Taking a suffiecent amount of factors in (like for horse racing track type etc.) and knowing how to value them. Data from last year is pretty much pointless already for current races.

*Tags: Strategies*

---

**AndyL** - *21:53:36*

thanks James for your help, it's given me lots to think about, cheers

*Tags: General Technical*

---

**Misha** - *23:52:30*

There is also no such thing as "too efficient". On UK racing there are a few syndicates making huge money on pre-off UK racing. And my view is that these models, while incredibly detailed and profitable, will all lead to different prices for different runners

*Tags: Strategies*

---

**Misha** - *23:53:50*

So player 1 might get a huge win on a race, while player 2 might make a huge loss. Sometimes profits will coincide, sometimes not. Some runners will be bet on by one model but not others

*Tags: Strategies*

---

## 2021-03-03

**MacrcoPolo** - *00:38:27*

Helps if you realize that not a lot of people are thinking in this value-seeking way. People trade for different reasons and at different times, and this creates the noise in the market.

*Tags: General Technical*

---

**birchy** - *09:34:37*

Always remember that the market efficiency increases as more money is traded (aka "wisdom of the crowd"), so in _theory,_ a £500K market will be more efficient than a £100K market. The probabilities for each runner will conclude at either 100% (winner) or 0% (loser). From market opening to market closure, the prices are constantly moving, which by it's very nature indicates that there is uncertainty and inefficiency in the market and _that_ is what you need to exploit. Breaking it down into micro markets triggered by price moves, money flow, etc helps. Think about _why_ the price moved and _how_ the market reports it. You don't have to know that the horse was playing up or the trainer made some comment on TV, all you need to know is if it has a positive or negative effect on the prices and volumes.



You _can_ try to predict the winners or losers using fundamental or statistical analysis but it's a highly competitive approach and would likely require some extremely complicated modelling to beat the market makers.



Simple (and obvious) mathematical approaches DO make a profit but can be difficult to execute optimally, e.g. lay the field at &gt; 100% book, lay 4 runners at &lt; 4.0, back the drifters, lay the steamers, etc. Don't be afraid to oppose what everyone else seems to doing. The horses don't know how much money is being traded and I can tell you with 100% certainty that "insider" knowledge doesn't make a horse win. I live in an agricultural area and know a handful of people that work in the racing industry. If I had £1 for every time I've been told "our horse will win today" (and it didn't), I'd be a lot richer than if I was backing them. Hell, I regularly see these horses get backed from 14/1 into 5/1 and they still don't win, so even the "insiders" get it wrong more often than they get it right.

*Tags: Deployment, Strategies*

---

**Misha** - *21:19:00*

Helps if you know how they bet :wink:

*Tags: General Technical*

---

**Misha** - *22:01:11*

It might be possible, if you have enough background knowledge and skill, to identify movements generated by the largest and best of the syndicates. Their POT% is high, their turnover is huge, and I know that getting a fraction of that POT% and turnover would easily beat a full-time wage. I had thought about it in the past, but decided that it's a pretty risky strategy totally reliant on someone else

*Tags: Strategies*

---

**Misha** - *22:35:09*

Last point - if a single player pushes a price towards their model price, that doesn't mean it is pushed towards 0EV. That assumes their model is correct, which we know cannot possibly be so all the time

*Tags: Strategies*

---

**Misha** - *22:36:56*

It's why two different models betting on the same races can both be very profitable. You only have to be right more often than not

*Tags: Strategies*

---

**Misha** - *22:37:18*

In terms of price movement: I have seen prices go from $10 to $5. Getting it at $9 (if that model was correct) is a good value bet

*Tags: Strategies*

---

**Michael** - *22:39:54*

At any given time there is a 0EV price. There must be or there couldn't be a +EV or -EV price. Any given model may or may not get it right but it still exists. Two models may produce different prices for a particular instant for a particular bet and both models may be profitable overall but that doesn't mean that they both priced that bet correctly.

*Tags: Strategies*

---

**Misha** - *22:46:34*

EV is only a concept for the model you are using. Everyone has a different value for EV. We only find out who is right after the event over a lot of events

*Tags: Strategies*

---

**Misha** - *23:03:47*

On any one event we will all have different values. Over a lot of events the person with the most "accurate" values has the best "model"

*Tags: Strategies*

---

**Michael** - *23:09:14*

It's kinda frustrating that I was trying to offer some genuine advice to the questioner and now it's swamped in all this crap.

*Tags: General Technical*

---

**Paul** - *23:19:16*

Knowing that the syndicate just found value does not mean what in front of you remains value. You’d need to model how often they move a price by what % and then your EV calc is not on the price you now see, but on the likelihood it will continue to move as you catch those whirling tailwinds

*Tags: Strategies*

---

**Paul** - *23:23:29*

I seem to recall Benter giving a talk where he said this was actually the harder bit of the problem he’d worked on. Finding winning horses was relatively simple, it was the bet sizing and then dealing with the money following them when the boards started flashing where it got tricky...

*Tags: General Technical*

---

## 2021-03-04

**liam** - *08:13:06*

Probably going to regret trying to continue this but the [good] syndicates / big players spend 80% of their time trying to not move the price so you need to think outside the box a little if you want to try and follow. I would disagree [@U016535QCJ2](@U016535QCJ2) as it really doesn’t take much money to move the market (assuming you have a good model in the first place) 



That’s not to say their isn’t an edge here as any modelling that looks to try and model ‘trading flow’ as it can be very valuable.

*Tags: Strategies*

---

**Beeblebrox** - *08:52:54*

Yes, but of course that's harder to do! What I'm thinking is, if the strategy I have actually works in practice, the next step would be to then refine it to identify the movements earlier to get more value. The problem I foresee with that is you're more likely to then get false positives.

*Tags: Strategies*

---

**James T** - *11:32:38*

Yeah, I would imagine if you’re not very price sensitive (ie. have a large margin in a fundamental model) then generally taking volume would enable you to match more. I expect they do both though.

*Tags: Strategies*

---

**PeterLe** - *19:44:52*

Given the two following hypothetical scenarios, which would you consider to return the most profits long term, or indeed would the profits be approx the same :



Scenario 1: 1 Betfair Account running a proven strategy , £20 Liability Stakes

Scenario 2: 4 Betfair Accounts running the same strategy as above, £5 Liability Stakes



Assume all PC paying accounts and same comms etc

Just interested in opinions/thoughts and personal experience. Thanks

*Tags: Strategies*

---

**Mo** - *19:50:37*

[@UQL0QDEKA](@UQL0QDEKA) 1) is probably a bit better because you avoid situations where orders you would have matched get cancelled or matched by someone else in between orders sent by 2). If you are betting both sides then 2) will generate more commission so may help if you need to avoid 50% or 60% PC rates

*Tags: Strategies*

---

**PeterLe** - *20:34:36*

Thanks Mo, yes I think on the face if it, it seems a simple question, but there's many aspects that has had me thinking lately. Anyway Cheers Mo for your thoughts

*Tags: General Technical*

---

## 2021-03-05

**Misha** - *21:13:33*

When you have a model that good you just go for it

*Tags: Strategies*

---

**AndyL** - *21:14:21*

blimey, can you ping me the "model" please ? :grin:

*Tags: Strategies*

---

**AndyL** - *21:16:03*

although I would love to understand these models, I have a very good -ev model...!

*Tags: Strategies*

---

**AndyL** - *21:28:03*

not unless your model coincides with theirs...

*Tags: Strategies*

---

**AndyL** - *21:31:12*

only because i'm a novice with no idea of how to make money in these markets at the moment, so i'm trying to pick up ideas/knowledge...

*Tags: General Technical*

---

**Misha** - *21:47:38*

Even if the model is winning at over 10% and they are 10% of the volume, there are still the vast majority of the money to be made

*Tags: Strategies*

---

**Misha** - *21:51:40*

Also, the prices for these runners were generated hours before the start of the meeting. Only on extremely rare occasions were probabilities updated after racing had started. I found that fascinating that such a successful model didn't take into account anything happening close to the start time of the race

*Tags: Strategies*

---

**D C** - *21:54:50*

Seriously? So the favourite is removed and they don't rerun the model?

*Tags: Strategies*

---

**D C** - *21:57:35*

So how do they cater for that [@U016535QCJ2](@U016535QCJ2) ? If I have a set of probabilities and the even money favourite is removed the model probabilities on the other runners are meaningless. 10-1 shots will come into 5-1 shots ?? You have to recalculate value prices

*Tags: Strategies*

---

**Misha** - *22:01:21*

The point here is that absolutely nothing is essential in any one model. Even if you think it is, someone will develop a successful model without it

*Tags: Strategies*

---

**D C** - *22:01:24*

NR can get pulled at any time. I just find it amazing that someone putting as much money through the market as you claim they do and who has as sophisticated a model as you claim they have would not re-run the model estimates if the most likely winner was removed from the race.

*Tags: Strategies*

---

**Aaron Smith** - *22:02:37*

rescaling to 100% sounds to vague to me, but i m by no means an expert on what they do. Couldnt they run the models for these situations beforehand to be prepared?

*Tags: Strategies*

---

**Misha** - *22:03:28*

If there is one thing that the model is, it is mathematically rigorous

*Tags: Strategies*

---

**Misha** - *22:05:33*

That's not to say that the model couldn't be improved, but at the POT% they are getting at volume, it works

*Tags: Strategies*

---

**Misha** - *22:09:58*

BTW, on the note that nothing is essential for a model, I have an in-play tennis model (running successfully) that doesn't take into account who is playing :wink:

*Tags: Strategies*

---

## 2021-03-06

**PeterLe** - *10:31:05*

The last 14 years of my in running racing strategy is based on there being a true price (at any moment from the start of the event to the close) for each selection. Sure, that changes on each market refresh, but it’s always there.

[@U016535QCJ2](@U016535QCJ2) Id just say keep you mind open that this may be a possibility. Take a walk down that path and check it out for yourself.

*Tags: Strategies*

---

**Misha** - *11:20:55*

I never gauge performance on any single match/bet. I gauge performance over at least 1000 matches (which might be at 50,000 bets)

*Tags: Performance*

---

**Misha** - *11:22:50*

So in effect I have no idea how to recognise where a price is inherently wrong. I only know when my price is different from the market

*Tags: General Technical*

---

**Michael** - *13:52:54*

Saying that you don't know whether X price has positive, negative or zero value isn't the same as insisting that 0EV doesn't exist, it's just recognising that you don't know it. There are many beginners reading this forum, it's confusing and damaging for them to have you repeating this error as if it's a fact. When you say that there is _'no single "right" price for a runner'_ you're arguing that there is either no 0EV price or more than one. The former conjecture (the idea that there is no 0EV price) is impossible to reconcile with the idea of +EV and -EV as you obviously can't have both without a mid-point. The latter (more than one 0EV price) would imply that a given price can simultaneously be both -EV and +EV, in other words both the layer and the backer can make a profit on the same bet, this can't be true either on a single instance or on hypothetical repetitions.  So: Since there can't be less than one 0EV price and there can't be more than one how many 0EV prices are there for a particular bet at a particular instant? ONE

*Tags: Getting Started, Errors Debugging*

---

**D C** - *14:07:26*

100% this -&gt; "_*There are many beginners reading this forum, it's confusing and damaging for them to have you repeating this error as if it's a fact.*_". Saying that a true value/0EV price does not exist is basically saying that you don't know anything about or don't believe in probability theory and statistical inference and it is dangerous to propagate that notion to people new to the game who may not have the confidence to call out this flawed thinking.

*Tags: Getting Started, Errors Debugging*

---

**Michael** - *14:35:54*

Having said the above - I think I know where your confusion is coming from. Let me explain what I think you're thinking and why it's wrong: Say you had a 3 runner horse race that you knew nothing else about, you could reasonably price all runners at 3 and if you then evaluated that prediction in an information vacuum over many events it would turn out to be correct because one runner of the 3 will always win. If you 'paper traded' in your own private exchange then hypothetical bets at odds &gt; 3 would show a profit. Great, all your predictions look perfect, 3 is always 0EV.  Meanwhile someone else is looking at the same 3 runner races but they have access to some basic information about the relative quality of each horse that allows them to better price the runners. Will they agree that all horses are correctly priced at 3?  No. They will make some other model and generate from it a 0EV price that will be different to yours. Let's assume they validate that by testing over many markets so that it performs perfectly. So from your perspective (information blind) it looks like 3 is always 0EV and from his with some limited information he's generating 0EV prices that are not 3. That can make it look like there's more than one 0EV price - but that's an error. Consider what happens if you bet against him in an exchange, you've priced every horse at 3 but he knows better. Where's your 0EV now? You're backing any price he'll offer you that's &gt;3 and he's putting a deposit on a Ferrari. In this case both players have an ESTIMATE for 0V which looks perfect when evaluated internally but it turns out that one estimate is better than the other. That's because the notion of a 'true' 0EV price is predicated on a hypothetical state where all possible information is known. Anything else is an estimate that gets as close to that 'truth' as possible. I think your mistake comes from a failure to recognise that whilst your estimates of 0EV can reconcile perfectly when evaluated internally they wouldn't stand up to superior information and ultimately to perfect 'true' information.

*Tags: Errors Debugging, Strategies*

---

**Misha** - *14:45:07*

Everything is done on a probability of winning. There can be a multitude of successful models all doing this on the same market, often betting on opposite sides. Because to be successful, you only need a small margin over which your bets win (I aim for &gt;5%). I can lose a lot and still come out ahead

*Tags: Strategies*

---

**D C** - *14:48:51*

Logistic regression - you model the probability of a selection winning. You get a CI around that for sure - its just an estimate. Regardless, the true probability DOES exist even if we cannot calculate it with precision.

*Tags: Strategies*

---

**D C** - *14:51:59*

You can only determine "value" if you have an estimate of what the right price is (from modelling/whatever) and exploit market inefficiency when the available prices differ from it.

*Tags: Strategies*

---

**Misha** - *14:52:02*

It exists for every model. And is different for every model. (which is what I have been saying all along)

*Tags: Strategies*

---

**D C** - *14:52:53*

The TRUE probability, whilst unknown is fixed.

*Tags: Errors Debugging*

---

**D C** - *14:53:07*

The only thing that will differ is estimates of it from different models.

*Tags: Strategies*

---

**Michael** - *14:54:09*

_'It exists for every model. And is different for every model'_ That's exactly what I dealt with above. Did you read it?

*Tags: Strategies*

---

**Misha** - *15:10:08*

I'm certainly not telling anyone how to bet, because every system is unique

*Tags: General Technical*

---

**Misha** - *15:13:51*

I doubt my model would be working if it was incompatible with probability theory :wink:

*Tags: Strategies*

---

**D C** - *15:14:10*

Is it your model though Misha? Or is it your partners?

*Tags: Strategies*

---

**D C** - *15:16:09*

Fine. I wish you well and hope you are profitable. I am only speaking out because you are spreading opinions that are at odds with fundamental probability theory and I question the legitimacy of your knowledge given that is the case.

*Tags: General Technical*

---

## 2021-03-07

**Misha** - *04:38:34*

You can say what you like about my knowledge about probability, but what I have done works. I started modelling on in-play tennis at the beginning of October, and have used no techniques I had used previously for any other sport. So in 5 months from scratch I have got to here (by October I did have the required data in a database). Here is a plot of my lifetime bets, normalised for size (483,000 in total) - lifetime net profit is 4.3% at this point in time:

*Tags: Strategies*

---

**Misha** - *05:04:25*

Some random notes about my model:

*Tags: Strategies*

---

**Misha** - *05:06:29*

-) My modelling input data stops at June 30, 2020 (like building a horse racing model for use now ignoring the last 8 months of racing)

*Tags: Strategies*

---

**Misha** - *05:07:35*

-) I don't use live prices in my model, only the "starting" price of the match. Obviously I compare with live prices at bet execution time to know whether to bet

*Tags: Deployment, Strategies*

---

**Dave** - *09:54:40*

that's pretty cool. What happened in first 150K bets? Just updating model calibration etc?

*Tags: Strategies*

---

**Dave** - *10:26:28*

QQ to racing specialists: Have you noticed any impact to your existing strategy performance when deploying another strategy in a seperate market on the same event? I'm deploying some stuff on the place markets and worried I will impact price action on win markets. Hopefully starting off with minimal stakes should show negligible impact...

*Tags: Performance, Deployment, Strategies*

---

**Dave** - *10:26:49*

annoyingly impact is very hard to model in backtest...

*Tags: Strategies*

---

**Dave** - *10:42:00*

i guess that perspective is dependent on how liquid your market is and how much you expect your strategy to trade. At least for most of my strategies currently 4% would be too thin to be worth the time I put into it :laughing:  but obviously I would be incredibly happy if I was yielding 4% from 700K in a week for sure

*Tags: Strategies*

---

**Atho55** - *13:10:54*

Do you think the return of courtsiders will have an adverse effect on your model?

*Tags: Strategies*

---

**Misha** - *23:03:46*

I don't think courtsiders, if they exist, make any real difference at all. It's very difficult to make money by being extra fast because almost all "risky" prices are removed between points before any smaller matches. Maybe you "could" take some on offer after a point has been completed if there is money there at a reasonable price, but from my observations there is still little to be had, and you still need a good model to be right. Tennis matches swing often late in the match

*Tags: Strategies*

---

**Misha** - *23:04:52*

In terms of my model, would make absolutely zero difference, even if there was a "free for all" for courtsiders

*Tags: Strategies*

---

## 2021-03-08

**Michael** - *16:16:06*

The scale of the place market barely compares to the win, so whilst I'm sure people must be calculating place prices from the win market I doubt they're doing it the other way round. I wouldn't expect a new place market strategy to affect the win market at all. The other way round might be possible but in reality I doubt you'd see it. You're always trying to minimise your market impact so it's hard to imagine your win market bets leading to effects in the place.

*Tags: Strategies*

---

## 2021-03-09

**IndikaE** - *14:46:22*

Hi. A short question about using Monte Carlo methods for estimating a p-value for your strategy outcome. I have done it several times before for simple (football match-odds) strategies that simply back one outcome before game start. In these cases I simply use the probability implied by the odds, and simulate a large number of histories, to estimate the probability

of a better outcome than my observed outcome (assumption: low p-value means edge). However, I am currently trying to validate a new strategy that will place several orders in play. Validation suddenly becomes a harder problem, since the events are not independent - and the implied

probabilities are ambiguous. Intuitively, it would not be meaningful to draw independent variables for each order, since the outcome of events for a single market are perfectly correlated. Right? How would you go about this?  What probabilities could function as 0-hypothesis? Maybe pregame odds?

*Tags: Strategies*

---

**Mo** - *14:48:30*

Correct, it's a tough problem

*Tags: General Technical*

---

**IndikaE** - *15:21:39*

Thanks for your input - that actually got me thinking. Perhaps I could choose a number of random sub-strategies - each with only one bet per market - then do vanilla simulation for each sub strategy for each market, and finally do some sort of meta analysis on the set of p-values obtained for the sub strategies. The assumption would be if the probability of hitting a low p-value sub strategy is low, then I have an edge! Could this make sense? Not completely clear how I would do it - nor can I justify the method theoretically yet. I think this would work best, if the number of bets were equal for all markets -  but even when they are not, it might be a useful indicator if nothing else.

*Tags: Strategies*

---

**Michael** - *16:12:40*

I'm not a stats person so this is a question not an opinion: If you had only one bet per market you wouldn't have this problem right? And if all your bets were stuck by different people such that each person had only one bet per market each person would happily evaluate their own single bet markets in isolation and we'd have no problem with that. So if there's no causal relationship between the bets themselves  (IE you're not 'greening up') and the only link between them is that you placed them isn't it reasonable to treat each bet as if it was a separate market with a separate outcome? Obviously this is wrong - but wouldn't it get you to an acceptable answer?

*Tags: General Technical*

---

**Mo** - *16:45:34*

But I think this is an interesting, unsolved problem

*Tags: General Technical*

---

**Mo** - *16:47:12*

It's also probably less acceptable if there's some position dependent element to the strategy such that later bets depend on earlier bets

*Tags: Strategies*

---

**Michael** - *16:56:08*

It would be interesting to design a simple hypothetical strategy and work through an example. I'm thinking 'lay every time the prices shortens 10%' or whatever.

*Tags: Strategies*

---

**MacrcoPolo** - *17:23:33*

I'd probably bootstrap sample over the matches, then draw one order from each match in this case - agreed it's not an easy problem

*Tags: General Technical*

---

**Michael** - *17:29:28*

Again - a question not an opinion: but isn't taking one sample per match quite a lot worse because it over represents events where you had few bets and under-represents others where you had many? Surely that's a bigger problem than the independence of the bets..

*Tags: General Technical*

---

**IndikaE** - *17:32:59*

Might be - good point. That’s also why I mentioned earlier that this approach would only be really good if the number of bets are exactly equal for each market. I think the characteristics of the strategy should be considered before choosing an approach.

*Tags: Strategies*

---

**Michael** - *17:43:20*

....maybe I'm not following you. _Edit for anyone struggling to follow: I was misunderstanding him, I thought he was randomly choosing a subset of markets based on how many bets were in each market. That would have addressed the problem of some markets having more bets than others at the cost of a lot of data. In fact he's re-using the markets over and over, hence my comment below._

*Tags: General Technical*

---

**MacrcoPolo** - *18:20:31*

Not quite, you thought correctly - sample proportional to the number of bets in the markets, but it's the same as treating each bet independently. The idea is you wouldn't run this procedure once, you'd do it over and over generating a lot of subsamples, but the problem is it doesn't solve the problem :disappointed:

*Tags: General Technical*

---

**IndikaE** - *19:29:23*

Yes - I will take your advice. I have probably been overthinking it a bit. I am really only seeking confirmation that my edge is there. I have no interest in calculating VAR or anything. But my back tests do not look random at all(either I have a pretty decent edge, or I am unwittingly leaking information from train to validation). So I guess I should just use one of the ‘naive’ approaches and get on with it. Looking forward to being able to use the “check £ in account”-approach as soon as I get this strategy deployed(first time for me).

*Tags: Getting Started, Deployment, Strategies*

---

## 2021-03-10

**Lee** - *18:40:19*

What are peoples thoughts on analysing/dealing with strategies that may overlap with each other? Does it even matter as it's a separate strategy which could be something someone else is running?

*Tags: Strategies*

---

**birchy** - *18:48:07*

Personally, I treat them as individuals because my existing strategies are already part of the market data that strategy X is working with.

*Tags: Strategies*

---

**AndyL** - *22:15:02*

bloody difficult trying to find a profitable strategy, everything I look at just behaves as random outcomes

*Tags: Strategies*

---

**Michael** - *22:41:56*

Analysis wise I think the only thing you can do is identify the specific overlaps and flag them for analysis. This is also the sort of thing where you can use market level AvB tests to good effect (if you're patient enough). That's all a bodge though, I think the best solution is to unify all of your betting. You can unify your betting either by taking strategy A into account when triggering strategy B - as in: If strategy_b.conditions_met and not strategy_a.conditions_met....  or by having a single model/strategy that generates whatever bets are appropriate in a oner. Personally I do a bit of both. There is an overlap between my TPD and non TPD betting so in TPD markets I take the former approach and if a market doesn't have TPD I have everything unified into one 'strategy'. After all, the effect of more strategies is just more bets at certain times so why not do that explicitly and in a way that structures the data well for analysis later?

*Tags: Strategies*

---

## 2021-03-11

**Misha** - *00:08:07*

It's in the same vein as day trading, or god help us, high frequency trading. I don't think there is any net benefit in it at all. However, in reality you aren't ever in a one-to-one transaction with anyone, and a lot of the time, even if successful, you are taking the opposing side to others who have a lot of money in the game. In my chosen sport, in-play tennis, I am sure with Challenger matches that I am betting against some large individuals/organisations with a lot of money behind them, so I can only assume they have some amount of success as well

*Tags: Strategies*

---

**Paul** - *08:38:55*

I think most successful betfair (manual) traders would automate if they could, and they would increase transaction volume if they could - does that mean manual trading == good and hft algotrading on the same strategy on the same market == bad?

*Tags: Strategies*

---

**Jonjonjon** - *08:46:24*

A lot of what we do on Betfair can be thought of as liquidity provision. If my counterparties did not trade against me, their prices would be worse. I give them a better price, and make a little bit of cash each time as I have a superior pricing model.

*Tags: Strategies*

---

**Paul** - *08:55:29*

To go back to [@U016535QCJ2](@U016535QCJ2)'s argument - the "purely beneficial" way to bet then might be to take a buy and hold investment position: back an outcome pre-event and sit back and wait. Billionaires have been made in Hong Kong on that strategy, but it's a tough game...

*Tags: Strategies*

---

**Mo** - *11:38:26*

I think when most people say HFT is bad what they mean is latency arbitrage is bad

*Tags: Performance*

---

**Jonjonjon** - *11:40:03*

Who is latency arbitrage bad for? And are they people we should feel sorry for?

*Tags: Performance*

---

**Jonjonjon** - *14:16:49*

I don't think it's as black and white as that.



Suppose a market has a BATB price of 2.0. Mr Flumine User improves the price by offering 2.02. Mr Caan comes along and takes the best price.



If Mr Flumine User wasn't there, MC would have received odds of 2.0 vs 2.02. So he is better off with lots of Flumine users offering prices.

*Tags: General Technical*

---

**Dave** - *15:48:47*

latency arb = Not useful for society.

high frequency MM/low latency liquidity provision: useful for society

*Tags: Performance*

---

**Unknown** - *16:29:40*

latency arb is bad for most market participants - unless there are mechanisms that protect the market maker, they'll be less confident about quoting a tighter spread due to fear of being picked off. Less tight spreads = higher transaction costs for everyone

*Tags: Performance*

---

**Dave** - *16:30:11*

betfair's speedbump is one of those glorious things that helps a market maker quote tighter than they otherwise would

*Tags: Performance*

---

**Lee** - *18:18:23*

I find it a potentially interesting problem. In my case it's not really affecting me yet as i'm not pushing stakes hard at all. But more from the point of view if you have two separate strategies that want different prices from the same tick update I could view that many ways. I could be asking for a better price on one strategy, maybe i'd rather a lower price to improve matching, maybe increasing stakes of the lower priced strategy might take away a decent chunk from the latter.

I guess it still goes back to what you was saying, a unified strategy that could then be AB tested.

*Tags: Strategies*

---

**James T** - *20:25:53*

There’s no getting away from sports betting being a zero sum game. Net winners offset net losers and bookmakers and exchanges take their cut. You can argue that “providing liquidity” adds value to the system, but ultimately if you didn’t take the other side of the bet and win then no one would lose. So financially, winners are clearly not contributing to the system. 



In my opinion, the only way to frame the “non-zero” game argument is to say that value is created through the entertainment of gambling. Losers are happy to pay a premium for the excitement of placing a bet even though they lose in the long run (otherwise why would they continue), and winners are compensated for offering that service (for the financial risk and work of pricing up the bets). So it is a net positive system in my opinion. As for whether people should be allowed to gamble or how, or whether it’s rational to gamble if you’re a net loser, is a separate question. 

*Tags: Strategies*

---

## 2021-03-12

**Unknown** - *12:39:42*

Hi Dave, what do you mean by speedbump and how is it helpful for a MM?

*Tags: Performance*

---

## 2021-03-13

**Peter C** - *11:34:08*

I've been exploring a single trigger for pre-off horse racing markets that I'm hoping to get some advice on. The strategy places a lay bet if the single trigger is met. Over 1800 markets the trigger returns a profit (beats fair value 56% of the time, and sum of lay bet prices is less than sum of fair value prices) if I look at a subset of all runners, but a loss if I look at all runners (warning sign?). Even though the trigger returns a profit, it is a significantly bumpy ride. I looked at the dose response in relation to the strength of the trigger, and there is a tiny positive correlation (size of trigger value/value found). Honestly, I think that the trigger may simply be skimming some noise before the market is particularly efficient because if it places bets only in the last hour before the off then it makes a loss, or may simply be overfit.

I tried to improve the consistency of the results by:

• encouraging the strategy to take similar total stakes across each runner bets are placed on

• increasing allowed selection exposure

• 'cashing out' bets that aren't value at the off (improved consistency but made the trigger produce a loss)

• only looking at bets in certain time periods

None of these things helped with consistency of results. I'm interested in what sort of things I should be thinking about to improve consistency of results (other than removing root causes of bad bets or a secondary trigger that confirms value), and whether the trigger itself appears worthless. I'd appreciate any advice anyone is willing to give, even if it's throw it all away and start again! Cheers

*Tags: Strategies*

---

**birchy** - *12:03:12*

I'm in a very similar boat to [@U01LD279D16](@U01LD279D16). I have a preplay, straight laying strategy that lays at &lt; BSP on &gt;80% of bets, so shouldn't that _theoretically_ produce a profit long term? The other 20% are slightly &gt;= BSP but only by small margins. There is an option to back at BSP to reduce volatility but that feels a bit dirty and I've not yet calculated if it would actually be +EV.

Also, what's the best way to calculate expected drawdown for a strategy?

*Tags: Strategies*

---

**birchy** - *12:07:51*

To add to the above, I've gone live with the strategy after extensive backtesting and am seeing much greater volatility than backtesting suggested.

*Tags: Deployment, Strategies*

---

**Peter C** - *12:12:32*

I meant equalising the stake of the back bet you've laid as far as possible. So, for example, if the strategy laid the base stake x3 on a single runner, when the chance arises to lay a different runner the strategy would take a bigger stake to try and even this up

*Tags: Strategies*

---

**Mo** - *12:33:51*

Yeah that's what I'm saying. I think the more important thing is what I mentioned in the main channel. The first thing to do is to work out if you are actually making value bets according to BSP. If not, then the problem is the strategy itself rather than the variance

*Tags: Strategies*

---

**James T** - *12:54:23*

Taking prices with 3% margin against BSP is decent. Especially pre-off. It will be limited by how scalable it is. If it’s trading an hour before off time, then your backtests might be over optimistic because of market impact.  



You said the returns are bumpy. Is that because you are betting outright rather than closing? Why don’t you close at BSP (or at the off) for now and verify this trigger work in reality? Treat your open bets an hour before the race separately (give them their own strategy ref) to your closing bets, so that it’s easier to analyse them afterwards. 



If you’ve got 3% margin against BSP you’ll still make a healthy profit by closing (if the trigger pans out in live betting). 

*Tags: Deployment, Strategies*

---

**Paul** - *14:03:02*

I'm surprised by the notion that BSP is straight called out as fair price. I've not looked but are you saying that BSP consistently reflects winning percentages (2.0 = 50% win rate, 1.5 = 66.6%, 10.0 = 10%, and so on)? I've not looked because I've assumed it's not helpful

*Tags: General Technical*

---

**Mo** - *14:08:59*

100% valid to question if BSP is the true price but these assessments are a starting point

*Tags: General Technical*

---

**birchy** - *17:06:49*

That's exactly how I've been operating. It's always a bit of a twitchy-bumhole moment when you go live with a strategy and it turns out that you started on a down trend. I'll man it out and see how it goes. :joy:

*Tags: Deployment, Strategies*

---

**James T** - *17:42:08*

Of course the other issue is how to close at BSP without knowing what it will be. That’s more of an execution puzzle, and there’s several ways to do it. None of them will be perfect, but all of them should be close enough. So you’ll still end up with some open bet going into the race. 



Or you could close perfectly at the scheduled off. But then you’d be taking prices which causes extra slippage. 

*Tags: General Technical*

---

**Michael** - *21:58:46*

[@U01LD279D16](@U01LD279D16) - I've just been reading the responses above to your question. you've received a tonne of excellent advice but there's one little thing in your question that's needling me that hasn't been addressed. You say that your strategy looks profitable _'if I look at a subset of all runners, but a loss if I look at all runners (warning sign?)'._ Well yeah - that might be a warning sign. I suppose the questions are; how is that subset generated, does it make sense and when did you think of it?

*Tags: Strategies*

---

**Michael** - *22:05:55*

However you test your data there's no substitute for standing back from both your strategy and your process and just asking whether it makes sense, whether you understand (or at least have an idea) where your value is coming from and whether your process was likely to lead to a false or real result.

*Tags: Strategies*

---

**Michael** - *22:28:01*

Thinking about it; we don't seem to talk much about evaluating your process but I think it's one of the best things you can do. _'Was what I've just done likely to produce a bullshit outcome'_ is a really important question that could either give confidence or raise your level of statistical vigilance.

*Tags: General Technical*

---

## 2021-03-14

**Michael** - *09:03:49*

The way I see it that would offer a different starting point - I'd look again at races with lots of quite evenly matched runners, which will include races that aren't handicaps and exclude some that are. That would pull your analysis back towards looking at market dynamics. Obviously if it were some kind of fundamentals model that would be different.

*Tags: Strategies*

---

**Michael** - *09:15:48*

If you said the same thing during the summer I'd point out that flat races are the main thing going on so they're bigger markets and suggest you check whether simple market size could be indicative, but if you're looking at recent markets then that's all weather racing and NHF, so I'd say it's not a great sign. In general; if your model is all about market behaviour it's probably safest to confine your analysis to market factors unless there's some plausible reason to expect a particular group of markets to behave in some particular way which you can then look for a way to verify.

*Tags: Strategies*

---

## 2021-03-15

**Tom** - *21:55:13*

Hi All, apologies for the noobie question. I'm looking to build a horse racing strategy and wonder about the amount of data that you would recommend I first use for researching and then backtesting with. I've had a look at the free data available from BF and only downloading the BASIC plan for a year or two, there is a ton of data there. What would you recommend please?

*Tags: Strategies*

---

## 2021-03-16

**birchy** - *08:51:21*

[@U01Q94VE1AB](@U01Q94VE1AB) make marketrecorder your first priority.

[https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py)

*Tags: General Technical*

---

**Jorge** - *10:29:11*

In the example above, does flumine use S3 to store the recorded data? I'm not using flumine right now but this looks very interesting! (I keep the data in my local machine so it is getting huge)

*Tags: General Technical*

---

**Jorge** - *10:38:19*

Flumine's MarketRecorder creates a 'market_id' file and appends the MarketChangeMessage ('mcm') messages that come from Streaming to it, right?

*Tags: General Technical*

---

**Jonjonjon** - *11:10:58*

Yes. I think I do. Your comment about flats and jumps inplay has also given me some food for though. Thanks for the helpful ideas!

*Tags: General Technical*

---

**Tom** - *12:00:26*

Thanks for the above [@U016TGY3676](@U016TGY3676). Are most here using Flumine or Betfairlightweight please?

*Tags: General Technical*

---

**birchy** - *12:11:46*

So BFLW was [@U4H19D1D2](@U4H19D1D2)'s first generous offering and is the lower level implementation of the betfair API. Flumine takes bflw and adds lots of features (including backtesting capability) for "real world" applications. If you're just starting out, Flumine is definitely the way to go.

*Tags: Feature Engineering*

---

**birchy** - *12:32:04*

The AWS thing was new to me when I started, but S3 is definitely worth setting up. I run a modified marketrecorder that saves the data into it's corresponding month based on projected start date/time. It just makes it easier to pull out specific months for backtesting, i.e. summer flat racing is &gt; winter racing for one of my strategies.

*Tags: Getting Started, Deployment*

---

**Jorge** - *17:25:52*

Okay thanks! I am using bflw for recording because I find it simpler to use the MarketBook object returned by Streaming instead of the 'mcm' messages

*Tags: General Technical*

---

**Jorge** - *17:27:40*

How do you avoid that your Python Script loads the complete fills to memory when storing files in S3? Do you always keep the connection with the cloud open?

*Tags: Performance*

---

**Jorge** - *17:30:25*

Yes, bflw is using deltas but converts these 'mcm' messages to a readable MarketBook object

*Tags: General Technical*

---

**liam** - *20:08:16*

You still get access to the MarketBook once processed through the streaming.historical function but the data is stored as deltas, space saving is huge compared to saving full market books 

*Tags: General Technical*

---

## 2021-03-17

**Jorge** - *07:58:44*

Okay I will use flumine and have another process convert the files to a csv readable format

*Tags: General Technical*

---

**Jorge** - *08:06:33*

Is it possible to run flumine just as a "market recorder", with all the other features turned off? (for ex. turning off OrderStreaming)

*Tags: Data Quality, Feature Engineering*

---

**liam** - *08:07:16*

Yep, this is a client setting [https://github.com/liampauling/flumine/blob/d9c0d51a94c32cb5972d45a0806791cd1e72e56b/flumine/clients/baseclient.py#L26](https://github.com/liampauling/flumine/blob/d9c0d51a94c32cb5972d45a0806791cd1e72e56b/flumine/clients/baseclient.py#L26)

*Tags: General Technical*

---

**Alessio** - *08:24:10*

And if yuo read the code: [https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py)

*Tags: General Technical*

---

**Jorge** - *08:37:38*

Looks simple, and does flumine keep the connection with S3 always open? And appends lines to the files? Or does it upload the local files?

*Tags: General Technical*

---

**Jorge** - *08:39:33*

Got it, and isn't the upload of "big" files slowing down the whole process? This is python so I guess this is not done in parallel

*Tags: Performance*

---

**Alessio** - *08:42:02*

Also, tbh, while liam graciously fixed a related bug last week, I think running a recorder in a different process from your strategies is better. 1) you can record more than you actually bet on 2) in case one of the two starts having trouble doesn't affect the other one

*Tags: Errors Debugging*

---

**Alessio** - *08:42:39*

(although flumine is robust enough since strategies run into separate exception blocks, I still find it better)

*Tags: Errors Debugging*

---

**liam** - *08:47:55*

flumine, simple single process event driven, I don't want to start this again though :joy:

*Tags: General Technical*

---

**liam** - *08:56:24*

Exactly, if on the flumine readme page it describes how you have to setup MySQL, Redis, streaming process and a event process before you can even start processing data I think the take up would be zero, instead its 10 lines of pure python with the ability to add all of the above if required

*Tags: Getting Started*

---

**Jonjonjon** - *08:57:54*

Flumine's single process keeps things simple. I've been playing with Bitfinex's Python API. It is multithreaded, and that causes a headache with preventing duplicate orders being fired off.

*Tags: General Technical*

---

**Alessio** - *08:59:14*

Aren't they getting investigated heavily with a risk of shutting down soon? (e.g.: [https://siliconangle.com/2021/02/23/ifinex-agrees-pay-18-5m-settle-new-york-tether-lawsuit/](https://siliconangle.com/2021/02/23/ifinex-agrees-pay-18-5m-settle-new-york-tether-lawsuit/))

*Tags: Deployment*

---

**Alessio** - *19:19:27*

and flumine re-scrapes. it dumps the latest as you can see

*Tags: General Technical*

---

## 2021-03-19

**IndikaE** - *21:03:14*

I am papertrading my first strategy implemented in flumine, and I occasionally have latency issues. My strategy is really a straight (football) value betting strategy that will just take a backprice if it is +EV according to my model and lay when it is -EV. Dead simple with no need for precise timing, since the model operates on a timescale of minutes. With such a strategy is latency even something to worry about? To me it seems obvious that it is not, but being a noob and all, I’m a little scared I might be missing something in my reasoning. Apologies for the vagueness of the question!

*Tags: Performance, Strategies*

---

**Dave** - *21:21:33*

The fact that you have latency issues is odd. Are you running on a machine in a data centre? Or is it because you're doing some heavy computation so it becomes a bottleneck? Probably best to solve those issues

*Tags: Performance*

---

**Dave** - *21:22:30*

(FWIW I run a very similarly structured strategy on football and just execute over the REST API, no flumine there altogether)

*Tags: Strategies*

---

**IndikaE** - *21:54:27*

Currently running it on my MacBook Air m1. I wouldn’t say I do heavy computation. The computation is done asynchronously in a different process, and the Flumine process reads the predictions from files(could switch to redis?), and does very little except for some simple bookkeeping. I do have some custom middleware for keeping track of  price movements that I might want to optimize, though. I’ve noticed that the warnings occur quite frequently right after starting Flumine, but then seem to disappear after a time. The reason why I want to use flumine is because I plan to add some HFT element to improve the prices I am getting in the future. Anyway, if you recommend that I eliminate latency issues whatever the cause, I will get to work straight away! Thanks for the input :slightly_smiling_face: 

*Tags: Performance*

---

**Dave** - *21:56:19*

Should be fairly straightforward to eliminate the latency warnings, it's not something I'd put up with tbh! Is your internet connection a bottleneck maybe? Perhaps try unning it on AWS and see if you get the same warnings.

*Tags: Performance, Deployment*

---

## 2021-03-20

**birchy** - *19:54:22*

[@U01H1ALH18F](@U01H1ALH18F) synchronised clock is important because Flumine compares local time to response timestamp. On Linux/Mac you can use an NTP daemon. I also had issues similar to yours on startup when subscribing to 1000's of markets.

*Tags: General Technical*

---

## 2021-03-22

**Michael** - *18:24:32*

Just lick of a finger but I'd say that's not that many for quite a thin edge, but it also depends on your development methodology. Doing a MC simulation won't do any harm but you should be aware that analysing multiple bets in the same market with MC is problematic. Where are you at with your betting more generally - do you have strategies betting live and making money?

*Tags: Deployment, Strategies*

---

**mandelbot** - *19:06:34*

I'll work on doing a Monte Carlo, never done one before. I do have a few profitable live strategies though I have to say my approach is pretty basic compared to many on here. I've only just moved to working with python and bflw in the last few months having come from automating strategies on betangel. Still learning the ropes as it were.



You're right 3k does seem a bit thin, unfortunately that's the extent of my historical database for now. I am also betting on bigger odds but &lt;100 so probably that makes 3k seem even thinner. I figured though since I was beating BSP then I might be doing something right.

*Tags: Data Quality, Deployment, Strategies*

---

**Michael** - *19:21:00*

The reason I asked about your existing betting is that that might be a simpler way of calibrating your confidence. If you did the same type of thing that you did last time then you'll probably get the same type of result. All the statistical tests are wide open to error if your development process is weak because so many development pathways move inexorably towards excessive overfitting. I think it's also worth asking what more or less confidence would do for you in terms of informing your actions. In all likelihood the best approach will be to just give it a go with minimal stakes, having more confidence wouldn't make starting off more aggressively into a good idea and having less confidence probably won't stop you from having a go even if only for the sake of learning.  You probably already know in your gut whether having a go is a good idea. Think carefully about your reasoning and where your value comes from, if you can say that your reasoning is sound and you can identify why you expect your bets to have value then you're good to go.

*Tags: Errors Debugging, Strategies*

---

**Michael** - *19:29:21*

Is this some sort of fundamentals model or a market based thing?

*Tags: Strategies*

---

**PeterLe** - *19:47:00*

[@U010GM77S4W](@U010GM77S4W) [@U016TGY3676](@U016TGY3676) uploaded a Monte Carlo python file last week. Check that out before writing your own, it’s easy to run

*Tags: General Technical*

---

**birchy** - *20:46:13*

Just to forewarn you, I had a very similar result from Flumine backtesting which also went in the opposite direction when live. Mine was for ~1700 markets and ~9000 matched bets out of ~17k bets placed in backtest and double those quantities for live bets. Monte Carlo suggested that the backtest P&amp;Ls were &lt; 0.1% chance of being random...i.e. this was a strong edge rather than a lucky streak.

Have since run the backtest on some new data and had the same result as live betting.

Conclusion: strategy was inadvertently over-fitted.

*Tags: Deployment, Strategies*

---

**birchy** - *21:40:57*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1615824484140500?thread_ts=1615824484.140500&amp;cid=C4HL6EZTQ|https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1615824484140500?thread_ts=1615824484.140500&amp;cid=C4HL6EZTQ](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1615824484140500?thread_ts=1615824484.140500&amp;cid=C4HL6EZTQ|https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1615824484140500?thread_ts=1615824484.140500&amp;cid=C4HL6EZTQ)

*Tags: General Technical*

---

## 2021-03-23

**mandelbot** - *09:09:11*

Im down about 500 pounds so far on test stakes though the bets that I have taken have beaten BSP so far. Will keep it running for the rest of the month. I doubt I'm over fitting as I haven't filtered it at all (except im not taking bets &gt; 100) and it is based on what I've personally noticed happen in the markets. I'll run your montecarlo code and see, might PM you with some noob questions at some point :D

*Tags: General Technical*

---

**mandelbot** - *09:16:10*

I've also been back testing the inverse of this strategy which also seems to be an edge. Would you say that is further evidence of an edge?

*Tags: Strategies*

---

**mandelbot** - *09:40:16*

Well it's not in great shape already, though it seems within the realms of what is to be expected for some periods (judging by the plot). How do I determine when it's time to hang in the towel for this particular strategy?

*Tags: Strategies*

---

**mandelbot** - *10:06:51*

Yes It's been betting 4 pounds because the strategy is firing 2 pounds twice, not sure how to fix that...

*Tags: Errors Debugging, Strategies*

---

**Michael** - *10:07:24*

Code bug. Switch it off and fix that.

*Tags: Errors Debugging*

---

**Jonjonjon** - *10:11:07*

There is a max trade count option, in flumine strategies, to limit how many bets you can fire off

*Tags: General Technical*

---

**Jonjonjon** - *10:40:56*

The source code is here: [https://github.com/liampauling/flumine/blob/7f6a4543b4239fa24b59c2a74b7df7f447fc9812/flumine/strategy/strategy.py](https://github.com/liampauling/flumine/blob/7f6a4543b4239fa24b59c2a74b7df7f447fc9812/flumine/strategy/strategy.py)

*Tags: Strategies*

---

**Paul** - *10:57:31*

To those using Kelly: are you using a derivative that includes commission as part of the calc, or are you using fractional so it's "covered", anyway? I'm also looking around for a derivative of Kelly for multiple bets in the same market, but only solutions I can find are rather "exotic" (e.g. the Edmund Noon paper which resorts to a level of maths I'm not up to yet).

*Tags: Strategies*

---

**Atho55** - *11:12:17*

It could be that the BSP`s you are selecting are trending in another direction. This is BSP floored Odd 4.1 for example [http://prntscr.com/10tduhx](http://prntscr.com/10tduhx) It`s been chosen because it does slightly favour Back rather than Lay which could be another reason why your strategy is taking a down turn. [http://prntscr.com/10te12r](http://prntscr.com/10te12r)

*Tags: Strategies*

---

**mandelbot** - *12:58:18*

So I ran [@U016TGY3676](@U016TGY3676)'s Montecarlo code and got "Montecarlo beats strategy 13.93%" of the time. Can someone explain please? Does this mean a random strategy beats mine ~14% of the time? Is that high or low? Also thanks for the code [@U016TGY3676](@U016TGY3676).

*Tags: Strategies*

---

**mandelbot** - *13:19:39*

I've now run on a subset (odds &lt; 100) and found MC beats strategy 0.39% of the time.

*Tags: Strategies*

---

**birchy** - *13:24:17*

It basically means that your strategy has a 14% that it is just a lucky streak. The lower the %, the more likely you have an edge. Less than 1% is where you want to be. FYI, my over-fitted strategy came out at 0% on 10K iterations and 39% for actual bets.

*Tags: Strategies*

---

**mandelbot** - *13:26:10*

Well that's encouraging! May I ask how many different parameters you had for this strategy?

*Tags: Strategies*

---

**Michael** - *16:57:56*

I think it might be worth considering your methodology. Every time you 'filter' or change your strategy then check again it becomes more likely that you'll over-fit and no MC will detect that - I'm sure that's what [@U016TGY3676](@U016TGY3676) experienced, it's what inevitably happens if you iterate strategy changes over a fixed data set. A different way to interrogate the soundness of your strategy might be to isolate the individual conjectures that it's based on and see if you can evidence them using simple reasoning, graphs and so on. In other words - step back from your bets and find ways to check if the underlying strategy is based on sound thinking.

*Tags: Errors Debugging, Strategies*

---

**Michael** - *16:59:48*

If the underlying thinking is sound then you'll be fine, but if your strategy is a tower of arbitrary filters that produced the best return on a particular data set then it won't work.

*Tags: Strategies*

---

**birchy** - *17:48:40*

With my strategy that I inadvertently over-fitted, I'd basically been using the same training data over and over (because I had no other data at the time), using various triggers and ideas until I ended up with a simulated P&amp;L that was satisfactory. I've never actually looked at the training data manually but unintentionally found the sets of triggers and values that hit the sweet spot _for that dataset._ Meanwhile, I was gathering more data for use as a testing sample and that was when I realised what I'd done. An expensive lesson was learnt. :grinning:

The other problem was that this was straight laying only and I'd calculated that a maximum drawdown of &gt; £1000 was to be expected, so even at -£700, I left it playing live. It went up, it went down, but the overall trend was slowly down. Luckily, I have some other strategies that earn small amounts at lower risks, but truth be told, I managed to undo 6 months of profit in a matter of weeks.

*Tags: Performance, Deployment, Strategies*

---

**birchy** - *19:02:52*

[@ULDAVFDRP](@ULDAVFDRP) Yeah, I've always used train/test data but this was for a new sport. Was a bit premature going live TBH, but then again I keep reading that live betting is far more valuable than backtesting...

[@UGV299K6H](@UGV299K6H) sometimes shit turns to gold. I'm not overly bothered about losing a few hundred quid and see that as a necessary expense in the search for the golden eggs. I lost ~£200 on the above strategy, so not a major issue. I find the problem with asking for too much value often ends up with only the "wrong" bets getting matched more frequently, so seems counter intuitive. There is always the option to hedge bets, but again that's counter intuitive. Surely _some_ loss is to be expected when testing strategies?

*Tags: Deployment, Strategies*

---

**Michael** - *19:40:12*

Yeah I hope so [@U016TGY3676](@U016TGY3676) I wasn't getting at you just sympathising. I'm not sure how to calibrate how much loss to expect. My position is a little different in that perhaps I have a bit more experience, but I don't really expect any significant loss. I'm not really suggesting _'asking for more value'_ I'm talking about only posting bets when there's a stronger signal of value, how much value you ask for is then a separate consideration. That shouldn't give you the adverse selection you're talking about. Say for example your signal was weight of money - there's obviously a range of possible values and you're going to have to make a decision that when WOM &gt; X that's when you bet. So if that's your new bot I'd set X nice and high for a kick off. Obviously that's a very simplistic example but you can apply the same principal to anything you might automate, the decision to bet is essentially binary but there are continuous thresholds underlying it that can be adjusted. This is super simple if you're estimating EV but even if you're not you should be able to identify when your signals are stronger and confine yourself to those opportunities to start with. I'd be really suspicious of a trigger that was totally binary, to me it seems like common sense that you should be able to characterise a given opportunity as more or less profitable.

*Tags: General Technical*

---

**birchy** - *20:24:49*

[@UGV299K6H](@UGV299K6H) Ultimately, all triggers end up being binary, though? For instance `if wom &gt; 0.85` ends up being either True or False. I generally fire off a range of bets a few ticks apart to catch any "overshoot" and (theoretically) improve my perceived value. Does that seem sensible? That does of course increase my risk, so I'm also curious about how you reduce liabilities, i.e. do you try to balance risk per runner, place bets on both sides (unintentional hedging) or simply let bets ride? It can take a long time for edges to show their true colours, so I'm wondering how you determine what is an acceptable amount to lose when starting a strategy that may take a few thousand bets to settle?

*Tags: Strategies*

---

**birchy** - *22:26:20*

As [@UGV299K6H](@UGV299K6H) correctly stated, it can be majorly complicated to MC bets by market, so the generally accepted "near enough" solution is to treat each bet as an individual. Ultimately, the MC simulation only tells us if each bet is, on average, +EV and is by no means a definitive answer to whether or not your strategy will profit long term.

*Tags: Strategies*

---

**Beeblebrox** - *22:27:56*

Yeah, I guess it's a good enough approximation as things will average out in the general mish mash.  Interesting problem though.

*Tags: General Technical*

---

## 2021-03-24

**Michael** - *08:20:34*

I think it's worth emphasising that whilst MC is a useful tool it can't safely be used to validate a poorly developed strategy, only to analyse a well developed one. It's no good doing a ridiculous over-fit then trying to Monte Carlo it into reality, 'garbage in, garbage out.'

*Tags: Strategies*

---

**Dave** - *23:22:24*

generally done most of my calibration looking at charts in sample, calibrating based on that and then measuring out of sample performance

*Tags: Performance*

---

**MacrcoPolo** - *23:26:17*

Mainly for building fundamental models

*Tags: Strategies*

---

**MacrcoPolo** - *23:37:19*

Yes. You can model interactions if you wish, but usually not necessary - Do you want perfection or 'its good enough'?

*Tags: Strategies*

---

## 2021-03-25

**river_shah** - *07:19:33*

How do you guys think about maximal scaling for your strategies? Have typically gone with heuristics such as don't be more than x% of expected matched volume and or bet size/ frequency needs to be less than y and z (usually derived from observed size and update rate of offers). What can I do to make this more rigorous and what other factors would you recommend considering? Thanks 

Please assume that for all practical purposes we are clear of kelly constraints i.e market capacity is the limiting factor.

*Tags: Strategies*

---

**PeterLe** - *08:01:01*

Morning, I have a set of real data for a strategy that has been running seven months. What would be the best way to calculate the most probable maximum drawdown in the future? Could I estimate this as some form of function of a MC simulation? (The liability (stakes) are increasing proportionally to the growing balance too). Thanks for any thoughts

*Tags: Strategies*

---

**Mo** - *08:09:52*

Easily solved with a Monte Carlo simulation, yes

*Tags: General Technical*

---

**PeterLe** - *08:25:58*

Yes thanks Gents; I was just making a cuppa and thought this was more easily calculated that I first thought. So to keep things simple assume I was using a fixed stake and i know that for eg a MC simulation on past results were 10% (ie MC beats strategy) then i would create a go forward model and use a random number generator. Then run over a large number of iterations and count the longest sequence of losing runs ? This would be better actually that trying to calculate it in monetary terms as the loss would be proportionate to the profits at that time. Thanks again.

*Tags: Errors Debugging, Strategies*

---

## 2021-04-04

**Atho55** - *10:02:06*

I have a football related question. At the start of a match there are odds for each correct score. Once a goal is scored and the dust has settled are the new correct score odds in any way relative to the original odds or are they now newly derived odds based upon the goal time and score. Have started to build a tool that returns the outcomes of past games based upon the Event Minute (goal time) and Actual Score and looking to do a check on the odds that BF offer during the game. This is 6m and the score is 1-0 from the Premiership. Is there a source of data anywhere that I can be pointed at. [http://prntscr.com/113swwm](http://prntscr.com/113swwm)

*Tags: General Technical*

---

**thambie1** - *10:22:56*

Hey [@U01PQ9SR9MJ](@U01PQ9SR9MJ), not sure I fully understand what you're trying to do. The available odds on the correct score market is simply the odds that people are willing to bet that the event will/will not occur. This can be based on a myriad of factors. It's definitely based on the current score of the match, maybe some people's models care about the time each goal was scored, maybe not. There is surely a statistical relationship between the odds before and after a goal is scored as well.

*Tags: Strategies*

---

**Oliver Varney** - *11:00:34*

[@U01PQ9SR9MJ](@U01PQ9SR9MJ) I still think you are data mining, i.e. applying filters to historical data till you get a profit

*Tags: Data Quality*

---

**Unknown** - *11:10:01*

Generating these kinds of "average" fair values means that you will most likely be filled at the worst instances. The odds in a given match are a function of a variety of parameters, including historical performance, current game state which takes into account which players are playing, who's injured etc. You cannot generate any reasonable fair value for a specific match given the time into the match and current score

*Tags: Performance*

---

**Oliver Varney** - *11:55:07*

I may be missing something but everything you posted today suggest you dont understand what data mining is. This statement has serious flaws:  "If the match is at 6m and the score is 1-0, 18.84% of the games have ended 2-1. If this represents odds of 5.31 and the BF odds for 2-1 are 7.2 for example, is this value is what I am trying to work out. I do not have the BF odds to see how they compare to what I have tried to calculate ie 5.31."

*Tags: Deployment*

---

**Michael** - *16:38:21*

There's nothing to stop you from deriving prices simply by taking the current score and starting prices, but you'd have to bear in mind that that's quite a simplistic model and there's a lot of other information out there that others have that you're not incorporating, so you wouldn't expect such a model to  perform all that well. However, models that perform poorly can still be used to generate profits particularly if they are attached to some basic market making, where they can act to shield you from placing terrible bets. That reduction in adverse selection can be enough to get you into profit just by exploiting the spread.

*Tags: Strategies*

---

## 2021-04-12

**Michael** - *11:57:00*

Yeah - there's a sliver of value just in the fact that you're offering and willing to wait to match. If you're not sure how that works imagine doing it constantly on both sides of the book. The problem is adverse selection, which is where your bad bets match and your good ones don't. If you've got a model that's good enough to reduce that just a little bit and then offer into the market that's enough to get you into the black. That's why so many people get off to what looks like a flying start but then stall when they scale. They imagine it's the amazing performance of their model that's generating their profits at small stakes but they're really just partial market makers. When they scale beyond the capacity of those forces they get a reality check. You can watch this play out live on these forums.

*Tags: Performance, Deployment, Strategies*

---

## 2021-04-14

**liam** - *10:32:28*

depends on the book structure and language or is this a more theoretical question?

*Tags: General Technical*

---

## 2021-04-15

**Tosin** - *07:18:50*

just generally for lower latency. would a dictionary in python work well for low latency?

*Tags: Performance*

---

**Mo** - *12:07:00*

If you care about latency why use Python at all?

*Tags: Performance*

---

**Jono** - *14:55:45*

Ive recently managed to implement a trading script able to turn a small profit when certain odds exist on the exchange but is extremely reliant on the speed at which it is able to check the exchange. I am wondering the best way to move forward with trying to upscale the program to run on several events at once and potentially tackle the speed issue in this second version i am writing. I need to be able to check as many 1X2 markets on the exchange as quickly as possible in order to be profitable. When testing, if the time between checking markets reaches ~20 seconds the profitability of this strat takes a large dip, so small scale running sequentially on a 3 or 4 events is do able but more than that results in barely breaking even/losses. I believe streaming the odds would be the first significant improvement to be made instead of calling the api in a long winded loop to look for changes in listed prices, and soon after looking into this as an option i ended up at the flumine docs - which look very promising. Before i dive in and commit to the flumine framework is this what is designed for or at least useful in this scenario; high speed retrieval of bf exchange info for efficient strategy execution?



Any help is really appreciated and if ive got the wrong idea about flumine or should try another avenue for what im looking to do please let me know your thoughts, cheers!

*Tags: Performance, Strategies*

---

**James T** - *15:18:28*

Betfair Streaming API is exactly what you want for this. And BetfairLightweight and Flumine is an easy way for you to get into that if you have no other means already. 

*Tags: General Technical*

---

**Jono** - *15:21:58*

Sounds great :slightly_smiling_face: are there different areas where one is better than the other? Why would someone choose flumine over their own bflw loops and routines and vice versa? Want to make sure the next version is built correctly

*Tags: General Technical*

---

**James T** - *15:24:37*

bflw and flumine are two different things. Bflw is a python wrapper around the streaming API allowing you to easily call API functions. Flumine is a trading framework (which uses bflw) which abstracts around the functions allowing you to create an easy trading loop, along with providing offline backtesting (simulating) with historical tick data. 

*Tags: Strategies*

---

**James T** - *15:26:00*

Bear in mind I don’t use either - that’s just my understanding of it. I’m more of a C# developer, but if I had no knowledge and didn’t have my own framework already, I’d have probably started with bflw and flumine. 

*Tags: General Technical*

---

**Jono** - *15:33:17*

Do you know if any of the operations are carried out in parallel either top level in flumine executing strategies simultaneousl or for bflw requesting market books for all events of a given eventTypeId? My own (very primitive) framework performs everything sequentially one event after another as im not terribly experienced in asynchronous programming but feel it would beneficial in this scenario. No problem if your uncertain of the inner workings

*Tags: General Technical*

---

**Peter** - *15:34:05*

I'd echo Mo's earlier comment ... "20 seconds!". flumine provides  near instantaneous and certainly sub second data from the Betfair data stream as it is pushed out to subscribers. I suspect (feel free to correct me if I'm wrong) that you're currently polling the Betfair API and so haven't yet experienced the speed of streaming.

*Tags: Performance*

---

**Jono** - *15:38:24*

their are two other apis i have to call for a successful iteration, firtst to a database then to a client who then finally executes the bf orders/alterations. Each back and forth is in the range of 3-5 seconds and since everything runs sequentially the times do begin to climb to worrying levels as number of events increase

*Tags: General Technical*

---

**James T** - *15:39:57*

I’m not familiar specifically about flumine’s inner workings so others will have to comment on its asynchronous / parallel processing...



But with regards to the streaming API, you get market price updates as soon as they happen on every market. So it’s not requesting data at defined intervals anymore. When a price changes on any market, you are notified within a few ms. You then process (eg. place a bet) based on that price update. 



There is a chance that two different markets or matches happen to update at the same time, and that is where you might need to consider asynchronous calls or parallel processing of placing / calculating bets if you don’t want one market to be processed sequentially behind another. 



But the simple fact that you are receiving information immediately as it changes will already be a massive gain on checking everything every 20 secs. 

*Tags: General Technical*

---

**Jono** - *15:40:03*

definietly want to restructure a lot of what has been written so far so want to make sure im putting the first foot right with the betfair side of things. Would you recommend flumine as the go to for building out a fast paced strat like what im talking about?

*Tags: General Technical*

---

**Jono** - *15:42:44*

yes, ive yet to bear witness to the majesty of streamed data and i get a little bit too excited thinking about the possibilities. the 20 second time range is the result of other obstructions and api calls i have to make in order for the strategy to run. Ill be mending and streamlining the vast majority of the process just want o make sure im getting started on the right foot with the bf side of things

*Tags: Getting Started, Strategies*

---

**James T** - *15:45:25*

With streaming, the “program loop” is no longer governed by fetching data and processing that fetched data, and more constantly receiving data and reacting to what’s in that data. 

*Tags: General Technical*

---

**James T** - *15:48:53*

If your strategy is based on stats or analytics on data of a fixed period, like calculating a moving average or something, then that does require a little rethinking since streaming data arrives at irregular intervals. 

*Tags: Errors Debugging, Feature Engineering, Strategies*

---

**Mo** - *15:50:38*

Can’t you make those calls in parallel? I measure latency in microseconds, not tens of seconds

*Tags: Performance*

---

**Jono** - *16:06:14*

hmmm im not sure if i would be able to do so. the strategy goes kind of like per iteration: middleware retrieves instructions/existing bet info from database api, use instructions to send payload to client api, client api then places new orders on bf based on instructions, finally order responses are then used to update values in database. The process then starts again

*Tags: Strategies*

---

**liam** - *16:08:36*

:point_up:dictionaries are quick (for python) have a look at the bflw streaming implementation 

*Tags: General Technical*

---

**Mo** - *16:08:38*

I suspect you can optimise a lot of it away. But short answer is yes flumine is the way to go

*Tags: General Technical*

---

**Jono** - *16:10:40*

sounds good are there any other docs beyond the github, or other example strats i should look into? no problem if not, the repo certainly has a lot to digest and get started on

*Tags: General Technical*

---

**liam** - *16:11:30*

Yeah check out flumine, at least as a starting point as it’s designed to be flexible, tick to trade is certainly measures in ms. You also have backtesting (market and now event) + paper trading straight out of the box 

*Tags: Strategies*

---

**Jono** - *16:13:25*

will get cracking on that then. thanks for the help everyone, as always its been invaluable :grin:

*Tags: General Technical*

---

**Dave** - *16:42:18*

Sounds like the database is the bottleneck here? Call to place orders/get a response should be in millis even in your current setup (ignoring any inplay delay)

*Tags: Getting Started*

---

**Dave** - *16:45:55*

Point being, flumine won't solve your problem if you are still going to your database at each tick (if anything maybe kill your db :sweat_smile: ). Maybe you need to replace your db with some kind of in-memory db, e.g. redis

*Tags: Performance*

---

## 2021-04-17

**Dave** - *16:09:01*

qq guys - is there anyway to precisely determine when an order was actually made available for matching? would `placed_date + estimated latency + inplay delay` be the best estimate?

*Tags: Performance*

---

## 2021-04-22

**IndikaE** - *07:09:06*

Football question: I have noticed that the hometeam win frequency has fallen a lot across all leagues since matches starting being played without crowds(no big surprise there). My model is performing alright, but much worse than in backtesting(using Jan-may data 2020). Towards the end of that period I see a performance decrease even in backtest(when some leagues started reopening with no crowd?). My training set stops in December 2019. I must do my split along the time axis, otherwise I will surely be leaking. How have other people been dealing with this significant change in base distribution of home/away wins? Do you have data on crowd attendance, and is that enough to fix it? I don’t like that this whole period should probably be held out when retraining in the future, so I’m building a new model that will predict probabilities relative to baseline win frequencies instead. I can only imagine that this has been debated before, so sorry if I’m reviving a dead topic.

*Tags: Errors Debugging, Performance, Strategies*

---

**Dave** - *09:08:52*

I think that it's a very difficult problem to fix given how extraordinary the pandemic effect has been. I have decided to just ditch the period Jan - Sept 2020 altogether for my more complex models. Then again our models must be very different...for _most_ of my models, anything further back than 6 months starts losing a lot of predictive power.

*Tags: Errors Debugging, Strategies*

---

## 2021-04-24

**Jonjonjon** - *16:35:59*

Very pretty charts! Though I don't have an answer to your question....

*Tags: General Technical*

---

## 2021-04-25

**PeterLe** - *11:41:57*

If you were fortunate enough to have a long term profitable system (hundreds of thousands of bets) and wanted to increase stakes as a proportion of the balance , let's forget Kelly for the time being. 

What level of governance and method would you adopt as time when by?

Ultimately, as the balance increased, (and Im borrowing a phrase by Michael which I like :grinning:), "your appetite for betting would be more than the market could sustain". Equally as stakes increased, you would be more likely for the whole stake to be taken when you are wrong. (Cant remember the phrase)

Initial thoughts are to monitor:



P&amp;L Curve

Matched V unmatched bets ratio

ROI



What other suggestions would you guys have to implement such a strategy?

Thanks in advance

*Tags: Strategies*

---

**PeterLe** - *18:43:01*

Thanks Mo. Well despite me saying initially forget about Kelly...my son and I spent some time this afternoon and created a graph forecasting the effect of Kelly at various percentages on a random subset of the data.

In the end, we just decided to spin up a sub account, and let it go on quarter Kelly. Even at quarter Kelly, the compounding effect is very powerful.

I was thinking that tomorrow I could create a go forward model too based on known strike rates and see where it ends up.

I think Ive answered mu own question, but appreciate the reply [@UBS7QANF3](@UBS7QANF3)

*Tags: Strategies*

---

**Mo** - *19:32:37*

I was going to tell you to just use Kelly

*Tags: Strategies*

---

**PeterLe** - *19:43:23*

OK thanks Mo..Just on Kelly..I place many bets on a single in play market and each bet is determined in that brief moment in time on what I perceive as value. So I could lay a selection and then 100ms later I may back it irrespective of what I had previously laid it at. Is it therefore wrong to base Kelly on each bet rather than on the overall result of a single market?

*Tags: Strategies*

---

**Unknown** - *19:51:16*

Yea he's my youngest (16) and just about to start his A level maths in the autumn. He loves maths (and good at it too) and I was trying to show him how Monte carlo works and kelly, this was one of the graphs we plotted on a theoretical bank on a random sample set). I just wanted to show him how you can apply maths to real world examples and it is a really exciting subject (Note: My qualifications are electronics and Business, not Maths!)

*Tags: Strategies*

---

## 2021-04-26

**Unknown** - *09:52:38*

Yeah, and that percentage will vary depending on the runner status, doing this right now on a strategy with these columns



```strategy_name	size	size_matched	profit	roi	market_count	profit_per_market	profit_per_day	size_matched_perc	size_matched_per_market```

And of course a graph with a line to signify when I changed something, in this case it was the bankroll which has resulted in my profit per market actually decreasing :face_palm:

*Tags: Strategies*

---

**liam** - *09:56:10*

Np, I think [@UGV299K6H](@UGV299K6H) does something similar but 'per bet', this doesn't really suit my style of execution/analysis so base it per market per strategy

*Tags: Strategies*

---

**PeterLe** - *10:00:58*

Yea as I mentioned earlier in the thread, I tend to treat ever bet in isolation on a market, so I do it the same way as MIchael. I think Im being too cautious with my staking and want to max the summer racing.  Im going to do some go forward modelling later today when I get the day job out of the way, cheers Liam :grinning:

*Tags: Strategies*

---

## 2021-04-27

**Oliver Varney** - *07:57:40*

question on execution, say you have two models that become active at the same time but operate on different parameters, do you group these strategies to run on the same flumine instance? Just wondering if execution would be improved  as it could be likely they both algorithms separate come to the same bet. Im guessing the answer is it is probably a good idea but was wondering if people had any experience / thoughts on this

*Tags: Strategies*

---

**liam** - *08:09:58*

~Note that if they are on the same instance the default behaviour is for the orders to be batched/placed in the same request,~ you can control this through the use of [https://liampauling.github.io/flumine/quickstart/#order-placement|transactions](https://liampauling.github.io/flumine/quickstart/#order-placement|transactions)

*Tags: General Technical*

---

**liam** - *08:15:46*

i would be tempted to have a single 'execution strategy' which handles the execution/placement/batching/icebering etc which your model strategies feed into using context or redis

*Tags: Strategies*

---

**Lee** - *10:22:34*

Just trying to understand how the default behaviour works in code.

Do the following two pieces of code not do the same thing as [https://github.com/liampauling/flumine/blob/master/flumine/markets/market.py#L79|this line](https://github.com/liampauling/flumine/blob/master/flumine/markets/market.py#L79|this line) is is made within the transaction context so execute will be called after every `place_order` call?

```market.place_order(order)```

and

```with market.transaction():

    market.place_order(order)```



*Tags: General Technical*

---

**liam** - *10:25:03*

this is a good [https://github.com/liampauling/flumine/blob/dfbad123dcdce6ee7573d23b53a5772dee9d7f38/flumine/execution/transaction.py#L12|read](https://github.com/liampauling/flumine/blob/dfbad123dcdce6ee7573d23b53a5772dee9d7f38/flumine/execution/transaction.py#L12|read)

*Tags: General Technical*

---

**liam** - *10:34:19*

[https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1170-2021-02-22|1.17.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1170-2021-02-22|1.17.0) things changed, previously orders where batched per update, from 1.17 they are executed immediately unless you control via the transaction context manager

*Tags: General Technical*

---

**Lee** - *10:41:31*

still not really sure how to take advantage of it but have and AB test running an idea

*Tags: General Technical*

---

## 2021-04-29

**AndyL** - *23:36:01*

Given how difficult it is to make long term profits from automation, do you guys do this for: "fun", or "for the aim of eventually finding that elusive profitable strategy", or "it's not difficult, I make a living from this, have done for years!" ?

*Tags: Strategies*

---

## 2021-04-30

**Oliver Varney** - *06:30:18*

Id be interested to see what the big guys respond to this [@U01PJ5YMFBJ](@U01PJ5YMFBJ) as everyone journey is different. For me at least it was an idea, that started as a hobby and I guess the end goal of making money if I was lucky but obviously at the time I was clueless about the landscape (and still am at times it feels).  After some small trail and error I found an edge but it took little a longer for a real break through which ultimately lead to this being a full time gig. I think like most things in life when you struggling to achieve a goal it looks very hard / impossible until you do it, then your mind switches and thinks that isn't so hard and sets a newer even harder goal to achieve.  I guess a last point is, that its quite easy to get drawn into a mind set where your comparing yourself against others, which sometimes can be good for motivation and to show you what is possible but setting a series of smaller more manageable goals that you can consistently hit is going to push you forward best

*Tags: Errors Debugging*

---

**PeterLe** - *09:12:12*

For me [@U01PJ5YMFBJ](@U01PJ5YMFBJ) [@ULDAVFDRP](@ULDAVFDRP), although I’m not one of the big guys, in 2007, I took a new role in the company I worked for at the time and was the account manager to Betfair. At a meeting in Hammersmith I met an ex colleague who told me about trading (gambling!) and how people were using the exchange.I decided to have a go trading manually, sounded easy. Two years later my £2k bank was at £200 and at that point I decided to try automation (Betangel) with the intention that once the money had gone that was it, I would never deposit any more. That Saturday afternoon, using a very simple technique I made £426 and I never looked back. Over the next few years the stakes grew higher and higher, even cross machine didn’t affect me.

That three year period is where 90% of my lifetime profits came from.

This slack channel has been great for me, as its reset my expectations and Ive had some fantastic results this past year, thanks to [@U4H19D1D2](@U4H19D1D2) [@U01DPK9PGMC](@U01DPK9PGMC) and especially [@USYQKE5HN](@USYQKE5HN)

The thing is [@U01PJ5YMFBJ](@U01PJ5YMFBJ), you don’t find a strategy and then sit back and collect the money, it's an eco system and constantly changing. It's not easy as everyone would be doing it and there are some VERY intelligent people in this game. However it is possible. My advice would be keep working at it, but try and do something different from the crowd, always be testing, compare and contrast. You never stop learning.

*Tags: Strategies*

---

**James T** - *10:32:14*

Thanks for the shout out [@UQL0QDEKA](@UQL0QDEKA), but you’re definitely overstating any help I’ve given you. You’ve done well recently purely due to your own self-motivation.



Generally, to do well, I think you either need to have the right background (programming, finance, maths, or have worked for syndicates) or you will need some advice and/or mentoring from someone who has. One exception to that rule is [@UGV299K6H](@UGV299K6H) though. Michael did you start with programming right from the start or did you use other methods of automation?



Personally it came to me as a hobby while working in banking. My first strategy didn’t work, and I was lucky with my second, so I was up and running within 3-4 months. Markets were probably easier to crack 10 years ago though. At first it did seem like a fun hobby or puzzle to be solved, and to see how far I could push it. But now after 10 years it definitely does feel more job-like to me and just the easiest way for me to make good money with the least effort. 



Long term it’s always relatively difficult though I think. You can’t expect edges to last forever so it’s a constant process of adapting and developing. So once the interest does disappear it can be hard to maintain the same level of profitability. A lot of it is luck and trial and error in digging for profitable strategies though, so you need a thick skin to continue through patches of “missed shots”. 

*Tags: Errors Debugging, Strategies*

---

**Michael** - *10:33:50*

I used Excel to start with - literally a spreadsheet linked to Gruss. Then VBA, then I moved to Python and started using BFLW when streaming came in.

*Tags: General Technical*

---

**James T** - *10:39:55*

I think the one underrated trait I can think of is “ingenuity”. That’s sort of being able to think of creative ways of solving problems. It’s one word to combine problem solving, creativity, initiative, and resourcefulness. People who don’t like to have “standard” day jobs like me and Michael I think tend to have this trait. Without some ingenuity I think a person would struggle. 

*Tags: General Technical*

---

**river_shah** - *12:40:24*

I managed to find success across a few sports. Some more successful than others, some sports not very consistent but value +.  Will second Michael. The path to success seems very varied for people so follow the maxim: `"You can't tell till you bet"`

Don’t bet money you can’t afford to lose, don’t quit a steady job till your models are steady and if too much stress for the psyche, then don’t do it. There are consistent biases in gambling markets, so much is sure. Building the right technological process for you to exploit those biases is the hard part (flumine reduces that lift massively). Gambling market edges are not going away any time soon, as long as we face off large waves of behaviour driven money. There is not enough pro money to keep, for example, Grand National efficient vs the recreational crowd at all moments. Good luck finding hot decks.

*Tags: Strategies*

---

**Mo** - *14:41:53*

I think they’re two very distinct stressors. As far as having high overheads, the strategies comfortably cover them every month so the only worry would be an existential threat that means you’re left having to pay the costs with no income. However, costs can be gotten out of (the power of the limited liability company). As for the pressure of not letting down colleagues, I think this is pretty much exclusively a positive thing as it can be hard to be motivated working for yourself in a vacuum. And then there are the many positive aspects to sharing the work load and the stresses and enjoying a collaborative problem solving environment that [@USYQKE5HN](@USYQKE5HN) has touched on

*Tags: General Technical*

---

**Michael** - *14:53:13*

Funny but I've never had a motivation problem, sometimes more the opposite - I can get a bit obsessed and have to take myself away from it. I can certainly see the value of having people to bounce off though.

*Tags: General Technical*

---

**AndyL** - *16:13:05*

Noone really tells you how to make it work... so as someone above said "ingenuity" is a real need I think, you can burn 1000s of hours, but without some clever something or other I think it's a struggle

*Tags: General Technical*

---

**Jonjonjon** - *16:13:53*

I know how you feel [@U01PJ5YMFBJ](@U01PJ5YMFBJ). I wasted a few months grinding away at IP data last year, and started a new PE strategy a the beginning of this year, but neither of the ideas seems to be going anywhere.

*Tags: Strategies*

---

**AndyL** - *23:38:57*

i've had a lot of advice, but to be honest it doesn't always help for whatever reason... maybe advice just out of context or different to what I think I want to do.. At the moment i've collected 2months of data on UK horses, and back fitting strategies, unfortunately "fitting" is probably the theme! I've sorted of ended up with a trigger when some technical event happens, eg.selection far from vwap, then have added loads of extra conditions, like the style of race "Hcap"(theory should revert more), front favs are competitive, relative volume of selection is great than others,... you get the drift. Then "hedge" at the off. So basically going round this sort of circle, attempt after attempt, find something that fits, then live test forward, and repeat... Sadly, the current bot strategy I think is half decent, I tweaked one parameter by 1% and it then became an instant losing strategy.... hm not happy sign

*Tags: Deployment, Strategies*

---

## 2021-05-01

**Oliver Varney** - *05:41:58*

Just a couple of things on coming up with a model, be careful your not over fitting or data mining whereby you apply filters till you hit a profit, yet are left with such a small sample size that it isnt really significant.  if you have historical data, split it into two sections (80-20% split is generally okay with a smaller sample which you have by the sounds), train / come up with the model on the 80% and test it out applying the logic on the 20%. The performance should only be measured against the 20% and any performance on the 80% should be disregarded.

*Tags: Data Quality, Performance, Strategies*

---

## 2021-05-03

**Mo** - *09:52:24*

Depends exactly what you are asking:



1. Do they use joint Poisson distributions? I highly doubt it. It's well understood that this does not model the actual distribution of goals

2. Do they parametrise team strengths in this way? Probably but I think most if not all of their value is in deep analysis of the strengths including qualitatively, rather than the exact model structure

*Tags: Strategies*

---

**Alessio** - *09:53:06*

There's no single practice or way of getting to a profitable strategy. What may work for a syndicate may not work for you and viceversa just because the details are usually many. You can definitely start from the lambdas but it's a one score per the entire team over multiple matches, you will need to refine from there.

*Tags: Strategies*

---

**Michael** - *10:35:00*

From the football outfits I've encountered (which admittedly is only a couple) I think [@UBS7QANF3](@UBS7QANF3) is right that a lot of their value comes from data and insight rather than particularly clever modelling. Some of them are quite remarkable in their data collection, huge numbers of people, scouts at all matches all over the world and so on. They go really into the detail of near misses, opportunities, success rate in passing, tackling and all sorts. If you're starting out with a small operation you can't compete with that so you might look to play another way and use your strengths. That might mean betting small and frequent, effectively market making but with a little bit of added edge from a basic model.

*Tags: Strategies*

---

**user34** - *10:41:45*

In the academic literature, a Weibull count distribution with copulas outperforms other parametric models for the goal distribution ([https://blogs.salford.ac.uk/business-school/wp-content/uploads/sites/7/2016/09/paper.pdf](https://blogs.salford.ac.uk/business-school/wp-content/uploads/sites/7/2016/09/paper.pdf), [https://core.ac.uk/download/pdf/220685085.pdf](https://core.ac.uk/download/pdf/220685085.pdf)). As others have said, the real value comes from the actual parameter estimates.

*Tags: Strategies*

---

**Dave** - *12:13:03*

There is some value in not modelling it as poission tbh...there are basic classification methods that give you reasonable probabilities. Edge comes from having clean and consistent data, good data science practices when it comes to fitting/tuning, and thoughtful execution in the market.

*Tags: Strategies*

---

**MacrcoPolo** - *16:07:42*

I'm in favor of modelling it as a distribution, but poisson (even joint) really isn't correct. Markov chains give much more flexibility

*Tags: Strategies*

---

## 2021-05-05

**AndyL** - *22:29:19*

Ok how about this question, what makes a profitable strategy hard to find? Is it its complexity? Or are they just simple but just so obscure that they are really difficult to find?

*Tags: Strategies*

---

**Mo** - *22:59:58*

I'll give you my personal opinion. I think that the ability to come up with ideas that are likely to lead to profitable strategies is a rare skill. It's hard in the same way that being creative is hard. It's not really something you can force yourself to do. I don't think I personally possess this skill, hence why I believe networking is so important. Taking a ground up approach of building a model with the idea that it will give you a better price than the market is very very tough, whether that is the latest machine learning model or some technical indicator borrowed from finance. You should be thinking about why does the market think the price is X and what are the situations where it may have gotten it wrong. For example, favourite-longshot bias, early markets, obscure sports/leagues, punters wanting to bet on goals, not wanting to bet on draws, etc. etc.  Work out what parameters are implied by the market eg goal scoring rates. When you think the market is correct then plug these into models. Find situations where there are discrepancies (=statistical arbitrage). Think about things in terms of a system you're trying to exploit. How can you probe it? How does the Betfair matcher work? How can you be first in the queue? How does crossmatching work? What are the rules for the markets you're betting on? First understand then attack

*Tags: Deployment, Strategies*

---

## 2021-05-06

**liam** - *06:22:19*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) it certainly sounds like you are falling into the trap of back fitting coupled with applying manual type 'trading' styles and filters. Sports betting might look similar to financial trading with its fancy graphs but it's very different and to [me] it should be treated very differently. You are correct in saying you need to be wired differently but I do believe it can be learnt assuming you are open to it.



First off to make money in sports betting you need to get value, there is no compromise to this, if you are not getting value then you are not going to make money. It doesn't matter if you are market marking, scalping, swing trading, auto betting, manual betting etc. it's mathematically impossible to make money without getting value. Write 'value' on something in front of your computer and anytime you do something think about whether it makes sense to do from a value point of view.



For example you mention 'hedging', lets say you develop a signal that you believe gets value 60-10mins out from start time at around 2% ROI, unless you are limited by bankroll (I doubt it in sports) there is no planet where randomly crossing the spread in order to hedge is going to make you money long term. It is likely that you will either completely remove that 2% ROI or go negative. Does randomly crossing the spread at post time give me value? No. However assuming you are getting a decent chunk of value in the initial order and hedging allows you to even out your pnl / take advantage of as much of your bankroll as possible etc. then I can see its benefits (aka [@ULDAVFDRP](@ULDAVFDRP)) But this is under the assumption that you know where the value is and you are taking the hit, this second order should not form part of your analysis when trying to find a profitable strategy.



If you developing an automated strategy that works pre race but find yourself having to filter on race types, fave odds etc. to show a profit then stop. Take a step back, I write a lot of stuff down and simply writing what I am thinking makes me realise that I am an idiot when I come up with daft stuff like race type filters / limiting the odds etc. To me this says either back fitting or that my signal is not correctly developed.



I have skimmed your on going thread on BA and the big thing that sticks out to me is your sample size relative to what you are doing coupled with I assume your execution on hedging. I tend not to make any changes or remove strategies until I have at least 1k orders through the market, and at that point I start to analyse [everything] and not just the pnl but matching rates, profit vs trigger / odds / time to start / volume etc. I don't think I have seen you go into this detail on your thread? (I might be wrong)



_I will talk about your latest inplay post next, to be continued.._

*Tags: Strategies*

---

**Oliver Varney** - *06:34:14*

Looking back at my naive hedging I am still glad I did it when starting. It gave me the possibility to daily compound on returns that were realised over at day at more then 10% on small stakes. It also focused the mind into beating BSP which sometimes can be lost if you leave the money on.  The instant feedback I think is more important for someone at the starting of there journey then that 1-2% assuming youve got strategies that can run greater then that. So many times in this slack ive seen the same message saying it worked for a month then collapsed but bsp was always show slight negative so was destined to fail. These days my hedge would take out upwards of 3-5 ticks jumping across the spread so its a no go. One part I still need to learn / improve on is offering more. Its far faster for me to build models can jump the spread for guaranteed value (in the models mind) then risk missing the good ones and getting picked off on the bad ones.

*Tags: Strategies*

---

**liam** - *06:38:04*

```Just been doing some "inplay" thinking... take this scenario:

- Typical flat 10 horse 1m race

- 2furlongs left horse1 is 6 lengths clear, price is 1.2

Is that "value" or not?

The answer has to be you haven't got a clue! Unless you really know how good horse2,3,4,5 are doing and have left in them from "fundamentals"

You could assume 1.2 is neutral value and offer 1.5 for 5 seconds, but 5seconds later horses 2,3 may close sharply, and at that time 1.5 is now no longer value.... So with my "crystal ball" 1.2 was in fact great Lay value, as fundamentally horses 2,3 had it in them...

So it all comes down to "fundamentals" really...?```

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) my thoughts on the above:



Ignoring TPD it is very tricky to accurately calculate the number of furlongs left, however you can estimate the time length. See [https://medium.com/@paulingliam/using-ml-to-predict-horse-race-time-duration-6340776536e4|here](https://medium.com/@paulingliam/using-ml-to-predict-horse-race-time-duration-6340776536e4|here) for a post I did on using ML to calculate horse race duration, I use this all day and everyday and it can be very powerful. Calculating the distance between two horses is not possible, if you are automated you only have the odds to go off (which is enough :wink:)



Personally I Ignore the horse, ignore the fundamentals, ignore the jockey, I don't have the skills to race read or understand how a horse will run so neither will my models. Tbh I have found that once a race starts everything goes out the window anyway to the point that I ignore BSP as I find it more of a hindrance in model developing (I believe [@UGV299K6H](@UGV299K6H) mentioned the same recently)



In your post what you are missing is that value is a function of time and that all of the odds you have mentioned are most likely value. For example your horse 1 is trading at 1.2|1.3, if I had to bet on this I would want my lays matched at 1.2 and backs matched at 1.3, due to the spread it is likely the true price at that *time* is somewhere between 1.2 and 1.3. Now if the price suddenly jumps to 1.3|1.5 because of something that has happened on the track I would get stuck in to laying between 1.2/1.3 taking advantage of what you describe as a crystal ball. Everyone is reacting and not everyone is as quick as everyone else, its what makes inplay so volatile and brilliant. The real trick is getting matched.

*Tags: Strategies*

---

**liam** - *06:40:15*

Agreed, but you come at this from knowing the above and knowing where you value is. Lots of people try and automate/analyse/make money from hedging being part of the strategy rather than the execution.



But yeah as you mentioned it comes down to market impact, I tiny change in matching rates can make or break a strategy (I get burnt weekly :joy:)

*Tags: Strategies*

---

**Oliver Varney** - *06:48:35*

I like to think I have no clue what value is :joy:, im the stupid human who managed to frame a learning task that predicts what the market will do with a few python libraries and some dodgy code, the brains is the algo.

*Tags: General Technical*

---

**Unknown** - *11:07:03*

[http://betfairlightweight.com|betfairlightweight.com](http://betfairlightweight.com|betfairlightweight.com) is available, and we could put sticky stuff on it. However, I don't know what web host would be good to use. Godaddy is VERY slow:

*Tags: Performance*

---

**Michael** - *11:29:07*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ): From where I am now I look back on when I started betting and I can see how much I benefitted from not really caring too much about it. I was fairly broke but I didn't really mind, I had other stuff on and it wasn't like I had any expectation of making significant money. I just tried some experiments out of curiosity, it was interesting and fun and winning a few quid was the cherry, not the cake. That left me free to learn without pressure which is the best way to learn anything. Had this channel existed then I don't think it would have helped me, I wouldn't have understood any of the maths or programming stuff (a lot of it still goes over my head), I would have just been overwhelmed and put off. So here's my advice: Look to get something out of this that isn't money - like learning programming or just having fun. If the enjoyment or the learning or whatever you're getting from it become not worth the cost in time, money or stress then consider doing something else that will reward you more. Most winners (especially the individual types) have their own special bits of proprietary Mojo, if you're going to be a winner you'll need to find yours so be curious, explore, and go your own way. Above all: Don't have a shitty time, in the end this is just gambling, it's worthless, it's pointless, it's a drain on humanity. It doesn't matter.

*Tags: Strategies*

---

**Michael** - *12:02:40*

I don't know if that always helps them. My technical skills have always been very weak and still are. I have almost no understanding of statistics as it's generally set out although I sometimes find that I've been using the same concepts, I just didn't know the names or the notation. I struggle with some really basic maths things like re-arranging equations and I find it very hard to understand academic papers so I don't usually bother with them. The good side of all that is that I had to learn about gambling from the ground up in ways that I could understand, so I discovered the significance of the over-round and 'invented' notions like expected value on the back of envelopes rather than being taught them. I think that helped me. I remember right at the start wondering why you couldn't make money just by laying horses because _"surely they lose more than they win."_ Then I went and figured out why that wasn't true (which took me hours). I gradually figured out the relationships between various market forces and how sometimes markets could be out of balance and so it went on.

*Tags: General Technical*

---

**AndyL** - *12:42:34*

Thank you for your down to earth advice [@UGV299K6H](@UGV299K6H), I do have a day job so this is part time when I have spare time, but being an engineer myself I have intrigue in trying to conquer this problem, although I do get frustrated easily! but I like your moto on life... cheers

*Tags: General Technical*

---

**AndyL** - *13:00:17*

[@U4H19D1D2](@U4H19D1D2) you're spot on, i fell for the back fitting attempt at automating manual strategies, although the BA forum being mostly manual traders it sort of inevitable push that way. But i'm learning now... I've also I think finally understood the "crossing the spread" cost, as that 2nd hedge bet has no strategy, so it is just a "cost" to keep pnl less volatile, and it is quite a cost, although not sure I quite understand how people figure the value 3-4 ticks suggested? But anyhow, with the sort of stuff I was looking at that was probably taking all my miniscule profit!

My backtest data covers &gt;2k UK races now, but because I was filtering so much that reduced to about 400 with the filter!

The other glaring omission now with my backtest data, is I just recorded (using BA) up to the scheduled start of each market, so I don't know which horse won!!  So I can't actually backtest with no hedge, i'm feeling very dumb now!

You may have seen in my thread I tried a while back a strategy with no hedge, but it very quickly went negative big time, so scared me off!

*Tags: Strategies*

---

**liam** - *13:06:12*

:thumbsup: you can still get the result of a market if you make a marketBook request (normally cached for 2/3 months) or just ask on here :wink:



Your test shouldn't have scared you off not hedging but scared you off the strategy itself! If the initial order isn't making money (value) on its own then adding another that crosses the spread certainly isn't. As [@ULDAVFDRP](@ULDAVFDRP) mentions you have the BSP as your base price to try and beat, if you aren't beating that then it is likely you won't make money.

*Tags: Strategies*

---

**Misha** - *13:22:31*

I have been in this field for about 11 years. As a software engineer I spent the first 10 working for two large clients where I was given "carte blanche" to develop systems based on rough ideas with absolutely no specification as to what technology to use. This is the holy grail for an experienced software engineer - the ability to research, trial, and implement using anything I wanted, as long as it worked. The first client was nuts and the second client moved his whole operation overseas, so about 18 months ago, after 30 years working, I was trying to work out what to do. I had a good maths background, dabbled in tennis with Betfair right at the start, and had seen the obscene amounts of money it was possible to make (and had a friend from University start a hedge fund based on his algorithms and make $200M AUD), so I thought, let's have a go (with one other partner). The goal is simple: develop a fully automated system that ensures I never have to earn a single dollar from anyone else for the rest of my life (i.e, make enough money to cover living from age 52 onwards, with a family). My view was that I was almost certain I could get there, but not sure of the timeframe. So far so good. As of right now, I am returning about double my best ever monthly earnings. But to get to the level I need, I really have to increase that by a factor of 5, because nothing lasts forever. Fortunately we chose in-play tennis, which if you bet after points, has ample liquidity to get there and more (if the model works). We are a long way from finished, and the trajectory is in no way a line straight upwards. We had our worst day ever yesterday, when we lost $60K AUD, $48K in the last 14 matches. But that happens if you are betting on an outcome, not trading to hedge (nothing like a huge amount +/- coming down to a third-set tiebreaker). Not sure as a risk-averse person I was really cut out to do this, but the potential reward is just too big not to have a go

*Tags: Strategies*

---

**AndyL** - *13:59:34*

So let me check, so beating bsp is say bsp is 4.0, if i get a back bet on at 5.0 i beat it...

The problem with my test was most of my  bets over a period of a week lost, so even if they beat bsp, over the week I may have been unlucky side of the probability curve...

*Tags: General Technical*

---

## 2021-05-07

**Greg** - *00:31:29*

I have only just started to beat SP regularly (via bot but more arse than class).It has taken me 10 years to start to understand value (even with a retired pro gambler for a father in law---slow learner:slightly_smiling_face:) and even then I am not always sure of it. I am up quite a bit this time but only after 110 bets (1k markets analysed by the bot) and variance is a bitch. On the plus side I haven't had to top up my BF account for ages-haven't taken money out either of course:laughing:..but to be fair working full time, family taking time as well as and not being able to stay up and watch UK markets slowed me a bit. I can print money on NSW tracks manually for some reason (predictable volatility maybe-55 bets a race usually) but the turnover charge limits you massively. Gawd I wish I could understand Python (dedicating hours everyday) better and ML of course. Chemo is not helping but hey gives me something to do instead of thinking about life and death:flushed:.Hang in [@U01PJ5YMFBJ](@U01PJ5YMFBJ)!!:grin: Its tough but tackle the bastard to the ground!!!:gorilla:

*Tags: Performance*

---

**Michael** - *23:18:39*

It's a question of your style of play. Ignoring TPD based betting I believe [@U4H19D1D2](@U4H19D1D2) and I illustrate the two extremes. I like to get my bets matched so I'm up at about 80% match rate, but I place comparatively few bets. [@U4H19D1D2](@U4H19D1D2) leans more towards a lot more bets with a lower match rate. I wait until I can see a bucket load of edge on a runner and then ask for only a portion of it, hence the high match rate whereas Liam is (presumably) requesting a lot of edge all the time. You can do either, both or something in between. The one thing I recommend that you avoid is posting a lot of bets with only a little bit of edge - that will go badly wrong.

*Tags: Strategies*

---

## 2021-05-08

**birchy** - *00:14:21*

Oh. Bugger. I've somehow ended up concentrating on "micro" markets within each individual race and tbh, have struggled to find the golden eggs within them. Up until ~12 months ago, I was always a firm believer in looking at aggregated markets and picking off those spots where "abnormalities" occurred. Since diverting to the streaming API, I somehow took a wrong turning and ended up inside some warp zone where I've dismissed everything I did previously. Shit. :man-facepalming:

Thank you [@UGV299K6H](@UGV299K6H), I've just realised that I already know what I need to be looking for but have been looking in the wrong places.

*Tags: Errors Debugging*

---

**Michael** - *00:17:15*

I wouldn't say you were wrong necessarily [@U016TGY3676](@U016TGY3676), we all find our own ways of doing things. Still - I'm glad if that's helped you find another perspective.

*Tags: General Technical*

---

**liam** - *11:56:58*

Ignoring TPD I am 100% price action of which 90% is based purely off single updates but I am a cowboy when it comes to strategy development where my edge is in execution 

*Tags: Strategies*

---

**Atho55** - *13:17:20*

[@U01L8D326AK](@U01L8D326AK) If you have a bit of Promo data it`s possible to compare past BSP to IPMIN and use it as a measure of performance like this [http://prntscr.com/12oaxpn](http://prntscr.com/12oaxpn) It produces a shortlist depending what you are looking for. Over Average picks work reasonably well.

*Tags: Performance*

---

**Unknown** - *13:36:48*

Just happened to notice this one :grinning: Can we have a strategy for catching them every time :grinning:

*Tags: Strategies*

---

**Oliver Varney** - *14:57:28*

I may be being stupid and asking a real noob question here as I dont know the ins and out of IP, but is it a forward project of what you think will occur and be value after the IP delay, i.e. someone laying at 1.03 thought that the chance of the horse falling was greater then 3%

*Tags: General Technical*

---

**Oliver Varney** - *15:04:54*

again maybe another stupid question, but by the time your bet hits the market the situation has changed? So if for example you had GPS and you had some formula to define value based of distance to line, distance deltas to other runners, speed, etc , are you not looking to build the value function for T+bet delay

*Tags: Performance*

---

**Michael** - *15:15:43*

In truth I've no idea how picture players come up with their prices, I just know how to spot when they're getting it wrong.

*Tags: General Technical*

---

**Mo** - *23:30:44*

If you're questioning things after 2 days and can't take that size of loss I think you need to ask whether this is for you

*Tags: General Technical*

---

**thambie1** - *23:35:50*

Backtest your strategy over a long enough period and you'll get an idea of what downswings you can expect. If you're unhappy with the risk profile, you can always work on it. For example employing the kelly criterion.

*Tags: Strategies*

---

## 2021-05-09

**Peter** - *06:50:37*

Any time I get a positive EV, I'm happy as that's an indicator hat there might be something there. But it's just a starting point. The next step is to look for other signals with which it could be combined to push the EV up and bring the variance down. But you need way more data that a handful of days of experience. I typically backtest several months worth of data (usually circa 10,000 active bets) to obtain data for training and development and then run the candidate strategy against as much again to validate that it holds up on previously unseen data. Only then does it get launched onto the exchange with small stakes to see how it performs in the real world.

*Tags: Strategies*

---

**birchy** - *18:33:16*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) I'm a bit late to the party, but have you compared what the P&amp;L outcome would of been if all of your bets were placed at BSP instead of your actual matched prices? Essentially, you need to:

a) prove that your bets are being matched at prices better than the BSP

b) determine if you were simply unlucky on your bets so far

I posted a Monte Carlo simulator a while ago that will give you a result that tells you if your strategy is better than picking random selections. Will repost when I find it...

*Tags: Strategies*

---

**ShaunW** - *23:15:48*

@goatwish if you're not hedging then don't measure your performance by actual pnl. You'll have weeks or months where that runs ahead or behind your theoretical return (the one that assumes you'd hedged at bsp). At your stage I think you'd be best to hedge and then analyse your bets to see where the profit is coming from. Prove you're actually profitable first otherwise this could get expensive.

*Tags: Performance*

---

**AndyL** - *23:53:52*

Better day today...

Theoretical BSP performance was good too

Will see how week goes, but thanks on hedging advice chaps

Just moved BA from failing Laptop to a VPS, great stuff 24x7 without noisy laptop fan i could hear from the other side of the house!

*Tags: Performance*

---

## 2021-05-11

**Peter C** - *09:29:26*

Really enjoyed a lot of the recent conversation - it kind of inspired me to have a go at modelling pre-off horse racing using market info. My model is trash, but it's been a good learning experience so far. I've been reading back through this slack and wanted to ask about using price data to fit a market.



I'm not quite sure what my thinking is - I have a few thoughts but I'm not sure. I have thought that maybe the win market is largely efficient - and therefore I could use price data to model a projected sp that I could then take to eg the place market.

I've also thought that perhaps I could use features that aren't derived from the current market price to fit my model - eg the sp far price, and then 'return' to the market (the current price) to amalgamate information a la Will Benter.

I've also thought that maybe I should include features that are derived from the current runner price to fit my model, with the intent of coming up with a projected sp interval that I could use to place bets when I see an edge - but I'm not sure how feasible this is given my assumption that the pre-off win market is largely efficient - I'm also not sure it makes sense to take the current price and +/- a bit to come up with an sp.



Not necessarily looking for specific answers to my questions but I would be interested in people's ethos surrounding this or perhaps someone is able to point me to some resources where I can learn more? I feel like I am missing a huge chunk of understanding on this topic. (I'm reluctant to go straight to google with 'price action' because I suspect there's a lot of bullshit out there; I've read the majority 'efficiency of horse track betting markets').

 [@UBS7QANF3](@UBS7QANF3) the other day you used the example of using bsp to come up with an implied distribution of runner's speeds for inplay betting. I understand this example but am not sure how this would be achieved within a single market - I need to think about it further, maybe this is the understanding I'm missing

*Tags: Feature Engineering, Performance, Strategies*

---

**Mo** - *09:32:14*

There’s a lot to unpack here but a couple of quick points. Firstly, using the win market to price the place market is a well trodden strategy. It works but is also likely to be very competitive. Secondly, you can’t just say the pre-off win market is largely efficient. There’s a huge difference between the market five minute before the race starts and five hours before

*Tags: Strategies*

---

**Peter C** - *09:50:21*

So perhaps I should view it as using information from the market that I view as effecicent to help me price information that I consider less efficient?

*Tags: General Technical*

---

**Peter C** - *09:58:35*

Thanks for your thoughts - helped me straighten out my thinking a bit

*Tags: General Technical*

---

**Greg** - *10:01:30*

I often used to wonder if my 5-600 odd manual bets/trades in one week, when I can be bothered but when I want to manually top up my balance, mess with efficiency models in Aus.I might do it twice one month and four the next.Guess it is a relative drop in the ocean though :grin:

*Tags: Strategies*

---

**Peter C** - *10:17:42*

I am not the person best placed to answer but the favourite/longshot bias is talked about extensively in 'efficiency of racetrack betting markets' from both a tote pool/fixed odds perspective. I think in terms of a tote pool it is talked about in terms of punters preferring high risk/high reward bets and so over betting on those horses. Not sure why it persists in a fixed odds environment

*Tags: Errors Debugging, Strategies*

---

**Peter C** - *10:26:35*

further back than I thought [https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1615899464083500](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1615899464083500)

*Tags: General Technical*

---

**AndyL** - *18:42:29*

So what is efficiency? I am assuming an efficient market wont deviate much, [http://ie.it|ie.it](http://ie.it|ie.it) has found neutral value and any deviation   reverts very quickly..? It's difficult to trade because prices offer little value..

So let's go the other end, 10am in the morning 4hours before a race, market volume total maybe 10k if lucky, so prices  meander and move a lot as the market is not efficient, which means noone knows where neutral value is, which also means you cant pick a value bet as neutral value is not known...so how can you trade it other than a fundamental model?

*Tags: Strategies*

---

**AndyL** - *18:56:32*

Can you have a model on price action alone? Or do I need to scrape form from some website...?

*Tags: Strategies*

---

**AndyL** - *18:58:37*

Must agree when people say dont trade in a bad mood, after losing £90, realising a stupid strategy and frustrated... i cant think straight...

*Tags: Strategies*

---

**Dave** - *18:59:22*

Many people here are successful from modelling off of price action alone I believe - I am not one of them (entirely fundamental) so I probably cannot help. Generally it is safe to assume that at current time fair value lies somewhere between the best back and best lay. A shift in the best back and best lay itself doesn't signal to me anything other than a move towards a 0EV price (in general). I guess the game is to guess when it has moved towards fair value, or has moved away (but a move itself in some direction without context of full market observations can hardly signal this to you I think)

*Tags: Strategies*

---

**liam** - *19:09:52*

No rush in this game, what did your back tests say on this strategy? £90 draw down expected?



Ignoring the hedge you can calculate your EV using a simple formula however make sure your sample size is large enough 



[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1618859325404000|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1618859325404000](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1618859325404000|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1618859325404000)

*Tags: Strategies*

---

**Michael** - *19:28:39*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ): As a new player you're very likely to have a lot of variability as your edge (if you have any edge at all) is likely very thin, so in your position 5 days of racing is almost nothing. Just step back a little bit and ask yourself if you're being sensible: Is the logic of your strategy sound? Have you done everything you could to research and validate your thinking - not just back-testing but looking at your strategy and trying to stand up your various conjectures with data? Are you betting at sensible odds? Are you betting with minimum stakes? Have you set the thresholds beyond which bets are triggered nice and high? Are you placing minimal bets per event? If you can't answer all that in a well reasoned way then stop and reassess.

*Tags: Strategies*

---

**Michael** - *19:49:48*

My suggestion would be that you forget about back testing for a bit and just try to think through the logic of what you are doing - what is the critical bit of insight that's going to find you your +EV bets? Then research, check and prove (or disprove) your thinking. Let's imagine you'd just 'invented' weight of money; you wouldn't have to back test some strategy to see if that was predictive, you could just take every instant of every market and see if more WOM = more value to back, 0 wom = 0 value and -WOM = lay value. No wishful thinking, no _'only maidens on a Tuesday'_ no money down the drain. Just a nice graph that tells you 'yes' or 'no'. If your strategy isn't well reasoned and proven to the greatest extent possible then you don't need to test it, I can tell you right now that your bets will be -EV.

*Tags: Strategies*

---

**AndyL** - *20:38:38*

I had to think a bit on that one, so yes, but that's partly as I enjoy the communities, bet angel and this slack. Ive had tons of help and advice from great people. I also enjoy the software, as im an engineer. What adds doubt is the continual lack of any success,  that weighs a bit..

*Tags: General Technical*

---

## 2021-05-12

**Unknown** - *08:32:48*

Graph showing why stake/bankroll is so important..



This was only a 25% increase in size resulting in only a 0.01% point reduction in matching but almost destroying all profit :face_palm:

*Tags: General Technical*

---

**Jonjonjon** - *09:25:11*

[@U01DPK9PGMC](@U01DPK9PGMC) That's quite a great suggestion. I've recently been having problems with slow backtests, as my strategy recalculates "fair value" with every market book update. But I could probably just calculate "fair value" at a single fixed time (e.g. 5 minutes before the race), and then start analysing the performance of the fair value signals.

*Tags: Errors Debugging, Performance, Strategies*

---

**AndyL** - *10:11:02*

So ive used vwap for that sort of strategy,  waiting for price to be a certain distance above or below vwap as a trigger then entering on theory of reversion to vwap, but that alone was not profitable, so i added all sorts of filters, race type, price of other horses etc..but this seemed to be back fitted, although it may have legs with time..not sure

*Tags: Strategies*

---

**AndyL** - *10:23:16*

Ok that is pretty much what my recent-£90 strategy was doing, my fair value was the 30sec SMA of mid price, when 6% above i would offer a Back, -6% Lay

*Tags: Strategies*

---

**Michael** - *11:04:46*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) I wasn't suggesting that you use WOM, I was just using  that as an example to illustrate how you could investigate a theory without getting bogged down in back-tests or losing money. Any strategy is made of a series of conjectures like _'signal X indicates back value'_ and _'this effect is stronger at shorter odds'_ (or whatever). There's more than one way to develop a strategy but the way I go about it is to look at each of those conjectures, check if they're true in isolation and see how they interact. Then I stitch them all together the best I can. That way you end up with a well reasoned strategy with little risk of overfitting. That approach also lends itself to adding in new signals into your strategy as you find them rather than ending up with a proliferation of strategies and no overview of how they relate to one-another. Shotgun back-testing with made up parameters will inevitably over-fit.

*Tags: Strategies*

---

**Aaron Smith** - *13:16:30*

[@U010GM77S4W](@U010GM77S4W) eh yea of course, switched them around there it seems :smile: is fixed

(looks like it doesnt fix it in the quote though)

*Tags: Errors Debugging*

---

**D C** - *19:01:08*

For many it is the allure of "beating the system". Don't forget that in this space there are lots of false traders selling the "dream" because they want you to buy their crap e-book that purports to show you how to be a successful "trader". Lot of people buy into that narrative, others are addicts. For others it can be a refusal to give up. That was the case for me - I thought I had the right mix of education and programming skill to beat the markets (initially through modelling).  I had limited success on american sports many years ago when the bookies prices were utter shit but they caught up with my relatively "simple" models after a few years and I was a student at the time and could only afford to bet pennies. I lost about £50K at Betfair alone and proably the same again (if not more) at bookies but it was only recently that I turned the tide on that one and started making profits. Small time compared with many here but profit nonetheless. But you would need to ask everyone here as to why they still try because everyone has a different backstory and life experience.

*Tags: Strategies*

---

**Beeblebrox** - *19:21:32*

I don't think everyone who is trying is spending years losing money though.  You don't need to be betting to work out if a strategy you're proposing to use has any legs, you can paper trade or backtest or do any of the other things [@UGV299K6H](@UGV299K6H) has suggested.  You do seem overly keen to bet, and then as soon as it goes wrong after a few days, you move on to the next idea.  I don't think success comes that easily - it takes time, but you don't have to be losing lots of money whilst you're trying to find what works.

*Tags: Strategies*

---

**ShaunW** - *20:13:44*

There might only be a finite of set of parameters but each can be viewed in many ways. Eg time, is that the time frame you're active, time between refreshing bets or time to measure price moves or acceleration. And price, is that %implied chance or relative to others, and is that touch price or one of the many fair pricing models (vwap vpoc MA WoM type things). There's more than enough permutations to last a lifetime. And more you're involved the more variations you find but like a lot of complex problems look for the elegant simple solutions.    Don't waste money using trial and error in live, you can be pretty sure before you commit cash.

*Tags: Errors Debugging, Deployment, Strategies*

---

**Mo** - *20:15:46*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) something I wanted to suggest based on a comment you made is that maybe you should focus more on writing software if that is what you enjoy. For example, build a dashboard or some other tool, something that is not directly strategy development but that lets you engage with the data and get the creative juices flowing while also being more enjoyable and less pressure

*Tags: Strategies*

---

**AndyL** - *20:29:09*

Bflw &amp; flumine sound a great project

*Tags: General Technical*

---

**D C** - *20:51:25*

This is very good advice, but I should point out that most of the losses were simply from recreational gambling rather than losing shitloads trying to invent systems and strategies. I spent years analysing and gathering market data but instead of waiting for something profitable to transpire and saving the money I would bet like a mug at the same time. Now I have documented a lot on twitter that at times I have been close to having a problem with gambling - always at bad personal times in life. While there is nothing wrong with having a bet with money you can afford to lose, I would always deter someone from betting to try and "beat" other problems in life. When chasing a system or trying out some automated strategies I have never really lost a lot of money, it is just the recreational betting that dragged that down. That and betting during "bad times". For example, one day while being a Phd student I was having some personal issues in life - debt, lack of self belief (something any phd student knows all about) topped off with some relationship issues. Basically spanked £4.5K of money off a barclaycard in a single afternoon making self destructive bets in-running. I'll never be able to explain why it happened - and thankfully you can no longer use credit cards to deposit. But for me this is the dark side of what we do and every day I make a profit I am really hoping that there is not someone on the other end of it in that same dark place I was that afternoon. This experience has always grounded me and was a low point. It is also the reason that I will only ever consider automated gambling now because there is always a part of my mind that questions if I can trust myself.

*Tags: Strategies*

---

**birchy** - *20:54:40*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) definitely get Flumine up and running with [@U4H19D1D2](@U4H19D1D2)'s marketrecorder example (it pretty much works straight out of the box). That will give you ALL of the market data for each race. Also setup a basic strategy to avoid account suspension for leeching. Again, [@U4H19D1D2](@U4H19D1D2)'s lowestlayer example will suffice, but I'd suggest setting the price trigger to something like 1.80. You won't lose much laying odds on runners.

You can also run backtests in Flumine once you have a month or 3 of recorded data. What I would suggest is to use this ability to parse the data files to gather statistics rather than run full backtests. From there, you may find some useful patterns, upon which you could run a few backtests to prove they work as a strategy. And then.... assuming that you've done all of the above and backtests show that your ideas are sensible, you can port the Flumine strategy to live betting, which will give you "real world" data to analyse.

*Tags: Getting Started, Deployment, Strategies*

---

**AndyL** - *21:00:50*

Its also somewhat to get my engineer head into rather than frustrating over a strategy!

*Tags: Strategies*

---

## 2021-05-13

**Greg** - *00:55:59*

Hi [@U016TGY3676](@U016TGY3676) -are you talking about this example : [https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py) ? I have an error on the "from strategies.marketrecorder import MarketRecorder" line. ...strategies is an unresolved reference and I can't seem to fix it. Any advice appreciated. Cheers

*Tags: Errors Debugging*

---

**Greg** - *08:45:32*

Thanks Liam. Not aware of anything I've changed and all the other imports seem to work as default .Sorry for another dumb questions-change the imports how?

*Tags: General Technical*

---

**birchy** - *21:52:13*

General question for the inplay horse racing players...was March a tough month and April the polar opposite?

*Tags: General Technical*

---

**D C** - *22:47:59*

It's pretty much break even and I am working on tweaking it. It's funny because I've heard a lot of people say that TPD is unlikely to help anyone who was not already profitable IP but for me its the opposite - getting something profitable on TPD has given me greater insight into the general IP markets. I feel I am close to getting it "right" but I have been here before. Totally agree about the diversity though and hopefully I will get something soon. Truth be told I have had quite a few health issues and an operation a couple of months ago and motivation for analysis and coding has been very low. Really been cranking the handle each day. My work ethic should improve over the coming weeks though.

*Tags: General Technical*

---

## 2021-05-14

**PeterLe** - *10:09:55*

[@UUE6E1LA1](@UUE6E1LA1) Sorry to hear about the health issues, hope you're on the mend soon. Its interesting about what you said about TPD (my best month is march too but not incorporating TPD). Ive had the feeds for a while now and whilst my normal bread and butter stuff is coming on leaps and bounds, I'm embarrassed to say that my TPD is yet to bear fruit. (In fact it hindered my results if anything)

I'm only recently been able to start recording the TPD data and building on that, but without giving away the crown jewels, could you give an idea about how you would approach modelling it in general?

We all work hard for our edges, so I dont expect you to give anything away as such, just a common sense approach and a sense of direction would help me! Thanks

*Tags: Strategies*

---

**D C** - *10:19:07*

I've written a lot of software GUI tools that allow me to "replay" inplay data and this is where I have got most of my initial ideas from. All I am willing to say really is that I have not yet got what I suppose most of the other TPD guys here have (and what TPD sell to bookies) and that is a ML (or not ML) model that models a probability for each runner at each point in time. I have tried this but my knowledge of ML is just not advanced enough. What I have is more simple than that but in some ways also more complicated. That sounds strange probably but hard to explain further without giving the game away.

*Tags: Strategies*

---

**D C** - *10:20:43*

I really want to throw multinomial logistic regression at it but I am not sure what the right approach is here as there are not a fixed amout of runners per race. I am comfortable with regular logistic regression but never actually performed multinomial. Any mathematicians here have any suggestions on how to use it on horses? Is partitioning the dataset into distinct sets that contain races with the same number of runners the way to go? Shame we can't get TPD and inplay markets on the dogs. Can you imagine the carnage there.....

*Tags: Errors Debugging*

---

**PeterLe** - *10:33:02*

Yes thats what has threw me to be fair. Ive tried to smooth the data out by using moving averages etc it helped but slowed things down too much

If I could strip out the outliers that would make sense.

*Tags: Feature Engineering, Performance*

---

**Mo** - *16:30:33*

Two things [@UUE6E1LA1](@UUE6E1LA1). Firstly, I don't think you should put much stock in TPD's ML model. Secondly, regarding your uncertainty about different numbers of runners per race, you can just train different models for each number of runners

*Tags: Strategies*

---

**D C** - *16:33:11*

Yeah that was pretty much the only thing I could think of. Never used MLR in anger before so I will try it out. I've no idea if the TPD ML model is any good and its no use to me anyway but I was just aware of it and I would be surprised if they are the only one with such a model. I do know though that not everyone on here goes down the ML path

*Tags: Strategies*

---

**Mo** - *16:34:21*

I will say it's not their own model

*Tags: Strategies*

---

**Mo** - *16:36:59*

Also [@U4H19D1D2](@U4H19D1D2)’s model is a series of if statements so ML is hardly required

*Tags: Strategies*

---

**D C** - *16:42:21*

What I have now is crude compared with a ML or a statistical model and its doing ok. Maths is intersting and its useful but I'll not turn down profit because my approach is less sophisticated thats for sure.

*Tags: Strategies*

---

**D C** - *16:47:54*

Yeah agreed. But I would really like to get a model that spits out probabilities of each runner winning given the current (and previous) states of the GPS info as this would give me a lot more confidence in my ask prices being value.

*Tags: Strategies*

---

**John A** - *16:53:39*

Question for you guys that use TPD, what’s the best indicator; speed or order?

*Tags: Performance*

---

**John A** - *16:59:08*

Thanks guys that’s a great help, on the speed front has anyone tried to calculate the acceleration over a number of seconds?

*Tags: Performance*

---

**Michael** - *17:01:09*

Calculating it is simple the question is how best to use it.

*Tags: General Technical*

---

**Oliver Varney** - *17:47:53*

I do find the questions on what features are good in a group of nearly 1000 people who are your competitors a tad strange :face_with_raised_eyebrow:

*Tags: Feature Engineering*

---

## 2021-05-19

**Mo** - *11:34:11*

Can anyone point me in the direction of material on using Kelly with multiple outcomes *when you already have a position on one or more runners?* So an extension to: [https://en.wikipedia.org/wiki/Kelly_criterion#Multiple_outcomes](https://en.wikipedia.org/wiki/Kelly_criterion#Multiple_outcomes)

*Tags: Strategies*

---

**Michael** - *11:57:54*

Questions about international racing and now complex Kelly stuff? Surely he can't be hitting Betfair with that. Looks like [@UBS7QANF3](@UBS7QANF3) is planning something epic.

*Tags: Strategies*

---

**Ewan** - *13:09:29*

Hey Guys, first post. Lots of info in that paper but no great conclusions?

As I see it, Kelly tries to optimise the holy trinity that is edge, bank and stake. In my experience it works better at &gt; 30% probabilities. Certainly no magic formula?

Not sure if I would apply it to multiple selections but maybe more the intent?

*Tags: Strategies*

---

**user34** - *13:11:43*

I read that thesis a while ago and I remember that most of the results are numerical for the interesting questions.

*Tags: General Technical*

---

**Ewan** - *13:14:46*

Yeah, I'm sure you do! Its like the old Monty Hall problem, always does my head in! :joy:

*Tags: General Technical*

---

**Paul** - *18:35:11*

To answer your question though, multiple people have claimed to come up with derivatives that take this into account, but the only one that looks like it had legs and spelled it out last time I looked was a PhD thesis from Imperial. That said some of those links are new to me.

*Tags: Getting Started*

---

**Mo** - *18:41:51*

Can stick to backing if that helps

*Tags: General Technical*

---

**Paul** - *18:43:56*

OK, so I think I found something on extending the stuff I fixed in the wiki article. Laying is an entirely new thing I know I couldn’t find anything on, but I’ll do some digging around tonight and tomorrow based on stuff I was doing at the time

*Tags: Errors Debugging*

---

**Paul** - *18:46:27*

My intuition says just factoring in existing positions into new bet sizing is going to be where this ends up, but will take a look.

*Tags: General Technical*

---

**MacrcoPolo** - *20:07:28*

You won't get something exact - the wiki algo is just a numerical optimization recipe

*Tags: Performance, Strategies*

---

**MacrcoPolo** - *20:08:00*

Maybe you can get a near-exact approximation, but your objective is maximizing the log-wealth function, and in general that's a numerical optimization

*Tags: Performance, Strategies*

---

**MacrcoPolo** - *20:10:04*

You can limit to a fixed number of iterations if you want to get an approximation that is reasonable in a short time

*Tags: Errors Debugging*

---

**MacrcoPolo** - *20:18:26*

Was this around the time that limit texas hold em was solved?

*Tags: General Technical*

---

**MacrcoPolo** - *20:19:49*

I think limit was solved pre-2010, but not 100%

*Tags: General Technical*

---

**Paul** - *20:57:15*

I’ve been digging through some papers and notes from last year to answer original question. Short version: I couldn’t find a paper with an answer and settled at the time (but have not had reason to implement) a solution that calcs what the bets would be right now, then look at the delta and move positions accordingly and optimise toward new liability limits (i.e. make sure not overbetting, even if that means not being able to maximise profit based on new Kelly calcs). No proofs, no papers, just me and my not very sophisticated maths

*Tags: Strategies*

---

## 2021-05-20

**Kai** - *17:20:47*

I don't know if this is stating the obvious, but if you have variable odds and increase positions then Proebsting's paradox should become relevant.

I'm not sure what exactly that implies but it might mean that not all results from classical Kelly carry over.

*Tags: Strategies*

---

## 2021-05-21

**Kai** - *10:49:53*

The latter. I think the way I dealt with it when I had a related problem was to just look at the payoffs and maximize expected growth directly

*Tags: General Technical*

---

**Paul** - *23:00:05*

There are situations where if you do the working out you might find you’re unable to get where you want to be. Solving for the bets you need might be complex enough - especially on multiple positions in the same market - is perhaps the numerical optimisation problem that [@U0135SLHQBW](@U0135SLHQBW) was pointing towards maybe. A dunce like me would reach for a green up algorithm or unwind, and go again, but that might not be the most efficient route by far.

*Tags: General Technical*

---

**Paul** - *23:16:22*

This paper describes a betting exchange simulator that simulates a large number of events and agents to help train ML. The theory has a long track record in fiscal markets but it still feels… odd… maybe useful to train and then use actual historical data for testing &amp; validation? [https://arxiv.org/pdf/2105.08310.pdf|https://arxiv.org/pdf/2105.08310.pdf](https://arxiv.org/pdf/2105.08310.pdf|https://arxiv.org/pdf/2105.08310.pdf)

*Tags: Data Quality, Strategies*

---

## 2021-05-25

**AndyL** - *21:26:33*

Ill iterate not a problem

*Tags: General Technical*

---

## 2021-05-29

**Dave** - *10:40:25*

Bit of an open question maybe...suppose you have a history of bets, how could one ascertain that one should have used Kelly for staking? My scaling up to date has been generally market-capacity based (i.e. if the market looks like I can accumulate larger positions without significant impact then I will, regardless of my perceived EV on this position), but I cannot tell how much "better" my results would be if I took advantage of Kelly criterion.

*Tags: Strategies*

---

**thambie1** - *10:54:01*

Kelly reduces variance in returns. If you look at your profit graph and see huge swings and are not happy about it, you could use Kelly to reduce the swings.

*Tags: Strategies*

---

**thambie1** - *10:55:27*

This can be especially helpful if swings/variance is the limiting factor preventing you from scaling up your strategy

*Tags: Strategies*

---

**MacrcoPolo** - *12:50:55*

No no... full kelly dramatically increases swings

*Tags: Strategies*

---

**MacrcoPolo** - *12:51:32*

(Your chance of doubling your bankroll before halving is ~2/3rds ... so at full kelly you will halve your bankroll 1/3rd of the time)

*Tags: Strategies*

---

**thambie1** - *13:03:30*

Yeah, but no-one sane uses full kelly

*Tags: Strategies*

---

## 2021-05-30

**PeterLe** - *09:47:50*

Re Kelly; I’m with [@U0135SLHQBW](@U0135SLHQBW) on that one. As I’m in play only, I have my own version of Kelly ie it performs the same function BUT it only applies it on the fly on races where I believe I have a bigger edge. It recirculates some of the profits back in, in real time. 

On the odd occasion it triggers you can have some spectacular results

So you don’t think you need to think of a Kelly like principle as a binary, you can choose when and if to use it.

I’ve named this system ‘my ratchet system’ :smile:

*Tags: Strategies*

---

**Dave** - *10:05:42*

Ah cool cool. I'm fairly happy with the vol of my returns (in fact could probably handle more vol if absolute return was larger), so I'll just continue to stake as a function of market capacity and my own risk limit, and introduce Kelly once I exhaust these and find I don't like my pnl path

*Tags: Strategies*

---

## 2021-05-31

**Ewan** - *13:17:10*

Using Kelly and to what fraction vs other staking plans really depends on your approach and results I've found.

Generally Kelly is good for lower-odds and say, percent-of-bank is better for larger odds.

Proportional staking can be used but might sacrifice overall profit for reduced draw down.

All I can suggest is graphing the plans out on your results and see what you like the look of?

I'll find some examples I did a while back

*Tags: Strategies*

---

**Unknown** - *13:23:54*

So, this was a 1370 bet sample with average odds of $8, didn't seem to suit Kelly (graph is half Kelly, 10% edge assumed, was about 10% pot) Y axis scale is different.

*Tags: Strategies*

---

**MacrcoPolo** - *13:35:09*

Kelly is mathematically optimal. The hard part is estimating your true edge. Kelly also assumes you're not at market capacity. If you are, then kelly doesn't matter - you're limited by the size you can execute in the market. Other staking systems are fine, and a simple, conservative, fractional system will get you most of the same benefits (no risk of ruin being the primary one, but you also don't need to reclaculate your bet-size). The issue is that your capital growth isn't as rapid as with kelly, which sizes your bets appropriately. But again, you need to know what your edge is. One of the reasons people use fractional kelly is because estimating this is hard, and fractional kelly gives you more wiggle room to be wrong without negatively affecting your log growth rate. Just as a final point, If you bet &gt; 2x true full kelly, your capital growth is actually negative and your bankroll will -&gt; 0 over the long run (but it will never actually hit zero because you're wagering fractionally).

*Tags: Strategies*

---

**Ewan** - *14:01:08*

More than likely! Always been my question with Kelly; what to input as the edge?

I would have thought pot would be an okay substitute but maybe not. I think in this particular dataset the profit was all in the higher odds runners (probably coincidence) and in that scenario Kelly didn’t work. 

Do you think that Kelly works in situations with high odds and high odds-ranges?

*Tags: Strategies*

---

**Dave** - *14:17:52*

`The hard part is estimating your true edge` - yup this has been a primary concern for me in terms of using Kelly. I usually take quite a generous margin over my forecast, so consider a relatively large margin of error essentially.

*Tags: Errors Debugging, Strategies*

---

**thambie1** - *14:32:44*

Kelly does work in high odds scenarios

*Tags: Strategies*

---

**Michael** - *15:09:57*

Could Kelly be adapted to account for a stake/value trade off? What would that look like? For what I do estimating edge  is quite easy if you're content with an estimate of your edge at the point of execution, it's the matching that messes it up.

*Tags: Strategies*

---

**Michael** - *15:10:36*

I doubt that Kelly has much practical application for most of us but it is interesting.

*Tags: Strategies*

---

**thambie1** - *15:19:35*

By stake value trade off do you mean some model accounting for adverse selection based on available sizes?

*Tags: Strategies*

---

**thambie1** - *15:43:50*

Yeah it's possible to perform kelly calculations given existing positions, and you'd just adjust your edge as it changed and incorporate your already existing stakes

*Tags: Strategies*

---

**Michael** - *15:48:54*

That's not what I mean. Say you estimate your edge at X then submit a bet with infinite stake, the amount that you get matched would cause you to revise your estimate of that bet's edge. So your Kelly optimal would be Y% of your bank, but if you could match that much then your edge estimate would be revised down and you'd have staked too much.

*Tags: Strategies*

---

**Michael** - *15:50:17*

Many of us will have worked out value/stake trade off curves from data, but can you adapt Kelly to serve this function?

*Tags: Strategies*

---

**Michael** - *15:55:45*

Or let me put it another way: Say you submit a huge bet at a random price and have no estimate of EV. If none of it matches it must have been +EV, if it all matched -EV and something in-between gives you central values. So you can infer value from a size matched (to a degree). Can you factor that into Kelly such that your optimum stake takes account of that adverse selection?

*Tags: Strategies*

---

**thambie1** - *15:58:33*

No, Kelly can't do that for you. The trade off curve you allude to is specific to the market you trade on, so there is no way for kelly to do it for you. You'd just have to perform your own estimate, and then adjust the inputs to kelly accordingly.

*Tags: Strategies*

---

**MacrcoPolo** - *15:59:43*

I think that yes you can make adaptions - the fact you managed to get filled does indeed adjust your estimate of your edge, but you would just need to model your edge as as function of the size you're filled at.

*Tags: Strategies*

---

**MacrcoPolo** - *16:00:38*

Kelly boils down to maximizing your log-wealth growth, that function is pretty flexible and you can adapt it a lot, provided you can do the maths for the maximization.

*Tags: Strategies*

---

**Kai** - *20:31:54*

That's an interesting question. If you have inferred a function that gives you an edge depending on stake size, then you could use Kelly to choose the size that gives you highest log wealth growth.

However if your bankroll is very large compared to the maximum possible +EV stake then the outcome should be "take everything with value" as your risk of ruin (or of reducing your bankroll to a tiny amount) is negligible.

So you could use it to balance risk vs +EV turnover, but if you have maxed out for market capacity already then Kelly obviously couldn't help you increase the market capacity.

In reality the problem would probably be more murky as you have a region where you don't know exactly at which size your bets start to leak value. I'm not sure how much Kelly could help you with that as it is ultimately about risk control and can't tell you where you reach the -EV region (it is always assumed that you know your edge).

I would love to have such a magic function for edge vs stake size for my strategies btw.

*Tags: Strategies*

---

## 2021-06-01

**AndyL** - *08:51:39*

As you point out i now have aus and greyhound stream data courtesy of flumine

*Tags: General Technical*

---

**Aaron Smith** - *13:56:53*

Its obvious that this is not an easy task. Just by how betting works (being 0 sum - commission) its impossible for it to be easy, as then everyone would be making money, which goes against the 0-sum nature of it all. Also note that the hints you get here, while most likely ppl really want to help you and get you to look into the right directions, will never be the full story. Noone is going to just give away their working strategies.

*Tags: Strategies*

---

## 2021-06-08

**Aaron Smith** - *08:04:53*

You probably dont have to be the very first to come up with the general idea behind your strategy (if you do that could be some nice money i guess, but its not necessary). Chances are you ll execute it slightly diffrent or maybe combine it with something else.

*Tags: Strategies*

---

**liam** - *08:31:59*

I think you need to change your attitude first, I have been doing this for about 10 years now, many on here have been doing it longer, it takes more than a few months of part time work. Random is a waste of time (as I thought you had already learnt) have you tried modelling what is normal and then going from there?

*Tags: Strategies*

---

**AndyL** - *08:40:45*

So yes, been trying normal models, but normal is based on what I think is normal, and ive found backtesting those don't seem very good, in fact doing the opposite of what I think would work is better!

*Tags: Strategies*

---

**PeterLe** - *09:29:48*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) In my opinion, you are missing an important step. [@UGV299K6H](@UGV299K6H)  has already given you the answer; he said: I"f you want to be really successful it's not about finding some magic combination of obvious parameters, it's about coming to some kind of understanding of market dynamics that other people don't have”



I don’t think you will better that advice. In the many years Ive been doing this, the single stand out attribute/trait of the very successful people Ive met or spoken to is encapsulated in that statement. They think differently having acquired that understanding. Fix that first (yes it may take you years), then move to step two, You cant fast track it

*Tags: Errors Debugging*

---

**liam** - *09:44:45*

You have data, use that to model normal, no thinking involved at this point

*Tags: Strategies*

---

**Michael** - *10:48:45*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ): The likelihood of your random strategy succeeding is vanishingly small.

*Tags: Strategies*

---

**Michael** - *10:49:15*

Actually this conversation and others like it have made me somewhat re-think my advice to new players. In the past I've told people just to write almost any strategy, get some bets on and then work on the results. Although that's right for some people it's obviously not right for everyone.

*Tags: Strategies*

---

**Jonjonjon** - *10:59:14*

I do horses pre-event. I'm currently taking your prior advice of going back to basics, and re-working my pricing model. It's been making a London software developers' salary for a few years, but when I revisited it, I found that due to a bug, the code doesn't actually produce a number a lot of the time, so once that is fixed I will return to my IP quest.

*Tags: Errors Debugging, Strategies*

---

**AndyL** - *11:19:24*

It's poluted my head though! For example when [@U4H19D1D2](@U4H19D1D2) mentions model the data i have, I don't really understand what that means other than basic aspects of manual traders and betangel videos...

*Tags: Strategies*

---

**Jonjonjon** - *12:21:06*

DOing this also helps reduce the number of orders I place.

*Tags: General Technical*

---

## 2021-06-09

**AndyL** - *15:32:41*

Ive even had BA members dm me offering to sell me a profitable strategy for £1k :rolling_on_the_floor_laughing: :rolling_on_the_floor_laughing: :rolling_on_the_floor_laughing:

*Tags: Strategies*

---

**Newbie99** - *15:58:56*

...it does, but especially when starting out, bankroll management is important too. There is a balance between maximum value and variance.



Everyone on here has differing bankroll sizes, some can take larger variance (or simply have a substantial edge) and therefore they have no need to ever green up, for some it may make sense (whilst accepting this will erode value) to hedge sometimes.

*Tags: General Technical*

---

**Jonjonjon** - *16:03:50*

Isn't it standard to green up? I'm not trolling. I usually green up, and believe that for my strategy it's the right thing to do.

*Tags: Strategies*

---

**D C** - *16:11:10*

The school of thought on the greening up seems to be that if you are on the winning side of an entry/exit trade, your entry will be value because the market prices have a tendency to correct themselves to where they "should" be. Therefore, if you are entering your position at value, why green up as you should be +EV. Furthermore, that greening up is seen as -EV. Obviously this requires vastly different bankroll management to what most punters see as manual trading with much higher percentage of their bankroll. I am just summarising how I understand the argument as presented by others on here, so as always someone else will correct this if it is wrong.

*Tags: Strategies*

---

**river_shah** - *16:12:46*

Depending on your strategy you should green up or not. There is not one blanket thing that is correct. If you are super good at predicting the next tick but have crap long term modelling skills, by all means green up.

*Tags: Strategies*

---

**river_shah** - *16:13:05*

If you have good fundamental models, never green up

*Tags: Strategies*

---

**river_shah** - *16:16:37*

Most players will have a blend of both skills and they should hedge accordingly. There are PC considerations too and there are also sharp money considerations (typically sharp money will put you in positions that will be -EV if you hold to end so may make sense to green up; you have to model this yourself because if you do it right you could be the sharp money in which case you want to keep all the risk on)

*Tags: Strategies*

---

**liam** - *20:07:16*

My problem with ‘sports trading’ is the idea sold around trading sports by opening and closing positions whilst completely ignoring how value makes it profitable. It seems that 95% of the BA forum (even those who are profitable) simply don’t understand this

*Tags: Strategies*

---

**Atho55** - *21:18:07*

An opinion on how to find value in the pre market would be appreciated [@U4H19D1D2](@U4H19D1D2)

*Tags: General Technical*

---

**V** - *22:15:29*

Backtests are important tools, but they should be as realistic as possible. Latency is quite important (depending on what you’re doing) you can quite easily find a great backtest with 0 latency but it’s for good reason… it’s impossible to trade it in real life! Or maybe the strategy is too slow for the message throughout and your queue gets backed up, so you can’t trade on all price updates in practice… the backtest won’t show the latter, production will.

*Tags: Performance, Deployment, Strategies*

---

**Jonjonjon** - *23:00:41*

Which markets take an hour to process? I think I can do 1,000 AU horse markets in a few minutes using Flumine.

*Tags: General Technical*

---

**AndyL** - *23:01:08*

I only got flumine up and running 3weeks ago..

*Tags: General Technical*

---

**V** - *23:02:51*

Your strategy is probably slow

*Tags: Performance, Strategies*

---

**AndyL** - *23:18:21*

I wouldn't know how to start to prove it!

*Tags: General Technical*

---

**Jonjonjon** - *23:19:45*

So try using this to profile the code: [https://docs.python.org/3/library/profile.html](https://docs.python.org/3/library/profile.html) , and then if it's a bottleneck, people here can help.

*Tags: General Technical*

---

**Jonjonjon** - *23:20:19*

People here will generally be able to help with better code. It's just that they are less likely to give you secret sauce. But an SMA implementation is not secret sauce.

*Tags: General Technical*

---

**Michael** - *23:32:50*

I know you're getting a lot of suggestions [@U01PJ5YMFBJ](@U01PJ5YMFBJ) but here's mine: Forget about back tests and see if you can answer that question - either go and find out how you evaluate the efficiency of a set of markets or invent your own way of doing it. At the trivial level it looks easy: Do half of all bets at 2 win? But when you think about it it's not that simple. Work that shit out until you understand it and at the end of that maybe you will have understood something about probability or the workings of markets that you didn't before and maybe you'll have had some kind of original idea or bit of insight that you can use to make a proper strategy.

*Tags: Strategies*

---

## 2021-06-10

**AndyL** - *00:10:07*

So basically I need to come up with a horse racing model that works better than the wisdom of the crowd...?

Not sure that will ever happen!

*Tags: Strategies*

---

**Mo** - *05:43:44*

1. Maybe pay more attention to the genuinely successful professional gambler trying to help you than a random article on the internet?

2. What are the requirements for wisdom of the crowd and do they apply to Betfair?

3. Are you betting with bookmakers? Because both of the academic papers referenced analyse bookmaker data and that's what the article focuses on

4. If you want to explore this idea further then go and read the two academic papers referenced in the article ([https://link.springer.com/article/10.1007/s12197-020-09507-7](https://link.springer.com/article/10.1007/s12197-020-09507-7) and [http://www.reading.ac.uk/web/files/economics/emdp201910.pdf](http://www.reading.ac.uk/web/files/economics/emdp201910.pdf)) yourself rather than relying on this guy's biased summary. Then reproduce their analysis. Then adapt it for Betfair data

5. Engage your brain more

*Tags: Strategies*

---

**Jonjonjon** - *08:38:18*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) To get you started at Michael's question:



"At the trivial level it looks easy: Do half of all bets at 2 win? But when you think about it it's not that simple."



What if the market (best back/lay) is at 2.0 / 10.0, and you get matched at 2.0. How often do you win?



What if the market is 1.5 / 2.0. How often do you win?



If people have been telling you to trade standard technical analysis nonsense, they might try to categorise strategies into mean reverting and trend following. But they don't point out that if you are constantly crossing the back/lay spread without good fundamental reason, then you're probably going to lose.

*Tags: General Technical*

---

**Unknown** - *09:07:35*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) I can`t help feeling that on your journey you have missed out taking some fundamental steps to understand how the market works and think that coming up with a horse racing model(s) is the solution. This is what helped me, setting up BA to display just money moving and how it looks as a %. Just a case then of recording a load of races, look when the odds move, rewind a few secs to see what caused the move and write it down. Then look for some links in what you have scribbled. Rinse and repeat then test and tweak. This was the template I used. Model at the end not the beginning.

*Tags: Strategies*

---

**Scott** - *14:00:13*

Regarding hedging, I assumed before coming here that most 'traders' were looking for models that got in and out of bets (best case laying below true odds and backing above), from reading back and the last couple of days talk it appears thats not the case, so when backtesting are the majority testing against the races result? (placing x number of bets that meet their criteria pre off and ‘letting them ride’) Im interested in the pre off guys as thats where I will be starting.

*Tags: Strategies*

---

**Oliver Varney** - *14:04:13*

I'll go against the grain, but with the condition that you are a newbie, I think all newbie should hedge. In the long run its a no no. When you are learning it will help you focus on beating bsp and you are very very likely to make mistakes which are amplified massively when betting

*Tags: Strategies*

---

**Scott** - *14:07:22*

Is that purely to reduce bankroll swings?

*Tags: General Technical*

---

**Oliver Varney** - *14:25:32*

Basically my view is that it's best to learn how to walk before running, suboptimal but it's about getting your foot in the door and not getting discouraged

*Tags: General Technical*

---

**AndyL** - *14:33:05*

With my current strategy i have a sub optimal hedge at schedules off, mainly just offering a hedge bet at reverse price with market_on_close 

But I'm thinking i need to more comparison with bsp, as the auction can change the price to a certain extent and im not achieving full value i may have ascertained with the original bet

*Tags: Strategies*

---

**Michael** - *15:00:44*

Yes - If you decide to hedge every bet then your hedging will inevitably be -EV so if your underlying strategy doesn't generate enough value you can turn it negative.

*Tags: Strategies*

---

**Scott** - *15:07:12*

more +ev bets = more profits.. think ive solved it

*Tags: General Technical*

---

**Unknown** - *23:27:00*

So ive just done a dual backtest of my current strategy hedge vs theoretical BSP hedge of same strategy, and it shows im quite poor +0.06% v +0.34%, so a -0.28%, of course might be variance, but here's how it looks:

*Tags: Strategies*

---

## 2021-06-11

**AndyL** - *07:45:54*

Thanks [@U9JHLMZB4](@U9JHLMZB4), yes realise the data issue but that's gathering in the background 

Need to spend a bit of time optimizing my python as it's taking a long time each run

*Tags: General Technical*

---

**Aaron Smith** - *09:23:00*

i assume he means you should try and come up with a strategy first and only if you believe you ve got something good, you use the backtest to confirm your belief, as opposed to throwing random strats into the backtest-machinery.

*Tags: Strategies*

---

**Michael** - *09:25:45*

The basic problem here is that [@U01PJ5YMFBJ](@U01PJ5YMFBJ) doesn't want to do the difficult/hard work bit. That's fine if he's not interested in succeeding.

*Tags: General Technical*

---

**Mo** - *09:29:12*

I don't think it helps that there are so many opinions going around that he can pick the ones that appeal to him rather than the ones that are going to help him

*Tags: General Technical*

---

**liam** - *09:49:52*

We do need some bflw stash…

*Tags: General Technical*

---

**Unknown** - *10:15:06*

So i spent 4months part time watching ladders with BA last year which helped alot

Ive had numerous discussions on market dynamics and getting value, which is where i am at with my current bot, and my main concentration. I want to work on  this more.

*Tags: General Technical*

---

**D C** - *10:22:40*

I would also add that when you are feeling frustrated with it all, taking a break for a couple of days can help. Nothing worse than getting pissed off because something you think should work doesn't work.

*Tags: General Technical*

---

**D C** - *10:33:26*

You are starting off in a stronger position than I did because you have BFLW/flumine and getting good advice from people on here who are making a shitload of profit every year.

*Tags: General Technical*

---

**mandelbot** - *11:43:42*

So I've been toying with a few LAY pre-off horse racing strategies which are shown to be profitable in backtests, but I've found that the BACK versions of them are not profitable. This has casted a bit of doubt in these strategies, should it? Should a strategy and it's inverse both be profitable?

*Tags: Strategies*

---

**Michael** - *12:11:23*

It would certainly be better if the inverse of a lay strategy worked and if you're re-pricing the whole market then you would expect it to. On the other hand if you're not re-pricing everything it might not be easy to say quite what the inverse of a lay strategy is in a multi runner market. So it's not great but I wouldn't despair.

*Tags: Strategies*

---

**Aaron Smith** - *12:47:10*

I suspect [@UBS7QANF3](@UBS7QANF3) is gonna throw a bunch of maths at us to make him look smart, when what i really want is an easy step by step guide on how to buy that yacht

*Tags: General Technical*

---

**thambie1** - *13:09:39*

It would be better if the inverse strategy works, but it a strategy can still be useful if it doesn't. It just means that there's some bias in your predictions that's pushing your odds in one direction. So when betting in the opposite direction of your bias, you're fine, you're going to make money. But if for example, you're biased to lower odds AND laying, you're going to have a problem. I have some strategies where I haven't fully resolved the bias, so I'm only betting on one side.

*Tags: Strategies*

---

**thambie1** - *13:18:40*

Yeah. If you're going off price action, it's a whole different ball game. Personally, I wouldn't be put off at all by only one side of a price action strategy working. Price action is in a way exploiting human psychology and patterns, which you could imagine tons of reasons why a given strategy of that style only works in one direction.

*Tags: Strategies*

---

**Scott** - *15:34:18*

Awaiting my live key [@U9JHLMZB4](@U9JHLMZB4), was going to place a few manual football bets on euros automatically. If that makes sense just so I can get used to what I’m sending and any feedback flumine/bflw gives me. 

*Tags: Deployment*

---

**ShaunW** - *16:30:42*

@goatwish you say the python is taking a long time to run. Compared to what?  



Sounds like a case of choosing something from your ToDo list you can do rather than what needs to be done? Looking busy might work in the PAYE job but you can't cherry pick the jobs now. :wink:

*Tags: General Technical*

---

**liam** - *17:33:04*

Error between keyboard and chair regarding speed, flumine/bflw is capable of about 1k markets per minute out the box 

*Tags: Errors Debugging, Performance*

---

**mandelbot** - *21:02:48*

Mine take significantly longer than this. Should I be using the C BFLW or normal?

*Tags: General Technical*

---

**liam** - *21:49:11*

As far as performance goes it’s brilliant, software compatibility has been a pain although things improved a lot since the beginning 

*Tags: Performance*

---

**AndyL** - *23:40:36*

[@UEA14GBRR](@UEA14GBRR) optimsed my python strategy, now runs in half the time... note im goatIwish !

*Tags: Strategies*

---

## 2021-06-12

**birchy** - *01:42:14*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) I've been sat on the sidelines reading the various threads on here and feel your frustration with this whole "easy money" malarkey. I've been interested in gambling for over 30 years. For the first couple of years, I was a typical "mug" punter who did a 25p EW Lucky 15 every Saturday and watched the horse racing on TV. It was great entertainment and not a particularly expensive hobby, because I was pretty decent at reading form and had a lot of "unusual" insights that gave me a slight edge. During that time, my father and I decided to put £10 each into a pot and we both picked a few horses every Saturday. I always placed mine as single bets, ranging from £1 to £10 depending on how strongly I felt about the horse's chance of winning. He always did a Lucky 15 or a 3 horse Patent. From that point onwards, I no longer did Lucky 15s or any other kind of multiple bet beyond a (very) occasional EW double. And that was because I realised quite early on, that when I had 1 or 2 winners out of the 4, that backing them as singles paid out far more often, albeit at the cost of missing out on "the big one". But more crucially, I also realised that you could build a bank by taking lots of small profits rather than waiting for occasional bigger payouts. After 18 months, we had just over £1000 in the pot, most of which came from my bets. We split it 50/50 because I'd just discovered beer and women. The money didn't last long.

Several years later, I started looking at betting again and developed many strategies, writing them all in a notepad. Around 1999/2000, I bought my first laptop and discovered spreadsheets. And porn. One pixel at a time, because internet was dial-up back then. I learnt VBA because Excel just wasn't cutting it. Then VB6 because VBA was too slow. Then I discovered Flutter, who were bought out by Betfair shortly after. While watching the markets in bewilderment, I noticed that the books would occasionally go the wrong side of 100%. Knowing how bookies made their money, I attempted to catch these spikes manually. It didn't go well. So I made it my mission to automate it. It took me the best part of a year to learn how HTTP, HTML, Ajax, etc worked. It was hard going as I was never academically minded. But I never gave up. Finally, I had a home-made web browser spoofing, 50 times a second polling, page scraping, data collecting, bet placing API that (so I thought) was totally unique. Another year or so of tweaking and testing followed. I made a few grand from that strategy, but it took me several years to get there. Then competition started to grow and finally, it died a long, slow death, particularly when cross-matching was introduced.



Roll forward to the present.... I still have lots of "wacky" strategy ideas that I need to test but rarely have enough time to give them the devotion they need. The markets are far more efficient nowadays than they were years ago, BUT small edges (&lt;=0.5% ROI) are still fairly easy to find and can make reasonable long-term profits with lower variance. The biggest issue with these is getting large quantities of bets matched. You won't become a millionaire overnight, but it's a step in the right direction.



To put things in perspective, the professionals on here are generally peaking at 4% or 5% ROI over millions of bets. Mere mortals like us can't even get close to that level without a LOT of work. Or a lucky find.



So my suggestion is: go right back to basics. Pick an obvious strategy like laying 3 runners at &lt;3.0, backing both players in 2-player markets to achieve a book &lt;100% or even laying any runner&lt;2.0. If executed correctly, all of these will make a small profit. And it's obvious why. Then work on increasing your match and hit rates. Pick ONE strategy and do everything you possibly can to make it work. Avoid shooting off at a tangent when you think of something else. Write it down. You can work on that next year.



Above all, never attempt to build a "trading" strategy where you are intentionally hedging your bets. Trading is for mugs. There's nothing wrong with hedging bets in order to reduce volatility, but, as [@UGV299K6H](@UGV299K6H) has already said, if your initial bets are not profitable, then your hedge bets certainly won't be.

*Tags: Performance, Strategies*

---

**PeterLe** - *11:59:45*

It was interesting reading [@U016TGY3676](@U016TGY3676) story. I remember my dad having a bet each Saturday (5p Yankee!, 55p). At the time I had just started my first paper round at 12 and he would often ask me to stop by the bookies to put a bet on for him at 12!

Just on a side topic, my youngest son has obviously noticed me toiling away learning Python and just recently he said "I think Id like to learn that dad whilst I have the summer free" so Ive set him up pycharm and he has access to my Udemy course

Just an interesting contrast between what I had with my dad and him checking the 'form' and ringing the selection with his pen and how things are more high tech today :grinning: I often wonder what my dad would think if he knew how many bets I was doing each day

*Tags: General Technical*

---

**thambie1** - *18:05:08*

Yeah, I did this full time for 9 months before breaking through :thinking_face:.. but then again I built my own version of bflw and flumine in that time too

*Tags: General Technical*

---

**AndyL** - *20:07:39*

Good point [@UGV299K6H](@UGV299K6H) the criteria for stopping... im not losing money now ive got bflw &amp; flumine setup, at some point ill need to go live with something but im going to set that stop point...

*Tags: Getting Started, Deployment*

---

## 2021-06-13

**Mo** - *15:31:01*

I have a strategy where I would like to lay at BSP.



I want to size each bet so that my prospective win amount is 1 unit. In other words, for each bet the backer's stake will be 1 unit.



My understanding is that for a Market On Close order, I can only specify the liability not the backer's stake. This makes sense as you don't know what price you will get filled at.



From discussions with [@U4H19D1D2](@U4H19D1D2), I've realised it is possible to place a Limit On Close order which would let me cap the maximum BSP I would lay at (with the bet being cancelled if BSP is above this price). I am thinking that I can use this to achieve my goal as follows:



1. Look to execute bets a small number of seconds before the market start time, e.g. 10 seconds before

2. Roughly predict what the BSP would be. I am thinking to use the near SP field

3. Take the near SP and add a few ticks to allow for some movement. This will be the `limit_price`

4. Work out `liability` as `unit_size * (limit_price - 1)`

5. Place a Limit On Close order at `limit_price` with liability `liability`



Thoughts? Anyone already doing something similar?

*Tags: Strategies*

---

**AP** - *19:09:59*

I backtested a strategy on IP AUS racing using flumine. It had a reasonable number of bets and based on 12 months of back testing the expected ROI was 6%, now after roughly 6 months of real betting the ROI is at 4% so I would say it’s reasonably accurate but you may want a buffer in case. 

*Tags: Strategies*

---

**liam** - *19:19:14*

[@UQL0QDEKA](@UQL0QDEKA)  very dependant on what you are doing, for example flumine has a few limitations in terms of doubling matching liquidity and delays etc. Inplay/volatility certainly makes things worse. As long as you factor this in and don’t go mad with stakes I actually find things to be better when live (inplay horse racing) 

*Tags: Deployment*

---

## 2021-06-15

**AndyL** - *19:31:19*

What are peoples views on these 2 ends of the spectrum?

1) A strategy that has a return of 1% and a trigger rate of 100 trades in say 1week

2) A strategy that has a return of 0.1% and a trigger rate of 1000 trades in 1week 

??

*Tags: Strategies*

---

**AndyL** - *20:15:37*

Obviously backtest, so small slippage or error and ....!

*Tags: Errors Debugging*

---

## 2021-06-16

**AndyL** - *07:51:31*

The edge size was my worry, hence questions yesterday.... 

To come up with these i actually took PeterLe's suggestion thinking about simpler situations and how crowd might be reacting, so seemed good it came up with something albeit small... but in reality im not sure im going to find much more..

*Tags: General Technical*

---

**Michael** - *07:52:43*

0.1% ? Yeah maybe you can work with that if you are experienced but you've got so little room for error that you really can't afford any mistakes or fudges. Any strategy you design WILL have less edge in real life than in testing because all testing tends toward over-fitting, you should build the assumption of an over-fit into your process. The more experienced you are the less allowance you need, with no disrespect I'd suggest you make a big allowance.

*Tags: Errors Debugging, Strategies*

---

**Michael** - *09:47:25*

Truthfully I'm not sure what the best mathematical way of doing it is. For my own part it's a 'lick of the finger' thing, I know what the difference has been in the past so I go with that. You always have to assume a bit of adverse selection, but what you're really looking for here is an overfit bias - which doubtless has a proper mathematical name but I don't know what it is.



There are statistical ways of estimating over fit that others can elucidate better than I can, my own approach would be more about evaluating the process and one's own level of experience. You can simply step back and ask yourself honestly _'what's the probability that I did this right?'_



Having done that; if you have promising results I'd still usually advocate a very slow and cautious switch on starting only with your best bets - i.e. those bets where your signals are strongest. Impatience never pays, if your strategy is sound there's no reason to rush and if it's not sound you'll be glad you didn't.

*Tags: Performance, Strategies*

---

**Beeblebrox** - *10:04:45*

Yeah, I have a couple of strategies that are working, one in the pre-race win market, which I put together using a genetic algorithm to determine the 'best' parameters to use for the strategy. Its been pretty consistent since last August, but I don't really know _*why*_ it works because of how it was put together!



The other one places bets in a couple of derivative football markets and was conceived via a logical thought process and it's returning between 5-10% since January, so I'm quite pleased with that one.  There's still room to scale it up I think, but it's not going to make me a millionaire!



I've got others that I'm testing now in the horse in-play market, but they're break-even at best at the moment.

*Tags: Strategies*

---

**Unknown** - *12:25:40*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) I struggle to see why you are flitting from one market type to another (Aus, Greys) other than hope you strike it lucky. Anyway, before the racing resumed last year I went into the data and selected a series of odds and rank/odds as groups then set them up to update to a table. So for over 1 year they have been updating, some good others so bad that they have been replaced. You are more than welcome to look at the list, pick any out that may interest you and I will post the underlying criteria. These are what I think this forum calls fundamentals so will probably be of no interest to many but someone may be generous enough to also test any selection you may make to at least check your newly developed Python skills. UK Racing only.

*Tags: General Technical*

---

**PeterLe** - *15:20:57*

"Current BOTS came out of Peterle suggestion?? :confused: No not quite what I said??

Im not quite sure what you mean, but if it was reference to "why not run both" then sure, If they are known well proven strategies.  I run 9 strategies each day and some only have a small ROI. but they all add up.



This initial piece of advice still holds true 8th June:



[@U01PJ5YMFBJ](@U01PJ5YMFBJ) In my opinion, you are missing an important step. Michael  has already given you the answer; he said: I"f you want to be really successful it's not about finding some magic combination of obvious parameters, it's about coming to some kind of understanding of market dynamics that other people don't have”

I don’t think you will better that advice. In the many years Ive been doing this, the single stand out attribute/trait of the very successful people Ive met or spoken to is encapsulated in that statement. They think differently having acquired that understanding. Fix that first (yes it may take you years), then move to step two, You cant fast track it

*Tags: Errors Debugging*

---

**AndyL** - *15:25:30*

[@UQL0QDEKA](@UQL0QDEKA) apologises wrong person! It was [@U016TGY3676](@U016TGY3676) s comment i was referring to: [https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1623458534154600?thread_ts=1623458534.154600&amp;cid=CTPL3R3FU|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1623458534154600?thread_ts=1623458534.154600&amp;cid=CTPL3R3FU](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1623458534154600?thread_ts=1623458534.154600&amp;cid=CTPL3R3FU|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1623458534154600?thread_ts=1623458534.154600&amp;cid=CTPL3R3FU)

*Tags: General Technical*

---

**Frankz** - *16:54:24*

Next question. How would create an embedding of the teams players? I have worked with creating embedding of text features quite often. there we have word2vec and doc2vec but in this case the order of the players doesn't matter too much.... but here I'm not to sure, I have looked at the paper playerank

*Tags: Feature Engineering*

---

**Frankz** - *16:56:32*

currently main problem is that from training with 10,000 rows and 100,000 rows the training speed increases quite a lot. I mean I could scrap the team id and replace it with a kickscore rating or try with catboost instead.

*Tags: Performance*

---

**Unknown** - *17:01:38*

basically you can predefine the models you want to try. mlp, xgboost, and most of sci kit learns one. in the preprocessing you can try various ways to make it less sparse. group features etc. but as it often uses one hot encoding it is not very good if you have &gt;1000 teams...

*Tags: Feature Engineering, Strategies*

---

**Mo** - *17:07:14*

Depends on the model

*Tags: Strategies*

---

**Dave** - *19:06:09*

Individual team performance has rather high variance, probably better to cluster teams first maybe and then use those clusters as features

*Tags: Feature Engineering, Performance*

---

**Unknown** - *21:44:54*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) just to clarify what I suggested, although low margin strategies are not ideal, they ARE easy to find and with sufficient quantity of bets produce returns like the attached graph. That's one of my strategy's last 10K actual bets, over ~13 days. ROI is 0.6% and my Monte Carlo analyser that I posted on this group a while ago says that there is a 0.03% chance this was a lucky streak. You should be able to find something like this fairly easily if you have the right mindset.

*Tags: Strategies*

---

## 2021-06-17

**birchy** - *07:39:01*

To add to the above, that strategy took several months to develop but is built upon market knowledge I've gained over the years and was NOT a "backtest today, go live tomorrow" strategy. As with all strategies, it started with a sensible idea but then I had to implement it and overcome lots of anomalies, execution issues and unforseen bugs. And then, when it worked as I wanted, I simply WAITED. I didn't touch anything, I didn't start "tweaking" parameters and I didn't obsessively check my P&amp;L every 5 minutes. Doing nothing is the hardest part. Learn that level of patience and everything else will fall into place.

*Tags: Errors Debugging, Deployment, Strategies*

---

**Peter** - *17:16:50*

Slightly embarrassed to admit this, but I have data for this question. In 239,794 greyhound races between December 2016 and (coughs) a few minutes ago, 712 went off before the scheduled start time. So it is a thing, but not very common as (hat tip to [@U01HPRM31G8](@U01HPRM31G8)) greyhound racing is existentially dependent on the bookies.

*Tags: General Technical*

---

**ShaunW** - *17:20:19*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) that's why actual pl doesn't mean much,  and why tracking your theoretical pl helps your sanity. Sh!t happens but thankfully is usually reasonably fair value sh!t.

*Tags: General Technical*

---

**Aaron Smith** - *22:09:30*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) variance gets lower when bet count goes up (assuming they are independent). Succesful players probably have a handful of strategies running at the same time, also those charts are generally over a longer period of time than any strategy you may have had running

*Tags: Strategies*

---

**D C** - *22:24:10*

Shape of a PnL chart will surely depend on what you are doing. A strategy looking for overpriced longshots will be very jagged compared with a scalping bot for example.

*Tags: Strategies*

---

## 2021-06-18

**Dave** - *20:03:55*

To anyone running ML models - how often do you retrain?

*Tags: Strategies*

---

**Oliver Varney** - *20:21:05*

Most likely that depends on the learning task and how much data you include in each model

*Tags: Strategies*

---

**Oliver Varney** - *20:21:54*

If you have a model with like 10 years of data , retraining every week I doubt would change too much unless you add huge bias to recent samples

*Tags: Strategies*

---

**Dave** - *20:27:03*

Yeah, I retrain about once or twice a month. My training set is never older than past 4 months, but I hate retraining the model and re-deploying even though out of sample results are still as good as before. Maybe an unreasonable fear that things won't continue like they did on the older model

*Tags: Deployment, Strategies*

---

**Dave** - *20:33:50*

Nah, football. For my forecasting target and features, anything over 4-6 months has never added additional predictability. 4 months gives me 20-30,000 samples too which is sufficient to obtain good OOS performance. I put it down to change in dynamics as a competition progresses through its stages tbh. Im sure there are people for whom 10 years of data in football is useful tho, but their features must be way diff to mine 

*Tags: Feature Engineering, Performance*

---

**Oliver Varney** - *20:39:45*

I guess if the whole process of cross validation , grid searching parameters, model selection and deployment is all automated there no reason why you couldn't do it daily

*Tags: Deployment, Strategies*

---

**birchy** - *20:41:26*

General question...what libraries do you guys prefer for modelling/training?

*Tags: Strategies*

---

**Dave** - *20:43:23*

Yeah - automation there is the way to goooo. You're right, could do it daily, altho it's difficult to then spot if you've broken your model or it's just today's variance

*Tags: Strategies*

---

**Mo** - *20:44:24*

Run backtests with yesterday’s and today’s model and assess agreement

*Tags: Strategies*

---

**Oliver Varney** - *20:45:27*

Generally you should have an idea of the maximum performance of your data, if its past this then generally stinks of something going wrong

*Tags: Performance*

---

**Dave** - *20:46:18*

Yeah...this is what I do actually when I do my retraining, I rerun my old model on the same out of sample set and assert performance at least doesn't degrade

*Tags: Performance, Strategies*

---

**Oliver Varney** - *20:52:34*

Model stacking / ensemble also an option

*Tags: Strategies*

---

**AndyL** - *23:00:01*

Dumb question warning! Q: why would ML be any good at predicting crowd behaviour? Ie.a market? How can it predict participants? What would you use as inputs, fundamentals or technicals?

*Tags: General Technical*

---

**Paul** - *23:27:10*

I’m diving into some reinforcement learning work but am some way off from having a viable model

*Tags: Strategies*

---

**Paul** - *23:28:54*

But can you imagine how algorithms that allows you to make classifications or that could predict prices can help as an input to trading problems?

*Tags: Strategies*

---

**Paul** - *23:30:31*

Your question is “how would ML solve these specific problems”. You need to change the way you think. A better question is “ML is good at x, y and z. How can I use that to get an edge?”

*Tags: General Technical*

---

**Paul** - *23:32:17*

You are assuming you know the answer and just need to figure out the tools to help you prove it. Change it around: assume you don’t know anything and use the tools to ask questions. If that doesn’t appeal to you, this is definitely going to be very hard work and not very enjoyable.

*Tags: General Technical*

---

## 2021-06-19

**Peter** - *07:00:38*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) As an example, ML can be useful for detecting systemic bias in the crowd. So if the question being asked was "are soccer teams with large followings overbet?" you might use ML to build a model with fundamental features such as differences in home attendance, ground size, number of twitter followers, size of supporters club, shirt sales between two opponents. Or if you were wondering whether favourites or longshots are overbet / underbet in certain types of race, you could build a model using technical price data from the type of markets that you suspect could be prone to this.

*Tags: Feature Engineering, Strategies*

---

**AndyL** - *07:56:32*

Thanks [@U012XF5CNPN](@U012XF5CNPN) [@U9JHLMZB4](@U9JHLMZB4) your answers help a lot, makes sense, as you say i was thinking about it the wrong way round!

*Tags: General Technical*

---

**birchy** - *08:20:07*

I really must get my ass into gear and look at this ML malarkey. While my `if book &gt; x and ltp &lt; y...`  type triggers do generate some profits, it's not really a "model" per se. My notepad is rammed with ideas for potential features similar to the kind of thing [@U9JHLMZB4](@U9JHLMZB4) mentions above. So my question is...where would a layman start with ML, given that they are familiar with Python but a complete luddite regarding statistics, ML, NN, etc? I've looked at some tutorials online, but they all seem to assume that the reader already uses sklearn/pandas/etc...which I don't. YET.

*Tags: Feature Engineering, Strategies*

---

**Oliver Varney** - *08:39:29*

Even before any of this though I think the advise was to go away and understand how the markets work first. This cant be done in a day then you come back ask a series of questions and hope someone will give away their golden recipe. Be honest with yourself, do you want to find the answer yourself or do you want someone to tell you the answer. From all the conversations ive seen its the latter.

*Tags: General Technical*

---

**AndyL** - *08:59:00*

I like asking questions and provoking discussions, i also hope it doesn't just help me

*Tags: General Technical*

---

**Oliver Varney** - *09:04:20*

me telling you your approach is very slow and flawed wont give away my advantage over you and will help you save time. Me telling you what data I input, what the learning task is, what are good indicators,  what to look out for / take advantage will give away my advantage over you and the 1000 others in here.

*Tags: Performance*

---

**Jonjonjon** - *09:07:05*

You could start by plotting your profit Vs book. Do that for a few features. And then ML will be useful for combing a few of those features together. If you understand the individual relationships first, it can help you fit the model more successfully.

*Tags: Feature Engineering, Strategies*

---

**AndyL** - *09:14:26*

Ill stop asking the stupid questions.. found  hole, back in it now!

*Tags: General Technical*

---

**Aaron Smith** - *09:29:31*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) That wasnt a stupid question, just a sign your knowledge of ML is limited (just like mine). If you read up a little on ML, you will surely understand why ML can theoretically be good at predicting crowd behaviour. Personally i wouldnt recommend you to go down that route yet though. I m rather certain there is easier ways to make your first profits, even if those would be small in the beginning and ML could give you higher returns. The first step should be making some profits at all, once that barrier is broken (which can be hard enough) you can look out for bigger projects. Also another downside i see to the ML approach as a starter is that you often times dont fully understand why you are making money (you probs have an idea if you managed to make a profitable strat with it, but its surely not clear what is going on).

*Tags: General Technical*

---

**AndyL** - *09:33:22*

[@U01DVUAE2G1](@U01DVUAE2G1) i was just being curious given previous discussions, not going to jump into it, but the YouTube refs i will take a look at for interest

I think my asking too many questions is annoying, i probably ought to let the few 100 other members have their turn

*Tags: General Technical*

---

**PeterLe** - *09:33:42*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) just curious what your background is? Did you say you are an engineer of some description? You seemed to pick up Python quickly

By the way I don’t think there is any such thing as a stupid question if you don’t know the answer. I for one am learning off your posts

*Tags: General Technical*

---

**birchy** - *12:11:08*

Same question as [@UUE6E1LA1](@UUE6E1LA1)...

When you "plotting" I presume you mean on a graph?

*Tags: General Technical*

---

**Unknown** - *13:44:24*

Okay, a few wines in down under but here is my book vs sum profit for a strategy, is this what you're talking about [@UPMUFSGCR](@UPMUFSGCR)?

Look like I should stick to $6.60 chances in this zone!

*Tags: Strategies*

---

**ShaunW** - *15:04:31*

Adding sophistication won't help if you haven't got a grip on the basics. It's not a magic bullet and all you'll do is make the same mistakes, but faster. It's on that list of "if it's that simple then we'd all be doing it". I think [@U4H19D1D2](@U4H19D1D2) and [@UBS7QANF3](@UBS7QANF3) have some interesting things to say about ML and it's not always complementary.

*Tags: General Technical*

---

**Unknown** - *21:39:08*

This is something I recently plotted, where edge="entry price" / "exit price" - 1.0.



The y-vs-x relationship is obviously a bit unclear. And shows that I need to normalise it in some way. Linear regression won't be useful for modelling it, in its current form.



A pearson correlation on the data will give spurious results.



However, it has a spearmans rank correlation of 32%. So there's definitely something in it. I just need to sit in my chair and think for a bit.

*Tags: Strategies*

---

**Jonjonjon** - *22:08:51*

I just use matplotlib because I'm used to it. There are prettier packages out there, but I keep forgetting how to use them.

*Tags: General Technical*

---

**Ke** - *23:40:54*

Where are your main data source to build up a betting model? Do you collect your data manually or from some api provider? I'm looking for the dog weight just before the game start, but seems those data is not public available on internet until the race finishes

*Tags: Strategies*

---

## 2021-06-20

**river_shah** - *09:37:07*

```[...I joined a hedged fund, Renaissance Technologies, I'll make a comment about that. It's funny that I think the most important thing to do on data analysis is to do the simple things right. So, here's a kind of non-secret about what we did at renaissance: in my opinion, our most important statistical tool was simple regression with one target and one independent variable. It's the simplest statistical model you can imagine. Any reasonably smart high school student could do it. Now we have some of the smartest people around, working in our hedge fund, we have string theorists we recruited from Harvard, and they're doing simple regression. Is this stupid and pointless? Should we be hiring stupider people and paying them less? And the answer is no. And the reason is nobody tells you what the variables you should be regressing [are]. What's the target. Should you do a nonlinear transform before you regress? What's the source? Should you clean your data? Do you notice when your results are obviously rubbish? And so on. And the smarter you are the less likely you are to make a stupid mistake. And that's why I think you often need smart people who appear to be doing something technically very easy, but actually usually not so easy.]```

*Tags: Strategies*

---

**mandelbot** - *09:57:42*

which book is that? the man who solved the market?

*Tags: General Technical*

---

**PeterLe** - *10:39:28*

[@U4H19D1D2](@U4H19D1D2) /All, So how might you go about doing this in practice? Lets say you had a laying system and one key variable you wanted to test (fill/kill). 

If the Fill/Kill was too fast, (and it can be) you would miss opportunities and too slow you’d get taken to the cleaners.

Over the years I have reached what I think is near an optimum through trail and error. 

What would be the steps you would take in practice (could you only do this by backtesting?) Or maybe you would use backtesting to get in a range and then fine tune by live testing perhaps?

Thanks in advance

*Tags: Errors Debugging, Performance, Deployment*

---

**PeterLe** - *11:01:00*

Morning [@UUE6E1LA1](@UUE6E1LA1) , Thanks for your reply. Yes I have tried something similar, Ie lets say, placing a £2 real bet at say 1 sec F/Kill, £2.01 at 1.1 sec, £2.02 at 1.2 sec etc and then testing over a period of time to see what is optimum by seeing which stake was the most successful over time..  I was more asking how you might use linear regression to model it.I was just wondering if this is how you would use the back test model.  Over the last week or so, its been said that just trying random things in backtest wouldnt work, but I was thinking that if you had a known profitable system, then you would simply run the backtest and then vary the single variable you were interested in (fill/kill in this case).

*Tags: Strategies*

---

**PeterLe** - *11:11:43*

Yes its given me a good idea and ive stuck with those settings for a long while now. Once I found what I thought was the best fit, I would then turn to the next variable, lets say bet position and then go through the same process. Then onto the next etc. In my mind a logical methodical approach.

I was wondering how some of you guys go about using linear regression to model rather than the blunt/slow  approach that I have taken.

*Tags: Performance, Strategies*

---

**D C** - *11:16:16*

I'd be interested to hear about that too - particularly in the example you have raised because it seems like a problem that would be very tricky to tackle in a linear fashion. Seems more like an optimisation problem but one that is wrapped up with how good/competitive your ask prices are (i.e. if you are offering a bad price (from your perspective) you'll get matched very fast).

*Tags: General Technical*

---

**D C** - *11:19:57*

Thinking about it though, for people who are regularly updating their fair price models, you would be happy for a +EV price to sit in the market until such a point in time that it becomes a -EV price?

*Tags: Strategies*

---

**PeterLe** - *11:48:18*

Yes, that question _"Thinking about it though, for people who are regularly updating their fair price models, you would be happy for a +EV price to sit in the market until such a point in time that it becomes a -EV price?",_ It only takes a few milliseconds to cancel them; I do an element of that too (as do others on here), but thats a whole different ball game :slightly_smiling_face:

*Tags: Strategies*

---

**PeterLe** - *19:45:27*

HI [@U4H19D1D2](@U4H19D1D2) Yes the fill/kill was just an example. Im not sure I put my original question across properly and it went off at a tangent a little; I was more interested in how to apply the linear regression technique in general. Now all the kids/grandchildren have gone home, Ill get some thinking time :grinning:

*Tags: General Technical*

---

**liam** - *20:31:19*

The proper way, take your pick on tools but pandas is the go to when it comes to data analysis / ML. Backtesting is more valuable when it comes to confirming your analysis whilst getting a rough idea on matching / real pnl 

*Tags: Feature Engineering*

---

**Peter** - *21:43:32*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) Completely agree with [@U4H19D1D2](@U4H19D1D2) re: Pandas. It's like a cross between a spreadsheet on steroids and a database without all the overhead, but with the added benefit of transparently supporting parallel processing across however many CPU (or GPU) cores are available to it. If you've not used it before, investing some time there would dramatically speed up your analyses, making it easy to load and analyse data and then keep tweaking your parameters as you home in on finding value. Backtesting general revolves around a strategy about which you already have some confidence and running it against a large number of market files - a much slower process.

*Tags: Feature Engineering, Performance, Strategies*

---

**AndyL** - *22:25:10*

[@U9JHLMZB4](@U9JHLMZB4) thank you for the very concise answer, makes sense except for one query, you indicate at the end backtesting  ... and running against a large  number of market files... This sort of implies by the way you say that a pandas db does not have a large amount of data. Im was sort of thinking create a strategy to pump the market data files all into DB/panda, but thinking you're implying something more subtle?

Thanks

*Tags: Feature Engineering, Strategies*

---

**Michael** - *23:01:43*

Hmm. So I certainly do regression separately on individual variables as part of the process but I wouldn't base a strategy off just one.... Once I've come to an understanding of each I look at how they interact because a lot of the time they're highly correlated so you'd just be duplicating your bets if you bet on each of them in isolation. The regression part is simple, but the whole business of putting a strategy together well is much more than that, you need to know the underlying value of the bets you might post, whether you can expect to get matched at a rate sufficient to overcome adverse selection, what price is optimal and maybe in which circumstances to cancel. All of those things are indicated by different variables all of which have relationships to each other. It ends up being quite multi-dimensional. In the midst of all that complexity I can't help thinking that just feeding my entire betting history into a ML box might not be such a bad idea even if it does move me the wrong way (or perhaps the right way) on [@U01B8031PM1](@U01B8031PM1)'s IQ plot.

*Tags: Strategies*

---

## 2021-06-21

**Mo** - *08:22:58*

Broadly speaking I would prefer not to apply any transformations and use an appropriate model for the data as is

*Tags: Strategies*

---

**Peter** - *11:28:14*

1) Pandas is very good at handling huge amounts of data. But a single market file also contains a huge amount of data, so if I'm developing a strategy with a year's worth of GB greyhound data, that's 40,000 x huge = overwhelming. So I work initially in Pandas with data from representative points in time from the start, just to make the process manageable.

*Tags: Feature Engineering, Strategies*

---

**Peter** - *11:28:35*

2) Flumine's backtesting strategy includes a pretty good simulator to help assess whether the orders your strategy identifies will be matched or not and assesses profitability. Unless you're simply taking the prices on offer, that's vital to understanding whether your strategy has a chance when it comes into contact with an actual market.

*Tags: Strategies*

---

**Michael** - *11:47:10*

In terms of workflow I also do all analysis in jupyter notebook using Pandas. What I actually do is process every market file into an equivalent compressed csv which includes all derived variables. Then I extract the subset of data points that I'm interested in from each market file into another CSV and load that up into my notebook.  So maybe I want 1/5th of all instances where 1.5 &lt; best available back price &lt; 2 or whatever. That way I don't have to re-process all my markets every time I want a different data set, just relatively quickly extract what I need. I do both the initial processing and the extraction using multiprocessing and when I get frustrated by slowness I buy a new computer.

*Tags: Feature Engineering, Performance*

---

**AndyL** - *22:07:23*

So ive done sone reading about pandas, linear regression and stuff, and now what youre saying [@U9JHLMZB4](@U9JHLMZB4) [@U4H19D1D2](@U4H19D1D2) [@UGV299K6H](@UGV299K6H) makes great sense, a bit of an eye opener actually!

So it all sounds too simple though in an odd sort of way, dump a selection of market files with what you see as useful derived values into a csv, load into pandas, LinearRegression using sklearn, hey presto you have an equation to predicate an outcome say "fair value" from market values. Obviously a lot of variables there, choice of derived variables, timeframes,.....potentially a huge number, needing a decent computer to crunch! But I didn't think it could come down to just computing an equation...?

Sounds great though, i will make some baby steps maybe with 1 variable just to understand the framework...

Cheers

*Tags: Feature Engineering*

---

## 2021-06-22

**Michael** - *08:14:34*

If you really want to re-make the book in order to price everything properly then you'll need to do a bit more than that but that's the basic workflow that I use for understanding signals. A lot of people (most?) don't bother re-pricing everything as it's sufficient to simply identify when one runner is 'out' and bet accordingly. You don't need to re price all runners or even the runner in question to do that.

*Tags: General Technical*

---

**Michael** - *08:18:39*

Just to be clear [@U01PJ5YMFBJ](@U01PJ5YMFBJ): I don't know anything much about data science - I'm just telling you what I do and not claiming it's 'best'. Others could certainly offer more qualified advice particularly on how to do things in computationally efficient ways.

*Tags: General Technical*

---

**AndyL** - *08:44:16*

It does make me think if there's a few 100 of you on here running pandas models, i wonder how similar each is..? Do we end up fighting thresholds, you put orders in at +6.4%, Liam at +5.8%, Peter at +4.8% gets in first.....! (Just made those up!) ?

*Tags: Feature Engineering, Strategies*

---

**Michael** - *08:50:56*

Well yeah there's inevitably a degree of competition. I know enough about [@U4H19D1D2](@U4H19D1D2)'s setup to know that we don't directly compete too much but there's undoubtedly other people out there. In a way though it's not the good players that are the problem; it's the bad ones who scrape up dribbles of profits on low margins with a lot of bets. They can swamp out your good bets whilst throwing away their value on a load of bad ones. Nobody wins from that.

*Tags: Getting Started*

---

**Michael** - *10:02:35*

I need to qualify the above by saying that it's only part of the process, ultimately I do re-price the whole market and I don't think you can do that with line data in Pandas. I use Pandas for what you might call 'signal discovery' as well as for analysing my actual betting. For the former I do a line for each runner at each interval with a 0 or 1 for the win and then pretend that they're all independent for the purposes of the exercise. Obviously that's wrong but it's adequate for the job I need to do. I then take that learning and look for a way to incorporate it into my pricing model.

*Tags: Feature Engineering, Strategies*

---

**Beeblebrox** - *10:11:53*

I suppose you could do it in pandas if each line at a certain timestamp was tagged with an id and then you could group by those ids to get all runners in one line.  I'm no expert in pandas though - need to read up more about it.

*Tags: Feature Engineering*

---

**Peter** - *12:03:37*

[@U01MPC0GUK1](@U01MPC0GUK1) No need for a new id, just use the timestamp. Shouldn't need to get all the runners on one line either, just group by timestamp and then run the analysis for that point in time against each group. This is one of the ways that Pandas scores over a spreadsheet.

*Tags: Feature Engineering*

---

**Michael** - *16:53:08*

[@U01PQ9SR9MJ](@U01PQ9SR9MJ) "price" normally means odds, but you can take your choice of which odds you're looking at and money available might be part of that. The rest of your question is for everyone to figure out on their own.

*Tags: General Technical*

---

## 2021-06-25

**Peter C** - *10:33:33*

Apologies, I know this has been discussed before, but I'm searching through the slack and I can't find what I'm looking for. I'm backtesting a strategy that is putting up offers and am pleased with the results. Since this is my first attempt at offering prices I am keen to understand how the flumine backtest might translate into real life betting. I'd really appreciate anyone's insight into how I might expect my strategy to translate from backtest to real life. Ultimately I will run this strategy live either way and find out for myself!

*Tags: Deployment, Strategies*

---

**Michael** - *10:48:16*

The answer to your question is 'it depends'. However, since you've made this decision: _'Ultimately I will run this strategy live either way and find out for myself!'_ it doesn't really matter does it?

*Tags: Deployment, Strategies*

---

**Michael** - *10:52:46*

I don't know what your experience level is but if you're a new-ish player then honestly I think you're better off not asking that question, don't expect anything just find out.

*Tags: General Technical*

---

**Jorge** - *11:08:19*

My strategy only places bets in max. one runner per event, even though it places many bets in the same runner. So I assume the bets are independent from each other and then I can apply resampling to them

*Tags: Strategies*

---

**Peter** - *11:18:57*

[@U0155J92A7Q](@U0155J92A7Q) You can't strictly make the assumption that they're independent, since the act of placing the first bet impacts the market for the 2nd. And backtesting won't have taken this into account. That said, unless your bets are big relative to the market, I'd do it any way. A number of people here run Monte Carlo simulations to understand the range of potential outcomes, even wen they're clearly +EV. I load the data into a Pandas dataframe and draw tens of thousands of samples and although that is computationally intensive, it really doesn't take very long to run (a minute or two).

*Tags: Feature Engineering*

---

**Michael** - *11:28:30*

As a general point not specifically addressing [@U0155J92A7Q](@U0155J92A7Q) or [@U01LD279D16](@U01LD279D16) - I think people often don't ask the right questions with regard to this sort of thing, or else they ask them for the wrong reasons. The best predictor of the success of a strategy is very likely the person who designed it. How successful is strategy X likely to be? How successful is strategy Y (designed by [@UBS7QANF3](@UBS7QANF3)) likely to be? If you've got a bit of sense you apply that logic to yourself.



In any case why do you want to know? So that you can fantasize? Always a bad idea. So that you can scale before your returns justify it? Always a bad idea.



Of course back-testing can be an important part of design and development but it's usually a mistake to look to it for confidence or money management.

*Tags: Strategies*

---

**Beeblebrox** - *11:40:27*

As someone who started less then a year ago, so relatively new, I always assume that any strategy I design will fail when it goes live! It helps to have realistic expectations.



[@UGV299K6H](@UGV299K6H) as someone with a lot of experience what is your success rate like for new strategies you design? Do the majority of them succeed or is it closer to 50/50?

*Tags: Deployment, Strategies*

---

**Michael** - *11:50:23*

The notion of a 'strategy' isn't usually that applicable to me because most of the time I'm just adding some new bit of insight into my primary operation so the better question might be: How frequently do my changes improve results? I guess that's about 75% of the time but it's hard to say because there's a lot of feeling around for an optimum.



I do try completely new things and if I get as far as betting on them they always make profits but they're not always as good as I hoped for. Assuming something works well I take the learning from it and absorb that into my main operation so I don't have an accumulation of strategies on one sport.

*Tags: Strategies*

---

**Peter C** - *12:02:16*

For what it's worth I asked the question because I am quite sceptical of backtest results and wanted to gain some knowledge about whether they can be trusted when offering. Your answer was probably more what I actually needed.

*Tags: General Technical*

---

**Jorge** - *12:04:51*

[@UGV299K6H](@UGV299K6H) to answer your question of why do you want to know? - I'd like to know it to compare different strategies

*Tags: General Technical*

---

**Beeblebrox** - *12:04:56*

I guess the success rate is going to depend not only on who's devising the strategy, but what you're trying to do and how you define success.

*Tags: Strategies*

---

**Beeblebrox** - *12:07:12*

Say if you've got a strategy that's returning 0.5% where you were aiming for 4%, you might deem it unsuccessful, but for someone who's not got any profitable strategies, they might count it as a success.

*Tags: Strategies*

---

**Michael** - *13:24:31*

Oh well that's easy you can just back-test both and see how the back-tests compare. How they compare to real performance is a different thing, but assuming the strategies are comparable the back-tests will be and likewise the respective relationship to real performance.

*Tags: Performance*

---

**birchy** - *15:05:39*

My personal experience has been that live betting P&amp;L is significantly _better_ than Flumine backtests, i.e. one particular strategy has been matching a lot more bets than the backtests suggested. IMO the _type_ of strategy makes a difference.

*Tags: Deployment, Strategies*

---

**Michael** - *15:49:14*

I'm not saying it's not useful, just that I think you have to ask yourself what you're hoping to get from it and that I think forecasting your profits is a poor application. It's better applied to (say) the optimisation of execution prices, in which case you're comparing back-test to back-test and the comparison between real and simulated results doesn't really arise.



Certainly the type of strategy will make a difference to how simulated and real results compare but even if the simulated matching was perfect it and couldn't possibly accommodate your effect on the market. Every participant alters the market and the more you're betting the bigger your effect. Although overfitting is a separate issue it's worth remembering that even a perfect matching simulation will do absolutely nothing to help you with that. For most newer players the over-fitting errors are likely to eclipse anything to do with simulated matching.

*Tags: Errors Debugging, Strategies*

---

**Unknown** - *15:59:02*

My question about resampling is assuming that your backtest results match your live results, which is the case for me right now

*Tags: Deployment*

---

**Mo** - *16:14:34*

Possibly. But hopefully the newbies can take something away from this:



1. It’s hard

2. Sometimes you work on a strategy for six months and by the end you have the square root of fuck all to show for it

*Tags: Strategies*

---

**AndyL** - *18:49:25*

[@U01LD279D16](@U01LD279D16) fyi as your question is quite apt for what ive experienced in the last couple of days. My greyhound bot had a 41 bet losing streak yesterday in live betting, i then ran the streams last night through the same strategy in bactest and backtest only had a 26 bet streak as it missed 15 matches...

My strategy has a 10 second fill or kill, and from comparing logs live matched quite a bit more often than backtest 

So as [@UGV299K6H](@UGV299K6H) says dont just rely on backtest, just shame I've not worked anything else out yet..!?!

*Tags: Deployment, Strategies*

---

**AndyL** - *18:55:40*

Im now trying to muster some energy to do some pandas, just not convinced i understand how to find value in a market to really make use of it

I keep hearing snippets of you guys throwing 100s of offers at a market, and that's what my 10s fill or kill was sort of doing, didn't seem to work for the dogs!

*Tags: Feature Engineering*

---

**AndyL** - *19:13:29*

Say you come up with a pandas model that predicts a fair value at time 'now' based on a set of historical values. Which way would people play it...if the actual price now is "out" say 5% higher then that value so back.. or would you set an offer target say 8% out from now fair value with fillnkill say 10seconds...? The assumption being in 10seconds fair value won't change too much, so 8% will most likely be +EV

*Tags: Feature Engineering, Strategies*

---

**Michael** - *19:19:39*

There's no simple answer to a question like that, you have lots of options and it depends on the scenario, but if there's edge available to take I'd generally just take it.

*Tags: General Technical*

---

**thambie1** - *19:20:50*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) You're getting way ahead of yourself. Problem 1: find value. Problem 2: Optimize capturing value. You're skipping to problem 2, when you haven't worked out problem 1.

*Tags: General Technical*

---

**Dave** - *19:29:42*

I think something that really helped me was making a simulation environment that closely matched real betting as much as possible. Backtest done poorly = worthless, backtest done well = can be very powerful.

*Tags: Strategies*

---

**AndyL** - *19:30:28*

Isn't that Flumine ?

*Tags: General Technical*

---

**Dave** - *19:59:14*

It's difficult to build a good general purpose backtest framework. Flumine does a decent job but as a general purpose backtester it cannot make assumptions that a specific backtester could make in order to better simulate reality.

*Tags: General Technical*

---

**Dave** - *20:06:59*

For example, suppose you have a strategy that bets on BSP, some time before the off. Suppose you also have some knowledge as to how presence of pending BSP orders impact the eventual uncrossing price. You can factor this logic into how the BSP uncrossing goes. Flumine on the other hand would not consider this market impact (nor should it as a general framework).

*Tags: Strategies*

---

**Mo** - *20:10:23*

It’s a good point, I don’t use flumine for backtesting either

*Tags: General Technical*

---

## 2021-06-26

**Andrew** - *03:02:27*

And stop this “pandas model” talk. Pandas is just data structure. It’s Pythons implementation of a dataframe.  But I may be wrong that as I don’t use Python

*Tags: Feature Engineering, Strategies*

---

**AndyL** - *22:15:15*

Anyone using pandas, did you have to build Python3 from source to resolve pandas dependency on _bz2 ?

*Tags: Feature Engineering*

---

**AndyL** - *22:31:02*

Rebuilt python3.8 seems to have resolved it

*Tags: General Technical*

---

**AndyL** - *23:54:56*

So that was a fun bit of programming..

Now i realise im back to where i was,in trying to figure what independent variables might be an indicator of a better  fair value............

I guess I now have the advantage i can try some assumptions run a model and compare predicted accuracy across different data

*Tags: Strategies*

---

## 2021-06-27

**AndyL** - *16:15:28*

Anyway worked out pandas, so that's progress...

*Tags: Feature Engineering*

---

**liam** - *18:53:17*

Agreed, note that flumine is designed so that you can plug and play your own simulation logic to optimise for certain conditions. However I think there certainly needs to be some better docs surrounding it’s limitations 

*Tags: General Technical*

---

**Michael** - *19:00:54*

I don't think that's fair [@U01PJ5YMFBJ](@U01PJ5YMFBJ) you've had a lot of help. This isn't some self help yoga goop bullshit forum.

*Tags: General Technical*

---

**AndyL** - *19:49:57*

No [@UBS7QANF3](@UBS7QANF3) i deserved it, i dont always realise when I say the wrong things, or go on too much.. Typical IT guy slightly on spectrum thing..

Thing is i dont realise i am flitting... pandas is just progression to me

*Tags: Feature Engineering*

---

**Mo** - *19:51:00*

Absolutely nothing wrong with learning pandas, it's a great move that has applications far beyond sports betting

*Tags: Feature Engineering, Strategies*

---

**AndyL** - *21:46:03*

Just done an interesting pandas regression, i added BSP as the Y, and a load of variables, a straight linear regression had an accuracy of 0 ! Hence no correlation at any time with my input variables ?!?

*Tags: Feature Engineering*

---

**AndyL** - *21:51:59*

Not convinced pandas is going to help me at the moment, sorry [@UBS7QANF3](@UBS7QANF3) im going to flit back again!

*Tags: Feature Engineering*

---

**D C** - *22:14:08*

I don't understand you [@U01PJ5YMFBJ](@U01PJ5YMFBJ) - it does not matter whether you use python, pandas, R etc - linear regression is linear regression. You've invested the time learning to use pandas what possible benefit is there switching now? Regression results will be shit if your explanatory variables are not right. Not everything has a linear relationship and not all response variables are well suited to linear regression.

*Tags: Feature Engineering*

---

**D C** - *22:28:11*

You can fit linear regression to any data but it might not be the right thing to do. You can take data from a quadratic curve and fit a straight line to it but there is no point because it isn't linear. You can solve for the parameters of gradient and intercept that minimise the error but that line is as much use as a waterproof teabag when it comes to describing your data points.

*Tags: Errors Debugging*

---

**AndyL** - *22:31:01*

Im not giving up on pandas, just putting in tool box til i know what to feed it...

*Tags: Feature Engineering*

---

**D C** - *22:34:57*

Consider this [@U01PJ5YMFBJ](@U01PJ5YMFBJ) - what is the price difference at one tick for prices under 2.0 and what is the price difference for one tick at prices over 100.0 ? That in itself does not preclude linearity but can you see why this difference MIGHT cause a problem?

*Tags: General Technical*

---

**D C** - *22:35:17*

Plotting data helps you understand relationships between variables visually

*Tags: General Technical*

---

**Unknown** - *23:43:05*

[@UBS7QANF3](@UBS7QANF3) so not sure i understand, here is a plot of a linear regression model applied a selection, generated model is essentially p=0.91×price10secago

*Tags: Strategies*

---

## 2021-06-28

**Andrew** - *00:14:13*

Life is very non-linear, and quite multi-dimensional. Here’s by tip for you [@U01PJ5YMFBJ](@U01PJ5YMFBJ): What is your theory? What hypothesis are you testing? Read up on data mining. Success, in my view will come from accurately modeling behaviour of either runners or punters. If you don’t have a view on what behaviour you are examining you’ll be staying on that bicycle.

*Tags: Strategies*

---

**Atho55** - *10:30:44*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) your PC Enigma model?

*Tags: Strategies*

---

**Andrew** - *10:36:25*

With respect, you are then unlikely to find anything. My approach is to start with a theory. You then test that theory. Others may follow similar approach. For example: “Faster dogs win races. I believe there would be a weight to box relationship, and heavier dogs have an advantage if mid starters”. [I’m not suggesting this is a valid theory, but you see I now would have something to test]. Next step, ask what data you need to collect - timeframe, tracks, starter numbers, sample size. Go and collect a rich dataset then test you theory. And along the way you’ll use pandas :wink:

*Tags: Feature Engineering*

---

**AndyL** - *11:46:06*

[@UBS7QANF3](@UBS7QANF3) [@U4H19D1D2](@U4H19D1D2) thank you both, you've been super helpful

*Tags: General Technical*

---

**MacrcoPolo** - *16:32:32*

Asking good questions to your data is better than knowing the answers goatiwish, keep plodding along :slightly_smiling_face:

*Tags: General Technical*

---

**AndyL** - *17:44:14*

So this may seem a silly question, but when you say "on each line", what do you mean by "line"?

*Tags: General Technical*

---

**AndyL** - *18:29:02*

All the info is here, so hopefully it's helping many..

*Tags: General Technical*

---

**Peter C** - *18:41:04*

I've definitely been helped by the conversations your questions spark

*Tags: General Technical*

---

**liam** - *18:47:19*

Yep that’s what I do, intermediary step of plotting the distribution to give a better idea of groups and graphs always help 

*Tags: General Technical*

---

**Andrew** - *22:57:22*

At the risk of my help again being misconstrued, help the reader by labelling your axes. 

*Tags: General Technical*

---

**Andrew** - *23:45:38*

It doesn’t need to be art. You need to add in a dimension to the analysis. How does it look at different times until the off? And also consider filtering by price range. So, is the behaviour different at different price levels and at different times before start? You may be looking for a strategy that only works for a period of the market.

*Tags: Strategies*

---

## 2021-06-29

**AndyL** - *20:44:02*

Hmm, got further but hit an issue, i tried creating a strategy to backtest based on a pattern from the data mining, and the results are totally different.....

The reason being bet selection in my data mining differs, doesn't take much fir strategy to be totally different, so projected profit is typically wrong...

*Tags: Strategies*

---

**AndyL** - *20:44:43*

Data mining needs to be as accurate as Flumine backtest emulation

*Tags: General Technical*

---

**AndyL** - *20:48:06*

Ill fit my mining with Flumine strategy

*Tags: Strategies*

---

**Mo** - *21:03:20*

Who is to say flumine is more accurate?

*Tags: General Technical*

---

## 2021-07-01

**AndyL** - *07:49:28*

[@UEA14GBRR](@UEA14GBRR) as eluded to in methods, used ALL mined variations by spike height, WoM, price bands, time bands, vwap diff, relative volume increase, relative volume increase selection volume ratio, relative selection volume to market volume ratio, and others..

Accurate means results agree with building a strategy and putting through Flumine over same data

*Tags: Strategies*

---

**D C** - *12:52:41*

Trouble is that when you are researching something that from your personal perspective you don't know much about, how do you know when "enough is enough" ? I mean you could hammer away at a shit idea for too long if you don't know when to stop. It's not like parameter selection in a model where fitting will tell you what parameters are significant and which you can throw away. How is someone new to the game supposed to know what is too little/much time?

*Tags: Getting Started, Strategies*

---

**Stefan** - *13:40:53*

Actually using ML is very easy, so if you have trouble to find your way, you maybe should spent some time on building your framework for strategy bot execution, as it seems you are skilled developer. This way you will not waste your time and build something reusable when your ML is ready. For instance I have got just one such bot using different ML models as trigger to execute bet placing, trading or dutching. Any ML model gives me as result dedicated selection/s, bet type and optionally price.

*Tags: Strategies*

---

**AndyL** - *14:52:35*

So I have my own WoM which is very similar to that, and yes that does help

*Tags: General Technical*

---

**ShaunW** - *15:18:04*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) when I said "how do you define a spike" and you said "height" that's a tad dismissive and not really providing enough info to help you. So "height" relative to what? Is that %diff or %relative to the high/low figure? What min/max volume? (presumably not 1p) , what duration? (30s isn't a spike 500ms is), what min overround and does anything vary over time? And many many other factors. As [@U01B8031PM1](@U01B8031PM1) says these things should take you much longer to think about and explore. Yes you need to fail fast but that only works when you can emphatically say you have covered everything. Far from having 'done' spikes I don't think you've even fully defined what one is? Maybe we're all a bit thick but you blaze through topics we'd take months to work on. I tried to keep that short but every tiny aspect could/should fill a book.

*Tags: General Technical*

---

**AndyL** - *22:05:11*

hi [@UEA14GBRR](@UEA14GBRR) sorry been busy with day job today! So when I said height i meant it, so my spike if thats what we call it, is defined as any change in midprice after 10seconds. Then using the methods [@UBS7QANF3](@UBS7QANF3)  [@U4H19D1D2](@U4H19D1D2) [@UGV299K6H](@UGV299K6H) suggested i analyse across the whole dataset returns vs spike height, then returns vs seconds from start,  then returns vs wom, then returns vs selection volume change over 10seconds, then....etc. The main "fix" is the 10seconds, although I have tried several variations on that, but it's a bit trickier to compare that one. From all these orthogonal variables i plot graphs to visualise affect vs returns...

I think you probably do similar i am sure...

Anyway ive made some progress this evening, wom variable helping [@U01PQ9SR9MJ](@U01PQ9SR9MJ) and ill plod on...but no late nights for me!

Cheers

*Tags: Errors Debugging*

---

## 2021-07-02

**ShaunW** - *06:29:01*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) that's not really similar to what I do.  I'd obviously do the whole returns vs time/price etc but I wouldn't start charting vs a whole new nest of snakes like wom.  I'm still not sure what you mean by spike "height" (and presumably depth), there's so many considerations and ways to measure something like that which is why I (and [@UN1497LJC](@UN1497LJC)) see them as so hard to define. What you seem to be desribing is just a change in price from 10s prior, a 'spike' would definately need some rate of change characteristic (calculus?).  Lastly try to be a bit more questioning about throwaway phrases like "volume change",  how many ways can you think of to measure it? Absolutes won't work as some markets are 1,000 and some 1,000,000.  Percentages aren't great either because £100 is big on a £1,000 market but you won't find many 50grand bets on the ½mil markets, so what would be a meaningful measure given the algo has to perform at scale?   In short, reduce the breadth and increase the depth of your thinking.

*Tags: General Technical*

---

**AndyL** - *16:40:42*

I found a 1% edge from my data mining setup, created strategy in Flumine ran it over night took 4 hours on 6weeks of aus horses, returned -1%, not happy with mining on market snapshots, looks like ill have to use Flumine and 4hours per iteration...

*Tags: Getting Started, Strategies*

---

**AndyL** - *17:28:32*

I made up a Flumine strategy using a bit of logic and ran it while I was working, came back with -0.2% with not a lot of effort so -1% is crap :grinning:

*Tags: Strategies*

---

**AndyL** - *21:42:23*

Stepping back a bit from this haystack search... thinking why would value or the destination of BSP be determined by some tiny pattern of wom, price action, or whatever tech...? Just doesn't sound sensible, the crowd knows what it wants at time "now", but I can't see how it can predict in any form of technical indicator future BSP

All i know is what is value "now", ie. A price above/below "now", but I cant have that or at least not enough to cover spread &amp; commission and some spare...

Hmmm,seems an impossible problem

*Tags: General Technical*

---

**Atho55** - *22:14:02*

I`m not sure why you picked Aus racing because even if you produce the best strategy ever, I`m sure there are some limitations on the Back bets you can place.

*Tags: Strategies*

---

**ShaunW** - *23:55:53*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) Strip it back to basics, if you were a punter backing blind would you be a winner, and if you weren't a winner who'd would be taking your money? And with your strategy, if it's not a winner then who's got your cash?

*Tags: Strategies*

---

## 2021-07-04

**D C** - *10:44:26*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) you are doing it again - flitting from one idea to another. If you were a baker there would just be piles of uncooked dough all over the place and not a loaf of bread in sight! Pursue an idea to completion before moving on to another sport. You've been given plenty of ideas and pointers of what direction to take. At this stage I will say that I don't think this slack is actually helping you at all - it may be that you are just getting too many suggestions. I would take ONE idea that has been suggested and logout of here for at least a week and work hard on that one area.

*Tags: General Technical*

---

**D C** - *11:03:03*

I agree [@UPMUFSGCR](@UPMUFSGCR) but you don't buy a book, read the first chapter then start another one do you? MAybe the book is crap but you won't know until you finish it. It's a risk you take. You can't possibly know something is shite before you experience it first hand. I've said I spent ages trying to force MACD to "work" with no success. Maybe someone else has but I have no idea. Point is that nobody successful is going to tell him exactly where to look or what to do. I mean look at BFLW/flumine for example - I don't use it myself, but the existence of this is saving people literally hundreds of hours in itself. Not just the labour required to write that software, but the many additional hours debugging it so that people can now use it safe in the knowledge that it is reliable for the task. Fact is that it is not easy and spending a day on an idea before discarding it is not a route to success unless you get VERY lucky indeed. Just my opinion.

*Tags: Errors Debugging*

---

**D C** - *11:12:37*

This is the trouble with trying to use purely market data and knowledge of how the exchange works. I've admitted to many that I am only in the green now because of the GPS feed. I still am at a loss as to how to price the market purely off the order book and trade history but I hope one day to get there. I've not worked flat out on things but I have been playing around with bots and the API since the mid 2000s. For me the only "obvious" way to get somewhere is through fundamentals. As a (former) mathematician, I am quite at home with the idea of using statistical models to spit out probability estimates using fundamental data but this is hard graft and is going to be close to impossible these days as a one man band without access to some bespoke high cost data because professional sports analytics is now such a big area. I can't say for sure why the modelling value from price action eludes me after so long. Could be just that I am not clever enough, could be that my brain just hasn't seen the alternative angle to view things from. My point is that there are no guarantees that you WILL become successful. Even if someone tells you the right area to look in, you still might lack the thought processes or intelligence to find the answers. That is just part of the landscape. Just because someone else is successful it doesn't mean you will be. Not meant to demotivate you [@U01PJ5YMFBJ](@U01PJ5YMFBJ) just trying to be realistic.

*Tags: Strategies*

---

**Mo** - *11:29:58*

Already I am surprised by the results. I voted just fundamentals. Obviously I pay attention to what the market prices are but like you I struggle to see how to make money without a fundamental model. My advice to [@U01PJ5YMFBJ](@U01PJ5YMFBJ) was going to be that maybe he is another one for which price action alone is simply not going to work but the current poll results weaken that argument

*Tags: Strategies*

---

**D C** - *12:12:02*

TLDR: NO



I think the biggest problem for me until quite recently (until I started dabbling in crypto tbh) was that everything I tried was based on traded prices alone without consideration of volume or what's on the order book. Groups like this slack were just not around when I started off and at the time I only know one other person who even know what Betfair was.  I recall one website [http://fred77.co.uk|fred77.co.uk](http://fred77.co.uk|fred77.co.uk) which was a forum where a few botters chatted about strats and bots but that was it. When I started I basically had tunnel vision - I wanted to use data to predict market movement over next X seconds and that was it. It never occurred to me that you could use market data to model value at all. So I looked at financial TA and MACD started to crop up a lot so I started off there. Obviously I widened my knowledge of software languages (back then bots were Perl and data was mysql) but I can honestly say that I did not extract anything useful from MACD, RSI, Bollinger bands etc. and moved on to US sports and logistic regression.

*Tags: Strategies*

---

**PeterLe** - *12:23:52*

Thanks [@UUE6E1LA1](@UUE6E1LA1) thats interesting. I do think that peoples backgrounds and normal MO's do have a big part to play in this. Ive never used backtesting (until very recent) and although Ive not reached the lofty heights that some on here have, Ive been very successful.  Back in the 80's and 90's I worked on some very large computer systems and we were taught a process called kepner tregoe (I think NASA used it too), which we used in trouble shooting to define the problem before beginning to fix it. Some of the large computer systems had hundreds of modules in them and quite often you could spend many hours just defining the problem. I still use some of those techniques that ingrained in me even today.

I think if I could distil why some people are successful and not others, it is the way people see the problem and the task to be solved. They merely think differently

*Tags: Errors Debugging*

---

**Dave** - *12:27:23*

I think it's quite difficult to classify as fundamental OR price driven. I would say market prices/depth etc become extremely important if you pay attention to your execution, even if your triggers/forecasts are derived from a fundamental model.

*Tags: Strategies*

---

**D C** - *12:30:51*

I see what you mean [@U0128E7BEHW](@U0128E7BEHW) but I kind of interpreted the question as "Are your methods of identifying value fundamental or price driven". I answered fundamental as I don't use market data as an input.

*Tags: General Technical*

---

**Andrew** - *13:36:05*

Or Prices is a fundamentals model of punter behaviour

*Tags: Strategies*

---

**Unknown** - *17:47:41*

That's an equity curve from a fill or kill strategy.

*Tags: Strategies*

---

**Mo** - *18:20:41*

The problem with how it’s been implemented in the Betfair API is you can be filled at prices below what you request provided your average price is as good or better than what you requested. This means you can get some negative EV junk thrown in which you avoid if you handle the cancellation yourself 

*Tags: General Technical*

---

## 2021-07-11

**Adrian** - *10:36:43*

I hope nobody is getting sick of me asking questions yet :sweat_smile:

For the more statistically adept of you here, would you be able to recommend a method of analysing predictive/leading indicators? Say I have one particular independent variable (x) and the last traded price (ltp) and I put these two together to make some assumption about x's effect on ltp. What is this method called? TIA!

*Tags: General Technical*

---

**Unknown** - *14:02:50*

For this kind of thing where you have interactions between features that don't necessarily have a totally linear relationship, you may also want tree-based models such as Random Forests (ancient) or XGBoost (the usually better replacement).

*Tags: Feature Engineering, Strategies*

---

**Dave** - *14:03:53*

As always, you'll want the assumptions inherent to the modelling technique to make some sense with regards to the values you're feeding in.

*Tags: Strategies*

---

## 2021-07-12

**Adrian** - *11:18:57*

If I wanted to track an order in flumine so that my follow up order only goes through once the first has been fully matched, which functions should I be looking at? Thank you!

*Tags: General Technical*

---

## 2021-07-13

**liam** - *06:44:19*

Few helper functions in the `market.blotter` have a look at the [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/markets/blotter.py#L85|code](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/markets/blotter.py#L85|code) 

*Tags: General Technical*

---

**AndyL** - *18:33:25*

So it's interesting the affect "random" variation can have psychologically. You can think blimey this strategy is just awful...but you could just be looking at random variation.

I suspect knowing what might expect in terms of drawdown and amplitude of noise in a strategy helps.

To say the least I was looking at my current down trend thinking crap! When it's hopefully just normal variation.

*Tags: Strategies*

---

**Michael** - *20:13:37*

The real question isn't what type of 'drawdown' (hate that word) to expect, it's; _"how likely is it that this strategy is good given that I'm losing money"_. You might ask yourself how someone else would address answering that question.

*Tags: Strategies*

---

**thambie1** - *20:16:06*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) The first thing I'd suggest doing is verifying the accuracy of your backtest. And understanding all differences between live and backtest performance. If you do that, you can then launch strategies with a high degree of confidence even if they hit an initial downswing

*Tags: Performance, Deployment*

---

**AndyL** - *20:28:19*

@Michael im not an MCfan, although there's several ways to skin that one...im not convinced all are helpful

*Tags: General Technical*

---

**Michael** - *20:42:51*

Right [@U01PJ5YMFBJ](@U01PJ5YMFBJ) but your back test is only 6k markets. I doubt that that's really enough to give you a good idea how volatile your returns should be. To be honest though the fact that this question is even arising is not a good sign.

*Tags: General Technical*

---

**Michael** - *20:55:26*

Generally if you write a good strategy the first question to arise isn't _"is this drawdown normal?"_

*Tags: Strategies*

---

**thambie1** - *21:00:16*

[@UGV299K6H](@UGV299K6H) I still ask myself that question when I get downswings on proven successful strategies :joy:. Some of us are just paranoid, I still can't believe my simple strategies haven't been overtaken yet

*Tags: General Technical*

---

**thambie1** - *22:08:22*

Initial incarnation of the strategy with the latest downswing started ~5 months ago

*Tags: Strategies*

---

**Michael** - *22:13:59*

In my experience (more than a decade at this) you don't suddenly get caught. Markets evolve but not faster than you can adapt to them. Obvious exceptions for new technology or whatever but in the normal run of things it all stays quite still.

*Tags: Errors Debugging*

---

**Andrew** - *23:09:44*

Why? Why not a fan? Bootstrapping reveals shape of distributions and more importantly the length of that dreaded left tail, which tells me “how likely is this strategy good”. I think drawdown (and distribution thereof) is a good metric to have, unless you have an unlimited bank then don’t bother.

*Tags: Strategies*

---

## 2021-07-14

**liam** - *07:46:48*

You deleted your message :joy:



But to answer your question I think its fine to hedge when starting off (yes I have changed my tune) however if you start going below the 20% lifetime then I would think about trying to pay more commission/maximising your profit by taking only value. Do you know what your lifetime comm percentage is?

*Tags: General Technical*

---

**Adrian** - *08:06:16*

Haha yes I did, wasn't sure how appropriate it was :woozy_face: I think my commission percentage is pretty high, since I was running a breakeven strategy for a few months with big turnovers. Couldn't give a figure though.

*Tags: Strategies*

---

**Dave** - *09:22:54*

Focussing on diversifying has helped

*Tags: General Technical*

---

## 2021-07-15

**Adrian** - *01:44:52*

[@ULDAVFDRP](@ULDAVFDRP) i haven't run a closing strategy long enough to know. I may need to though because the variance of my latest strategy is too high for testing purposes

*Tags: Strategies*

---

**Oliver Varney** - *08:03:44*

[@U01S1VB9X9P](@U01S1VB9X9P) is it a pre-off horses strategy? if so you can compare whether your bets (entry) are beating BSP.  This is normally the fastest feedback, especially if your bets are at the higher ends of the odds scale.

*Tags: Strategies*

---

**Adrian** - *09:58:00*

i could bring it up on the betfair website/api if it's available otherwise i have the market recorder going where it's captured

*Tags: Data Quality*

---

## 2021-07-22

**liam** - *08:13:04*

Stupid question alert, historically my modelling (inplay racing) involves cowboy regression where I have a signal and optimise for profit/roi. However I have been working on something this week with 2 signals that both have a relationship with the price. So this then becomes a 3 variable problem, without using the word ML how can I optimise these?

*Tags: Strategies*

---

**Adrian** - *08:32:05*

my models fit better with non-linear regression

*Tags: Strategies*

---

**liam** - *08:32:32*

i am not very prepared for these questions, let me come back with some graphs

*Tags: General Technical*

---

**Jonjonjon** - *08:33:09*

I've had problems making tree methods hold out-of-sample.

*Tags: General Technical*

---

**Oliver Varney** - *08:34:24*

yes train/test performance diverges fairly quickly in my experience, but thats most likely due to features not being predictive enough. That will apply to pritty much all optimisations though I would of thought.

*Tags: Feature Engineering, Performance*

---

**Oliver Varney** - *08:53:18*

one problem with win/lose is there a massive class imbalance, which you will have to offset / deal with if you want your optimisation to work

*Tags: General Technical*

---

**Michael** - *10:37:34*

My experience of this sort of thing for IP racing is that too much optimisation becomes futile quite quickly, especially if your variables behave differently at different prices because optimal execution pricing and matching rates change at the same time. After years of face rubbing I now take a rough and ready approach and then go to the park safe in the knowledge that that'll get me 95% of the benefit with 2% of the effort.



Specifically: I might approach your problem by splitting the price ranges up into sensible chunks and treat each chunk separately. For each chunk make thresholds for each variable (assuming each can return an adequate profit on its own) then look at each variable below that threshold and make a function of the two variables that gets you to a good profit just in that chunk of prices. Keep the format of that function the same across the price chunks but vary the vectors (I think that's the right term, you know - the multipliers). Once you've done all the chunks you can either just use that as a trigger as is or you can look at your vectors and write a reasonable function to get each vector from the price. Job done.



I know that's not mathematically very sound and it's not particularly satisfying but you know as well as I do that the real problem is optimising execution prices and stakes rather than trigger thresholds anyway.

*Tags: General Technical*

---

**liam** - *10:52:07*

I plug in matching rates for price/win/loss when modelling this so it gets me half way before confirming with backtesting and then live

*Tags: Deployment, Strategies*

---

**Dave** - *22:39:37*

Oh I just remembered... The bayes_opt package is kind of a stepping stone in between basic regression and ML. You can define a custom optimization target as a function, and provide a search space of parameters and it will optimize for you. [https://github.com/fmfn/BayesianOptimization](https://github.com/fmfn/BayesianOptimization) (usual data science practices apply ofc to avoid overfitting etc).

*Tags: Performance, Strategies*

---

## 2021-07-26

**Unknown** - *20:34:31*

Hi, I have an in-play market making horse racing strategy which has been running for 75days now and placed bets in 1500 markets. It's my first one in in-play, so it's fairly selective in what it bets on and was really just a way to get some bets in the market to see what happened.



It's in profit, but if I plot time executable v cumulative profit you can see from the graph that bets which get matched in less than 0.1seconds are unprofitable.  I'm trying to work out what makes those bets unprofitable, but is the 0.1 seconds significant in some way, as it seems suspicious to me that it's pretty much bang on 0.1 seconds where it flips from unprofitable to profitable?  Or is it just randomly at 0.1 that it occurs and I shouldn't read too much into it?

*Tags: Strategies*

---

**thambie1** - *20:40:45*

Is there an in-play delay for horse racing? If not, I'd suspect that someone's taking strategy may be firing upon seeing your bets

*Tags: Strategies*

---

**Beeblebrox** - *20:47:59*

[@U4H19D1D2](@U4H19D1D2), no I'm going to look at other factors to see if they point to why they're bad bets. It's a fairly simple strategy, it just takes the current prices to work out a fair price for each runner, so there's certain to be room for improvement.  The fact it's actually in profit though is a start.

*Tags: Strategies*

---

**Aaron Smith** - *20:48:02*

This feels a lot like my experience with In-Play. I also was able to place bets that would be profitable once in the market, but getting them in the market was a pain. Say the odds where shown at back 5; 6 lay and i would place a back bet at 8 (which is obviously, according to the market, value at this moment), but in that second the bet takes to be placed, a lot can happen, so i had like 5%-10% of bets just being utterly terrible once they ve reached the market turning the whole thing in a +/- 0 strategy

*Tags: Strategies*

---

## 2021-07-27

**Jono** - *12:26:03*

I am working on bringing a strategy to flumine which involves backing then laying. What would be the best way to place the follow up bet once the first one has been matched? Would it be from the "process_orders" function once the back order is no longer of executable status and fully matched or from the "process_market_book" routine (but cant think how i would determine if the first order is matched from this func)? Or is there some other better way i havent considered? Any help is greatly appreciated. Cheers :grin:

*Tags: Strategies*

---

**liam** - *12:28:11*

Either works tbh, `process_orders` is the 'flumine' way of doing it as that function will be called whenever there is an update to the order so you will be able to fire in the second straight away rather than wait for a marketBook update

*Tags: General Technical*

---

**liam** - *12:29:47*

To answer your question the orders are in the blotter, so you can get them by either



```selection_orders = market.blotter.strategy_selection_orders(self, selection_id, handicap)```

```strategy_orders = market.blotter.strategy_orders(self)```

*Tags: Strategies*

---

**liam** - *13:01:40*

yeah, so any update to order will result in the order stream sending an update through and then flumine will [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/streams/orderstream.py#L62|snap](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/streams/orderstream.py#L62|snap) the stream anyway every 0.25s if there are live orders or every 5s regardless, this is to cater for fill/kill etc.

*Tags: Deployment*

---

**Oliver Varney** - *13:16:13*

yup cool, what I thought but thought id double check. plenty of :beer: coming your way  tomorrow for all the help

*Tags: General Technical*

---

## 2021-07-31

**liam** - *07:25:39*

I pipe straight to aws cloudwatch 

*Tags: Deployment*

---

## 2021-08-02

**Peter C** - *16:55:30*

Hi all. I've been running a horse racing strategy pre-off. It's placed ~1000 bets over almost as many markets at a healthy edge against sp, and had been remarkably consistent. However, over the last three days the strategy hasn't taken any value at all. Although in total the strategy is still ahead, this 'correction' has cost about 10% of total expected value. I'm concerned that I have fallen into the trap of 'it looks like it works for a month then blows up in your face'. I'm not really sure what my question is but I'm curious about other people's experiences in similar situations. I suppose I will have to wait and see if the strategy resumes positive returns.

*Tags: Strategies*

---

**Jonjonjon** - *17:09:28*

I'm not one of the top guys here. So maybe my advice won't be the best. But I tend to bet very small relative to my bank and have several parameterizations of similar models, so it is quite rare to have downturns that significantly impact my overall PnL.

*Tags: Strategies*

---

**thambie1** - *17:11:57*

When I first started and was concerned about my strategy's performance, I'd just buy more backtest data :sweat_smile:. Can never have too much data

*Tags: Performance, Strategies*

---

**Peter C** - *17:18:55*

That's not a bad idea. I have backtested over approximately 6000 recorded markets so I'm not sure whether I feel that further backtesting would change my opinion of the strategy or not to be honest, unless I get my hands on enough data to make a sensible comparison

*Tags: Strategies*

---

**Michael** - *17:27:58*

Assuming you haven't changed your strategy it doesn't really make sense to say that your bets before had value and your recent ones don't.

*Tags: Strategies*

---

**Peter C** - *17:29:43*

When I say lost 10% of expected value I mean 10% of the total value since strategy start calculated as ((price_matched / sp) -1) * matched_size. I have been closing at SP since I am relatively new so my loss in profit is the same 10% of total. When I say that the strategy hasn't taken value I mean in comparison to SP it has been behind, whereas before it was ahead. The last 150 bets have been 25% worse against sp than the preceding 1000

*Tags: Strategies*

---

**Michael** - *17:50:58*

Yes it's a problem - well you've got 6 months to a year to wait so you have time to develop something more mid-market. Maybe see if you can extract the critical bit of insight that (you hope) makes this work and apply it to some more moderate odds bets. Bet at 1.5 to 3 and you'll never have to wait more than a month.

*Tags: General Technical*

---

**mandelbot** - *18:24:32*

I'm in the same boat although maybe a bit in the deeper end. Sitting on 2k losses this month on a strategy with on average 1.8% better than SP. It can be definitely be hard to eat the losses but I find it hard to let go of a strategy that's been backtested heavily + beating SP.

*Tags: Strategies*

---

**Jonjonjon** - *18:24:35*

No. A back-to-lay strategy. But I had a deployment issue and it stopped laying the long odds.

*Tags: Deployment, Strategies*

---

**mandelbot** - *18:55:48*

Down about 1k on this strategy for the last 3 months

*Tags: Strategies*

---

**Peter** - *19:16:26*

It had been thoroughly analysed and then backtested before going live, so I was pretty confident that it was a solid strategy and the first 20k of bets rather backed that up. Couldn't see anything in the market that had changed fundamentally, so kept the faith that it was just a bad run, and it's back doing what I would generally expect it to do.

*Tags: Deployment, Strategies*

---

**Peter C** - *19:25:44*

That's really cool. I'm impressed you held your nerve through that but it gives me some reassurance to let my strategy run. I'm sure you were pleased to see it return to normal!

*Tags: Strategies*

---

**ShaunW** - *21:18:11*

Can't give concrete advice cos I don't know exactly what you're doing but is there a 2nd way to measure your performance? Simplistic example would be backing at fractionally better than SP, the gods of random could give you nightmares but you know your theoretical returns are sound and given enough time you'll catch up. Is your net position at the off looking good even though the actual return isn't?

*Tags: Performance*

---

**Unknown** - *21:35:59*

[@U01LD279D16](@U01LD279D16) my MC simulation will give you basic stats, P&amp;L graphs, ROI, etc but more importantly, it'll tell you how your strategy fairs compared to picking random runners. Given that the markets are generally efficient over a large sample, then picking at random should be roughly = 0EV pre-commission. It's very basic, but does the job it was intended for. [@UQL0QDEKA](@UQL0QDEKA) loves it, if you hadn't guessed already. :grinning:

It just needs to be pointed at a BetHistory.csv file of settled bets, which you can download from your Betfair account.

*Tags: Strategies*

---

## 2021-08-03

**ShaunW** - *03:31:54*

It was [@U01LD279D16](@U01LD279D16) (i think :slightly_smiling_face:)..but I'm not sure I said much you probably didn't already know.   The trouble with charts is that they can be like fractals, you never quite know what part you're looking at and at what zoom level. Even grim times lived in real-time can be not much more than a ripple after a year or two.  I once saw an interview with a quant, and worked with a few too back in another life, strategy death despite being the subject of plenty of research is very much a finger in the wind job, and that inexplicable 'feel' you get from experience. Taking on a comment from [@UGV299K6H](@UGV299K6H) and my own limitted experience, they tend to fade rather than do an about turn so maybe that's just a bit of random weirdness. Good luck with it, no easy anwsers that I know of.

*Tags: Deployment, Strategies*

---

## 2021-08-04

**Unknown** - *07:06:36*

here is a diagram of my latest strategy:

*Tags: Strategies*

---

**Adrian** - *09:57:11*

how long should you let a losing strategy run before you cut it off?

*Tags: Strategies*

---

**Mo** - *09:59:01*

I don't think it's a function of time. You should be asking questions like is the recent performance in line with the variance observed in the backtest? Have any metrics changed that might indicate a change in market conditions?

*Tags: Performance*

---

**liam** - *11:28:43*

Also very dependant on market, ie liquidity, when I start asking myself this question I spend weeks looking into it before realising it was me causing the issues; over staking, creating other [similar] strategies, basically reaching market capacity

*Tags: General Technical*

---

**Mo** - *11:46:51*

Bit of a fundamental question here but not something I've ever had to worry about: How do you handle removing your orders from your view of the market when the order stream and price stream may not be aligned? For example, the order stream update comes in first so your order is not actually in the price stream to remove or, vice versa, the price stream update comes in first so you don't realise your order is live yet?

*Tags: Deployment*

---

**Mo** - *13:41:41*

So this handles the case when the order stream update comes in before the price update. One way to avoid the case where the price stream update comes in before the order update is to not make any decisions while you have orders in flight - do you do that?



Doing things this way also means you cannot treat a negative size after removing your orders as an error state and must rely on other protections to avoid you e.g. losing track of orders

*Tags: Errors Debugging*

---

**Jono** - *14:17:21*

If a stream ending error is encountered during a flumine strategy, should restarting the same strategy in the same environment allow for it to pick up exisitng bets where it left off? for example i am currently holding a partially matched back position in a market and part of the strategy is to either lay this same selection/cancel remaining amount but a crash (not related to flumine) occurred. Im wondering how to incorporate this back bet into the strategy from the earliest point proceeding the restart, as previous testing i have performed have shown that flumine starts from a blank slate upon starting up again.



Additionally is it safe to open a seperate thread within process_market_book/process_orders or is this best to do with a background worker? Id like to send requests to a separate api every minute or so

*Tags: Errors Debugging, Strategies*

---

**Newbie99** - *15:43:47*

For your first point [@U4H19D1D2](@U4H19D1D2) created a Middleware example here: [https://github.com/liampauling/flumine/blob/master/examples/middleware/orders.py](https://github.com/liampauling/flumine/blob/master/examples/middleware/orders.py)



I use (my own adapted version of) that to get orders on startup.

*Tags: General Technical*

---

**Jono** - *16:04:41*

Thank you guys its been incredibly helpful

*Tags: General Technical*

---

**Michael** - *17:34:19*

This is why I'm so glad bflw exists and rest easy in the confidence that superior minds are taking care of things.

*Tags: General Technical*

---

## 2021-08-05

**Adrian** - *00:59:52*

[@UBS7QANF3](@UBS7QANF3) [@U4H19D1D2](@U4H19D1D2) this type of error Liam, it's obviously not something you can backtest for right? So would you only see the adjustments you need to make after a few weeks? Or any other strategies where backtesting isn't feasible, there must be some visual or statistical indication?

*Tags: Errors Debugging*

---

**S G** - *19:11:01*

I m a newbie for flumine and betfairlightweight. I would like to setup a simple strategy, any ideas on where to start?

*Tags: Getting Started, Strategies*

---

**Mo** - *19:16:18*

Have you read the [https://liampauling.github.io/flumine/|documentation](https://liampauling.github.io/flumine/|documentation) and looked at the [https://github.com/liampauling/flumine/tree/master/examples|examples](https://github.com/liampauling/flumine/tree/master/examples|examples) ?

*Tags: General Technical*

---

**S G** - *20:09:17*

Thanks for the reply, Mo. I will read through the docs and will ask more questions if I have any

*Tags: General Technical*

---

## 2021-08-06

**Adrian** - *00:56:47*

Depending on your level of experience, the documentation and examples may or may not make much sense. As a beginner of python that was the case for me. I do not have a programming background so some of the assumed knowledge was missing, simple things like having the necessary scripts placed in a local directory, logging in and (not) logging out, and using an IDE to navigate the relationships between modules. I think a FAQ channel would be very handy.

*Tags: Getting Started*

---

**Unknown** - *01:19:54*

Also, I just realised that there is way more documentation than I thought. When you go to [https://liampauling.github.io/flumine/|documentation](https://liampauling.github.io/flumine/|documentation) it looks like it is limited to the menu on the right (highlighted in red below). There is another menu at the top left, which isn't obvious. I think I clicked on it once and just saw the same "getting started" stuff, but had totally glossed over the "advanced" section, which happens to be very juicy indeed. So I am only just looking at it now after nearly 2 months not even realising it was there :man-facepalming:

*Tags: Getting Started*

---

**PeterLe** - *10:17:09*

I think there are many on here in the same boat [@U01S1VB9X9P](@U01S1VB9X9P); ie not having any experience in programming (nor Python)

Going off at at tangent slightly but keeping on the topic of being new to python, Ive been lucky in that Ive had a quiet time at work, so ive been able to invest some time in learning python, which I have enjoyed doing.

What Ive found is that people are very open to helping others (thanks [@U016TGY3676](@U016TGY3676) and [@UUCD6P13J](@UUCD6P13J) in my case and [@USYQKE5HN](@USYQKE5HN) explanation of OOP using a teapot analogy :grinning:), but only when you have put some effort in yourself and put some 'spade work in'

Quite often you will see a .."how do I do this"..and the response is "what have you tried so far and have you looked at the examples". That is a very fair comment.

(Im not saying this is true in your case by the way; I'm just talking in general)

Ive finally got a working strategy going about a week ago and bolted a few new bits onto it. Its not doing as well as my main stuff but its working and making money, so to anyone who's completely new; you CAN do it, but it isnt going to happen overnight

Just one last thing; Im sure there are some super complex programs out there, but in my experience a well though out simple one can be just as effective so you dont need to be a python guru, good enough is good enough

*Tags: Getting Started, Strategies*

---

**Jonjonjon** - *10:54:32*

The story above was when I was a futures trader at a firm.



I'm now employed as a quant Dev at a bank doing risk management stuff. There isn't much crossover, but it's all Python work.



What do you do?

*Tags: General Technical*

---

**liam** - *11:04:24*

I think it is very dependant on where your edge lies, my weak link is strategy development so my focus is on execution, infra/ease of deployment, rt analysis and speed (probably in that order)

*Tags: Performance, Deployment, Strategies*

---

**liam** - *11:10:53*

execution is the most important part of any strategy, yeah iceberg is one type

*Tags: Strategies*

---

**liam** - *11:11:43*

talk to any of the syndicates, model building is easy

*Tags: Strategies*

---

**Michael** - *11:25:21*

Hmmm. I'm gonna half agree - I mean whatever you do you have to get matched or it's all pointless but different types of approach might or might not depend on clever models. Knowing that a value price has just matched for someone else isn't the same as getting one matched for yourself, so in that sense modelling is easy but then what are you trying to model?

*Tags: Strategies*

---

**Oliver Varney** - *11:42:16*

I think on the "model" building side (coming from a ML perspective), id partially agree with the statement that its easy, but thats once you got it working in the first instance. Im sure no matter of everyones current size most peoples first model didnt work. Once you finally cracked it variations are easy yes, but I wouldnt describe an new original idea (that completely changes the way you go about it) isnt exactly easy.

*Tags: Strategies*

---

**Michael** - *11:45:30*

I think of you ([@ULDAVFDRP](@ULDAVFDRP)) as being an example of the sort of person who priorities modelling value over anything complicated execution wise - as I recall you're basically taking available money a lot? So you really need an accurate price and you've got to out perform the market but once you've done that you're filling your boots. Or have I got that wrong?

*Tags: Strategies*

---

**Oliver Varney** - *11:51:52*

I do take alot, my journey I feel is still relatively young and that optimisation will hopefully happen over the winter into next year. I wouldnt recommend anything I do as something to follow! :joy: initially my focus was on modelling yes, now its shifted more into designing a faster, more scaleable architecture (probably at the slight harm of the models), which is a working progress

*Tags: Strategies*

---

**liam** - *12:00:35*

yeah its certainly a journey, I guess the point I am making is that it takes experience to really understand matching/execution from the point that it is very easy to impact it (good and bad) as well as how to take advantage, lets be honest, so called 'pros' with decades of experience are oblivious and currently paying an almighty price

*Tags: General Technical*

---

**Mo** - *13:08:01*

I figured it was not really a sensitive question unlike past polls

*Tags: General Technical*

---

**Jonjonjon** - *13:42:56*

Is the question deliberately vague? Are people willing to put out some examples of what they mean? At some point I want to work limit orders a set distance from fair value, but the logic for that will be more complicated than just hitting stuff, so I have not done it yet.

*Tags: General Technical*

---

**Dave** - *13:45:27*

It's difficult to generalise this I think. For example, in football you have new leagues starting every few weeks in all sorts of regions....I spend a fair bit of time maintaining models for so many leagues, investigating if a model is ready to be deployed for a recently started league etc. Per-league effort is probably quite small but it adds up, and then you need time for execution optimization, and r&amp;d into new features. Maybe racing markets are more stable in this regard (idk). This is coming from a truly fundamental approach.

*Tags: Feature Engineering, Performance, Deployment, Strategies*

---

**Michael** - *14:16:23*

[@UPMUFSGCR](@UPMUFSGCR) the question for me is: How are you deriving your fair value? You can just assume that ltp == 0EV and bracket that price with offers, you'll certainly be posting value bets. Then your task is to understand in what circumstances you'll get enough noise to get matched at a rate sufficient to overcome adverse selection. Fine - but is that the best you can do? If you can improve your estimate of 0EV you'll do a lot better even if the 'bracketing' is the essence of your strategy.

*Tags: Strategies*

---

**gbettle** - *14:20:12*

Hope this is the right channel for a quick question about chi - I'm getting some really weird results.

*Tags: General Technical*

---

**Mo** - *14:36:05*

To actually answer your question, you're just calculating the test statistic correct? That is not limited to [0, 1]

*Tags: General Technical*

---

**gbettle** - *14:37:20*

I googled and found this [https://www.pinnacle.com/en/betting-articles/Betting-Strategy/how-to-analyse-your-betting-history-with-monte-carlo-simulation/W2M2YAWJHUW5LUZ5](https://www.pinnacle.com/en/betting-articles/Betting-Strategy/how-to-analyse-your-betting-history-with-monte-carlo-simulation/W2M2YAWJHUW5LUZ5)

*Tags: Strategies*

---

**liam** - *14:39:20*

Would make a good section in the strategy FAQ :wink:

*Tags: Strategies*

---

**liam** - *15:06:07*

[@UGV299K6H](@UGV299K6H) going back to what you are saying, I agree, however your style/scenario is very specific where modelling EV and your execution are intrinsically linked, from what you describe to me I would say you are 100% execution and the +ev is a byproduct :joy:

*Tags: Strategies*

---

**liam** - *16:41:07*

Yeah he certainly needs an invite to the next bflw meet

*Tags: General Technical*

---

**birchy** - *17:39:29*

I'm probably asking this question on behalf of others who are too scared to ask because they don't want to appear foolish...but how would someone (myself included) go about learning how to "optimise execution", "model EV", etc? For me, it may just be the terminology that's confusing, but I also feel like I'm somehow overlooking market fundamentals that I _should_ know already...

*Tags: Strategies*

---

**Jonjonjon** - *18:46:44*

[@U016TGY3676](@U016TGY3676) good question!

*Tags: General Technical*

---

**Michael** - *18:48:23*

[@U016TGY3676](@U016TGY3676) As far as I'm concerned optimising execution is mostly just deciding what prices and sizes to bet at. There are a couple of little tricks that can help but price and size is most of it. Some people look for speed edges but that's a specialist thing and honestly beyond most of us - it's certainly beyond me so I choose my battles. The basics of it are totally obvious - if you lay too long you lose on the price and if you lay too short you don't get matched so it's finding the optimum. Mostly I try to do that with graphs and I use quite a bit of randomisation but it's always rather unsatisfactory. Likewise stakes. Similarly obvious dynamics and similar strategy. It's not unusual for beginners to assume that they can take a model that makes a bit of money on small bets and scale it in a linear way to make life changing returns but it should be quite obvious to a moderately smart person that that's not how it works. I don't think anyone who's ever posted on here would make such a moronic mistake and if they did I certainly wouldn't point and laugh.



The other sorts of questions you're thinking about would be whether to spread your bets over a range of prices, whether there's a back equivalent for your lays on another runner, that sort of thing. There are some tricks to do with delaying bets and there are some other sprinkles but that's most of it.

*Tags: Getting Started, Performance, Strategies*

---

**Michael** - *19:55:12*

In all seriousness though - I would suggest that you think about this. I'm going to guess that your stakes are somehow proportional to prices? You could consider randomising them in a range that would keep your average exposure the same for your comfort but would generate stake/return data. Use discrete price increments and track the performance of each increment in each price bracket against the 'ideal' performance of the bets if fully matched at requested odds. You'll learn a lot from it.

*Tags: Performance*

---

## 2021-08-07

**PeterLe** - *11:23:16*

Yea that was an extreme example! I know for instance that you can get different results running a second instance using exactly the same program (Stakes etc) on the same system. I guess this is why is is so difficult to model and understand completely. I was surprised that £2 to £4 would be so significantly different though?

*Tags: Strategies*

---

**Michael** - *11:28:45*

It totally depends on what kind of markets you're betting in. For what [@U4H19D1D2](@U4H19D1D2) and I do every penny counts, in other scenario's it'll be different scales but the mechanisms are the same. As a broad brush: If you have a big modelling, speed or data edge then you'll be a bit less stake sensitive because you'll be taking all you can get whereas if you're more about betting off market dynamics phenomena you'll be more so.

*Tags: Performance, Strategies*

---

**S G** - *18:48:49*

Any github repos worth mentioning about specific sport strategy like tennis etc?

*Tags: Strategies*

---

**John** - *23:18:45*

Hi [@U016TGY3676](@U016TGY3676) many thanks for the MC script. I just ran it for my bet history file of the past 7 days (happen to have a +£1000 P&amp;L). The result says the MC beats strategy 1800 times in 10,000 runs (18%). Sorry I haven't worked on MC before. Not sure how to interpret it. Does it mean my +£1000 is sitting at the right handside of the P&amp;L distribution (82 percentile = 1-18%), so I am pretty lucky (as it's not at the mean or median)? Any handy way I could output P&amp;L distribution of the 10K runs quickly I wonder?

*Tags: Strategies*

---

## 2021-08-08

**Dave** - *09:50:44*

those trading football - are you doing anything to your models to exclude any performance from the recent international friendlies matches?

*Tags: Performance, Strategies*

---

**thambie1** - *10:04:45*

As far as I can tell, you and I are the only ones doing football on this slack. And the type of modelling I'm doing, a few odd matches here or there doesn't matter.

*Tags: Strategies*

---

**Dave** - *10:10:16*

Probably a good thing competition-wise, maybe not so much a good thing learning-wise :sweat_smile: and ahh fair enough, I will consider my question answered hah. Cheers.

*Tags: General Technical*

---

**birchy** - *10:33:36*

So the 18% is basically how many times you would of won more than £1k by betting on random runners. It's also an indication of how many times you _wouldn't_ of ended up with that profit. The lower the %, the better as that indicates skill vs luck. Ideally it should be &lt; 5% and preferably &lt; 1%. It's not uncommon for a good strategy to be 0% over a large sample of bets.

*Tags: Strategies*

---

**Mo** - *11:02:26*

I guess this question only applies if you're trying to derive your own estimate of team strengths?

*Tags: General Technical*

---

**Dave** - *11:08:23*

[@UBS7QANF3](@UBS7QANF3) - yup. I guess similar principles apply in tennis - I currently do not have much intuition on the usefullness of data from friendly matches, but they do seem to have some adverse effect on my models (at least given my current feature set).

*Tags: Feature Engineering, Strategies*

---

**Mo** - *11:09:29*

Would these international performances only affect your team strengths for international teams or would they also filter down to club level?

*Tags: Performance*

---

**Mo** - *11:11:14*

I suppose an equivalent question would be do you model individual player performance

*Tags: Performance, Strategies*

---

**Dave** - *11:24:38*

Yeah that's a good question for me - nope I don't model individual player performance. I guess the real question is what should I expect to be similar between team A vs B in a friendly league when compared to team A vs B in a competitive league. I don't follow football enough (or much at all really) to know this nuance, but perhaps it might be more common domain-specific knowledge for more religious football followers.

*Tags: Performance, Strategies*

---

**Dave** - *11:37:38*

[@UBS7QANF3](@UBS7QANF3) yeah I think the liklehood of something useful being in the friendlies that isn't already covered by data from competitive leagues is quite low. So in the absence of any expert saying "no, wait, you can extract X which is very useful", I will just ignore friendlies :slightly_smiling_face:



[@UGV299K6H](@UGV299K6H) indeed, I suppose my question should have been phrased "from common knowledge, would one be an idiot to exclude friendlies from assessing the strength/scoring ability/etc of a team, or would one not be missing out".



[@ULDAVFDRP](@ULDAVFDRP) they are quite a huge chunk of a the games in the past few months, probably if you sorted a list of competitions by game count over the last month or two they'd have the highest game count. At the same time the last few months are the quietest periods for football so they are quite irrelevant for most of the year

*Tags: General Technical*

---

**Dave** - *11:38:22*

I will just exclude them and see how my models fare, it's unlikely to have a significant effect on margins on turnover/pnl volatility, and hopefully any effect is positive :smile:

*Tags: Strategies*

---

**Oliver Varney** - *11:44:04*

would be great to see the results if you created two models, your normal one, then one including these and just set the stakes to minimum.

*Tags: Strategies*

---

**Unknown** - *11:47:56*

these Friendly games are dying out, they're roughly 2.5% of games currently and excluding them from the model also means excluding them from betting so it's as if they never happened (and the hypothesis is that the presence of them does not have some predictive ability on performance in competitive leagues). The top chart is total number of games per day across all competitions in my dataset, and the bottom is Friendlies only

*Tags: Performance, Strategies*

---

**Dave** - *11:49:36*

&gt;  would be great to see the results if you created two models, your normal one, then one including these and just set the stakes to minimum.

hmm yeah, or even a model including friendlies _only_ if I can source some more historical data as there are very few samples currently, or bring in some league-invariant features from other leagues and let it loose on small stakes.

*Tags: Data Quality, Feature Engineering, Strategies*

---

**thambie1** - *13:22:12*

From a price action stand point, I imagine it would be a tough market. Both me and [@U0128E7BEHW](@U0128E7BEHW)  have some kind of modelling edge

*Tags: Strategies*

---

**Jonjonjon** - *13:24:45*

What sort of consistency can those models deliver? Is it possible to be profitable every day of the week?

*Tags: Deployment, Strategies*

---

**thambie1** - *13:25:34*

Modelling edges usually have variance. I make bets and stick with them. Can lose big on some games

*Tags: Strategies*

---

**Jonjonjon** - *13:26:41*

Why would certain games cause a big loss? I tend to have a max risk per strategy and like to keep it small.

*Tags: Strategies*

---

**Dave** - *13:29:41*

Also, what if your model says fair value at the moment is 1.7? (Unlikely given how tight and deep this market is, but you might be smart)

*Tags: Strategies*

---

**Oliver Varney** - *13:31:16*

It looks from that chart that someone "model" said the price was too short they they laid heavily probably taking it in a few big matches from the bars.

*Tags: Strategies*

---

**Jonjonjon** - *20:22:49*

I didn't do microstructure stuff, but I traded futures via a basic model and staring at the limit order book. There's not much difference.

*Tags: Strategies*

---

**birchy** - *22:52:58*

MC is as simple as comparing your actual P&amp;L to what it would be if you had no strategy and simply bet on random runners. I'm awaiting this to be release: [https://www.amazon.co.uk/dp/B093BDGHXT/ref=cm_sw_r_cp_apa_glt_AGBPESG0VJBT2WGZGFG7|https://www.amazon.co.uk/dp/B093BDGHXT/ref=cm_sw_r_cp_apa_glt_AGBPESG0VJBT2WGZGFG7](https://www.amazon.co.uk/dp/B093BDGHXT/ref=cm_sw_r_cp_apa_glt_AGBPESG0VJBT2WGZGFG7|https://www.amazon.co.uk/dp/B093BDGHXT/ref=cm_sw_r_cp_apa_glt_AGBPESG0VJBT2WGZGFG7)

*Tags: Strategies*

---

## 2021-08-09

**S G** - *20:29:55*

Am I missing something here, I created a simple strategy with market filter for a specific market id

*Tags: Strategies*

---

**Unknown** - *23:09:02*

I've been doing some backtesting with flumine.



With hedging, the equity curve is a straight line.

Without hedging it is very bumpy.



Using hedging gives me a better night's sleep.

*Tags: General Technical*

---

## 2021-08-10

**Adrian** - *02:00:58*

The problem with hedging is execution. It's not viable in low liquidity markets as you usually just end up getting matched on the shitty side more often than the ev side

*Tags: General Technical*

---

**Unknown** - *10:49:32*

And when I run the bet_analysis.py



$ python bet_analysis.py

*Tags: General Technical*

---

**birchy** - *12:00:17*

Tidy. That strategy needs some work. It's currently worse than random betting. :grinning:

*Tags: Strategies*

---

**birchy** - *13:25:13*

Either way, that strategy has no edge.

*Tags: Strategies*

---

**gbettle** - *13:27:02*

Yes, I've found another flaw already in the strategy. Hope I'm not taking up too much of your time - I really appreciate the comments \ input. It's a brilliant script

*Tags: Strategies*

---

## 2021-08-11

**mandelbot** - *03:13:31*

Once BSP is compiled it will adjust your stake according to the liability you requested. This is done by betfair not flumine.

*Tags: General Technical*

---

**Peter** - *08:12:20*

Oh yes, there's that. I have one strategy that uses hedges and had to constrain it to only work withi odds ranges where the hedge was actually possible. So sorry, don't know of a way around it.

*Tags: Strategies*

---

**Peter** - *08:17:54*

[@ULDAVFDRP](@ULDAVFDRP) Might I gently suggest that you're confusing Greening Up with hedging. Hedging is just about removing risk. So my strategy returns mostly zeros. A few positives and fewer negatives (if the BSP moves against) so overall a small profit with minimal risk.

*Tags: Strategies*

---

**Adrian** - *08:22:53*

[@U9JHLMZB4](@U9JHLMZB4) yep that's what I'm trying to do, simply getting the value between my order and bsp. it will help me reduce my commission, which is my main competitor atm

*Tags: General Technical*

---

**Michael** - *11:41:12*

Whilst [@UEA14GBRR](@UEA14GBRR)'s approach isn't strictly optimal I can see why it might make sense for where he's at. However once you've reached a point where you're more consistent paying to minimise comm is markedly disadvantageous which is why it's not generally advocated. Comm naturally minimises itself as you improve your betting and as the winnings stack up that can become a problem in itself. I don't think anyone should pile up comm for the future at the cost of taking money home in the here and now but if you're thinking ahead it doesn't make sense to reduce it either.

*Tags: Strategies*

---

## 2021-08-16

**Unknown** - *06:04:29*

Second question, I'm struggling to get my balance heading in the right direction with this one strategy. Even though these graphs are promising. They are my actual results over 10 days and it's only using $5 stakes so there's lots of potential. But every time I try and scale it up it heads in the wrong direction.



You can see in the first graph (backing) the last couple of days where I tried to increase the stakes it went against the trend, also the biggest dip in the middle. I haven't really touched the laying side of things (second graph). With commission, these profits basically evaporate and I've had trouble implementing hedging. Any pointers how I can scale this up? I've thought maybe being more selective with my bets so I'm not paying commission so often? Any tips/help/pointers appreciated :hugging_face:

*Tags: Strategies*

---

**Adrian** - *06:37:19*

yeah if i feel like it's worth it to keep going i will. i've got a feeling looking at the post-commission versions of these graphs is going to tell me exactly why my balance is going down not up, and that's going to put a real dent in my motivation since this is the only strategy i have left that's going up before commission

*Tags: Strategies*

---

**S G** - *07:05:55*

Guys anyone has historical data in github or in a shared location? betfair seems to charge a fortune for a months worth of data with ms ticks

*Tags: Data Quality*

---

**S G** - *09:50:26*

One more question, any best practices for storing historic data? like use a database with normalisation or store in file per market etc

*Tags: General Technical*

---

**Mo** - *09:51:49*

Both can work, personally I prefer to keep them in flat files. I think [@ULDAVFDRP](@ULDAVFDRP) puts them in a database

*Tags: General Technical*

---

**S G** - *10:31:15*

Thanks Mo, always helpful

*Tags: General Technical*

---

**S G** - *10:31:39*

how to best organise the files?

*Tags: General Technical*

---

**Mo** - *11:26:41*

However the flumine market recorder does it [https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1629066953248900](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1629066953248900)

*Tags: Data Quality*

---

**Van** - *12:01:07*

Dumb question, are you all trying to model outcomes of sports, or making bets based on the flow of market data..?

*Tags: Strategies*

---

**Van** - *15:42:40*

What type of models do you employ and for what sport? Care to share an example ?

*Tags: Strategies*

---

**Unknown** - *15:58:30*

Seminal paper for football: [https://betfairlightweight.slack.com/files/UBS7QANF3/F010NTJJK88/dixoncoles.pdf](https://betfairlightweight.slack.com/files/UBS7QANF3/F010NTJJK88/dixoncoles.pdf)

*Tags: General Technical*

---

**Unknown** - *15:59:13*

Something to get you started in horse racing: [https://betfairlightweight.slack.com/files/UBS7QANF3/FV582N4CE/lessmann2010.pdf](https://betfairlightweight.slack.com/files/UBS7QANF3/FV582N4CE/lessmann2010.pdf)

*Tags: General Technical*

---

**Van** - *16:55:12*

Question about a site like [http://footystats.org|footystats.org](http://footystats.org|footystats.org). Any idea whether I can just pay for 1 month, download all the data and then unsubscribe?

*Tags: General Technical*

---

**Jonjonjon** - *17:13:46*

How would someone running a horse racing model actually decide when to place a bet? Would they need to rely on technicals at that point?

*Tags: Strategies*

---

**Mo** - *17:28:59*

Just personal stash. I find it helps to become very familiar with the really important papers, like Dixon and Coles, then you can more easily find stuff by looking up what references it (a feature of Google Scholar)

*Tags: Feature Engineering*

---

**admiral** - *23:10:23*

Would you not need a stream of live data to keep the data your feeding into your model for predictions up to date?

*Tags: Deployment, Strategies*

---

## 2021-08-17

**Adrian** - *04:06:09*

i've got `trading.betting.list_cleared_orders` but i don't know how to implement it

*Tags: Strategies*

---

**Mo** - *05:53:19*

1. Use `group_by='MARKET'` argument

2. Call the function once an hour and save the results to database

3. Your database table should have 2 columns: betfair_market_id and cleared_order_summary

4. I suggest using postgresql because then your cleared_order_summary can be of type json(b)

5. Do an upsert when updating: if the market ID doesn't already exist then insert. If it does exist then replace the cleared_order_summary with what you just received from Betfair

6. For efficiency you can utilise the `settled_date_range` argument. For example, only get markets settled in the last 24 hours when your script runs

*Tags: General Technical*

---

**Jonjonjon** - *08:43:00*

Suppose someone new to fundamental modelling wanted to start on UK Horse Racing.



How much of a project would it be to:



• Get historical form data and odds, for developing a model.

• Getting up-to-date data, feeding that into a model, lining up that data with Betfair markets/selections, then placing bets?

*Tags: Getting Started, Strategies*

---

**liam** - *08:54:23*

Depends, some have entire teams on each individual part, infra/data/model/execution and some are one man bands doing it all themselves and not far off syndicate levels. I personally think experience is the most important factor in how long something like this would take.



But tbh play to your strengths, if you aren't an expert in fundamental modelling do you really want to try and compete with those that are?

*Tags: Strategies*

---

**Van** - *09:04:21*

[@UPMUFSGCR](@UPMUFSGCR) I’m in the same position, except with football. I want to start algorithmic betting. I’m reading up on football models like dixon-coles, etc. There was a thread above which indicates most people use price, which is interesting..

*Tags: Strategies*

---

**Jonjonjon** - *09:11:46*

Do you do much price action trading on football? I've found the price movements a bit too smooth for my models.

*Tags: Strategies*

---

**Van** - *09:20:52*

No.. I’m very new.. My initial idea was a trading bot that

1. consumes football stats

2. analyses price time series

3. ???

4. Profit… :sweat_smile:

*Tags: Strategies*

---

**Van** - *11:00:56*

Would this be a viable strategy?

1. Use ML or other method to determine fixtures to predict high likelihood of one or both teams scoring.

2. If 0-0 at half time, lay the draw.

3. Trade out as soon as the first goal is scored.

*Tags: Errors Debugging, Strategies*

---

**jp** - *11:07:28*

For a strategy to be profitable you need to take +EV bets. Why do you believe that laying 0-0 half time for these matches (at any price?) is a +EV bet? Why do you believe that trading out after the first goal is a +EV bet?

*Tags: Strategies*

---

**Van** - *15:38:01*

[@UEA14GBRR](@UEA14GBRR) Sorry, I don’t quite understand what I’m missing.  I thought this is a common strategy that is/was profitable for beginners. Say I had a model that told me one of two teams are likely to score,  which would also be confirmed by inplay stats, why wouldn’t laying the draw at half-time and cashing out when a goal is scored (to lock in a profit) by a +EV bet? I guess I’m trying to reconcile gambling theory with googled material :wink:

*Tags: Getting Started, Strategies*

---

**D C** - *15:45:03*

The issue is this: what do you mean by "likely" ? You need to get a probability from that. Once you have a probability estimate you can use that to assess +/- EV. Now if you want to model the same thing only on games that are 0-0 at half time, then do that but the principle is the same. You need to decide what you mean by likely. You see this kind of stuff from people selling ebooks and "systems" where you won't hear or see the word "value" anywhere.

*Tags: Strategies*

---

## 2021-08-18

**Van** - *01:51:27*

I think I see what you are saying. You need a model to output a probability in order to determine what is a good bet, and to find value between similar situations. In addition your model needs to be close to the true probability (i.e has ‘alpha’).. correct?

*Tags: Strategies*

---

**Van** - *02:29:47*

[@U01S1VB9X9P](@U01S1VB9X9P) I am actually a CS nerd in general :wink: What I’m missing is statistical modelling and gambling theory - i.e how to go about converting historical data into a probability.

*Tags: Data Quality, Strategies*

---

**Van** - *02:31:40*

I think this might be similar to slippage, which is relevant to algo (financial) trading. Not getting the quoted price. Some suggest adding 0.15% fee to model it, which might turn a strategy from profitable to lossmaking

*Tags: Strategies*

---

**Adrian** - *02:42:16*

You don't need any theory if you can create a model that predict actual odds vs market odds. This is where my experimenting has taken me, from initially trying to predict price movements to now just predicting value. But I guess you have a different approach - top down vs bottom up

*Tags: Strategies*

---

**liam** - *12:14:02*

3 is certainly something I want to look at developing as an alternative middleware in flumine, I just wonder if it would actually be more accurate than the current logic (obviously stake size dependant)

*Tags: General Technical*

---

**S G** - *13:56:37*

Has anyone tried running strategies/python code in raspberry pi?

*Tags: General Technical*

---

**thambie1** - *14:29:26*

Lots (probably most) people on this slack are. You can rent a sufficient server for between $5 and $20 per month for basic setups. Many use aws ec2

*Tags: Getting Started, Deployment*

---

**thambie1** - *14:51:29*

If you want to save $5 a month, go for it. A VM will be easier to maintain. It will have stable power and internet. It will likely have lower latency too.

*Tags: Performance*

---

## 2021-08-21

**S G** - *11:10:31*

Hi All, trying to import MarketRecorder class from [https://github.com/liampauling/flumine/tree/720eb8168a18bbea62042d3fd0a67a0550b19e9e|flumine](https://github.com/liampauling/flumine/tree/720eb8168a18bbea62042d3fd0a67a0550b19e9e|flumine)/[https://github.com/liampauling/flumine/tree/720eb8168a18bbea62042d3fd0a67a0550b19e9e/examples|examples](https://github.com/liampauling/flumine/tree/720eb8168a18bbea62042d3fd0a67a0550b19e9e/examples|examples)/[https://github.com/liampauling/flumine/tree/720eb8168a18bbea62042d3fd0a67a0550b19e9e/examples/strategies|strategies](https://github.com/liampauling/flumine/tree/720eb8168a18bbea62042d3fd0a67a0550b19e9e/examples/strategies|strategies)/marketrecorder.py

*Tags: General Technical*

---

**S G** - *11:11:01*

FYI, I added flumine to virtual env in pycharm, but it doesnt seem to have examples folder

*Tags: General Technical*

---

**Jono** - *12:24:47*

whats the best way to pull scores or access the inplayservice info for a corresponding football match/market in flumine? Use a background worker and somehow fill the scores and other desired details into the blotter? Bit new to flumine but its already proving incredibly useful! Cheers for any help

*Tags: Getting Started*

---

**S G** - *15:12:44*

I have started market recorder for tennis, but this is only getting updates on 3 markets

*Tags: Data Quality*

---

**S G** - *15:13:04*

since I started market recorder more events turned in-play, but market recorder didnt automatically subscribe to new events

*Tags: Data Quality*

---

**S G** - *15:15:43*

```strategy = MarketRecorder(

    name="MATCH_ODDS",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["2"],

        country_codes=["GB"],

        market_types=["MATCH_ODDS"],

    ),

    stream_class=DataStream,

    context={

        "local_dir": "/data",

        "force_update": False,

        "remove_file": False,

    },

)```



*Tags: Strategies*

---

**Adrian** - *23:20:53*

So my big idea of 3 months work has failed. I've tried multiple angles to get it working. Feelsbadman. I don't know what I can salvage from it, apart from some knowledge of how machine learning works and the general idea bflw/flumine. And being able to read/write Python came from learning all this too.



It was an ambitious project and a cool idea but in the end just didn't work out how i intended. I'm not sure where to go from here, whether to pursue ML from a completely different angle or just drop it altogether and focus on execution. I have some ideas around execution that I've seen while manually "trading" but feel too deflated at the moment to bother.

*Tags: Errors Debugging, Strategies*

---

**Adrian** - *23:24:07*

One of the major problems I faced was being unable to backtest due to the way ML is a black box, so that was never going to end well flying blind. By blind I mean real world results not just getting good results on a test/validation set, because that's easy enough to fool onself

*Tags: General Technical*

---

**Jonjonjon** - *23:27:20*

If you've learned Python in 3 months, that is a very useful skill.

*Tags: General Technical*

---

**Jonjonjon** - *23:27:46*

If you don't already have a working strategy, I wouldn't bother with ML. Human eyeballs are better.

*Tags: Strategies*

---

## 2021-08-22

**Adrian** - *01:19:09*

[@UPMUFSGCR](@UPMUFSGCR) you say human eyeballs are better.. what do you mean? I don't have the skills to do things like multiple linear regression or the market knowledge to come up with arbitrary rules. ML is my one tool i know how to use, i just don't know where to apply it

*Tags: General Technical*

---

**Adrian** - *06:25:39*

I asked that question once in the BA forum and was told it doesn't necessarily work like that. I've also heard it's not the best place to get informed.



My issue is why would laying in both instances appear to beat BSP when the model is predicting the opposite outcome for each

*Tags: Strategies*

---

**Adrian** - *06:26:17*

Does it just mean my model is rubbish?

*Tags: Strategies*

---

**Oliver Varney** - *06:39:55*

not sure I understand this also, but ignoring fully flumine backtesting did you do historical testing based purely on the best availables and bsp? ML prediction should be instant basically and id guess training should take no more then a hour or so unless your DeepMind :joy:

*Tags: General Technical*

---

**Adrian** - *06:46:59*

Initially yes I was running through a month of data but moved towards focussing more on alpha within markets. All I had was my eyes to see it did pretty good at predicting ltp movements but when I put it in practice the execution broke down. This was before I learned about bflw/flumine backtesting capabilities

*Tags: General Technical*

---

**Mo** - *07:56:23*

I still don’t understand. Are you talking about training the model or making predictions? Surely the latter is very fast?

*Tags: Strategies*

---

**Jonjonjon** - *08:38:04*

One mistake I've made, and that I've seen in many academic papers on trading, is that people throw a bunch of indicators at an ML model and expect it to find good results. Their findings are usually very weak.

*Tags: Strategies*

---

**Mo** - *08:42:18*

You train a new model for every market?

*Tags: Strategies*

---

**Mo** - *08:46:46*

And what's the theory for needing to do it that way rather than pre-training a global model across markets?

*Tags: Strategies*

---

**Adrian** - *08:47:52*

i struggled to build a global model that predicted anything valuable

*Tags: Strategies*

---

**Mo** - *08:54:36*

Sounds like you've fallen foul of focusing too much on making the predictions and not enough on the entire pipeline of developing a trading strategy. If your model is so computationally intensive you can't use it to backtest then it's not a very good model

*Tags: Strategies*

---

**Adrian** - *08:57:11*

100% and that's something i'm slowly coming to realise as i learn form you guys how you approach it. The whole strategy and execution side of things wasn't something I considered as a beginner

*Tags: Getting Started, Performance, Strategies*

---

**Unknown** - *09:05:32*

I'm not one of the top guys on here, but I'd prefer to go back to basics, and try to identify some sort of repeatable pattern manually. ML can be used to optimise the parameters.



I've recently started a new strategy from eyeballed parameters. The signal was clear enough for me to pick the parameters (initially) by hand (it's a 1-dimensional linear regression). The optimal parameters aren't that different to the fitted numbers.



Here's the first few datapoints. Linear regression works.

*Tags: Strategies*

---

**Oliver Varney** - *09:24:26*

A model per market doesnt really make much sense to me tbh, not sure what testing or validation would look like. If your enjoying the experience then keep going pal. If its a hobby and keep you interested then no reason to give up in my opinion.

*Tags: Strategies*

---

**Jonjonjon** - *09:36:31*

Profit by market

 Out of sample performance.

*Tags: Performance*

---

**Adrian** - *10:10:26*

it can pick up on micro movements better than a global model

*Tags: Strategies*

---

**Van** - *12:58:49*

I question the utility of these funds though.

&gt; Priomha’s Fund returns are not correlated to the performance of traditional investment products such as stocks, bonds, property etc. Subsequently, Priomha’s Fund can provide your investment portfolio with valuable diversification.

If COVID stopped all sports, is it really uncorrelated?

It also seems to be a good way to turn tax-free money to taxable money

*Tags: Performance*

---

**ShaunW** - *14:04:19*

If flipping a losing strategy was straightforward then this game would be easy. [@U9JHLMZB4](@U9JHLMZB4) is right about the sample size and like him I have losing days doing 3x that a day.

*Tags: Strategies*

---

## 2021-08-23

**S G** - *08:49:45*

I have got basic historic data files and they are all in .bz2 format. Looks like there is a bz2 package in python that can read file without decompressing. What do you recommend for reading these file decompress manually vs using bz2 package?

*Tags: General Technical*

---

**Adrian** - *11:18:42*

that's just what i learned first. how to use json

*Tags: General Technical*

---

**Mo** - *11:29:00*

Why don't you want to run the deltas through betfairlightweight so you're dealing with the full market books?

*Tags: General Technical*

---

**Adrian** - *11:30:56*

Well there's that. I presume bflw can do that. But I interpreted S G's question like he wanted to manually extract it

*Tags: General Technical*

---

**Mo** - *11:38:15*

The big question is what do you want to do with the files:



If you're using flumine to backtest then you can patch open with smart_open so it doesn't matter if your prices files are bz2 (historic) gzip (scraped) or uncompressed. It also handles reading directly from S3 and other cloud services.



If you just want a list of market book dictionaries:



```from betfairutil import read_prices_file

market_books = read_prices_file(path_to_prices_file)```

Again, the prices file can be compressed, uncompressed, in S3, stored locally, whatever.



If you just want to look at the file and you're on a Unix based operating system (including I assume Windows subsystem for Linux) then you can just do



```bzless historic_data_file.bz2```

in your terminal

*Tags: General Technical*

---

**Jeff Waters** - *11:52:43*

I've written a backtesting program using flumine, which uses JSON files. I currently manually extract each file to a directory, one by one.



From this conversation, it sounds like I'd be better writing a script using Betfair Lightweight, and working with the bz2 files?



Is that correct?

*Tags: General Technical*

---

**Mo** - *11:55:45*

You should be able to do everything in flumine

*Tags: General Technical*

---

**Mo** - *11:56:37*

You want to use a construct like



```with mock.patch("builtins.open", smart_open.open):

    framework.add_strategy(strat)

    framework.run()```

*Tags: Strategies*

---

## 2021-08-24

**Steve Roach** - *07:04:55*

Ok, that’s no good. I was hoping to be able to build an initial history without having to pay at this stage. Of course, I’ll pay for access when I get closer to a live system but it’s all analysis and modeling at the moment.



Don’t suppose there’s a free way of building a history of pre-start Aus horse racing data?

*Tags: Deployment, Strategies*

---

**Steve Roach** - *22:43:07*

If the amount of time into a season is a feature, should it be incorporated into the ML model?

*Tags: Feature Engineering, Strategies*

---

## 2021-08-25

**Van** - *00:59:53*

Don’t see why not. That could incorporate fatigue and explain some of the performance..

*Tags: Performance*

---

**Van** - *02:10:47*

The “run it” is the hard part. I tried RNN years ago, but I could not find a standard set of practices how to organise the input data, so I was flailing. I also couldn’t “get” how it was supposed to incorporate time besides hand-wavy explanations

*Tags: General Technical*

---

## 2021-08-28

**Jonjonjon** - *10:49:12*

Yes. Or edge. A possible problem with using profit is that, if you are doing single bets (or back to lay) per market, the analysis might not take into account partial fills.

*Tags: General Technical*

---

## 2021-08-29

**Adrian** - *12:13:13*

Maybe I just have the parameters wrong. But my strategy goes in one direction for greyhounds and the opposite direction for horses, but they are not really sport dependent. I am talking about strategies close to the off

*Tags: Strategies*

---

## 2021-08-30

**ShaunW** - *02:10:31*

Is this chunking a (by)product of the tools you're using ie simpler, faster, indicative enough? The output from my analysed data extracts and backtest/sims is very granular and viewing the output as smooth or chunky are equally easy.  I don't use the Flumine/Python stuff, so I'm not alway sure how much the methods advocated are driven by how that works.  Time is valuable though so if it's primarily about quick and dirty I get it, you can usually see the story in 640x480 without having to go full 4k.

*Tags: General Technical*

---

**Adrian** - *06:00:28*

Seems I'm just confused. Like, I'm looking at something that's telling me some basic truth but I don't know how to interpret it

*Tags: General Technical*

---

**Michael** - *11:55:07*

[@UEA14GBRR](@UEA14GBRR) if you want to view means across a range you've got no choice but to chunk. The other way is to do a running total which you might prefer (fair enough) but that adds a layer of interpretation to understanding the plot as you're having to interpret the change in gradient rather than just looking at the height of the line. For my part I don't see the merit of it, if you're looking at the performance of a variable against profit I think using a running total is quite counter intuitive compared to just looking at the means.

*Tags: Performance*

---

**Michael** - *21:22:14*

Yeah - profit in one column signal values in the other.



My pandas skills are not great so I'm sure this could be done more elegantly but the overall is that you make a new dataframe by grouping the one you have, something like this:

```lower_bound = 0

upper_bound = 1

interval = 0.1

ys = ['profit', 'fm_profit']

feature = signal



groups = np.arange(lower_bound, upper_bound + interval, interval) 

labels = groups[:-1]



df_1 = df.loc[:, ys].groupby(pd.cut(df[feature], groups, labels=labels)).mean().reset_index()```

Then just plot the new dataframe however you want to:

```df_1.profit.plot.bar()

df_1.fm_profit.plot()```

I find myself always needing the same types of plots so I keep a set of pre-made plotting functions in a separate file to the notebook and just pull them in with something like:

```%run ./analysis_tools.ipynb```

*Tags: Feature Engineering*

---

## 2021-08-31

**Adrian** - *09:46:45*

[@UGV299K6H](@UGV299K6H) thanks for sharing your graphs. I'm still trying to think how I would apply that to my data. I have a strategy that is showing a strong profit on the backtests but I don't know if it means real world results. I haven't identified the the main variable within the set of parameters that is producing the profit, so perhaps that is the next step. How do you know you can rely on your graphs from a glance?

*Tags: Strategies*

---

**Michael** - *10:57:14*

Whether your back-test results will cross over to real world profits will likely depend on how well thought out your strategy is. If your strategy is well reasoned then you just try it and see.



You know whether you can rely on your plots because you know how much data you have - I picked the plot above out of something that I'm working on because it's a nice typical example: You can see at a glace that the signal is predicting the profit of bets if they matched very nicely (the red line) but you can also see that that's not translating to actual profits quite as well, so maybe there's some pricing optimisation to do or maybe that's just the way it is (it does tend to be that way).

*Tags: Strategies*

---

**Michael** - *11:03:06*

Yeah - and the ratios between actual and fully-matched performance. 90% of all analysis time is spent on this.

*Tags: Performance*

---

## 2021-09-01

**Unknown** - *20:56:54*

Calling all Monte Carlo fans...

I have been developing a strategy and I am running it now over [@U016TGY3676](@U016TGY3676)’s MC analyser (which is great by the way thank you [@U016TGY3676](@U016TGY3676))

I'm getting 0% as a result and I'm wondering whether this is too good to be true or if it is indeed indicating a good strategy?

Any advice would be much appreciated.

*Tags: Strategies*

---

**Unknown** - *21:22:39*

Thanks [@U016TGY3676](@U016TGY3676) still at 0% on 10k iterations :+1:

I was also concerned by the 2k loss - but does a spikey graph necessarily mean the strategy needs more work? Or could it just be a strategy with a high variance but a good overall trend?

*Tags: Strategies*

---

**Unknown** - *22:43:27*

It's self-generated from backtesting. This is from backtesting over about 4k recorded markets over the last 6months or so (2.5k which my strategy played on as you can see from this graph - markets x axis).

*Tags: Strategies*

---

## 2021-09-02

**birchy** - *09:46:39*

[@U01S1VB9X9P](@U01S1VB9X9P) In theory, yes, but match rates, market impact, etc will paint a very different picture in the real world. The MC analysis is just a very basic setup to get a rough idea of how a strategy performs by comparing luck vs skill. Don't over value it.

*Tags: Getting Started, Strategies*

---

**Michael** - *11:44:46*

Remember that MC won't alert you either to a logical error in your strategy or an overfit on your data. Either one of those will return a fantastic looking result on your MC test and a crushing loss in real life.

*Tags: Errors Debugging, Strategies*

---

**Jack** - *11:55:49*

[@UGV299K6H](@UGV299K6H) wouldn't the logical error be caught by the backtest itself? Or is there an example of an error that would only present itself in a live scenario?

*Tags: Errors Debugging, Deployment*

---

**Michael** - *12:23:13*

The sort of error I mean would be something where your code is fine but your underlying reasoning is faulty, for example using a forward looking data field.

*Tags: Errors Debugging*

---

**Adrian** - *13:19:28*

I just can't wrap my head around why you wouldn't go for any strategy that is showing an uptrend, even if it's wonky. You slap a few of those together and it evens itself out. I'm starting to doubt my strategies but if I add more like then the chances of them ALL going to zero diminishes by orders of magnitude. Unless your sample size is so small that it creates issues, or you're worried about your bankroll

*Tags: Strategies*

---

**Jack** - *15:08:00*

[@UGV299K6H](@UGV299K6H)/[@U016TGY3676](@U016TGY3676) All makes sense thank you. Out of interest, and without asking you to spill any secret sauce, what kind of additional things would you be looking at to encompass a robust process?



In this case for context this strategy is:

• pre-play

• purely price-driven (that is to say no data is being used outside of the market stream info)

• bet-sizes are fixed minimum stake £2

I have no strategies running so certainly no profits being made just yet :joy: any advice appreciated.

*Tags: Errors Debugging, Strategies*

---

## 2021-09-03

**Adrian** - *00:00:22*

Yes thanks for all the help! I might be falling into the trap of overfitting. I'm running strategies live that I couldn't get lower than 4-5% in the MC and didn't split the data into test/validation as I only have 2 months worth

*Tags: Deployment*

---

**Beeblebrox** - *14:01:08*

I've got a strategy that's been running for a few months now and I want to try and see what effect using higher stakes will have.



I'm laying horses in-play up to odds of 5.0 for £2.  I'd like to take Michael's approach of betting with randomly different stakes, but I'm not sure what a sensible range of bet sizes to use is.  Maybe bets in the range £2-£3, with 20p increments?  Maybe a higher range for lower odds?  Anyone got any thoughts?  Or just try it and see?

*Tags: Strategies*

---

**Michael** - *18:31:32*

[@U01MPC0GUK1](@U01MPC0GUK1) I'd lean towards £2 and £4 if that were the question - but £2 and £3 is probably a better idea.

*Tags: General Technical*

---

**Oliver Varney** - *18:34:56*

naive question, but IP does £2 or £4 really make a difference, are volumes that tight?

*Tags: General Technical*

---

**Beeblebrox** - *19:28:28*

:joy: great... Genuine question, do you actually enjoy what you do?  For me, it's just a side project with a chance to learn some new things and make a bit of money.  But if it was my sole income I'm not sure I'd particularly enjoy it.

*Tags: General Technical*

---

**Beeblebrox** - *19:36:56*

I guess like any job really.  Anyway, thanks for your help again.

*Tags: General Technical*

---

## 2021-09-04

**birchy** - *12:07:29*

For laying, would there be anything wrong with using either `stake = target / price` or `stake = target / (price - 1)`, where `target = random.choice([£10, £15])` at the market level?



To be more specific, the question is: regarding market impact/exploitation, is it better to use a flatter stake such as £5 rather than a calculated £4.85?

*Tags: General Technical*

---

## 2021-09-05

**Ke** - *11:00:33*

I'm doing some simple market making strategy. Which bleeding consistently. I wonder if there are more informed bettor in this market than other markets

*Tags: Strategies*

---

**Oliver Varney** - *11:14:56*

Maybe a stupid one here (which I probably should of thought about more), but do people take strategies and separate them out into sub strategies (but effectively the same strategy) and partition by lets say odds and provide separate banks. For example if a strategy was partitions by odds, in a month where favourites are winning the shorter odds sub strategy will be able to grow and the longer odds would scale down. Is this optimal?

*Tags: Strategies*

---

**Newbie99** - *11:19:45*

so although its 1 single flumine strategy

*Tags: Strategies*

---

**Newbie99** - *11:20:03*

and performance monitored for each sub category

*Tags: Performance*

---

**Oliver Varney** - *11:23:21*

cool thanks, something Ill look into modelling for sure

*Tags: Strategies*

---

**Newbie99** - *11:38:34*

The above was an attempt at some kind of stat arb (its not an active strategy), I think the above was actually something like number of runners and difference between the nth and n+1 runner by odds/volume or whatever

*Tags: Strategies*

---

**Ke** - *11:52:38*

Thanks, Aaron. Your comments give me some confidence which i really need now. Do you mind let me know if your strategy is speculating is more market neutral, i.e. market making?

*Tags: Strategies*

---

**Aaron Smith** - *12:07:17*

the 2 main money makers (not talking about any big numbers still, but enough to pay my bills so i can be chill while playing around with different stuff :smile: ) are one market making and the other taking, so both works. Its difficult to say which is easier, as from what i have seen, different ppl have a really different perception of what is easy to find and what isnt. Both strategies i got running without any backtesting (this is surely not best practice, but i was lacking the data at the time). Backtesting is a strong tool, but sometimes i feel like ppl get lost in it. I still have an intentionally very slow backtest (single core), so i focus more on ideas and only use the backtest to validate an already worked out strategy.

*Tags: Performance, Strategies*

---

**birchy** - *17:10:12*

That makes sense [@UGV299K6H](@UGV299K6H) . I've found that level liabilities generally produce inferior results compared to plain old level stakes. As you say, your average punter is likely betting £2/3/5/10 rather than £2.36. The main exception being those using hedging software, cash out, etc.

*Tags: Errors Debugging, Strategies*

---

## 2021-09-08

**Jack** - *21:29:02*

Sorry for the noob question - how would I filter out Hurdle races within a flumine strategy? I was using

```    def process_market_book(self, market, market_book):

        if market.market_definition.race_type == "Hurdle":

            return```

But I have realised it must be added to the market book later on because this is throwing tonnes of errors out.

*Tags: Errors Debugging, Strategies*

---

## 2021-09-09

**liam** - *06:35:53*

What errors? These are the list of raceTypes which you can also use on the market filter 



Hurdle, Flat, Bumper, Harness, Chase

*Tags: Errors Debugging*

---

**Jack** - *20:21:48*

Thanks [@U4H19D1D2](@U4H19D1D2) - error was `'Market' object has no attribute 'market_definition'` which of course makes sense given your reply.

The reason I went with `market.market_definition.race_type == "Hurdle"` is because it works in my logging controller I use that under `_process_closed_market()`  so I thought it was part of the market object during the race.

Certainly also makes sense for this to be in `check_market_book` !

*Tags: Errors Debugging*

---

## 2021-09-10

**Mo** - *08:34:30*

Not at all, it's just a way of smoothing the data which helps with the sample sizes involved

*Tags: General Technical*

---

## 2021-09-11

**Unknown** - *13:14:41*

I was just looking at the GPS data from this race..

The attached is a screenshot from a playback..Everything seemed to be updating with the exception of the progress data on "Hoo ya Mal". It never changed through the race. yet everything else did ie stride freq etc

I was wondering if the "Start" signal is set for each selection and it was just this single horse that didnt have it set? Or whether the "start Signal" is set for the whole market? Would you know [@U4H19D1D2](@U4H19D1D2)

*Tags: Errors Debugging*

---

**birchy** - *17:28:40*

When needing to change some hard-coded parameters for a "live" strategy, is it best to stop -&gt; change -&gt; restart, or use a more dynamic setup, i.e. have an external settings file which is polled each time a market strategy is initiated?

*Tags: Getting Started, Deployment, Strategies*

---

**birchy** - *20:44:37*

Was thinking about it today. Before I started using Flumine, I accessed the API with my own polling library and always used a 'settings.pkl' file which was basically a pickled dictionary of parameters/trigger values. I had a separate script that I ran when I wanted to change anything (due to having to re-pickle the file) but nowadays I'm much less paranoid about security and would probably just use a CSV. Don't know why I didn't port it to my Flumine bots. :thinking_face:

*Tags: General Technical*

---

**birchy** - *21:41:43*

Does this warrant a worker? I would of thought a simple `load_settings()` in the strategy `__init__()` would suffice? Obviously that would only load when markets are added but would be less likely to cause any issues with any already running strategies. Would be interested to hear what [@U4H19D1D2](@U4H19D1D2), [@UBS7QANF3](@UBS7QANF3), etc do regarding minor parameter tweaking. I suspect they have fully automated models that tweak parameters dynamically?

*Tags: Strategies*

---

**PeterLe** - *21:49:56*

Thanks [@UUE6E1LA1](@UUE6E1LA1)  I only ever use their value too. It makes my head hurt just thinking about how to calculate it myself :smile:

*Tags: General Technical*

---

## 2021-09-12

**S G** - *10:11:23*

In some production softwares, dynamic configuration is read from DB, and we a add a cache time to queries say 5 seconds, meaning your config changes will take a max of 5 seconds to kick in to your strategy. This also means you read config from DB every 5 seconds

*Tags: Deployment, Strategies*

---

**liam** - *10:42:22*

I currently just restart because I am normally changing the code rather than the parameters/context but maybe we could make things easier in flumine for this to be possible 

*Tags: General Technical*

---

**Oliver Varney** - *10:43:32*

I just use a worker to a database personally

*Tags: General Technical*

---

**Oliver Varney** - *10:44:37*

imo I wouldnt think its worth building out something complicated into flumine itself

*Tags: General Technical*

---

**birchy** - *10:45:49*

[@U4H19D1D2](@U4H19D1D2) not sure how that would work in Flumine as most of my settings are specific to my models.

*Tags: Strategies*

---

**Oliver Varney** - *10:46:14*

I assume we talking about model params also and not stuff like filters

*Tags: Strategies*

---

**Mo** - *13:44:59*

There is a Trader Manager component which monitors one or more data streams to identify events to trade. When a new one is encountered it runs a Trader process, specifically Python pointed at a particular module. The code that gets executed is just whatever is in the virtual environment at the moment the process is launched. So if a code update is deployed then it will get picked up by any new processes starting up after that deployment

*Tags: Deployment*

---

**S G** - *17:28:02*

I could read all json files ahead and can add meta data to the above output. I was wondering if there is any trick  or functionality in flumine that can do this automatically

*Tags: General Technical*

---

**liam** - *18:29:23*

Not sure I understand but there is middleware for ingesting the marketCatalogues when backtesting



 [https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py|https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py](https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py|https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py)

*Tags: General Technical*

---

**S G** - *21:02:44*

Cheers [@U4H19D1D2](@U4H19D1D2) That should do. guess thats exactly what I am after. I am using marketrecorder from flumine examples. I presume market_id.json (ex: 1.187440878.json) is the market catelogue file

*Tags: General Technical*

---

## 2021-09-13

**Unknown** - *13:25:46*

All my bot strategies are implemented with different set of parameters, this allows me to set different strategies just by setting different entry parameters/values, and of course changing the strategy behavior in runtime when bot is actively running on a market. I do not restart my system at all (my app) app manages any problems with betfair api, and is able to re-login and so on. Well yes I restart the app only when hosting server restarted.   Screenshot better explains what I tried to explain.

*Tags: Deployment, Strategies*

---

## 2021-09-15

**AndyL** - *10:07:58*

[@U4H19D1D2](@U4H19D1D2) hi Liam, quick question on a warning message ive seen in logs. I occasionally get Order nnn not present in blotter...followed by Strategy not available to create order...

I have 3 separate strategy processes running, and from the betid ref i can see the order is from one of the other processes. It looks as though all 3 are using the same OrderStream? It doesn't seem to affect things as the orders are unique, but wondering if this is expected in this scenario?

*Tags: Strategies*

---

**Jonjonjon** - *10:09:33*

It's due to this bit:

[https://github.com/liampauling/flumine/blob/ec060b7d98a9f15ec229fab73644d5ad970e05fe/flumine/config.py#L8-L12](https://github.com/liampauling/flumine/blob/ec060b7d98a9f15ec229fab73644d5ad970e05fe/flumine/config.py#L8-L12)

*Tags: Deployment*

---

**Jonjonjon** - *10:10:45*

Please note that we recently changed `hostname` to `customer_strategy_ref`, so please check your flumine version before proceeding.

*Tags: Strategies*

---

**Oliver Varney** - *14:07:47*

Ah fair, I thought you meant model training

*Tags: Strategies*

---

## 2021-09-16

**birchy** - *17:40:58*

Anyone got any advice for increasing matching rates? So I have a horses inplay strategy that uses price signals. For post-race analysis, I plot the "what if" cumulative P&amp;L of 100% match rate vs my actual cumulative P&amp;L (which is ~50% match rate). The theoretical 100% match rate gives a lovely straight upwards line, so that does at least show that I'm not pissing _directly_ into the wind. This strategy is right in the thick of the action, so fairly time sensitive.

Obviously increasing the cancellation "age" of the bet is an option, but that does seem to result in increased adverse selection. I'm considering using a more dynamic cancelling procedure (i.e. signal driven rather than a fixed time). The other option is moving my price closer to the middle of the spread, but backtests suggest that's going to reduce my edge closer to 0EV. My signals are fairly optimal, well... they're somewhere in the middle of the high volume/low margin : low volume/high margin range. Soooo many combinations. Doing my head in TBH. :face_with_rolling_eyes:

*Tags: Errors Debugging, Strategies*

---

## 2021-09-17

**Michael** - *11:05:34*

Yeah personally I randomise everything in sensible ranges even when I think I'm getting it right because you'll always want the data in the future. The time it takes to for this stuff to work through depends on the complexity of the question you're answering and how good an answer you need. If it's a straight A v B type question with a well constructed trial you can get an adequate answer in a thousand markets, if you're tracking something more complex and you want a nice clean graph you might need 10k. Sometimes you'll think you've got a clear answer with a small number of markets but that tends to be illusory and you'll always be doubting your conclusion. Don't tinker too much as you'll confound your data set and your tinkering probably won't increase your profits anyway, set your thing up and leave it alone. Chances are your bets are already optimal for some part of the market, if you can work out what part that is then you can build from that.

*Tags: General Technical*

---

**birchy** - *14:47:17*

Yes, was thinking about using something along those lines with a fixed time as the fallback.

*Tags: Errors Debugging*

---

**ShaunW** - *14:55:39*

Thanks [@UGV299K6H](@UGV299K6H) theoretical ideal vs practical implementation? Yep that seems to characterise my problems in general. [@U01DJ4DBF9R](@U01DJ4DBF9R) I like the sound of that, a real world relationship other than the angle the earth has turned through. Although I can see issues with that too from one event to the next so the time-only option is actually becoming more appealing by the minute (or n thought cycles :blush:)

*Tags: General Technical*

---

## 2021-09-21

**Adrian** - *01:45:51*

Hey guys i was hoping someone with more experience could help me understand this weird problem I'm having while backtesting. I have identified some market parameters that occur on my most profitable bets. Problem is, when I make the bet placement require those conditions, my number of matched bets goes up substantially relative to those identified bets. Like double. E.g. my backtest shows 600 matched bets as the most profitable within the range of the specified parameters. But when rerun the backtest using those parameters I get 1200 bets matched. Those extra 600 do not add to the profit.



I just don't understand because I putting in parameters that limit the range of circumstances in which a bet is placed. I have even simulated removing all my other bets from the market and it still shows the same behaviour. Just wondering if anyone else has seen this behaviour? I can't understand why my matched bets number goes up when I use the same parameters?? :pleading_face:

*Tags: General Technical*

---

**Michael** - *07:53:27*

[@U01S1VB9X9P](@U01S1VB9X9P) do I understand your question correctly:



You have some parameters: A, B and C which produce (say) 900 bets. When you add parameter D to those 900 bets you're left with 600, but when you re-run your back-test with parameters A, B, C and D you see 1200 bets?



Could it be something to do with exposure limits? This might especially be the case if you're laying and if the subset A, B C and D generates bets at a different range of odds (shorter perhaps).

*Tags: General Technical*

---

**Adrian** - *08:04:57*

Hi Michael thanks for helping out. Yes pretty much, except the first run I don't have any limits, I just capture as many bets as possible. Then when I say, "filter" using A, B, C, D i cut it down from 900 to 600 as you say. Then rerun with those same filters and get 1200

Just one matched bet per runner, backing only so the exposure doesn't change.

*Tags: General Technical*

---

**Beeblebrox** - *08:33:52*

I think the best thing is to pick a market that this happens on and use the debugger to step through it to see what's happening.  It's difficult to explain as it's a bit counter-intuitive.

*Tags: Errors Debugging*

---

**Adrian** - *08:48:53*

I feel my brain ceasing up :exploding_head: but i think i'm starting to get it. i'll look into getting the debugger going. thanks for all your help everyone :grin:

*Tags: Errors Debugging*

---

**Oliver Varney** - *08:49:34*

pycharm is decent IDE for python

*Tags: General Technical*

---

**Dave** - *19:48:43*

Are your filters time based by any chance? This often happens when you are imposing time based restrictions since stuff that would have matched before (which you hope to filter out) is just instead being matched later on still at -ev (provided you are not running your simulation on infinite bankroll)

*Tags: General Technical*

---

## 2021-09-29

**Jack** - *00:14:37*

Hello :wave:  Is there a way to set a strategy to only play out of the games wallet balance? I've heard it mentioned here a few times but never seen how it's done.

*Tags: Strategies*

---

**Jack** - *09:07:19*

Wanted to go live with a new strategy and protect my main balance on case it starts throwing money away. 

I suppose when it was mentioned before the suggestion was actually just to move money across from main to games wallet to keep it safe.

*Tags: Deployment, Strategies*

---

**Oliver Varney** - *09:17:43*

I feel your pain, dont know why betfair cant give at least two accounts (instead of a full TMSA) for all api key holders. I think flumine has a number of variables to control as much as possible.

*Tags: General Technical*

---

**Jack** - *12:50:59*

I'm logging all my orders into a database by strategy so I'll just watch it closely for a while. I haven't got much in the account to lose anyway :joy::sob:

*Tags: Strategies*

---

## 2021-09-30

**nthypes** - *00:40:23*

folks, what kinda of strategy can be develop when you have +60% strike rate in predict (+/-)2% moves in the market

*Tags: Strategies*

---

**Beeblebrox** - *10:38:30*

I guess my question is what conclusions can I draw from the match rate v signal?  And is there anything else people do to analyse it, other than plot a graph?

*Tags: General Technical*

---

**Beeblebrox** - *11:15:57*

:joy: That's good to know! Basically - plot stuff, try and make sense of it, tweak your strategy and see what happens?

*Tags: Strategies*

---

**liam** - *11:21:47*

you need to be aware of your impact along with feedback from your bets, the latter gets very complicated and impossible to model, ideally you start small and nudge up until your profit levels. Keep stakes as low as possible and instead think outside the box in how you can get more matched, for examples £5+ bets are massive inplay

*Tags: Strategies*

---

**liam** - *11:38:01*

flumine?

*Tags: General Technical*

---

**liam** - *11:42:55*

default strategy [https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/strategy/strategy.py#L52|settings](https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/strategy/strategy.py#L52|settings) are one live order per runner, you can bump or remove this, but you also have the following on the trade/order itself:



```place_reset_seconds: float = 0.0,  # seconds to wait since `runner_context.reset` before allowing another order

reset_seconds: float = 0.0,  # seconds to wait since `runner_context.place` before allowing another order```

*Tags: Deployment, Strategies*

---

**liam** - *11:54:16*

strategy dependant and part of the art, anything I deploy live would have been through backtesting yes

*Tags: Deployment, Strategies*

---

**PeterLe** - *12:16:19*

Just on the reset; I have mine set to :

```place_reset_seconds=0.14```

I think that this may be too generous as Im missing out compared with other strategies that Im not running on Flumine.

What would be the lowest you could set this to, just before it becomes irrelevant? (Ie if the code takes 10ms to run then there would be no point attributing a value to the variable anyway). It would be interesting to know how low you would go with something like for the fast inplay horse markets.

*Tags: General Technical*

---

**PeterLe** - *14:03:39*

Thanks Liam, I have used zero on other programs. I was just being cautious with this one on Flumine. Ill have a play around with it.

*Tags: General Technical*

---

**Beeblebrox** - *19:59:27*

Also [@U4H19D1D2](@U4H19D1D2), are the comments wrong here: [https://github.com/liampauling/flumine/blob/3674986e14d266acc739a8288d6c50183d42db14/flumine/order/trade.py#L32](https://github.com/liampauling/flumine/blob/3674986e14d266acc739a8288d6c50183d42db14/flumine/order/trade.py#L32)



It says `place_reset_seconds` is the time to wait since `runner_context.reset` and `reset_seconds` the time to wait since `runner_context.place` , but isn't it the other way round?



Similarly for the docs: [https://github.com/liampauling/flumine/blob/62b40ad11988ecab039b3b822bc70779aed64bca/docs/trades.md](https://github.com/liampauling/flumine/blob/62b40ad11988ecab039b3b822bc70779aed64bca/docs/trades.md)

*Tags: General Technical*

---

**Beeblebrox** - *20:04:19*

Because `place_reset_seconds`  is being compared to `placed_elapsed_seconds` here  [https://github.com/liampauling/flumine/blob/bae349672f543d1973b706e5efb72f6bc05fd5e5/flumine/strategy/strategy.py#L154](https://github.com/liampauling/flumine/blob/bae349672f543d1973b706e5efb72f6bc05fd5e5/flumine/strategy/strategy.py#L154)



and `reset_seconds`  is being compared to `reset_elapsed_seconds`[https://github.com/liampauling/flumine/blob/bae349672f543d1973b706e5efb72f6bc05fd5e5/flumine/strategy/strategy.py#L163](https://github.com/liampauling/flumine/blob/bae349672f543d1973b706e5efb72f6bc05fd5e5/flumine/strategy/strategy.py#L163)

*Tags: Strategies*

---

## 2021-10-08

**Aaron Smith** - *17:14:34*

If i want to put my placed orders in a database, what would be the best practice? processed_closed_market seems reasonable, but i dont want it doing stuff there when it could be processing market_books, so maybe a backgroundworker?

*Tags: General Technical*

---

**Aaron Smith** - *17:16:25*

never used that/dont really know what it is. I ll have a look at it and come back if any questions are left. Thanks [@UUCD6P13J](@UUCD6P13J)

*Tags: General Technical*

---

**Aaron Smith** - *18:53:50*

[@U9JHLMZB4](@U9JHLMZB4) thanks for input on how to use it, i ll sit down tmr and try and get a grasp of what the best approach is, so that kind of input will surely make my day tmr easier :slightly_smiling_face:

*Tags: General Technical*

---

## 2021-10-09

**Aaron Smith** - *11:30:26*

When using logging control, is this going to interfer with flumine's main run thread? Can i write stuff into the db straight from the _process_event functions or should i hand it over to something seperate to not slow down the important stuff?

*Tags: Performance*

---

**Aaron Smith** - *11:35:45*

My technical knowledge is very limited, so i ll dig a little deeper with some probably stupid questions :smile:

If the flumine instance is running on a single core, wouldnt it still have to priotize one thread over the other?

*Tags: General Technical*

---

**liam** - *11:37:20*

No, have a read up on processes and threads, python runs by default on a single process so the only blocker would be the GIL but tbh you can ignore that for now. Threads run in the same process and share memory but don’t block each other (ignoring the GIL)

*Tags: Performance*

---

## 2021-10-10

**ShaunW** - *18:47:42*

Sounds good to me [@UNQGKT0CR](@UNQGKT0CR) and how I do it, although not being a Flumine guy I don't know if there's a slicker way to do it with that. And you can get a MVWAP by refreshing your T1-snapshot with one of a certain age rather than one from a fixed time like at the off, if you wanted too.

*Tags: Errors Debugging*

---

## 2021-10-16

**PeterLe** - *10:48:07*

Morning

Im just looking to relax some of these params now Im getting more confident..



max_order_exposure: float = 10,

max_trade_count: int = 300,

max_live_trade_count: int = 2,

multi_order_trades: bool = False,



So (and I apologise for the basic question!) ...assuming the settings above am I correct in thinking that using the exposure setting and max live trades, my exposure would be £20 (Each time my trigger is activated, ie 2 orders at £10 exposure each)?

Also, if the multi_order_trades bool was set to False this would only ever allow one live trade anyway?

Essentially I want to send more orders but still have control on max exposure. Thanks in advance...

*Tags: Deployment*

---

## 2021-10-19

**Adrian** - *11:50:33*

I have a strategy that performs really well over 2 months of backtesting data. Looks great on the MC analysis as well. However it performs dismally on the verification/test data, to the point od only breaking even. I can't figure out if I'm overfitting (I don't want to look at any of the verification data so I dont overfit that too), but i dont think my parameters are overly complicated (only 4 variables). Is it normal that this happens or could something else be going wrong? Thanks

*Tags: Strategies*

---

**Oliver Varney** - *11:51:21*

When your training / fitting are you evaluating its performance against the training or test set ?

*Tags: Performance*

---

**Adrian** - *11:54:32*

I'm not sure what you're asking. I'm not doing any machine learning. Just running a backtest over all my markets from 2-4 months ago (~10k markets) then verifying with with the last 1-2 months of markets with the exact same strategies. Not changing anything, just the target path for the data files

*Tags: General Technical*

---

**Adrian** - *11:57:54*

i got the same problem when i used all runners instead of just top 3 and had 1700 matched bets total. exact same problem

*Tags: General Technical*

---

**Oliver Varney** - *12:01:26*

Typically you would change a value(s) of signal then at each step evaluate the performance in the test set. This should help as it sounds like your data mining essentially

*Tags: Performance*

---

**Oliver Varney** - *12:02:27*

looking at the performance in the training set has little value normally

*Tags: Performance*

---

**Adrian** - *12:05:29*

ok thanks. that's a different way to look at it. rather than trying to poinpoint a set of parameters for the first set as a whole, I should make small adjustments one by one then test eash in the test set.



How do i avoid becoming too familiar with the test set? As in how do i avoid overfitting the test set so that im not repeating the same problem

*Tags: General Technical*

---

**Oliver Varney** - *12:09:26*

from a ML perspective but I think it generally applies, is that you have some form of loss function (a way to measure performance), you take iterative steps changing values of the signal which decrease this loss function in training set. Then you would predict the estimates for the test set in that step and if its loss decreases also then you continue repeating, until teh point where a change to the signal value decreases the loss in the training set but increases the loss in the test set

*Tags: Performance*

---

**James T** - *12:10:15*

Sounds like you’re overfitting, but the real problem is that it’s just not enough bets to confirm whether the strategy works or not. 



Over a 2 month period really you need to be aiming for 10 or 100 times that number of bets. Otherwise you’re just not going to be able to scale it unless you have some massive edge. 

*Tags: Strategies*

---

## 2021-10-21

**Adrian** - *23:32:51*

I should have been more specific - do you analyse historical data separately from backtesting? I know how to backtest. I just dont really know how to approach the historical data.

I guess I'm just looking for a more reliable approach to what I've been doing, which is identify a strategy (from observed market behaviour), backtest, adjust parameters, validate.

So far the validations have fallen down. So I have a more fundamental issue.

Liam said start with a known strategy and adjust to make it work. So how does historical data analysis work with that? It's not just adjusting the backtest, right?

*Tags: Data Quality, Strategies*

---

## 2021-10-22

**Oliver Varney** - *10:01:30*

I think it depends on your approach to strategy design, what you are trying to predict and the type of market (for example I would think modelling pre off vs inplay would be different approaches). For me at least backtesting (in the sense of simulating matches) is the last stage, knowing prices available at entry vs benchmark is the fastest way to iterate I find.

*Tags: Strategies*

---

## 2021-10-23

**Adrian** - *10:01:00*

Thanks Oliver. I don't really know how to approach historical data analysis. My only skill right now is backtesting, then iterating the backtest for better performance. THere must be a way to make it work

*Tags: Data Quality, Performance, Deployment*

---

**Adrian** - *10:22:09*

yeah for this one strategy i have been working on. i mean, i make a profit, it's just not as big

*Tags: Strategies*

---

**Oliver Varney** - *10:35:59*

5% in real life is good if you can scale it to high stakes. It sounds however that you could be overfitting slightly. Remember you should only keep refining a signal if it increases both the performance of the training and crucially the test set.

*Tags: Performance*

---

**Alex Bella** - *10:47:02*

Hi Everyone, I'm Alex and new to all this.

I know ML, Python and APIs but am a bit lost as to where to start. Any advice or doc for someone trying to get started?

*Tags: Getting Started*

---

**Adrian** - *10:51:03*

Ok that makes sense. Thank you for your help!

*Tags: General Technical*

---

**Newbie99** - *11:16:15*

In terms of docs, the best places are:



Betfair docs: [https://docs.developer.betfair.com/pages/viewpage.action?pageId=4392320](https://docs.developer.betfair.com/pages/viewpage.action?pageId=4392320)



Betfair Lightweight (API wrapper): [https://github.com/liampauling/betfair](https://github.com/liampauling/betfair)



Flumine (Betfair 'Trading' Framework): [https://github.com/liampauling/flumine](https://github.com/liampauling/flumine)



Other than that, there are plenty of experts to ask on here!

*Tags: Strategies*

---

**Peter** - *12:17:37*

I'd recommend taking a look at the strategy examples in the flumine repository and then trying to get the lowest layer strategy working using

```client = clients.BetfairClient(trading, paper_trade=True) ```

so that you can be confident that you're not risking real money.

*Tags: Strategies*

---

## 2021-10-24

**Amit Patel** - *18:35:27*

its one of the free horse racings files from betfairs historical data site, 29717254 from 1st May 2020

*Tags: Data Quality*

---

**Amit Patel** - *20:00:40*

good question, not sure how to verify



just to check, when running backtesting, should I run all files for a day or can I run single files?

*Tags: General Technical*

---

**Amit Patel** - *20:07:41*

works using the file PRO-1.170258213 in the github repo so likely as you mentioned, still would love an answer for the question above though if anyone knows!



Appreciate all the help :)

*Tags: General Technical*

---

**MacrcoPolo** - *20:09:16*

The files are human readable, you should be able to get hints by looking at the messages with 'marketDefinition' in them. I'd go direct to the answer here for you, but it's a moment to teach you how to understand what's going on :slightly_smiling_face:

*Tags: General Technical*

---

## 2021-10-25

**liam** - *09:37:38*

Thats an event file full of 'OTHER_PLACE' markets, as the docs state you need to add the event flag if providing an event file



[https://liampauling.github.io/flumine/quickstart/#event-processing](https://liampauling.github.io/flumine/quickstart/#event-processing)



Note this will be slower than processing single files

*Tags: Performance*

---

## 2021-10-27

**AndyL** - *22:09:45*

[@U01S1VB9X9P](@U01S1VB9X9P) just being reading your posts, you sound like you're doing what i was, ie.making a pure backtest created strategy work, they don't!!

I had a backtest dog betting strategy, worked great over the 3months/3000selections, 6.5%ROI, went live.....lost money....

I think what works is come up with a strategy based on your experience of how the market works, then backtest to validate

*Tags: Deployment, Strategies*

---

## 2021-10-28

**JazzMan** - *11:10:11*

To make money from a live strategy you have to be doing something unique. If you’re doing the same as everyone else then you’re all waiting in line to place the same bet. This maybe the problem with everyone using the same backtesting program. Backtesting works fine for me but I’ve taken a long time writing my own.

*Tags: Deployment, Strategies*

---

**Aaron Smith** - *15:40:11*

I dont agree here. The goal of backtesting is to replicate as good as possible what would have happened if you had the strategy running live. There is no need for it to be very unique. I m rather sure goats problem was (and the problem he is warning of) that he just threw so many strategies into the backtest machinery until he got a positive results by chance. Even if you were to throw the same strategy in the backtest machinery and always give it a different patch to backtest on, assuming its not horribly bad, it will eventually show you good results, even though its not a good strategy.

*Tags: Deployment, Strategies*

---

**Aaron Smith** - *16:21:36*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) regarding your strategy that lost money on live: just as a strategy can be lucky during a backtest and indicate that its good even though its not, it may just be unlucky when live and be good even though its current results arent. It may be worth to keep that strategy in mind and once you got another good batch of data, throw another backtest at it. If it performs well again in that backtest you may give it another shot at live, or in case you want to be more cautious, ask yourself where and why it could perform better during the backtest than live.

*Tags: Deployment, Strategies*

---

**JazzMan** - *17:03:20*

So if your strategy is not unique would you be able to run it on 10 different machines and make 10 times the profit?

*Tags: Strategies*

---

## 2021-10-31

**Ruben** - *17:04:14*

I'm thiking there's always going to be some degree of ambiguity simply because of the latency right? at any point an order that I believe is executable, could be matched and hence no longer available. maybe there's no perfect solution to the problem, but rather some "best effort" solution

*Tags: Performance*

---

**Ruben** - *18:15:48*

is it possible that flumine is already doing this removal of orders? I'm trying to do a controlled experiment on a market that isnt moving and I'm not seeing an order that I placed through the api as available, even though I can see it on the web version of the exchange

*Tags: General Technical*

---

**Ruben** - *18:55:24*

nvm, was an issue on my end due to using the debugger

*Tags: Errors Debugging*

---

## 2021-11-01

**Štefan** - *07:18:03*

[@U011VL3CA2Y](@U011VL3CA2Y) my app has such bot strategy I called it [http://bfexplorer.net/Articles/Search?text=Be%20the%20first%20in%20queue|Be the first in queue](http://bfexplorer.net/Articles/Search?text=Be%20the%20first%20in%20queue|Be the first in queue) you can test it or just reverse engineer it, or if you develop your strategy on .net you can directly use it

*Tags: Strategies*

---

**Ruben** - *19:12:08*

this is probably a very stupid question, but I notice that when I place a back bet, it appears on the marketbook as available to lay with a smaller size than what I placed, why? some liability computation on the background? e.g. back of size 10 becomes available to lay with size 8.5

*Tags: General Technical*

---

**Ruben** - *19:28:07*

so in the website the lay appears with the same size of the back i placed through the api, but looking at the marketbook in the debugger, the available_to_lay size is smaller than the size of the back I placed

*Tags: Errors Debugging*

---

**thambie1** - *20:31:40*

I use a different currency than GBP as well. All streaming market data is in GBP, but streaming order data is in your currency. So when you are trying to remove your liquidity from the market book, you will need to do currency conversions. It sucks :cry:

*Tags: General Technical*

---

**Ruben** - *20:59:47*

thanks for the help :slightly_smiling_face:

*Tags: General Technical*

---

**thambie1** - *21:01:48*

I'd suggest contacting support just to make sure. I'm telling what I've heard second hand about how to do it

*Tags: General Technical*

---

## 2021-11-03

**Peter** - *15:33:44*

[@U01DVUAE2G1](@U01DVUAE2G1) I think that's sometimes yes/ sometime no kind of a question. Do you have a specific event type and country in mind?

*Tags: General Technical*

---

**Aaron Smith** - *16:00:35*

[@U9JHLMZB4](@U9JHLMZB4) Not really, any horse/hound market really, thought i d have a look at those markets. yes/no i assume is regarding the 2nd question? First one i assume its the number from the name rather than the sortPrio? Also, if they change, how would i be able to notice that, as neither names nor sortPrio are given with a market_book  i think?

*Tags: General Technical*

---

**Aaron Smith** - *16:31:03*

[@U9JHLMZB4](@U9JHLMZB4) i checked for GB/IR dogs, and while sortPrio equals the leading digit of his name most of the times, i do find occasional cases where they dont match. I m not yet sure how to match by selection_id. If i want to place an order on the FC market, i would first have to know what that selection actually is, for which i would need to know the runners represented by the number-tuple given in the fc_selection name. For this i seemingly can not check sortPrio as they are unreliable and names i can only see in the market_catalogue. I could make a dictionary from the market_catalogue where i take the leading digit from the runners name (is this always equal to its race card number? or is there a better way to get this number?) and assign it to the runners selection_id. But then i need to be able to trust that this is not going to change, as i wouldnt see that in my market_book update.

*Tags: General Technical*

---

**Peter** - *16:53:48*

Just to fill in some extra detail in case it's helpful ... For GB dogs, they will always be listed in order from 1-6 for win and place markets, so the order trap will match sort priority order, though if there's an empty trap sort priorities may only go up to 5 (or even 4). Selections in the forecast market will also always be in the same order and have the same selection_ids in every race e.g. 1-2 will always have selection_id 41419585. Note that the ids aren't contiguous, there are some gaps, even when all traps are in use. Note too that when backtesting you don't have to go back too far to find a different set of selection_ids, though the older ones were also used for every race at that time. When a trap is empty, the selection_ids combining that trap with each of the others may still appear in the forecast market data but with status "REMOVED".

*Tags: General Technical*

---

## 2021-11-05

**Adrian** - *09:14:30*

What's the best way to analyse market data? Is there a separate program or something? How would one even start to analyse the historical data?

*Tags: Data Quality*

---

**Adrian** - *09:22:18*

I'm really at my whit's end here. I have all these strategies that of course don't work straight out of the box but I have no idea how to improve them. Just tweaking the parameters and backtesting is a crapshoot. Analysing the backtest results and tweaking off that is a crapshoot. Everything is a crapshoot and the only way i can think to discover anything useful before I throw my fucking computer out the window is to pour over the historical  data using some kind of software or program other than flumine/bflw.

*Tags: General Technical*

---

**Adrian** - *09:30:04*

even better i'll give you the exact strategy i've been working on [https://forum.betangel.com/viewtopic.php?f=53&amp;t=16731&amp;sid=6a9993465c324059eb7363d075c5887a](https://forum.betangel.com/viewtopic.php?f=53&amp;t=16731&amp;sid=6a9993465c324059eb7363d075c5887a)

*Tags: Strategies*

---

**Mo** - *09:55:19*

Doesn’t this strategy involve leaving up orders?

*Tags: Strategies*

---

**liam** - *09:58:55*

Have you tried to analyse the strategy described by Dallas? For example:



• Get some pre race data

• Push through bflw and record WOM% / price / odds / … / result into a csv

• Pull into pandas see if there is any relationship between WOM/price/result and thus value 

• The above can be done with graphs/regression/ml etc.

My assumption would be that the above is a half day job and would probably lead you down another route. Also note that we are ignoring matching etc but it would give you a good idea if the idea has any value.

*Tags: Feature Engineering, Strategies*

---

**liam** - *10:12:50*

my first question would be is that actually a positive signal, if no can it be adjusted to be positive

*Tags: General Technical*

---

**liam** - *10:28:02*

python

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

*Tags: Feature Engineering*

---

**liam** - *10:31:52*

if you want I can go through this in a jupyter notebook if you think it would be helpful?

*Tags: General Technical*

---

**Adrian** - *10:32:58*

that would be awesome! i was just about to ask how you extract the data from the stream/historical data

*Tags: Data Quality*

---

**Adrian** - *10:36:25*

make it as light as you want. i'm not here to steal anyone's ideas, just want to learn how to do the analysis

*Tags: General Technical*

---

**liam** - *12:17:34*

[@U01S1VB9X9P](@U01S1VB9X9P) here is the first part in using flumine to collect some raw data



[https://github.com/liampauling/flumine-strategy-development](https://github.com/liampauling/flumine-strategy-development)

*Tags: Strategies*

---

**Adrian** - *22:49:15*

[@U4H19D1D2](@U4H19D1D2)

&gt; {"asctime": "2021-11-05 22:49:00,368", "levelname": "CRITICAL", "message": "Unknown error list index out of range in process_market_book (1.186625686)", "exc_info": "Traceback (most recent call last):\n  File \"/Users/adrian/opt/anaconda3/envs/betfair/lib/python3.8/site-packages/flumine/utils.py\", line 204, in call_strategy_error_handling\n    return func(market, market_book)\n  File \"/Users/adrian/Projects/betfair/code/dataanalysis/flumine-strategy-development-master/strategies/datacollectwom.py\", line 60, in process_market_book\n    wom_one = _calculate_wom(runner, 1)\n  File \"/Users/adrian/Projects/betfair/code/dataanalysis/flumine-strategy-development-master/strategies/datacollectwom.py\", line 115, in _calculate_wom\n    p = runner_book.ex.available_to_lay[i]\nIndexError: list index out of range"}

*Tags: Errors Debugging, Strategies*

---

**Adrian** - *22:56:25*

thanks yeah i've wrapped my head around liam's version of wom. i didnt get that error with mine

*Tags: Errors Debugging*

---

**Adrian** - *23:42:11*

[@U4H19D1D2](@U4H19D1D2) thanks for this awesome work! just what i needed. now just have to figure out how to analyse it

*Tags: General Technical*

---

## 2021-11-06

**Oliver Varney** - *08:40:59*

btw I think my model uses both although im not sure off memory which has a higher weight

*Tags: Performance, Strategies*

---

## 2021-11-07

**AndyL** - *21:24:45*

Now im working on models

*Tags: Strategies*

---

**Adrian** - *22:09:35*

[@ULDAVFDRP](@ULDAVFDRP) i am doing this part time so it's not that it's overwhelming me. it's just that I've been doing this so long for no reward is pretty shitty. i'm trading crypto and nft's now and seeing instant profit, so it really makes me question the validity of this pursuit

*Tags: Strategies*

---

**Adrian** - *22:10:32*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) thanks for sharing that. so you abandoned wom altogether? what do you mean by models?

*Tags: Strategies*

---

**Adrian** - *22:12:35*

[@U4H19D1D2](@U4H19D1D2) how did you progress from looking at data output like the one from this strategy to being able to see if something is profitable or not?

*Tags: Strategies*

---

## 2021-11-08

**AndyL** - *02:08:02*

[@U01S1VB9X9P](@U01S1VB9X9P) yes, wom i couldn't find profitability, in fact like all the tech analysis ive tried...

Which is why im looking at modelling my own prices, and then value betting

*Tags: Strategies*

---

**Oliver Varney** - *06:17:08*

[@U01S1VB9X9P](@U01S1VB9X9P) and [@U01PJ5YMFBJ](@U01PJ5YMFBJ),genuine question here (and not meant in a  negative way) , If you took a step back, do you view it as a positive that its hard to crack, knowing its a zero sum game (ignoring comms)? [@U01PJ5YMFBJ](@U01PJ5YMFBJ) personally I think the model approach has alot more legs to it, so if you get it going then there should be some profit out there for you for sure.

*Tags: Strategies*

---

**Adrian** - *09:17:36*

[@ULDAVFDRP](@ULDAVFDRP) thanks mate i appreciate the feedback. i'm not pessimistic about what it's possible to earn, i'm just frustrated that i can't crack it. i don't know if i'm learning the right things. and as for doing the world any good, how do you know? your trading supports Betfair, which helps hundreds (thousands?) of people put food on their table. And gives joy to people who like to gamble at the best possible prices.

*Tags: Strategies*

---

## 2021-11-09

**V** - *11:14:44*

Anyone have any strong opinions (and willing to share!) around the various overround removal methods? Noticed that [https://cran.r-project.org/web/packages/implied/vignettes/introduction.html|https://cran.r-project.org/web/packages/implied/vignettes/introduction.html](https://cran.r-project.org/web/packages/implied/vignettes/introduction.html|https://cran.r-project.org/web/packages/implied/vignettes/introduction.html) has a new addition, using Jensen-Shannon distance. Shin seems like a go-to choice, partly because [@UBS7QANF3](@UBS7QANF3) has a Python implementation [https://github.com/mberk/shin|https://github.com/mberk/shin](https://github.com/mberk/shin|https://github.com/mberk/shin) ;)

*Tags: General Technical*

---

## 2021-11-11

**liam** - *15:54:19*

Liquidity is the problem, doubt anyone would say US is better than UK in terms of pnl

*Tags: General Technical*

---

**liam** - *16:03:35*

I found my UK stuff didn't work out the box also much harder to catch errors/issues due to the market being so illiquid

*Tags: Errors Debugging*

---

## 2021-11-16

**Unknown** - *22:11:51*

Any ideas on how to extract goal scored information from the betfair historical data? I was wondering what your approaches are. Obv one could monitore the correct score market but are there any other more efficient approaches? What I try to achieve is deduct from the over 1.5 market the point a goal was scored. As you can see in the linked image it is obvious that the goal was scored after around 67min. However in the end you can also see some weird behavior where the odds drop and then return to a higher point. So what would be the best way to distinguish between those two scenarios in a less perfect odd plot.

*Tags: Data Quality*

---

## 2021-11-30

**AndyL** - *22:51:31*

How do you guys use models?

Ive created my own model for uk greyhound prices, from recent form, ratings, times, etc.. so i get my prices and compare with the market and they'll typically differ in some way...! The odds are the market is more accurate than my prices, and ive demonstrated that by trying to enter prices when mt model thinks there's value....it isnt!!

Do I just need to refine and improve my model until it is better than 95% of all other people's models, ie.mine is the better?! But then how do i know?

*Tags: Strategies*

---

**AndyL** - *23:11:11*

Ok so if you're aiming to beat BSP or say the price at 5seconds to go on uk dogs, on average, is a model going to help do that?

As that implies the bsp is efficient on average for MY selections. What if say my model happens to pick half the market selections that are below true value?

*Tags: Strategies*

---

**AndyL** - *23:19:30*

The other issue with models is 100s of other very smart traders are doing the same...so mug me and 95% of the others create a valid model, we think a selection has value so we back it, but realistically the opposite is going to happen the 5% smart traders lay those offers and the price drifts rather than steam...even though fair value is probably lower...

But the market overall is still efficient because these trades occur both up or down, so even the the individual prices are nothing like fair value overall averaged they are efficient!!

*Tags: Strategies*

---

## 2021-12-01

**Michael** - *08:32:31*

Regarding EMH: We all know that markets are not totally efficient, so what we're seeing is "approximate efficiency." We know that overall markets appear entirely efficient so that shows us that the inefficiencies go both ways, in other words as much value is given away by layers as by backers.



Regarding models: The problem is not with models, just your model.

*Tags: Strategies*

---

**Mo** - *08:49:49*

What is your theory for why your model is better than that being used by the market makers? If you don’t have one then there’s no reason to think you have an edge. Either you need to be using data they’re not using and/or a modelling technique that can better exploit the data you are both using. The likelihood of this is greater on less efficient markets

*Tags: Strategies*

---

**Mo** - *08:54:32*

My point is I use models to make value bets. If your model is not able to identify value bets then it's back to the drawing board...

*Tags: Strategies*

---

**AndyL** - *08:57:29*

Right...but my extra question is on bsp, i currently find value bets with model then close near off/bsp. My model might be good but the bsp of my selections might be -value, like [@UGV299K6H](@UGV299K6H) said i think..?

*Tags: Strategies*

---

**D C** - *09:08:15*

I suspect that the word "model" means different things to different people.

*Tags: Strategies*

---

**James T** - *09:24:09*

Try not to overthink what everyone else is or isn't doing. Just try to think of ways to make your own price calculation/prediction more accurate. Like you say, you're not beating the market at this point, you just need to make your model better. 

*Tags: Strategies*

---

**liam** - *09:25:06*

+1, if you assume the market is as efficient as its going to get at post time, then as you move further back in time then the market should be less efficient. I think you are trying to apply the model / value logic (correct) to your old way of 'trading' the markets.



My understanding is that you were using market signals on racing but it sounds like you are now using historic form on UK dogs? How are you then creating this model?

*Tags: Strategies*

---

**James T** - *09:35:35*

BSP is just a good benchmark. It's a prerequisite. Firstly your strategy should beat BSP on average. (If it doesn't then it would have made more sense to place your bets at BSP because you'd be getting a better price). (It might be a little more nuanced than that, but broadly that's what to aim for). 



Once you're beating BSP you can decide whether to place an (opposite) closing bet at BSP or not. You might find that generates more or less value (normally it's less), but it also relates to the variance of your returns like you have already said. 

*Tags: Strategies*

---

**AndyL** - *09:36:55*

Using previous form and performance of the dogs

*Tags: Performance*

---

**Beeblebrox** - *09:54:02*

A greyhound machine learning modelling process was discussed in one of the Aus meetups. Might  be useful for you:



[https://www.youtube.com/watch?v=MLalkyPDiQE&amp;list=PLvw8KRdyfOY19ys_5lpSpcbjpy_PBoZEZ&amp;index=13](https://www.youtube.com/watch?v=MLalkyPDiQE&amp;list=PLvw8KRdyfOY19ys_5lpSpcbjpy_PBoZEZ&amp;index=13)

*Tags: Strategies*

---

**AndyL** - *18:07:54*

Thanks [@U01MPC0GUK1](@U01MPC0GUK1) for the video link, good

One thing it highlighted i found odd, they evaluate strike rate based on betting on the predicted model selection with the lowest price, no comparison of market price to predicted price for value evaluation...?

Just bet on most probable winner, their model showed a 32% strike rate at that, where as doing the same using bsp showed a 42% strike rate, hence model needs improvement...

Just find it odd there's no value check?

*Tags: Strategies*

---

**Peter** - *20:31:21*

A lot of skill when using ML is determining the appropriate type of model to use. Choose a model that takes a set of features and gives you a simplistic answer, and that's what you'll get. But there are other model types, e.g. logistic regression, that do assign probabilities for each possible outcome.

*Tags: Feature Engineering, Strategies*

---

**birchy** - *20:57:19*

Given that markets are efficient long term, you could literally create a completely random strategy to gather a load of data and then analyse the results to find the combinations of signals that provide the best results.

FYI: I don't practice what I preach. :face_with_rolling_eyes:

*Tags: Strategies*

---

**AP** - *21:29:14*

You could take a step back from building a pricing model (which I think is quite a difficult task) and try to isolate some variables that the market is mispricing and building a strategy around one or more of them

*Tags: Strategies*

---

**AndyL** - *22:35:30*

[@U4H19D1D2](@U4H19D1D2) correct i am doing a simple approach. Basically ive been watching loads of races and corresponding odds and looking at recent form, and ive created my own price formula from the standard features, recent finish positions, recent normalised speed avg and max, recent normalised sectional timings...

I actually quite like the prices it produces, i watch races and compare mine v market, and quite often think mine are more true. For example i quite often see a dog go up a grade and the market immediately gives it a very short fav price say 2.0,but my model has it say more at 3.5+, and to me the market has it so wrong the dog typically finishes near last...

This is where i think my model falls down it's trying to cover all prices and all dogs. Taking what [@UNQGKT0CR](@UNQGKT0CR) i think said isolate more specific variables, eg.short priced upgrade...

*Tags: Feature Engineering, Performance, Strategies*

---

## 2021-12-02

**ShaunW** - *02:42:34*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) You're not going to develop a greyhound betting model by watching a few races and saying you often think your prices are more true.

*Tags: Strategies*

---

**Michael** - *07:34:09*

I think you're doing ok [@U01PJ5YMFBJ](@U01PJ5YMFBJ) - you might not be there yet but you've certainly come a long way. However the test of whether your prices are better than market prices is whether you win by betting on the difference, if you don't then it doesn't make much sense to "prefer" yours.



It might be that your model doesn't cut the mustard but the process of building it might have given you new ideas or ways of thinking about things, if so it was a worthwhile exercise. Don't sweat the model not out performing the market, it's really difficult to build a good fundamentals model with just basic inputs and I'm always really impressed by people who manage it. I don't think I could.

*Tags: Strategies*

---

**Michael** - *07:36:45*

All of that said, I doubt that a fundamentals model is your shortest route to making a profit.

*Tags: Strategies*

---

**Mo** - *07:58:39*

How does it perform on those dogs that you "like" the price of? If it still performs poorly then the market hasn't got it wrong. It's perfectly acceptable for your model to be more accurate on some particular subset of dogs

*Tags: Strategies*

---

**AndyL** - *09:37:49*

Thanks [@UGV299K6H](@UGV299K6H), i was curious from the [http://betfair.au|betfair.au](http://betfair.au|betfair.au) videos and LR analysis why they were judging it purely on how good each model was at picking the winning favourite, as maybe they do but surely it depends on the price. Say they win 32% of the time and profit 1R, but lose 68% of the time and lose 0.4R, then that's still a profitable strategy?

*Tags: Strategies*

---

**AndyL** - *09:42:09*

Must say it's never as good getting advice online as it is face to face, so hoping to get to the next meetup, and ill buy all you guys who have helped me on this journey a pint!

*Tags: General Technical*

---

**Peter** - *10:15:31*

That's a 4.8% return, so sounds good to me. Even if that's gross profit, you could take out 8% commission and still have a return of 2.2%, better still of course at lower commission rates, So provided the model had been derived from a sufficiently large sample and produced the same returns against data not used in it's construction, I'd go with it.

*Tags: Strategies*

---

**Peter** - *10:57:13*

The numbers were clearly made up :wink:. If you had a strategy capable of that, you'd be quietly raking in the profits rather than seeking advice here :grinning:.



I think that by number of winners in this context, you're referring to hit rate, and that's generally at the heart of whether a strategy has value, i.e. whether the odds at which you can match and the frequency with which they come in, are good enough for the winnings to overcome the losses, because, unlike gamblers, we carefully balance the two.



I believe that Liam's earlier point was that as you get closer to the start time the market gets more efficient. So by moving further away from the start and being selective, you can hope to find more mispriced runners with better odds and increase your hit rate.



After the event you can compare the price you got with the BSP as a proxy for the fair price to see whether the odds that you though were good actually had value, or not, as you can't tell that from the outcome of the race itself.



I very much doubt that you will find a strategy that delivers consistent value, i.e. over a large number of races, but has prices that, on average, are worse than BSP.

*Tags: Deployment, Strategies*

---

**Peter** - *11:00:10*

Being selective is important too. When I find a profitable strategy, I continue looking at signals to see whether I can achieve the same absolute profit from less bets, and often I can shave 30-40% off, with minimal reductions in profit, giving me a higher hit rate, better ROI and less volatility (i.e. a more comfortable ride).

*Tags: Strategies*

---

**Peter** - *11:31:57*

He's starting with logistic regression, so is training a model that gives win percentages for each runner. Although he then works out the strike rate by seeing how often the runner with the highest win probability actually wins, he describes that as one of the most important metrics, rather than the objective of using the model. This seems to me fair. The first step in assessing any model is whether it returns results that are appreciatively better than random, otherwise ditch it and move on.



However, you're then going on to compare the results with BSP. That's a good instinct, but sets the bar rather too high. In practise most models will tend to perform better over some subsets of the data than others, so for me a more useful question to ask of each model is can I find range of inputs over which it performs particularly well, and focus in on that. Examples might be models that are good at finding runners unlikely to have an impact on an event (and quantifying unlikely) or runners whose early price is wildly out-of-line with where BSP is likely to end up - even if it's off by a few percent from the final BSP price.

*Tags: Strategies*

---

**Peter** - *11:35:07*

But you are absolutely right that he's not giving too much away. As [@UBS7QANF3](@UBS7QANF3) hinted earlier, the key to success is often coming up with a feature or features that other people aren't using to give you a small edge. He's not giving us anything from his personal secret sauce. Indeed he's showing in his predictions calibration section that there's still rather a lot of work needed to make that model usable.

*Tags: Feature Engineering, Strategies*

---

**birchy** - *18:58:40*

If I was asked to give just ONE piece of advice, it would definitely be to NOT look at short term performance. And I learnt that the hard way. One of my best ever strategies lost over £500 in the first week and left me with around £100 in my account. I was pretty pissed off at the time, so logged out in frustration, thinking I'd never return. I left the bot running. A month or two later, I logged in expecting to see my account empty. It wasn't. It had placed 1000's of bets during my absence and turned out to be a £3K a month strategy at £2 stakes and ran for around 18 months before it started to decline. So the moral of the story is NOT to write-off a strategy just because it's showing a short term loss. Variance can take several months to shake out and you need some fairly large bollocks and a lot of patience to ride on this rollercoaster.

*Tags: Performance, Strategies*

---

**birchy** - *22:24:24*

Oooh...that was probably around 2003/2004 at a guess. I was "aware" that sample sizes need to be fairly large _before_ that, but was very blinkered and would often pull a strategy that started off badly.

Pretty much everything I look at nowadays has some elements of those early strategies but of course, things are a _lot_ harder nowadays.

*Tags: Strategies*

---

## 2021-12-06

**TT** - *20:52:33*

I'm looking at the example for the BackgroundWorker, and it shows each market context getting updated. I've got a single variable I'd like to make available to all markets but I'd rather not copy it to each market context as it is quite large. Does it make sense to add this to the strategy context directly? I.e.



`def background_worker_function(context, flumine):`

    `x = query_db()`

    `flumine.strategies._strategies[0].context["foo"] = x`



and then you can just access it within the strategy using `self.context["foo"]`

Does that make sense?

*Tags: Strategies*

---

**liam** - *20:57:46*

But do it properly by looping the strategies and updating by name rather than using the private var



`for s in flumine.strategies:`

*Tags: General Technical*

---

**TT** - *20:59:27*

Yeah, good shout. I'm only at the stage of having one strategy so I could get away with it for now :joy:

*Tags: Strategies*

---

**liam** - *21:14:41*

Famous last words, one way ticket to a day wasted debugging :joy:

*Tags: Errors Debugging*

---

## 2021-12-07

**C0rnyFlak3s** - *09:03:10*

Has anyone experience with developing a strategy around the ‘lay the draw’ idea in soccer? I know most of you guys focus on horses and dogs, but maybe there are some soccer guys out here too. Obviously the plain lay the draw strategy will not make any profit nowadays, but I think with considering some game parameters and maybe analyse the correct entry time for the lay may proof to be profitable. Any experiences with it?

*Tags: Strategies*

---

**D C** - *09:24:43*

I am sure there are ways to make money from it somehow, but whenever I hear "lay the draw" and "strategy" in the same sentence I can't help but get ebook salesman vibes.

*Tags: Strategies*

---

**C0rnyFlak3s** - *09:28:30*

haha ^^ I don’t say I am fully convinced by this idea myself, and I am well aware, that this is the most basic and beginner friendly strategy that will be praised by those ebook salesman as you call them. However I think, most of them do not really analyze the parameters around the games with it. But yeah I may try to come up with something the next days, and see how it fairs :stuck_out_tongue:

*Tags: Getting Started, Strategies*

---

**Aaron Smith** - *09:57:17*

surely given you find and analyse the right parameters, you can make a profit from laying the draw. That is kinda true for any market. But why are you limiting yourself to laying the draw? why not back the draw? Feels like there is an underlying assumption that draws are generally underpriced and that sort of assumption surely needs a decent foundation if you want to build a strategy around it.

*Tags: Deployment, Strategies*

---

**C0rnyFlak3s** - *10:00:42*

Well I am not limiting myself to the lay side of this strategy strictly. If my first analysis concludes, that there are games where the back side is underpriced, I will surely investigate this side too. :slightly_smiling_face: I just used the lay side, to start with. But yeah thanks for your input, definitely this assumption of the draw being underpriced either needs to be validated or refuted

*Tags: Strategies*

---

## 2021-12-11

**Ruben** - *22:06:24*

Good night everyone. I am beginning to test my first strategy live, and I am seeing that it matches about 3% of orders it offers. It offered ~1700 orders and only 50 orders were matched. Is this too low for Betfair's standards?

*Tags: Deployment, Strategies*

---

**Oliver Varney** - *23:07:48*

Only base profit on small stakes with flumine

*Tags: General Technical*

---

**ShaunW** - *23:08:23*

No problems there [@U011VL3CA2Y](@U011VL3CA2Y). As long as you're making some sort of effort to generate commission you're golden.

*Tags: General Technical*

---

**Oliver Varney** - *23:08:40*

Nothing against [@U4H19D1D2](@U4H19D1D2) but its impossible to model

*Tags: Strategies*

---

## 2021-12-12

**Mo** - *08:57:46*

There is a transaction charge that is supposed to discourage excessive numbers of bets but beyond that there aren't any policies on spoofing etc. Your strategy’s behaviour sounds entirely reasonable 

*Tags: Strategies*

---

## 2021-12-15

**birchy** - *18:44:18*

When running Flumine strategies 24/7, I generally use a hard coded dict to store signal parameters, e.g:

`self.params = {

    'min_book': 102,

    'max_risk': 20, etc...

}`

This means that when I decide to tweak some value, I have to edit the source code and then restart the bot. In the past (pre Flumine), I've put the parameters dict in a plain text `.json` file and then dynamically load on market initiation:

 `with open(filepath) as f:

    self.params = json.load(f)`

This obviously allowed me to update parameters without stopping the bot. Is this kind of dynamic loading something that others do? Is there a nicer way to achieve this? Nowadays, I rarely change parameters once a bot is running, so not even sure if it's necessary...

*Tags: General Technical*

---

**Mo** - *19:08:58*

Each match traded as a separate Python process. Config file read on start up

*Tags: General Technical*

---

## 2021-12-19

**Ruben** - *18:53:05*

hello everyone, I am experimenting with running my first strategy live, and I am running it in an EC2 t3.micro instance (2 vCPUs, 1gb memory). However, looking at the logs, I am seeing a lot of latency warnings, some with very high latency, like 60s. Could this be due to the CPU of the instance not being fast enough to process updates at the rate they come in?

*Tags: Performance, Deployment, Strategies*

---

**liam** - *18:54:18*

How many markets? CPU intensive strategy? Spikes of warnings or gradual increase? Look at monitoring in AWS 

*Tags: Deployment, Strategies*

---

**Ruben** - *18:57:16*

atm I am just trying 1 market, but the strategy places/cancels orders constantly. In fact it often reaches the 5k tx limit with just 1 market (but that's another problem)

*Tags: Strategies*

---

**Aaron Smith** - *18:58:23*

[@U011VL3CA2Y](@U011VL3CA2Y) i am using the same instance running multiple strategies on it with no problems (lately), however placing that many orders is costly

*Tags: General Technical*

---

**Aaron Smith** - *18:59:01*

I had a strategy placing a lot of orders (not reaching the 5k though) and immediatly got latency issued

*Tags: Performance, Strategies*

---

**Aaron Smith** - *19:01:14*

check [https://github.com/liampauling/flumine/blob/master/flumine/execution/transaction.py](https://github.com/liampauling/flumine/blob/master/flumine/execution/transaction.py)

*Tags: General Technical*

---

**Aaron Smith** - *19:01:26*

no, it still counts per order/cancel/etc. (the transaction in flumine is a different concept than the one in the transaction limit)

*Tags: General Technical*

---

**Aaron Smith** - *19:01:36*

however, latency issues will likely be resolved

*Tags: Performance*

---

## 2021-12-21

**Ruben** - *07:47:34*

the use of transactions seems to be helping quite a bit! :slightly_smiling_face:

*Tags: General Technical*

---

## 2021-12-28

**Peter** - *22:03:27*

Market.blotter has the orders for the current strategy / market combination.

*Tags: Strategies*

---

## 2021-12-30

**Shashi Khaya** - *12:36:03*

Hi all, my particular interest is in tennis markets - predicting outcomes for different levels of the scoring system (I.e. match, sets, games and points) and then opening positions (based on confidence of prediction) and closing them after the predicted event has occurred (E.g. player A wins/loses a point/game/set). I figured as there is rarely liquidity in the dedicated markets for predicting these events, it makes more sense to bet on the match_odds market and then 'green/red out' based on whether the prediction was correct - In this way you could place multiple trades on a single market. From your experience do you think this type of approach could work with Flumine or am I being naïve trying to build something of this sort?

*Tags: General Technical*

---

**Ruben** - *12:46:08*

From a capacity/workload perspective I think flumine is perfectly suited for that. On the other hand, I know some members of this group will want to have a word about "greening out" a bet

*Tags: General Technical*

---

**liam** - *16:22:33*

Bflw/flumine will do this for you 

*Tags: General Technical*

---

**liam** - *16:23:23*

[https://liampauling.github.io/flumine/quickstart/#listener-kwargs|https://liampauling.github.io/flumine/quickstart/#listener-kwargs](https://liampauling.github.io/flumine/quickstart/#listener-kwargs|https://liampauling.github.io/flumine/quickstart/#listener-kwargs)

*Tags: General Technical*

---

## 2021-12-31

**birchy** - *14:25:38*

In Flumine, what's the recommended way to use

`lookup = (market.market_id, runner.selection_id, runner.handicap)

exposure = market.blotter.selection_exposure(self, lookup)`

If all of our bets are one side only? Let's assume that we've backed OR layed all of the runners and now have an exposure &lt;= 0 for each runner. The current calc accumulates the liabilities at runner level. Is there a way to reset it to avoid validation errors, or is it easier to set `max_selection_exposure` to some big number and self-handle the exposure at market level?

*Tags: Errors Debugging*

---

**birchy** - *14:27:59*

Ah... [https://github.com/liampauling/flumine/pull/517|https://github.com/liampauling/flumine/pull/517](https://github.com/liampauling/flumine/pull/517|https://github.com/liampauling/flumine/pull/517) :flushed:

*Tags: General Technical*

---

## 2022-01-06

**JC** - *16:45:18*

Hi guys, might be a strange question but hoping it can get some discussion started. Is it possible to be profitable with a fundamentals model that is technically worse than the market by placing bets when there is over x % estimated value? Let's say your model's brier score and accuracy are a little worse than the market-implied probabilities, but that your prices deviate from the market - can you still make a profit from betting in the direction that your model suggests the market is wrong? I imagine this is especially possible if you are offering prices and avoiding the spread. Or is the only way for a fundamentals model to be profitable long-term for it to have a lower brier score than the market?

*Tags: Strategies*

---

**Mo** - *17:01:43*

Can your model be worse than the market ON AVERAGE and better than the market in some specific scenarios? Yes. If you only bet in those scenarios can you make money with a model that is worse than the market on average? Yes. Is it likely that you are right when your prices deviate most strongly from the market? No, these are probably the worst situations to bet in

*Tags: Strategies*

---

**JC** - *17:20:20*

Thanks Mo, this makes sense. In theory though, could your model be worse than the market on average but you bet _every_ time in the direction that your model suggests the market is wrong, and you still make a profit? Presumably, the market would be closer to the 'true' probability than your model on average if it has a lower brier score, but if it is slightly wrong then would it not be more likely to be wrong in the direction suggested by your model? Of course, when there is a massive discrepancy it suggests your model is missing some key information and this may not be the case.

*Tags: Strategies*

---

**Aaron Smith** - *18:07:32*

[@U0160E9HS2G](@U0160E9HS2G) Yes, that is possible. To make a oversimplified example, suppose you found a significant parameter that the market doesnt take into account. Now, you dont know anything else but this single parameter, so your modeled probabilities would be nonsense. However, you could still let the market create its price and when the spread is sufficiently small, go in the direction of your "model".

*Tags: Strategies*

---

**Aaron Smith** - *18:09:41*

On another note, being more precise than the market is rather vague without specifying the time. Making a model that creates probabilites better than the market right before the off seems nearly impossible, however you may still find value earlier on, when the market isnt as efficient yet.

*Tags: Strategies*

---

**Peter** - *18:24:08*

I share [@UBS7QANF3](@UBS7QANF3)’s view that it's unlikely that you could make money by betting against the market based on triggers from a model that you know to be inferior to the market (as most are). That seems to be inviting adverse selection.



But let's look at a hypothetical that illustrates how it might be made to work. Suppose you built a fundamental model that gets into the ballpark for soccer match odds markets. Upon closer examination you spot that it performs well for teams in major leagues with large followings, especially for televised games. You could rationalise this as lots of fans backing their team and driving the price below what your model's idea of value and indeed "true value". In this scenario you could use your model to trigger trades with +EV.



However, if you started using it to trade outside that specific scenario, you're likely to come unstuck very quickly as the market snaps up each trade where it knows better than your model, and ignores those where your model has actually taken you in the right direction.

*Tags: Strategies*

---

**JC** - *19:11:24*

[@U01DVUAE2G1](@U01DVUAE2G1) thanks this is a good illustrative example. I see now that it may only be profitable in this scenario because you have an edge or are doing something different to most/the rest of the market. If you were only using a predictive parameter that is well-established and already baked into the market price, then there is no reason the market would be more likely to be wrong in the direction your model suggests. It may even be more likely that the market is wrong in the opposite direction, since your indicator may be so commonly used that it is overbet?

*Tags: Strategies*

---

**JC** - *19:20:44*

Also cheers [@U9JHLMZB4](@U9JHLMZB4),  I was thinking about markets that have very small spreads so that adverse selection can be minimised and you benefit from being a market taker, or just by getting to the front of the offer queue. In this scenario you can expect a return of zero/breakeven in the long run by betting randomly (ignoring commission). So logically, if you are betting in a direction nominated by a good (but not as good as the market) model, you are likely to do better, and possibly generate a profit?

*Tags: Strategies*

---

**birchy** - *23:42:38*

[@U0160E9HS2G](@U0160E9HS2G) The way I see it is that no model is 100% accurate, so we have to overcome this by using a margin of error big enough to give us confidence that the ends of our probability intervals are +EV. Then we pick the spots where the market overreacts and crosses our approximated +EV prices. The difficult part is optimising the model and narrowing the margin to achieve sufficient volumes of bets that are still +EV.

*Tags: Errors Debugging, Strategies*

---

## 2022-01-07

**D C** - *07:55:45*

Don't forget it is not just about model data but also speed of getting hold of that data. Consider a team sport using a model that has parameters that are team aggregated statistics versus one that uses the selected players individual statistics. Model A won't change if a player is injured/dropped from the team but model B will - and people with the first knowledge of lineups/injuries will update their model and have the advantage. So much depends on the type of sport and whether you use exchange or bookie. For example, the former big syndicate employee Misha used to inform us regularly that in horses, the big players don't get involved until 2 minutes before the race off - but in other sports betting with bookies, chances are you want to get in early before all the sharps snap up the value.

*Tags: Performance, Strategies*

---

**Peter** - *08:51:57*

[@U0160E9HS2G](@U0160E9HS2G) In the scenario you posited, you would still suffer from adverse selection. Your offer is taken or you're able to take that price, because on balance the market (which is slightly better informed than your model) recognises a -EV opportunity. Ignoring commission helps, but in markets with very small spreads it becomes disproportionately important, so ignoring it helps with your hypothesis, but will adversely affect your bank.



To recast slightly what I was saying earlier, I believe that there's a consensus here that the market is pretty darned good, but not perfect. So most of the time it's pretty much accurate, but sometimes it's a bit off. Classic financial theory holds that efficient markets incorporate all the publicly available information and reflect it in their prices. By contrast your model incorporates only the information that it can infer from the features that you have given it. So most of the time the market performs better than your model 'cos it knows more, but sometimes your model may perform better than the market 'cos what it does know is more important in specific circumstances.



So if you use your model all of the time, you'll sometimes win, but mostly lose. The trick is to identify those (I'll use [@UBS7QANF3](@UBS7QANF3)'s word here) scenarios where your model outperforms the market and then focus all your attention there.



I agree with [@UBS7QANF3](@UBS7QANF3) too that the times when your model deviates the most from the market are probably the times when your model is the least performant.

*Tags: Feature Engineering, Strategies*

---

**D C** - *09:02:13*

_"I agree with [@UBS7QANF3](@UBS7QANF3) too that the times when your model deviates the most from the market are probably the times when your model is the least performant."_



That is an interesting point [@U9JHLMZB4](@U9JHLMZB4) because it makes for an interesting expirement for people who want to dabble with any kind of kelly based stake calculation. If the hypothesis is correct and your model is most shit when it is furthest from the market then you will get rinsed a lot faster using kelly.

*Tags: Strategies*

---

**Peter** - *09:20:20*

A while back I took the results data for one of my most active strategies and plotted cumulative profit against the underlying model's calculated EV. At first it climbed, but surprisingly quickly it turned and headed down again. So now that strategy has an EV limit beyond which it doesn't trade (though I also paper trade it without the limit so that I can make future adjustments if anything changes or more data allows a more refined approach).

*Tags: Strategies*

---

**Ruben** - *14:16:23*

in flumine, is it possible to pass the parameter customerStrategyRef somewhere to tag orders from the same strategy? I haven't been able to find it looking at the method definitions

*Tags: Strategies*

---

**liam** - *14:19:35*

flumine uses it to separate running instances per market, it is then used to filter orders so it cannot be used by the user

*Tags: General Technical*

---

**Ruben** - *14:27:20*

understood, no problem, in practical terms this means that us users must keep track of which orders belong to each strategy, correct? no way to do that with flumine

*Tags: Strategies*

---

**JC** - *19:13:43*

Interesting stuff, thanks for your thoughts. Guess the trick definitely is in identifying inefficiencies rather than building a perfect model. I do wonder if there are many here who have built fundamental models that outperform the market on average though!

*Tags: Strategies*

---

## 2022-01-08

**birchy** - *00:08:22*

Oh...so is this wrong? `config.customer_strategy_ref = strategy.name`

*Tags: Strategies*

---

**birchy** - *00:16:00*

I recently added this to my strategies because I discovered that when using bflw to poll `listClearedOrders` in order to save the history beyond the 3 months the website offers, I was unable to identify which strategy had placed which bets because they all used the same host name. I use `loggingControl` but also wanted the 'official' numbers from betfair.

*Tags: Strategies*

---

**Oliver Varney** - *06:52:24*

What's wrong with the customer order ref for this (unless I'm misunderstanding). Can't remember how flumines property looks like now but I just override it with a custom value encoding strategy and trade information, everything I need to rebuild the trade basically

*Tags: Strategies*

---

**liam** - *08:20:30*

The order stream filters on it, fine if you only have one strategy 

*Tags: Strategies*

---

**birchy** - *08:42:51*

[@U4H19D1D2](@U4H19D1D2) I'm a bit confused.... The reason I changed the customer_strategy_ref was because I have one deployment server which runs 2 independent frameworks (one per betfair account), each of which has 1+ independent strategies. Obviously they're all on the same hostname, so all of the orders had the same reference which was a PITA. So what's best practice for this kind of setup?

*Tags: Getting Started, Deployment, Strategies*

---

**birchy** - *08:57:21*

Like this?

`strategy = A1(

    ...

)

config.customer_strategy_ref = strategy.name

framework.add_strategy(strategy)`



`strategy = A2(

    ...

)

config.customer_strategy_ref = strategy.name

framework.add_strategy(strategy)`

*Tags: Strategies*

---

**birchy** - *09:18:32*

I was just thinking that after I'd posted. :man-facepalming: So in a nutshell, the only way to separate orders is to run a separate AWS instance per strategy? OR...use one instance with logging control for each strategy (which I do anyway) and only use the polled listClearedOrders data for basic analysis at account level.

*Tags: Deployment, Strategies*

---

## 2022-01-15

**foxwood** - *12:33:56*

I'm developing some analysis based on the example datacollectinwom.py and am gathering lists of data per runner for graphing etc. I hadn't allowed for getting a None price from non-runners so it blew up (expected behaviour lol). However, that got me thinking about some key assumptions I had made and I wonder if they were safe, particularly with non-runners - maybe those with more knowledge could comment.

Assumptions:

1) the strategy callback argument "market_book" is the same object as could be accessed at "market.market_book"

2) the strategy callback "check_market_book" is made for every "mcm" streaming packet received and if that returns True then "process_market_book" is called - this means the strategy is offered every packet

3) for each callback all the data from the supplied market/market_book pointers has been updated to reflect the changes implied by the packet

4) the list of runners in the market_book and their index order is invariant ie runner Fred will be at the same index in "market_book" from first recording through to closed market



I think the first three are correct but not sure relying on the index position is safe - perhaps selection_id has to be used as a dict key for my saved lists which would slow things down quite a bit - important if it was a live betting strategy ?



I have noticed that BF change the "sort_priority" field for runners when there are withdrawals but not sure how/if that impacts flumine.



One area of difference I have found is with "market.market_book.market_definition.runners" which appears to be in BF sort order with N/R at the end whereas "market.market_catalogue.runners" and "market.market_book.runners" appear to remain in fixed positions regardless of withdrawal.

*Tags: Errors Debugging, Performance, Deployment, Strategies*

---

**MacrcoPolo** - *13:13:50*

(also, dictionary lookup is basically the fastest operation python can do. If you time it, you'll see it's typically on the order of nanos for simple keys and moderately sized dicts - cache miss depending)

*Tags: General Technical*

---

## 2022-01-18

**liam** - *08:52:51*

Yeah its a handy trigger to update anything you have cached, flumine uses it to trigger a new marketCatalogue request, dramatically cutting down requests

*Tags: General Technical*

---

**foxwood** - *19:54:32*

I have built a strategy using "datacollectwom.py" as a template. In that example it adds a new dictionary variable named "data" to market.context the first time any new market is passed to check_market_book. I've copied that approach for a strategy that watches market movements and prices and bets when appropriate. That's testing out reasonably so far with history files but I've hit some questions / doubts ...



1) is there a special reason for using market.context for the "data" variable or could I do something safely at a higher level without interfering with bflw/flumine eg market.mydatavar = {} ?

2) is it common / naughty in Python to add a variable dynamically like that ?

3) if the "enhanced" market object is deleted then any data in that variable would be lost. In bflw or flumine is there a case of a market object being deleted and recreated from scratch ie some sort of mcm case where the approach is to discard what was held and rebuild the market object from scratch ?

4) the main.py for the datacollectwom is processing history files - would the same structure /code for a strategy (less mock_patch) work with live streaming data ie would it just be simply a change of input source for the json ?



Sorry if some of these questions seem basic but trying to fill in some of my python and flumine gaps

*Tags: Getting Started, Deployment, Strategies*

---

**liam** - *19:59:21*

Context is your safe place, just the flumine way of storing your own data in flumine objects that won’t be cleared etc.



Not naughty but not very pythonic and your IDE might struggle with hints 



No, only on a restart of the python process would the data be cleared. There would be a case of this if betfair closed a market and then recreated after 1hr but not sure that has ever happened



Designed so that strategies can be used live, paper and simulated with no changes 

*Tags: Deployment*

---

**foxwood** - *20:51:42*

Great that's where I'll store my stuff then and sounds robust as well - saves having to have a parallel dict of markets. Yes, I'm using PyCharm and it takes a while to catch up with it's hints when its not sure what the variable is. I think your design is very effective and python does suit it even though the language frustrates me at times. Thanks :slightly_smiling_face:

*Tags: General Technical*

---

## 2022-01-19

**birchy** - *10:49:27*

Also take a look at the logging controls... [https://github.com/liampauling/flumine/tree/master/examples/controls|https://github.com/liampauling/flumine/tree/master/examples/controls](https://github.com/liampauling/flumine/tree/master/examples/controls|https://github.com/liampauling/flumine/tree/master/examples/controls)

*Tags: General Technical*

---

**Lee** - *17:31:14*

i think the market recorder takes the last cached catalogue at the time of market save/upload

*Tags: Data Quality*

---

## 2022-01-23

**foxwood** - *23:11:43*

I want to shift a backtest strategy to live - my first live flumine play. The strategy is based on the DataCollectWOM strategy example at [https://github.com/liampauling/flumine-strategy-development/blob/master/main.py](https://github.com/liampauling/flumine-strategy-development/blob/master/main.py)



Obvious things I need to do is change the wrapper to use live client login, remove mock_patch and provide a bflw streaming market filter.



There are 2 possible gotchas in my strategy where I'm not sure if things exist in live ie will it crash if these references are left in:

a) market_catalogue is used to get the runner name - is this just a backtest feature and should this middleware be removed if a catalogue is constructed during live streaming ?

b) in function process_closed_market the field "order.simulated.profit" is printed and logged - will that field exist or will this crash things if the reference is left in ?



Any advice or thoughts about other things I might not think of first time more than welcome

*Tags: Getting Started, Feature Engineering, Deployment, Strategies*

---

## 2022-01-24

**foxwood** - *21:22:02*

Was worth doing as [@U4H19D1D2](@U4H19D1D2) said lol. Many confusing issues. My connection to BF from server stopped late afternoon. At that same time I had logged in to test and live at least 7 times in 20 mins after fixing issues from a local PC and also started a BA instance. Think I blew the connection limits since no graceful termination - just killed flumine. Local ran for the rest of the day but server didn't get any live data it seems and stuck cycling non-stop with Latency high: 3317.0....."". Not sure if they simply killed that IP. See what tomorrow brings.

*Tags: Errors Debugging, Performance, Deployment*

---

## 2022-01-25

**liam** - *09:09:24*

[@U02RN7YDRQ9](@U02RN7YDRQ9) what are your strategies doing? If you are getting latency issues its likely you are maxing CPU or doing something slow in the strategy (network or db call etc)

*Tags: Performance, Strategies*

---

**foxwood** - *09:42:52*

[@U4H19D1D2](@U4H19D1D2) Was a dead connection I believe. I'd think I'd bust the connection limit - I had 5 open on a VPS (2 x mktrecord + 1 BA) and then my paper testing crashed or I thought of something to change so killed it to make a quick change. That meant 2 connections were abandoned each time and I read here somewhere that it takes about 15 mins for them to clear them. I know I killed it about 10 times in 30 mins lol. End result was BF pulled the plug on the earliest connections or maybe all connections for a short time. Killed all the server jobs including BA. The latency message was on server with a dead connection. Must find a way of doing a graceful terminate that closes connections - running under PyCharm currently. Thanks anyway - see what today brings lol.

*Tags: Performance, Deployment*

---

**liam** - *09:54:44*

flumine/bflw trys but tbh I think its betfair side that is the problem

*Tags: General Technical*

---

**foxwood** - *16:17:28*

VPS running 2 x mktrec + 1 x BA fine today - 5 connections. Tried strategy live locally today (simply removed paper_trade=True). Seemed to log in ok but after 20 mins log had about 18k lines all like this `{"asctime": "2022-01-25 10:21:47,343", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.8610439300537109"}` No other message about set up or starting of flumine which appears normal in mktrec. Message printed "starting strategy xxx"  when strat class started so it was starting off ok but that was all. Killed it after 20 mins and went for long walk lol. Any suggestions/clues - I can see the time it gives exceeds max latency of 0.5 but presume things should still function but quite slowly.

*Tags: Performance, Deployment, Strategies*

---

**liam** - *16:18:22*

is the latency increasing?

*Tags: Performance*

---

**Unknown** - *16:40:31*

Curiouser and curiouser. Restarted PyCharm and closed  1drive. Last 2 lines in attached file keep repeating. The "runners" problem line reported reads `for x in market.market_catalogue.runners::`

*Tags: General Technical*

---

**liam** - *16:43:24*

that error is because you need to wait for the catalogue to be populated by the worker



```if market.market_catalogue:

    crap```

*Tags: Errors Debugging*

---

**foxwood** - *17:31:29*

ok fixed that safely. strategy prints out immediate info from process_closed_market for markets still on BF but closed so getting there. Actually made some bets but I'd changed stake to £1 so not taken but no error message in log - maybe it doesn't - no probs. However, nothing in log except high latency messages for {MarketStream: 2201] with typical time of 0.84 - no flumine setup messages / bet errors / markets or anything like that - not sure what to expect lol. What section of processing / transmission is the latency referring to ?

*Tags: Getting Started, Errors Debugging, Performance, Strategies*

---

**birchy** - *17:48:36*

Yeah, check that clock is syncing to an NTP server. Should be automatic on Linux but can be flakey on Windows. Also in PyCharm, if you run in debug mode, there's extra overheads.

*Tags: Errors Debugging, Deployment*

---

**foxwood** - *18:36:15*

Latency is always around 0.8ish it seems. Clock updates regularly (win10) but tried forced update but got same result. VPS connections are to 84.20.208.166 NET-BETFAIR-001 and same for local PC. Local PC ping time to that IP is 17ms and VPS is 13/14/19ms so both near enough same so unlikely network latency. If I patch down MAX_LATENCY to 1.0 will the message just go away - seems to be overriding normal logging output and certainly generating miles of log :upside_down_face:

*Tags: Performance*

---

**birchy** - *19:06:26*

Are you getting the same latency on the VPS?

*Tags: Performance*

---

**foxwood** - *19:14:14*

Well, well, well. Changed the time server from [http://time.nist.gov|time.nist.gov](http://time.nist.gov|time.nist.gov) (win default) to [http://ntp1.npl.co.uk|ntp1.npl.co.uk](http://ntp1.npl.co.uk|ntp1.npl.co.uk) and the log is sitting there nicely empty (no flumine logging though ??). That is a stratum 2 ntp server ie more accurate than most. Maybe tomorrow I will get to run live properly :joy:

*Tags: Deployment*

---

## 2022-02-05

**rjj** - *10:41:24*

Hi everyone, is there a way of speeding up the backtesting? I'm running 10K (flumine scraped) markets across 12 CPUs but its still taking 24+ hours. I'm not sure what performance I should be expecting but wondered if maybe there was a way to serialised the market ticks as flumine objects or something, to save parsing the data files.

*Tags: Performance*

---

**liam** - *10:42:00*

[https://liampauling.github.io/flumine/performance/](https://liampauling.github.io/flumine/performance/)

*Tags: Performance*

---

**rjj** - *10:56:43*

Thanks, totally missed that page. Very helpful!

*Tags: General Technical*

---

**liam** - *11:08:59*

`listener_kwargs` will help there, and then I would look at profiling your code, something is very slow

*Tags: Performance*

---

## 2022-02-06

**rjj** - *11:59:53*

Turns out in my case it is the sheer volume of ticks, even with the base strategy (i.e. do nothing) it still takes O(hours) most of the time in _process_runner and creating MarketBook objects

*Tags: Strategies*

---

**rjj** - *18:58:14*

Yeah it's on an external drive but should be pretty quick. With logging off, I can get the base strategy to process a market in about 10-20 seconds depending on how active the market was. I'm guessing this is more in line with your expectations?

*Tags: Strategies*

---

## 2022-02-09

**Nacho Uve** - *13:01:07*

Hi [@U02KWHK2H89](@U02KWHK2H89),



Sorry, no ideas for your questions... I guess you've managed it with the monitoring other O/U goals markets.



But I'm going to struggle with graphs right now, and I like this one.



Are you plotting with matplotlib, right?

Could you share how do you config that nice x-ticks labels?





Thank you.

*Tags: General Technical*

---

## 2022-02-10

**Mo** - *07:09:30*

Relevant? [https://betfairlightweight.slack.com/archives/C4H05KKMY/p1643843408591949|https://betfairlightweight.slack.com/archives/C4H05KKMY/p1643843408591949](https://betfairlightweight.slack.com/archives/C4H05KKMY/p1643843408591949|https://betfairlightweight.slack.com/archives/C4H05KKMY/p1643843408591949)

*Tags: General Technical*

---

## 2022-02-12

**AndyL** - *08:20:41*

Ive being running an order intensive backtest overnight on a EC2 instance, and twice now the python process has been killed after a couple of hours, no exit msgs or backtrace, indicative of kill -9?

Has anyone seen this sort of issue?

Does EC2 have any rogue process detection maybe?

*Tags: Deployment*

---

**liam** - *08:22:43*

Are you batching up the markets as per the [https://liampauling.github.io/flumine/performance/#multiprocessing|performance](https://liampauling.github.io/flumine/performance/#multiprocessing|performance) docs?

*Tags: Performance*

---

**AndyL** - *10:13:34*

Ah yes, i think it's memory exhaustion. Just looked at performance doc, I'll do that, thanks [@U4H19D1D2](@U4H19D1D2) demsg confirms out of memory thanks [@U01D36QEBSB](@U01D36QEBSB)

*Tags: Performance*

---

**AndyL** - *18:17:28*

Multi process works great, all 4 cores zooming

I also cProfiled my strategy and improved its performance by another factor of x4     :rocket:

*Tags: Performance, Strategies*

---

## 2022-02-18

**foxwood** - *13:27:24*

If I want to ignore a market that can't be excluded by the stream filter (eg wrong race grade / distance / time etc) etc is there any way of telling bflw/flumine to ignore that market data as soon as it sees it (ie a stop_market call) or does check_market_book have to keep rejecting every message for that market (seems a lot of unnecessary calls if that is the case) ?

*Tags: General Technical*

---

**foxwood** - *21:49:47*

Yes, I was looking at running dogs and horses through a parameterized general purpose strategy to keep number of streams down (assume each strategy gets it's own pair of streams). Given the differences between the strategies I have it's going to be easier to just run two dedicated strategies for now with their own filtered streams.

*Tags: Strategies*

---

## 2022-02-22

**foxwood** - *21:22:25*

I've finally worked out a combination of market filters and strategy parameters and as I read it flumine provides one common order stream and a discreet market stream for each strategy ie if the framework is running 2 strategies it uses up 3 connections.



Question: I'm running some instances of the sample marketrecorder and each one has an open order stream. How can I close that order stream and release a connection - or must it stay open ? I've looked in the debugger, docs etc but can't see an obvious way to reach the stream object and call stop() - I thought I had it with client.order_stream but that was a bool flag ! (I know I should wrap the recorders in one instance but that's a job for another day)

*Tags: Errors Debugging, Strategies*

---

**foxwood** - *22:11:19*

There's a flumine.client.market_recording_mode bool. Nothing in the recorders sets it to True - if it was set it stops the workers poll_account_balance and poll_market_closure being created. I'll try setting it to True and see what happens :slightly_smiling_face: Thanks.

*Tags: General Technical*

---

## 2022-03-01

**J** - *10:50:01*

Silly question, is there a variable anywhere that shows the change in the LTP for a runner?

*Tags: General Technical*

---

**liam** - *10:58:53*

flumine?

*Tags: General Technical*

---

**J** - *10:59:54*

Yep, but on digging it seems it's in betfairlightweight (runnerbookcache)? Essentially I'm trying to make a strategy that triggers on a % change in price in in play races

*Tags: Strategies*

---

**J** - *11:07:59*

Oh, I worked it out, n00b question sorry

*Tags: General Technical*

---

**J** - *11:09:12*

Out of interest, is detecting price changes through process market book in a strategy the fastest way to detect a runner price change? Or should I be hooking directly into something underlying?

*Tags: Strategies*

---

**liam** - *11:38:13*

so you can do it in the strategy and store in `strategy.context` or `market.context` but you also have middleware that you can create/use so that it can be shared with all strategies (this is how the simulated logic works)

*Tags: Strategies*

---

## 2022-03-02

**J** - *05:58:21*

Whipped up a bot/strategy today in flumine but it's not placing bets. The market.place_order(order) call goes through without error (and on to the next line of code fine so there's no silent exception in the main code) but no order is placed. Is there a place that order errors are logged?

*Tags: Errors Debugging, Strategies*

---

**J** - *06:09:44*

Exact code is below. Is this the correct way to place a back order? The only examples I can find are lays





                                        print('Placing bet')

                                        market.context['betstatus'] = 'y'

                                        betstatus = 'y'

                                        trade = Trade(

                                            market_id=market_book.market_id,

                                            selection_id=r.selection_id,

                                            handicap=1.0,

                                            strategy=self,

                                        )

                                        order = trade.create_order(side="BACK", order_type=LimitOrder(price=1.01, size=5.00))

                                        market.place_order(order)

                                        print('Bet placed.')

*Tags: Strategies*

---

**Jonjonjon** - *07:46:13*

I had to set my logging level to warning before it told me what a recent problem was. When spamming many orders, I had rounding errors. So it thought a potential exposure of 15 was greater than my limit of 15. I'll probably get around it by adding a penny to my exposure limit

*Tags: Errors Debugging*

---

**AndyL** - *11:39:37*

Aus greyhound market start times are sometimes wrong, ie.flumine market.seconds_to_start is out by 5 or so minutes. Both the 09:08 and 10:08 races showed as starting 5+ mins later.. my bot tried to hedge several mins after race had finished!!

*Tags: General Technical*

---

## 2022-03-04

**liam** - *10:47:03*

the flumine catalogue worker uses the marketBook updates to trigger a new request, this is dependant on how you have flumine setup but if you stop getting streaming updates like you have then you wouldn't get catalogue updates

*Tags: Getting Started*

---

**AndyL** - *10:48:24*

Stop getting streaming updates? What does that mean? Im not aware of stopping..

*Tags: General Technical*

---

**AndyL** - *11:00:22*

Im using default flumine client worker poll settings i believe

*Tags: General Technical*

---

**AndyL** - *11:07:07*

Thanks. in going to restart one of my live strategies with info logging to check the flumine startup polls

*Tags: Deployment*

---

**liam** - *12:50:56*

[@UUUAJQZ5J](@UUUAJQZ5J) I assume an update like this would result in the `marketVersion` being bumped? (thanks for the help on this btw)

*Tags: General Technical*

---

## 2022-03-25

**liam** - *08:27:46*

ROI is much higher for me, strategy/risk tolerance dependant on how you play the game. My results tend to be majority break even (slightly +ve) with some nice wins on top, although recently I have found it's very easy to get the opposite when pushing things too hard... :joy:

*Tags: Strategies*

---

**PeterLe** - *10:37:28*

Yea I did see something on here saying that yesterday and update from Neil

I have the feed direct from the betfair API (and use particular stake sizes) with my own program and I also have the TPD feed through betangel. It was only the bets submitted by betangel/TPD on that race.

I can then tell by the stake size where the bet originated from.

It will be interesting to see how I fair when you boys are in the market too :grinning:

*Tags: General Technical*

---

**liam** - *14:18:27*

do the battering's/big wins coincide with errors/discrepancies in the accuracy or just bad luck? Back in 2018 I found it was common to have every single market as +++ve, in hindsight I don't think I took advantage, much harder now

*Tags: Errors Debugging*

---

**D C** - *14:24:30*

I suppose accuracy depends on how good my strategy is. Hard to verify actual GPS accuracy without watching the race back. The more I speak with other GPS players, the more I feel I am doing something slightly different to the crowd. I tend to get my worst results on races with small fields and very short odds on favourites that don't rise above BFSP during the race.

*Tags: Strategies*

---

## 2022-03-29

**liam** - *10:44:02*

Making is more realistic in matching (can prevent double counting of liquidity) but discounts impact, taking theoretically is impacted less by impact but doesn't handle double counting of liquidity.



However if you have reached this point you should be doing the analysis live, simulating is more for confirming a model/backtest and as a final integration test although I am very guilty of using it too much as it works so well..

*Tags: Deployment, Strategies*

---

**Mo** - *11:25:47*

Personally I would not bother backtesting a making strategy. It's impossible to know exactly how the market's behaviour would change with the presence of your orders. At least if you are testing taking then you know for sure that a price was available to take at a time. Provided you don't get yourself in a position where you try to take the same bet twice in a backtest

*Tags: Strategies*

---

**Ruben** - *11:35:03*

was asking because I chose to go live with my first strategy, which happens to be a making strategy, and I had not backtested (mainly because at the time I was not recording anything) and I see that it has quite a large variance and it will take a while to tell if the strategy makes money or not

*Tags: Deployment, Strategies*

---

## 2022-04-08

**J** - *13:39:20*

Can flumine strategies go across markets? As in, can I directly run calculations comparing win and place data on a runner in the same place?

*Tags: General Technical*

---

**J** - *13:40:04*

Is there a good way to go about it? I thought the strategy operates only within the context of the market so I must be missing something

*Tags: Strategies*

---

**liam** - *13:42:45*

yeah its really handy, assumes you have subscribed to that marketType as well for that strategy

*Tags: Strategies*

---

**J** - *14:52:18*

My dumb question for the day

*Tags: General Technical*

---

## 2022-04-09

**J** - *15:16:09*

Next dumb question: is there some kind of throttle on the rate at which orders can be placed? I seem to have random dropping of orders (no doubt through my own doing somehow)

*Tags: General Technical*

---

**Aaron Smith** - *15:44:22*

`runner_context.placed_elapsed_seconds`  lets you know how many seconds passed since you last placed an order on the runner, so you can use this to adjust the rate at which orders can be placed. RunnerContext also shows orders placed on the runner, so you can use that in your decision of if you want to place more orders aswell:

[https://github.com/betcode-org/flumine/blob/0f12a0f9d200b4a4c36c8bd26c9f4fbe82f8cce1/flumine/strategy/runnercontext.py](https://github.com/betcode-org/flumine/blob/0f12a0f9d200b4a4c36c8bd26c9f4fbe82f8cce1/flumine/strategy/runnercontext.py)

*Tags: Strategies*

---

## 2022-04-18

**Ruben** - *13:00:13*

any necessary adaptations to port a strategy that has only been running in preplay to inplay? from what I gather reading examples and documentation, the instructions to place/cancel orders should be the same, and there will be an additional inplay delay for placing/cancellations, but nothing more than that

*Tags: Strategies*

---

**liam** - *19:35:21*

I think flumine actually orders the files on the file name (again need to double check) but I always order on top 

*Tags: General Technical*

---

## 2022-04-19

**birchy** - *07:31:13*

Thanks [@U4H19D1D2](@U4H19D1D2). Let's say I wanted to use the WIN markets to calculate prices for the PLACE markets, how would we set that up? I'm assuming it would require 2 separate strategies and some form of IPC? Or can it be done within one strategy? Also, where would we find the "Betfair created event file"?

*Tags: Strategies*

---

**liam** - *08:11:41*

Betfair event file is from the historical data, it is basically all markets for an event in a single file. You can replicate this by appending all the markets you want to process into a single file (order doesn't matter) flumine will then process chronologically with markets available as per the code above assuming `event_processing` is True



[https://betcode-org.github.io/flumine/quickstart/#event-processing](https://betcode-org.github.io/flumine/quickstart/#event-processing)

*Tags: Data Quality*

---

**liam** - *08:40:13*

Its been there for a while, needs its own section really, here is the [https://github.com/betcode-org/flumine/blob/049caf051593d301e0ae6eaba64aed5e9e489f26/flumine/simulation/simulation.py#L50|code](https://github.com/betcode-org/flumine/blob/049caf051593d301e0ae6eaba64aed5e9e489f26/flumine/simulation/simulation.py#L50|code)

*Tags: General Technical*

---

**thambie1** - *15:05:47*

[@UGV299K6H](@UGV299K6H) Not for all of us. I tend to consider betting on each runner in sequence, rather than all at once. Kelly criterion does the job

*Tags: Strategies*

---

**Douglas Hickling** - *15:22:02*

Thanks, I have been doing that with a form of kelly, was just wondering if there was a better way

*Tags: Strategies*

---

## 2022-04-21

**Jonjonjon** - *12:10:41*

Why not just try it with a small strategy?

*Tags: Strategies*

---

**PeterLe** - *12:20:30*

Hi Liam, yes I do have an element of AB testing as I have multiple versions of the same strategy and always comparing/contrasting anyway. The advantage of doing it as mentioned is that as you know, the stakes will increase/compound rapidly whenever you apply to a successful strategy. If the account stated with a small balance say £200, htat would be my maximum loss and i would be less worried about things when it stated throwing in large stakes. Stakes that I would not normally be comfortable with.  Now execution..that's a whole new ball game :grinning:

*Tags: Strategies*

---

**PeterLe** - *12:21:22*

It would be a know good strategy JonJon, but starting from a low balance.

*Tags: Strategies*

---

## 2022-04-26

**PirellOne** - *11:12:15*

hi All. I am Marco, from Italy, so no possibility to register on the [http://Betfair.com|Betfair.com](http://Betfair.com|Betfair.com) Api or even open account. So my question is: there's nobody from UK could help me just to get notifications (telegram, mail or whatelse..) when a massive bet is occurring only Soccer, only last 30 minutes before match starting PREGAME,  only MATCH ODDS market.. very easy task for someone having Api and know how to use Zapier or similar...i guess.. if you are interested just write..is something that would not break any betting law or any BETFAIR law.. example of this could be: 

GET notified when 10000£ bet  is made on HOME_WIN



..i am new to Slack. If i wrote in the wrong place just let me know..

*Tags: Getting Started, Strategies*

---

## 2022-05-04

**liam** - *10:51:09*

up to the user really, if you have it in process_market_book there is a chance you might miss things if the market doesn't update often unless you use the `streaming_timeout`

*Tags: General Technical*

---

**Oliver Varney** - *10:57:52*

so if streaming_timeout is set to None it will block untill something arrives ? therefore snap will never be called if im understanding correctly? and order snap is defined with SNAP_DELTA as 5 seconds

*Tags: General Technical*

---

**Oliver Varney** - *10:59:05*

what kind of streaming_timeout are people typically using then, inline with matching cycles or something?

*Tags: General Technical*

---

**liam** - *10:59:23*

snap_delta is a `or` if you look at the code it makes sense but it basically uses `streaming_timeout` on the queue and then snap_delta is used to send an update regardless

*Tags: General Technical*

---

## 2022-05-07

**foxwood** - *18:22:23*

I have a derived base strategy which does all sorts of parameter driven grunt work and provides helpers for various tasks. Deriving from this makes it easy to create and modify strategies quickly but each strategy ends up with it's own stream as part of the flumine BaseStrategy  construction.



When different strategies have identical market filter requirements is there any way to share a single stream to save on sockets and duplicate processing ? I could use lots of if/elif code but that quickly becomes a maintenance / debug 'mare.



One solution I'm considering is introducing another layer that feeds an array of child strategies in some way - just seems a bit messy compared to having a shared common feeder stream. Any ideas / Python tricks / alternative design patterns welcome :)

*Tags: Errors Debugging, Strategies*

---

**liam** - *18:24:26*

Flumine does this by default, why do you think it doesn't? 

*Tags: General Technical*

---

**foxwood** - *19:14:45*

Might be a doh moment then - I've not tried just assuming !

eg I derive mystrat from BaseStrategy and add it to the framework - since (from logs) that takes 2 sockets I presumed that's the way things worked - I've never added more than 1 strategy to framework ie I'm running one strategy per top module - maybe that's the bit I should study ?!



So are you saying the framework will recognize identical streaming_market_filters across all strategies added to it ?

```framework.add_strategy(

    mystrat(

        name=mystrat.__class__.__name__,

        market_filter = betfairlightweight.filters.streaming_market_filter(

            event_type_ids=["4339"],

            country_codes=["GB"],

            market_types=["WIN"],

            ),

        market_data_filter = streaming_market_data_filter( fields=[

            "EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP",

            "EX_MARKET_DEF", "SP_TRADED","SP_PROJECTED",]

            ),

        context = { "stratparms": mystratParms, "client": client, },

    )

)```



*Tags: Strategies*

---

## 2022-06-01

**Rudeger Jamison** - *03:12:27*

Can I confirm that from reading the code the setting `max_selection_exposure`  that is handled in the `StrategyExposure` control works that for the LAY side it takes the `to_lose` amount and for the back side it takes the `stake` ?



sample:



```LAY $5 @ 5.0

BACK $10 @ 3.0



selection_exposure = $30```

If I wanted to adjust this so that my back exposure is not the stake and instead the `to_win` amount `( (odds - 1 ) * stake )`  is there any easy way?



I notice you have some of these fields in the blotter already ?

*Tags: Strategies*

---

**liam** - *06:20:08*

Just patch or create your own StrategyExposure control 

*Tags: Strategies*

---

## 2022-06-08

**nthypes** - *20:48:04*

Question: is expected to Booking/Overround of the VWAP prices be exactly 100%?

*Tags: General Technical*

---

## 2022-06-15

**nthypes** - *00:28:49*

Folks, whats the best way to test a model profitability? There is any guidance about this subject?

*Tags: Strategies*

---

**Peter** - *09:40:35*

It depends somewhat on the type of model, but in general I’d argue that models don’t have profitability. Expected profitability is calculated for strategies, which wrap predictive models with staking plans and triggers.



So one approach to assessing profitability is to take a large number of events whose outcomes weren’t used to build the model and use it to make predictions (usually in the form of probabilities) for those outcomes. Then use the chosen triggers to evaluate which selections should be traded and the side for that trade, and use the staking plan to determine its size. The available odds and actual outcomes can then be used to calculate whether each trade would have been a profit or a loss. Finally profitability can be calculated by summing the profits and losses and dividing by the total exposure (i.e. the stakes for back bets and the liability for lay bets).



I also like to order trades by datetime and plot the cumulative profit as that gives a clue as to the frequency and size of drawdowns.



In practice though the whole process is much more iterative than this and a lot of work (for me at least) goes into determining the triggers and evaluating different staking plans.



Moreover, if you’re offering, rather than taking prices, you can’t assume that you’re going to be matched for all trades. Running the strategy through the Flumine backtesting process (assuming you have the data for those events) can give an indication as to which are likely to be matched, which can have a huge impact on the profitability of a strategy.



There’s also a step in my process, way before I even start to consider profitability, where I evaluate the model’s “effectiveness”. One way of doing this is to calculate the Brier Score for its predictions and compare it to the available odds. Rarely will it be better (if it is it’s time to break out the champagne) but you may find types of bet in the overall population where the model outperforms the odds, and that’s where you’ll find value.

*Tags: Strategies*

---

**nthypes** - *13:06:24*

`favoriteProbability`: my model probability prediction for the horse with the higher predicted probability of win



`favoriteBsp`: the bsp of the horse above

*Tags: Strategies*

---

**Unknown** - *13:18:39*

*Strategy P/L Critic*



The following graph is a simulated strategy (implemented with 1/4 Kelly) bankroll cumulative profit/losses for ~1300 events. Any insights about the profit curve? The most intriguing for me are these downtrends.



Initial Bankroll: *$100*



Also, one part that intrigues me is that until the event no *400* it's look a solid growth pace, but after event *600* the growth explodes.

*Tags: Strategies*

---

**D C** - *13:27:48*

I'd personally use flat staking to try and get a feel for how the strategy performs. Don't try stake manipulations to try and force something. I know Kelly is "optimal" but try and find out the quality of your probabilities first. Just an opinion - I'm pretty low on the food chain.

*Tags: Strategies*

---

**Unknown** - *13:32:18*

This is the version with *$1* fix stake.

*Tags: Errors Debugging*

---

**Mo** - *13:37:45*

What odds are you putting into your Kelly calculation? You don't know what BSP is going to be in advance...

*Tags: Strategies*

---

**Mo** - *13:43:23*

I don't think it will be too much of a problem because of your $1 graph but it is an important consideration if you want a realistic backtest

*Tags: General Technical*

---

**Unknown** - *13:53:38*

I made another selection rule, to restrict the model selections using VWAP.



With $1 fixed stake:

*Tags: Errors Debugging, Strategies*

---

**nthypes** - *13:55:31*

About the volume, maybe I can put a trigger to only simulate Kelly stakes of maximum $10? Maybe this can solve "partially" the volume problem with a realistic assumption?

*Tags: Strategies*

---

**Unknown** - *13:59:57*

With 1/4 Kelly, only on stakes &lt;= $10

*Tags: Strategies*

---

**Michael** - *14:10:31*

Ok, so that's a small percentage of all the HR markets in that time, is that because only some small set of markets qualify (only 6f flat races or whatever) or does your strategy bet on only a small percentage of 'qualifying' markets?

*Tags: Strategies*

---

**nthypes** - *14:12:18*

Second one: strategy bet on only a small percentage of 'qualifying' markets



First I qualify the selection and run 1/4 kelly, after that I qualify a second time to consider only selections that produced &lt;= $10 kelly stake.

*Tags: Strategies*

---

**Michael** - *14:32:38*

Regarding how your plot looks, realistically just the look of a backtest plot doesn't tell you anything in isolation. Say for example you had massively overfitted your model, you could have a brilliant looking plot but it would be wrong to put any faith in it. If you want to have confidence in your strategy performing I think the best way is to develop it using a process that you can have confidence in. Only you can say whether you've done that.

*Tags: Strategies*

---

**Unknown** - *14:58:30*

after all adjustments, with $1 fixed stake.

*Tags: Errors Debugging*

---

**D C** - *15:18:53*

[@U9JHLMZB4](@U9JHLMZB4) If it is scaled correctly yes. For example, I can see the wisdom of weighting stake according to ask price but not using Kelly. If I have a model that the 1000-1 available is huge value when in reality it is not value then I could end up placing much larger chunks. I would want to make sure my probabilities were pretty accurate before I went anywhere near Kelly. It's only optimal if your probabilities are up to the task.

*Tags: Strategies*

---

**birchy** - *19:41:09*

[@U01A64T6DJQ](@U01A64T6DJQ) as suggested above, try it live with minimum flat stakes ($1 or whatever). More importantly, make sure you have setup bet logging as that will give you "real world" data that you can analyse later.

[https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: Getting Started, Deployment*

---

## 2022-06-18

**Ruben** - *08:39:13*

What are your thoughts on evaluating a strategy that places preplay bets by taking the set of bets placed during a period of time and randomly sampling bets placed and simulating the event outcome by the market inferred probabilities from the BSP, right before the off. As a way to create multiple realizations of the bets observed, as opposed to just keeping the single realization that we observed

*Tags: Strategies*

---

**Mo** - *08:42:43*

Otherwise I would say this is standard practice for evaluating a strategy that trades pre-event in sports where there is either a literal BSP or the market can be assumed to be efficient at the off

*Tags: Strategies*

---

## 2022-06-20

**Jorge** - *09:48:12*

Adding to Ruben's question. If I calculate the expected value from BSP, does it make sense to estimate the variance that my strategy has using: Var[X] = E[X^2] - E[X]^2 ?

*Tags: Strategies*

---

**Jorge** - *09:50:35*

I basically look at expected value and std when evaluating preplay strategies. And also when checking if a strategy that I have put live is performing as it "should"

*Tags: Deployment, Strategies*

---

**LF** - *10:17:32*

By the way, most libraries(pandas/numpy) already give you back sample variance by default

*Tags: Feature Engineering*

---

**LF** - *13:31:30*

I think there was just some misunderstanding, so when you say you want to use E[X^2] - E[X]^2 the assumption is that you are solving this analytically and not numerically, but from my understanding, you are already computing the mean numerically using the sample mean 1/n * sum(X_i), so for the variance I would go again with the sample variance, if you are a python user you can do it using pandas/numpy

*Tags: Feature Engineering*

---

**LF** - *13:43:26*

Hope this helps [@U0155J92A7Q](@U0155J92A7Q)

*Tags: General Technical*

---

## 2022-06-22

**nthypes** - *12:39:20*

Let's suppose that you found a model that his accuracy is equal to the BSP. What are your thoughts on the best strategy to exploit this model? Looks like that a simple BACK if odds are above the predicted value has a poor performance/high variance.

*Tags: Performance, Strategies*

---

**nthypes** - *12:45:22*

Actually, let's say is not equal, but quasi-equal.



Given that BSP has a Brier Score of ~0.08, and the model has a Brier Score of 0.09.

*Tags: Strategies*

---

**mandelbot** - *12:48:27*

Is your model price based or fundamentals based?

*Tags: Strategies*

---

**mandelbot** - *12:50:08*

Perhaps filter/combine with some sort of pricing model.

*Tags: Strategies*

---

**ShaunW** - *12:51:13*

I'll bow out [@U01A64T6DJQ](@U01A64T6DJQ) I've never used Brier so no idea how to use it, so 0.08 to 0.09 looks like a 12.5% difference to the uninitiated.

*Tags: General Technical*

---

**Michael** - *15:10:44*

if your model doesn't return more accurate prices than the market at the moment of analysis then you can't do anything with it. Just having a brier score of X isn't of any consequence.



That said just saying "if odds &gt; model odds" is a bit simple, you could start by looking for the greatest differences and work from that point.



There's no magic anywhere though, you've got to price better than the market.

*Tags: Strategies*

---

**Unknown** - *22:48:16*

`winProb` is the predicted probability of win.

`total` is the total number of horses that the model identified within this range of probability of win

`winners` is the numbers of horses from the total that finished in 1st place

`ratio` is winners/total



It's ok to quantify the model quality/accuracy based on winProb vs ratio?

*Tags: Strategies*

---

**Mo** - *22:53:52*

No this just tells you the model is well _calibrated_, nothing about the accuracy

*Tags: Strategies*

---

**nthypes** - *22:54:51*

Maybe is a dumb question, but what a _calibrated_ model means [@UBS7QANF3](@UBS7QANF3)?

*Tags: Strategies*

---

**nthypes** - *23:09:57*

Found this: [https://neptune.ai/blog/brier-score-and-model-calibration|https://neptune.ai/blog/brier-score-and-model-calibration](https://neptune.ai/blog/brier-score-and-model-calibration|https://neptune.ai/blog/brier-score-and-model-calibration)

*Tags: Strategies*

---

## 2022-06-23

**James T** - *10:18:59*

You're still better off benchmarking against market prices like BSP since, like Michael said in the other thread, if you can't beat market prices then it's pointless anyway. How can you best test if your model is better than market prices? Well just bet against them. Market price &gt; model then back, market price &lt; model then lay. You said when you did this that your returns were either zero or very volatile. That just means your edge doesn't exist or isn't big enough against the market. 



My alternative recommendation is betting earlier in the day to match less efficient prices, but there's limited liquidity. 

*Tags: Strategies*

---

## 2022-06-28

**John Foley** - *17:22:25*

Enjoying reading some of the discussion above in relation to backtesting. Very helpful to anyone like myself who has made the humbling decision to try modelling from fundamentals and betting pre-live close to the off :slightly_smiling_face:

Say you have a model and you’re pretty sure your probabilities are better than BSP (humour me). Next sensible step would be to bet against BSP historically and hope for a profit. What strategy/judgement do people use to be sure that the profit is due to edge and not noise? Is there a quantitative way to combine the sample size, the prices you’re taking, your staking plan and coming up with a “degree of certainty” that your profit is definitely due to an edge? In my head what I’m trying to get to is almost a bayesian result where you say “im x% certain that I have an edge here”

*Tags: Deployment, Strategies*

---

**Mr West** - *18:02:07*

The only way to know if a strategy works is to run it in the live market. There's only so much you can do with back testing. Use small stake and be prepared to lose some money.

*Tags: Deployment, Strategies*

---

**AndyL** - *23:17:13*

Ive heard that said before that assume your model will not be as good as BSP, ie straight betting against it will be -ve.

But a model is a model,ie you are hoping it is close to the truth... so that helps you to then combine it with factor X, to get an edge. Because you know betting on both +X and -X is -ve, but maybe just +X has an edge.

*Tags: Strategies*

---

## 2022-06-29

**D C** - *17:09:19*

Sorry if this is a bit of a thick question, but what is meant by betting "against" BSP? You don't know what BSP is going to be before the off so you just use it as a benchmark to establish if you are getting CLV ?? I've never placed a BSP bet in my life and I am not even sure how they calculate it so can someone explain what betting "against" it means?

*Tags: Strategies*

---

**ShaunW** - *17:15:25*

[@UUE6E1LA1](@UUE6E1LA1) BSP is going to be the last traded mid or thereabouts. If its x away from your model price get on it. If the BSP est comes back towards your price then cancel. Something like that.

*Tags: Strategies*

---

## 2022-06-30

**nthypes** - *01:45:53*

The best way to model odds is individual modeling, which means find a win probability for every and each horse in a race, or, find the favorite, predict the odds for the favorite and all other horses odds will derivate from this odd?

*Tags: Strategies*

---

**nthypes** - *11:56:35*

The problem with the first approach it is that the probability sum is always more than 1, which seems inaccurate to me.

*Tags: General Technical*

---

**D C** - *12:12:23*

That sounds more like the way you are doing the modelling than the approach itself.

*Tags: Strategies*

---

**D C** - *12:16:55*

What technique are you using to model?

*Tags: Strategies*

---

**D C** - *12:28:47*

What modelling technique are you using to estimate the probabilities that you say are summing to more than 1

*Tags: Strategies*

---

**nthypes** - *12:32:00*

No. Based on a model who gives each horse a rating.

*Tags: Strategies*

---

**Tom Cannon** - *21:10:59*

Hey all, I've been working on and off with price only indicators for awhile. For short term price changes, my predictions are not too bad, but i'm finding it hard to get matched at predictions to make any decent money (even in back-testing).



So I'm looking to change strategy to a value based approach. I'm wondering if any of you have a suggestion of where to gather runner (or team) data, and a ball-park estimation of how much the data costs to have enough for predicting successful value? I'm currently doing a Machine Learning PhD and hoping to see how well I can apply it, but this does mean I'm on somewhat of a budget!



Thank you!

*Tags: Strategies*

---

**Tom Cannon** - *21:28:30*

My predictions of price movement is only accurate for a short period of time. I've found I can rarely get matched on the correct side before the price moves.



I've been working with UK horse-racing and filtering by a over a certain matched to ensure fluidity.



I'd take any to be honest, it's all just data to me - as long as it's descriptive enough to predict value. I took a look at Total performance data but couldn't find any prices or many reviews if this service was worth it.

*Tags: Performance*

---

**liam** - *21:35:03*

Any strategy that makes money needs to get value so you are heading in the right direction. Your question is very open but many (including myself) are profitable working purely off market data and many others are profitable off free form data, you don't have to spend money to find an edge 

*Tags: Strategies*

---

**Tom Cannon** - *21:48:01*

Interesting, thanks for the reply. I've been mainly focused on taking small profits from selection greening back/lay over short periods of time. What do you count as value? Is just value on BSP?



So my objective would be to model the current state of the market and predict an accurate BSP, and then I can make value decisions based on this prediction, instead of short term price movements?

*Tags: Strategies*

---

## 2022-07-01

**Unknown** - *16:00:04*

Suppose the following dataframe, filtered by horses who model predicted odds was between `8 &lt;&gt; 8.02`. `winOdd` is the model projected odds for the horse.



From the universe of *1127* horses, *128* horses finished in 1st (~10.5%).



In the second example filtered by horses who model predicted odds was between `4 &lt;&gt; 4.02`



From the universe of *889* horses, *213* horses finished in 1st (~24%).



It's a valid approach to confirm that the model has an edge?

*Tags: Feature Engineering, Strategies*

---

**Alessio** - *16:06:26*

On one hand, you may have a calibration issue imho: your model says it should have ~12.5% of winning probability and you get 22% ?  It seems to scale correctly from this example (double the odds, half the probability), but the real question is not whether the model has an edge on its own, but whether it has an edge w.r.t. what other models do. When do odds of your model and odds of betfair disagree, what happens, for example?

*Tags: Strategies*

---

**nthypes** - *16:13:00*

&gt; When do odds of your model and odds of betfair disagree, what happens, for example

Hm, I didn't understand that part.

*Tags: Strategies*

---

**Alessio** - *16:19:32*

If your model predicts odd X you expect the horse to win 1/X. In your first case, the 95% CI is 9.6% to 13.3%, which includes the 10.5% you have. Great. This means your model is not that bad at computing odds. This is the calibration and prediction aspect.

*Tags: Strategies*

---

**Alessio** - *16:21:50*

Now the question is: edge against which baseline? Because an edge means you are better at predicting something, but you need a baseline to compute against. You need to identify horses in which the wisdom of the crowd is odd X but you spit out odd Y, and you are right, to say you have an edge

*Tags: General Technical*

---

**nthypes** - *16:33:17*

When my `winOdd` predicted by the model is greather than pre-play vwap, the average BSP is 14 and median of 7.6.



My model predicted odd is 4 or 4.02

*Tags: Strategies*

---

**James T** - *20:05:37*

I think you're overthinking it. Just backtest it over 10,000 runners for example - if modelWinOdd &gt; bsp then back at bsp price, if modelWinOdd &lt; bsp then lay at bsp price. Make sure you're calculating your P&amp;L correctly!



1. Use simple staking proportional to price.

2. Try adding a threshold where you only back / lay if model disagrees with market &gt; certain impl %

3. Stake relative to your estimated edge, so proportional to price but also abs(modelImpl - marketImpl)



You need to do this for each individual runner - don't bucket runners into a "price range". Treat each individual runner as its own bet. Only then will you properly be benchmarking against bsp.



For your model to stand a chance as a working strategy then the above should really work.

*Tags: Strategies*

---

**James T** - *20:09:11*

When is your model price calculated? Is it calculable at any time, or only at the off, or does it use BSP as an input as well?



If it's calculable at any time then you could also try backtesting betting earlier in the day. Prices tend to less efficient then, but there is less volume available. But to do that you'll need to use tick data.

*Tags: Strategies*

---

**nthypes** - *20:10:01*

it uses historical data

*Tags: Data Quality*

---

**James T** - *20:12:11*

Personally, to keep it simple I'll just use bsp for backtesting sometimes. You can achieve something close to bsp if you think about execution strategy.



vwap calculated a few seconds before the off is going to be very close to bsp. But that is equally impossible to know since you don't know when it's going to go off!



vwap across the whole history of pre-race is not going to be close to bsp.

*Tags: Strategies*

---

**James T** - *20:14:40*

I would try backtesting earlier in the day, say 1 hour before race start, or at around 9am, for example. With those bets that get placed (assume same strategy of model vs market price) do they beat bsp? i.e. can you close at bsp for a better price consistently?

*Tags: Strategies*

---

## 2022-07-02

**birchy** - *13:18:25*

[@U01A64T6DJQ](@U01A64T6DJQ) 

As [@USYQKE5HN](@USYQKE5HN) has said, keep it simple. Getting matched at value prices is a bigger issue than generating probabilities. First thing I would do is run a simple simulation where you assume that you had 100% of your bets matched at your generated prices Vs actual win/lose outcomes. That will tell you how your prices compare to real world results and should generate a profit/loss close to zero if your model is well calibrated.



Assuming the above generates expected results, THEN you can run backtests in Flumine:

`if betfair_available_price &gt; my_price:

    side = 'BACK'

if betfair_available_price &lt; my_price:

    side = 'LAY'`

The Flumine simulation engine will give you a good approximation of which bets will get matched, which ones won't and a simulated "real world" profit/loss which will hopefully be &gt; 0. You should also calculate the "what if I bet at BSP instead" profit/loss, where you replace the simulated matched price with BSP, keeping the matched amount the same. If the BSP performance is better, it's back to the drawing board.



FYI: The above assumes straight betting and no greening/hedging nonsense.

*Tags: Performance, Strategies*

---

**nthypes** - *13:56:16*

&gt; First thing I would do is run a simple simulation where you assume that you had 100% of your bets matched at your generated prices Vs actual win/lose outcomes. That will tell you how your prices compare to real world results and should generate a profit/loss close to zero if your model is well calibrated.

If shows a profit in this scenario, the model is not calibrated / bad?

*Tags: Strategies*

---

**Dave** - *21:17:29*

To football traders: if you don't mind disclosing, what is the ratio of your volume last weekend to the biggest-weekend-volume you have experienced so far this year? My volume is at this years' low currently but it kinda feels too low in a way, hence the question (i.e. expected to be low given the number of games, but feels too low still)

*Tags: General Technical*

---

**thambie1** - *22:06:22*

My models have changed way too much for that to be a fair comparison. That said, I'm not getting the impression that my volumes are unreasonably low

*Tags: Strategies*

---

## 2022-07-07

**nthypes** - *00:09:19*

*question:* runners prices are perfect correlated?

*Tags: General Technical*

---

## 2022-07-15

**Tom Cannon** - *16:54:31*

Hey all,



Following from my previous post (5 posts above).  Thank you all for the pointers I'm making far better progress!



I have changed strategy and am now successful matching better than BSP (On FlumineSimulation). In fact ... I match about 100 trades correctly and 20 incorrectly per race. Which is great!



But I have vast differences in expected profit from calculation and expected profit from Flumine. Calculations make me rich :slightly_smiling_face: and simulation makes me poor :grimacing:

I'm simulating on a months worth of Horses Win data - and so expect that this is statistically significant enough to tend them to the same expectation. (If BSP is a good indicator of probability?)



For each back trade matched I calculate expected return as:



E_back = Pr(Win) * return - Pr(Lose) * stake

E_lay = Pr(Lose) * stake - Pr(Win) * return



Where:  Pr(Win) = 1/bsp,       Pr(Lose) = 1 - (1/bsp),       return = (price-1)*(stake)



Profit is calculated directly from Flumines market.blotter order.simulated.profit (which I'm assuming works from race results)



Can anyone see any issues / had any similar realisations with using BSP as an estimate of Pr(win), or have any suggestions of where I might have made an error?



Ofcourse ... I could just engineer a green at BSP method and remove the need for expected returns ... #teamgreen :face_with_rolling_eyes:

*Tags: Errors Debugging, Strategies*

---

**Mo** - *17:07:07*

1. I'd expect it to be quite likely for most strategies to underperform expected returns in any given month

2. You can very easily do a simulation study or use the central limit theorem to work out the probability of observing the flumine simulated return if BSP is correct

3. You're assuming the BSP overround is 0 which I'd guess is not quite correct although should probably only make a minor difference to the calculations

*Tags: General Technical*

---

**Aaron Smith** - *17:10:07*

[@U01EQMX079B](@U01EQMX079B) I assume the expected return is also based on the flumine simulation?

[@UBS7QANF3](@UBS7QANF3) why 1. ? Only reason i see rn (ignoring commission) would be the effect you yourself have on bsp, which at smaller stakes, which i would assume Tom is, shouldnt have much of an effect?

*Tags: General Technical*

---

**Tom Cannon** - *17:22:31*

My expected returns based on EV and simulation are so wildly off I think both of your points probably means I've either made an error or discovered a very niche area of BSP's that are not representing Pr(Win). If I were a betting man :face_with_rolling_eyes: I'd assume the error :joy:.

*Tags: Errors Debugging, Strategies*

---

**Mo** - *17:30:18*

[@U4H19D1D2](@U4H19D1D2) can comment about how good flumine is at preventing double matching of liquidity when simulating but I would start by limiting it to one bet per race and then comparing again

*Tags: General Technical*

---

**Aaron Smith** - *17:31:57*

i could imagine this number dropping in actual production if the market reacts to the new money. Anyways, looks good to me. I do have months like that. Personally i d give it a go and calculate EV each day from production. But thats just me, a more careful type would probs wait some more and collect more data

*Tags: Deployment*

---

**Tom Cannon** - *17:38:21*

Interesting, I'll give it a try with one bet per race and one per runner and see if makes any difference.



I'm somewhat weary of my own EV calculations, but I'm confident that I'm being matched better than BSP.  So would have expected simulation to still be +ve.



I'll double check everything for mistakes, before I use any more of your guys time.



Thank you, really helpful to get your thoughts.

*Tags: General Technical*

---

**George** - *17:44:12*

One possibility is that you have a strategy which is highly predictive for the BSP but not necessarily highly predictive for the actual race result, and therefore greening is actually the correct way to go in this case.

However, I wouldn't base any decisions at all off of a month of data.

*Tags: Strategies*

---

## 2022-07-16

**Michael** - *09:11:18*

Is one months worth of data all you have or is your strategy developed using a bigger data set and you're just using a month's worth as a hold out set for testing?



If it's the former then you simply may not have enough markets. Depending on your odds you might be asking too much of too small a data set.



If you were to bet randomly at BSP would you expect to be at zero after &lt; 1000 markets?

*Tags: Strategies*

---

**Tom Cannon** - *10:43:21*

[@UCQB6S222](@UCQB6S222) this would be ideal. I've just implemented a green strategy and unfortunatly I'm getting similar negative profits in simulation - which doesn't make much sense to me ...



[@U4H19D1D2](@U4H19D1D2)

Am I correct in thinking that OrderStatus.EXECUTION_COMPLETE is only true when an order has been fully matched? So laying immediately at the BSP using MarketOnCloseOrder should give me a margin if BSP &lt; matched_back_price (which it does when I track it) but in simulation It's as if my lay order is never fulfilled. The only similar debugging I can find is here [https://github.com/betcode-org/flumine/issues/354](https://github.com/betcode-org/flumine/issues/354) Of which I think is still present in FlumineSimulation.__process_s_simulation_orders().



I also seem to be getting matched in simulation at strange prices. For ~95% it's sensible (matched at 5 &amp; BSP = 4.677) but some are far too high in my opinion (for instance matched at 170 for BSP 26) I suspect that these aren't actually getting matched - are there other reasons that EXECUTION_COMPLETE would be triggered?

*Tags: Errors Debugging, Strategies*

---

**Tom Cannon** - *10:49:52*

[@UGV299K6H](@UGV299K6H) I only have 4 months of data and using one month for simulation. I could form this model / strategy completely theoretically though to be honest; although I have tuned it with the data I have. But I see your point regarding simulation size.

*Tags: Strategies*

---

**Michael** - *11:15:16*

Four months is a reasonable amount to develop with, so maybe you're just looking at a relatively high volatility / low margin strategy that just needs quite a lot of bets to shake out. The simple answer is more data, which will resolve all questions. Like [@UBS7QANF3](@UBS7QANF3) says, one month of horse racing data is not as much as you might think.

*Tags: Strategies*

---

## 2022-07-17

**Tom Cannon** - *17:11:41*

Hey, thanks all! Assuming that EXECUTION_COMPLETE was matched was clearly a mistake. I have now fixed these issues and have a very steady profit of ~1200 a month  :tada:



I have similar responses from flumine.simulation and my EV - however my data doesn't have any runner.sp.actual_sp (I got this from historic data) which causes issues with the flumine simulation?



I've had to add a hack to simulatedorder._ process_sp_ to read sp_prices from a file I'd compiled from [https://promo.betfair.com/betfairsp/prices](https://promo.betfair.com/betfairsp/prices) I'm assuming (hoping) this data is reliable.



Feel free to direct message me if anyone else doesn't have the runner.sp.actual_sp in their data,  I'll share you the code :slightly_smiling_face:



Again, thanks for the help.

*Tags: Errors Debugging*

---

**liam** - *17:20:21*

Correct not having the sp would cause issues in simulation! Out of interest where you getting an error or just negative pnl

*Tags: Errors Debugging*

---

## 2022-07-19

**Mo** - *08:09:41*

[https://betcode-org.github.io/flumine/performance/#file-location](https://betcode-org.github.io/flumine/performance/#file-location)

*Tags: Performance*

---

**Rudeger Jamison** - *08:36:52*

Thanks for your help

*Tags: General Technical*

---

**liam** - *08:44:24*

[https://betcode-org.github.io/flumine/quickstart/#market-filter](https://betcode-org.github.io/flumine/quickstart/#market-filter)

*Tags: General Technical*

---

**Rudeger Jamison** - *11:31:47*

Last silly question, I'm assuming all simulation profit is just net. We would have to work out a crude % to factor in commission?

*Tags: General Technical*

---

## 2022-07-21

**Henry E** - *11:52:15*

Do many people do cross venue arbitrage with flumine / betcode or is it mostly focused on within Betfair market making?

*Tags: General Technical*

---

**liam** - *12:01:47*

Fairly sure the days of low hanging fruit on that strategy are long gone across all the venues 

*Tags: Strategies*

---

**liam** - *12:25:52*

Yeah I am anti crypto / blockchain / NFT / web3 as a whole however I don't really see the problem it is solving when it comes to gambling + many struggle with the idea of an exchange 

*Tags: General Technical*

---

**Newbie99** - *12:32:23*

It doesn't matter too much what the actual coin is, you still have the same problem

*Tags: General Technical*

---

**Henry E** - *12:35:53*

it's not clear what you're trying to say and if the problem still exists if you take BTC out of the equation

*Tags: General Technical*

---

**Newbie99** - *12:36:19*

that is the problem

*Tags: General Technical*

---

**D C** - *13:04:25*

Another aspect that makes it unattractive is that if it is TRULY decentralised then there must be a consensus mechanism where the result can be established truthfully and be safe from bad actors. Augur used to have the REP token for this so that holders would "vote" to verify settlement results but I have no idea if this is still the case as I have not looked at it for ages. As others have said though, its hard enough to drive punters to the exchange anyway through better prices. My father is a lifelong recreational punter and is far from stupid, but he still doesn't really "get" the exchange model.

*Tags: Strategies*

---

**Unknown** - *13:07:46*

Automated trading strategies - actually any kind of trading - needs two parties to agree on the price



Who is going to think that these automated trading strategies are posting prices they want to match with? Either it's dumb money that's price insensitive, it's other sharks who think the price is wrong or it's people with different incentives - ie they want to lay off some risk and are happy to pay the margin to do so



With the crypto angle you are heavily disincentivising the laying off of risk. No regulated bookmaker or sports entity is going to be doing this on a black/grey market exchange



Sharks eating sharks does not lead to sustainable liquidity



So now you have pretty much all of the same problems that Betfair has attracting dumb liquidity - and they are big problems - and crypto doesn't really help you solve them. Punters find it hard enough to understand the exchange model and now you want them to educate themselves around crypto on boarding etc? If you want to remain black market then you have some advantage but it's not something I would personally risk jail time for





*Tags: Strategies*

---

**D C** - *13:17:26*

I am not anti crypto in general but agree with [@U4H19D1D2](@U4H19D1D2) in general - with the exception that it WOULD allow a workaround for people who get on the arse end of affordability checks (if they come to pass). Problem is that it then creates so many more problems and most people who are NOT addicts would just say that gambling is just too much hassle.

*Tags: Errors Debugging*

---

**Unknown** - *13:39:38*

I appreciate the stimulating discussion, things have been pretty quiet on here lately. I do think there's a future for crypto but I think betting exchanges - both traditional and crypto - have major problems and crypto is not currently offering solutions to these - in fact, making them worse on balance. Better to focus on things crypto is suited to solving IMHO

*Tags: Strategies*

---

**Unknown** - *13:46:39*

It definitely offers ways to change up how incentives work. Maybe if betfair was reskinned more like a regular bookies and people other than betfair had direct incentivizes to market it, then that might help popularize order book exchanges. Again, all just a big experiment, may or may not work

*Tags: General Technical*

---

**Henry E** - *13:47:58*

The two big problems with a new exchange seems to be liquidity and marketing. That's something that maybe could be solved with the right incentives :shrug:¯

*Tags: General Technical*

---

**Henry E** - *13:49:33*

I'd like to at least try to design our exchange in such a way that if people want to arb against betfair or another exchange, it is straightforward to do. Although the practical problems with this have been well noted above

*Tags: General Technical*

---

**Jonjonjon** - *13:52:19*

Yes. But...



Unless I go through the dex code, which would involve learning a new language, I wouldn't be able to trust the dex either.



When I did my recent transactions using electrum wallet (do you consider that proper crypto usage?) I trusted the software would not steal my coins.



Before I used Bflw or Flumine, I went through every line to make sure it wasn't sending me details off to Liam!

*Tags: General Technical*

---

**Newbie99** - *13:56:00*

still the same problem

*Tags: General Technical*

---

**DeeJenn** - *14:02:55*

You don't have to report losses though (obviously one normally would so they can claim them)



I haven't found an answer to the question of whether you have to report a gain of 0. I would hazard a guess as to what most people do in that situation

*Tags: General Technical*

---

**Newbie99** - *14:04:26*

Yes...but the problem comes if its hundreds of transactions over a year...obviously if its a small number its not a problem, but we're talking about automated betting here, so realistically its likely to be a lot of transactions.



Now admittedly not all need to involve converting to Fiat, so you can keep it down to some degree, but its still an unnecessary headache!

*Tags: Strategies*

---

**DeeJenn** - *14:07:05*

100*0=0



Granted, this is more complicated when we’re talking about GBP instead of USD, but crypto exchanges let you convert between USD and USDC for no fees at a fixed rate of 1

*Tags: Errors Debugging*

---

## 2022-07-22

**Newbie99** - *08:12:29*

the real problem they need to solve is the issue of a taxable event on every transaction, if it was only on entry and exit to crypto that would make everything so much easier!!!

*Tags: General Technical*

---

## 2022-07-27

**birchy** - *13:46:10*

When using Flumine LoggingControl, I'm ok with building a dict from the orders for the CSV, but am wondering where is the best place to store associated order signals? We have order.notes and trade.notes but I'm unsure what the difference/preference is for storing a "signals" dict that I can unpack in LoggingControl?

*Tags: General Technical*

---

**birchy** - *21:03:19*

To answer my own question, I chose `order.notes` so that I can pick up my signal notes while looping the orders, which equates to one order per CSV row.

*Tags: General Technical*

---

## 2022-07-31

**Troy Edwards** - *23:44:21*

Hi all long time since chatting.  I have been trying some strategies with Aussie and NZ greyhound racing and still don't have an edge yet.  So back to learning and I have come across SVM - Support Vector Machine regression tool which I want to apply to greyhound data.  Some of the X inputs will be form, speed, rating, starting odds etc and some of my own inputs.  However for the Y inputs which are the results, should I enter say 1 (True) for the winner and then -1 (False) for the other 7 runners?  OR should I enter the actual finishing positions 1 thru 8?    I am wondering if training the model with positions 1-8 may introduce errors OR be too difficult?  I was also thinking I could just set positions 1-3 True and everything else False as most of my strategies are for Lay bets?   I  have started any coding yet for the model but am going to use Accord.MachineLearning.VectorMachines.Learning in the .NET library.

*Tags: Errors Debugging, Performance, Strategies*

---

## 2022-08-01

**Tom Cannon** - *09:41:33*

Hey Troy, It's hard to say which is best, SVM supports binary, multi-class and regression - so you should have no trouble implementing either. However, on a problem like this I would try all and then keep the most expressive (binary &lt; positions &lt; "true probabilities") which is still suitable for your use case (probably max profit ... )



You would want to use an SVM classifier for your True / False styles. SVM regressor for positions. Unless you're very familiar with SVM's I'd experiment with just the linear and default RBF kernels (depending on if you data is linear or non-linear).



It's also probably a good idea to have a "baseline" to know if SVM is sensible. Usually I would just use Scikit-learn's ridge regression for this. You'd be surprised how often ridge regression will give you better (or very similar) results to other, more complicated, methods. Also compare with out of the box XGBoost (a good, and easy to implement, decision tree - which will give you a good baseline for non-linear).



Good luck :)

*Tags: General Technical*

---

**Mo** - *10:53:00*

Some thoughts:



1. SVM is an odd choice; if you want state of the art then use XGBoost. If you want simplicity, speed, ease of implementation and a method that has stood the test of time then use Logistic Regression

2. I'm not sure about including starting odds - you mention form, speed, rating: all of these will already be incorporated into the odds. So it will come down to whether there is any marginal value in your "own inputs" over and above the odds

3. Another downside to starting odds is they are only available when the race starts. Typical setup for a pre-race strategy would be to use the starting odds (implied probabilities) as the target for your model rather than binary win/loss

4. I think you're thinking along the right lines with what I'll summarise as "truncated finishing positions" although I still think just using the implied probabilities from the starting odds would be way better

*Tags: Getting Started, Performance, Strategies*

---

**Alessio** - *11:05:04*

In general if i may add, the edge of a complex modelling technology like SVM or NNs pays off only with (1) a lot of data (like millions of examples) and/or (2) a loss function that is specialized for your task. XGBoost for non-linearity and classic regression will probably squeeze 90% of it. What is much more important is defining the objective, engineer feature and clean data like no tomorrow.

*Tags: Feature Engineering, Strategies*

---

## 2022-08-02

**Johnny Boston** - *00:08:48*

If I wanted to create a worker that appends some information to every markets context dict from `flumine.market` how often should I run that worker to ensure that all markets have been populated with this information ?

*Tags: General Technical*

---

**liam** - *08:22:17*

How quickly do you need the information? If it doesn't require any long requests you can put it in middleware instead and use the `add_market` function which is called when a market is added but this is in the main queue so would cause latency issues. If you use a worker to just set it to 60s? and check for new markets, I imagine a delay of 60s on new markets isn't a problem?

*Tags: Performance*

---

**Johnny Boston** - *23:40:36*

Hey - I am using this filter for a simulation on the pro files



```    strategy.market_filter = {

        "markets": markets,

        "market_types": ["WIN"],

        "listener_kwargs": {

            "inplay": False,

            "seconds_to_start": 600,

            "calculate_market_tv": True,

        },

    }```

I am still getting `PLACE` markets through in my *`process_closed_market` function of the simulation strategy class, is that expected?*

*Tags: Strategies*

---

## 2022-08-03

**Tiago Augusto Ferreira** - *05:01:24*

Hello. I am try filter my strategy using the competition_ids. But i receive a error SUBSCRIPTION_LIMIT_EXCEEDED. What is the correct form to filter by this field?

`strategy_over = MyStrategy(`

    market_filter=market_filter(competition_ids=[141]),

    name='OVER05HT_MODELO',

    max_trade_count=3,

    max_live_trade_count=3,

    max_order_exposure=12,

    conflate_ms=1000,



)



The error:



2022-08-03 00:57:07 *ERROR*    [MarketStream: 2001]: SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 14995 markets whereas max allowed number was: 200

2022-08-03 00:57:07 INFO     [MarketStream: 2001]: *FAILURE* (9 connections available)

*Tags: Errors Debugging, Deployment, Strategies*

---

**Mo** - *05:37:19*

You can't filter by competition ID in streaming so you have attempted to subscribe to every market across all sports

*Tags: General Technical*

---

**Mo** - *07:19:39*

You'll have to find an appropriate streaming filter and then filter by competition ID locally

*Tags: General Technical*

---

**liam** - *08:14:12*

I am unable to replicate this but then the way you are setting the `market_filter` like that is confusing me, do you have some code I can run to replicate? For example here is the simulation.py code I am running



```strategy = LowestLayer(

    market_filter={"markets": markets, "market_types": ["PLACE"]},

    max_order_exposure=1000,

    max_selection_exposure=105,

    context={"stake": 2},

)

framework.add_strategy(strategy)```

*Tags: Strategies*

---

**Johnny Boston** - *08:24:45*

Yes, I'll get it to you. I do it like this to not change a strategy I have in prod.

*Tags: Strategies*

---

**liam** - *08:49:35*

Yeah assuming its done before the `add_strategy` but something odd is happening

*Tags: Strategies*

---

**liam** - *08:50:16*

from memory it's a big change in the streaming backend to make it work, must be asked for a lot though as it is very annoying...

*Tags: Performance*

---

**Johnny Boston** - *08:50:19*

Yeah it's done before the add strategy

*Tags: Strategies*

---

**Johnny Boston** - *09:42:11*

Hey [@U4H19D1D2](@U4H19D1D2) - so this is the gist of it.



In prod I am recording for win and place markets, and in the simulation I wanted to record for all the PRO files for back data but only `WIN` markets



Here is the entire strategy and the class being instantiated...

*Tags: Strategies*

---

**liam** - *09:49:37*

That's a recorder strategy for recording live data 

*Tags: Deployment, Strategies*

---

**Johnny Boston** - *09:50:03*

Yeah, but can't you put a recorder strategy through a simulation to record back data?

*Tags: Strategies*

---

**liam** - *09:51:58*

Yes but that strategy is setup for live with the filters 

*Tags: Getting Started, Deployment, Strategies*

---

**Johnny Boston** - *09:53:02*

```    client = clients.SimulatedClient(min_bet_validation=False, transaction_limit=9999999999999)

    framework = FlumineSimulation(client=client)



    strategy = aus_flucs_recorder

    strategy.market_filter = {

        "markets": markets,

        "market_types": ["WIN"],

        "listener_kwargs": {

            "inplay": False,

            "seconds_to_start": 600,

            "calculate_market_tv": True,

        },

    }

    strategy.context = {"SIMULATED": True}



    logger.warning(f"Processing markets: {[m for m in markets]}")



    with mock.patch("builtins.open", smart_open.open):

        framework.add_strategy(strategy)

        framework.run()```

*Tags: Strategies*

---

**Johnny Boston** - *09:55:26*

```    strategy = TradingFlucsRecorder(

        market_filter={

            "markets": markets,

            "market_types": ["WIN"],

            "listener_kwargs": {

                "inplay": False,

                "seconds_to_start": 600,

                "calculate_market_tv": True,

            },

        }

    )```

*Tags: Strategies*

---

**Johnny Boston** - *11:12:18*

yeah that is what I ended up doing, thanks for helping out

*Tags: General Technical*

---

## 2022-08-08

**Alessio** - *11:20:44*

Monday morning curiosity. I have now a few things that work-ish a bit (my understanding is that ~3% profit on average is within expectations, but they are not scaling at the moment). The ways i feel i could continue exploring are more or less three axes: (1) low latency -- try to acquire fresher data and try to be earlier than others, but do not try to get "better" odds, just get to them earlier  --,  (2) more high precision strategies that try to figure out when odds are missing a piece of information or (3) simple strategies with complex fundamentals that run frequently  -- try to get better at pricing than BF, and any time there's mis-pricing, just enter/exit the market.  I don't do scalping, is boring and not enough liquidity :)

My feeling is that on the downside (1) has intrinsic limits unless you go for expensive data at some point, but may be very fruitful;  (2) looks like the simplest to go forward, but has the risk of black swan events and (3) is probably an endless pit unless you have a team of 4 working on it or stuff like that.

Is there some common wisdom or common ways to understand what's best? Or personal experience you guys want to share (within the limits of not telling, of course) ?

*Tags: Performance*

---

**thambie1** - *11:32:14*

It's much easier to to grow an already working but small scale strategy than to start a new one. It really depends on what you're existing strategy is, and whether there's room for growth. Bet sizing, latency, and modifications to the strategy can all be potential options. That being said, if you do decide to go down a completely different rabbit hole, I would pursue option #2. It's still very hard, but easier than #3, and more likely to be a stable competitive advantage compared to #1.

*Tags: Performance, Strategies*

---

**Alessio** - *13:07:07*

Thought so for (1)., thanks for confirming. For (2) vs (3) is more about the approach: what I consider (2) is to find specific moments where you see mispricing, recognize them and act on them (say, go for player injuries and see when they are not priced in;  you don't care too much about the actual correct odds, it's more a signal to tell the strategy to act, and a lot of the logic is hand-tuned in the strategy) while (3) is more like a fundamentals-first approach -- basically building your own odds, but then trusting them most of the time. I see your point about them being connected, it's more of classifying bad odds/good odds -- and probably going for more rare events --  vs trying to shave every single penny out of each mispricing.

*Tags: Strategies*

---

**Mo** - *16:37:39*

Yeah so essentially you have (at least) 2 competing methods for making a prediction: the near price and the mid point. Take your 10 am snapshot and calculate the 2 values and also record where the BSP ended up. Score the two methods according to some reasonable criterion such as mean squared error. Repeat for all the races and runners in your data set then add up the scores and see which method has the best overall score

*Tags: Errors Debugging*

---

## 2022-08-10

**George** - *09:35:23*

If I want to simulate an inplay strategy for Place markets, with TPD data, how should I set that up? The TPD files are named (by default) with the Market ID of the win-market, so it won't work without making some kind of intervention in the Flumine code itself, I think.

*Tags: Strategies*

---

**George** - *11:44:20*

Sorry [@U4H19D1D2](@U4H19D1D2) but it still doesn't work. [https://github.com/betcode-org/flumine/blob/master/flumine/markets/middleware.py#L306](https://github.com/betcode-org/flumine/blob/master/flumine/markets/middleware.py#L306) says that if the market ID from the update (i.e. the TPD data) does not match the market ID from the market book (i.e. the BF Exchange Place market data) then the update should be ignored

*Tags: General Technical*

---

**George** - *11:48:08*

Of course, but, now we are getting into the realms of making "changes" to Flumine which I'd rather not do. And it feels odd to subclass just to change one line...

Also, I expect that line is in there for a good reason, and by removing it without knowing why it's there, I'm going to cause unexpected knock-on effects.

Surely there is a better way to resolve this?

*Tags: General Technical*

---

## 2022-08-11

**George** - *09:38:34*

does the Flumine simulation replicate the inplay delay?

*Tags: General Technical*

---

**liam** - *09:39:19*

[https://github.com/betcode-org/flumine/blob/fd527626accbddd7782477f420855f57e928dae9/flumine/simulation/simulation.py#L182|yep](https://github.com/betcode-org/flumine/blob/fd527626accbddd7782477f420855f57e928dae9/flumine/simulation/simulation.py#L182|yep)

*Tags: General Technical*

---

**liam** - *09:44:27*

FYI that delay includes the inplay delay if present and network/match latency which you can adjust in the [https://github.com/betcode-org/flumine/blob/fd527626accbddd7782477f420855f57e928dae9/flumine/config.py#L25|config](https://github.com/betcode-org/flumine/blob/fd527626accbddd7782477f420855f57e928dae9/flumine/config.py#L25|config)

*Tags: Performance*

---

**Dario Scardina** - *18:44:51*

[@U02QDLFCYHG](@U02QDLFCYHG) did you find an alternative? I think we would implement a similar strategy :D

*Tags: Strategies*

---

**birchy** - *19:03:48*

For those of you with profitable strategies, how often do you all have to carry out maintenance or "tweak" settings/signals/etc? Personally I have one particular strategy that was my first live one with Flumine that's been untouched for ~2 years now. It's nothing adventurous but has been slowly ticking along and in recent months has performed exceptionally well. No doubt there will be an equal downturn at some point....

*Tags: Errors Debugging, Performance, Deployment, Strategies*

---

**Dario Scardina** - *19:17:56*

```trading = betfairlightweight.APIClient(username=my_username,

                                           password=my_password,

                                           app_key=app_key,

                                           cert_files=(cert, key),

                                           locale="italy",

                                           lightweight=False

                                           )



trading.login()



competitions = trading.betting.list_competitions(

    filter=betfairlightweight.filters.market_filter(

    event_type_ids=[1],  # Soccer's event type id is 1

    in_play_only=True # In play only

    )

)



live_events = trading.betting.list_events(betfairlightweight.filters.market_filter(

    event_type_ids=[1],  # Soccer's event type id is 1

    in_play_only=True,  # in play

    competition_ids=[competition.competition.id for competition in competitions], # get events only for competitions selected

    ))



strategy = ExampleStrategy(

    name="BackUnderSH",

    market_filter=streaming_market_filter(

        event_type_ids=["1"],

        event_ids=[live_event.event.id for live_event in live_events],

        market_types=["OVER_UNDER_05", "OVER_UNDER_15", "OVER_UNDER_25", "OVER_UNDER_35", "OVER_UNDER_45",

                           "OVER_UNDER_55", "OVER_UNDER_65"],

    )

)```

[@U02QDLFCYHG](@U02QDLFCYHG) maybe that could help... Works for me to consider only live events for soccer competitions.

*Tags: Deployment, Strategies*

---

**birchy** - *20:39:25*

[@U4H19D1D2](@U4H19D1D2) do you make adjustments manually, or do you have some automation that evolves with the markets? I did a very rough version of the latter many years ago that basically started off betting "randomly", then checked the P&amp;L every 1000 bets, adjusted the signals and ran for another 1000 bets. Can't remember what the outcome was, but assume it was shit because I don't do that anymore. :grinning: 

Of course, now we can simulate with Flumine instead.

*Tags: Strategies*

---

**liam** - *20:46:32*

Manually based off simulations or what the live data was saying 

*Tags: Deployment*

---

## 2022-08-12

**Michael** - *09:41:44*

I'm surprised that people find such frequent changes worthwhile, I think I would find them a problem as they really muddy the waters in terms of analysis. Even with frequent events like horse racing a few weeks of data is a trivial amount and easily affected by simple randomness.



Personally I find it very difficult to attribute success or failure to my own strategy changes rather than other factors with confidence.



These days I lean more towards establishing some kind of hypothesis and then setting up long running tests to confirm or refute it and I basically just ignore my betting whilst I'm waiting.

*Tags: Strategies*

---

**liam** - *10:04:18*

I guess it depends on what you constitute a change to be, I find that changes to test matching rates take just a few days to become obvious (80% of my testing) whilst triggers/strategy ifs can take months.



I try and isolate the other factors and just focus on the change and if it has been positive.



I think you are in the rare minority who is happy to go and do something else whilst you wait :wink:

*Tags: Strategies*

---

## 2022-08-13

**Ruben** - *17:23:24*

is the general consensus in favour of grouping strategies under the same flumine instance, or running each strategy in a separate flumine process?

*Tags: Strategies*

---

**Jonjonjon** - *18:33:30*

If you use separate Flumine processes you will use up lots of Betfair connections.

*Tags: General Technical*

---

**Peter** - *19:06:49*

Flumine is designed to handle multiple strategies and if they're targeting the same markets it normally makes sense to run them on a single process.



When it won't usually make sense is when the markets are different, e.g. don't mix football and horse racing, or if you have a strategy that is very CPU intensive and may slow up the responsiveness of other strategies.



I think it's pretty normal also to keep data collection separate.

*Tags: Performance, Strategies*

---

## 2022-08-15

**Mo** - *09:21:21*

I have a price taking strategy that produces good backtest results when backing (ROI 9% over approximately 20000 events) and bad results when laying (ROI -11% over the same events). I'm a bit perplexed why this is - my only theory is that the predictions are in some way biased towards 1 but in such a way that doesn't destroy all value when backing. This doesn't seem very convincing. Anyone experienced anything similar and were you able to resolve it? Any suggestions for where to start working out what's going on?

*Tags: Strategies*

---

**Mo** - *10:42:33*

&gt; So could you just have other players beating you to the lay side - all your good bets there simply aren't matching?

Could be - seems strange that would apply to one side only. I can test this by reducing my simulated bet latency and seeing how the results change

*Tags: Performance*

---

**Mo** - *14:24:17*

The bets range across the entire price ladder and the performance is equally bad in all regions of it

*Tags: Performance*

---

**Ruben** - *20:52:54*

is a 2% margin on backtest too low to consider a strategy?

*Tags: Strategies*

---

**Ruben** - *20:55:39*

this is with flumine, my understanding is that by default there is no commission applied

*Tags: General Technical*

---

## 2022-08-18

**Mo** - *06:08:04*

I've looked at spread and over/underround - no relationship with quality of bets. Ditto distance run/remaining in the race



Everything is pointing to the model is just failing to identify value bets on the lay side. I still can't reconcile how it can do so on the back but not lay side but for now I'm focusing on improving the predictions

*Tags: Strategies*

---

**Mo** - *09:21:13*

Same picture at 100% match rate: backs do very well, lays do very badly. So doesn't seem to be execution or price movement related. Hence the focus on the model

*Tags: Strategies*

---

**liam** - *09:38:59*

Reading between the lines I do things very differently when it comes to model/pricing, on average I will place 60ish bets per second at key times which is where matching/spread/price comes crucial but at 1 selection / side this mean reverts out to leave just the model.



Is it a market based or fundamental?

*Tags: Strategies*

---

**Alessio** - *10:24:12*

Very stupid question, now that i think about it. Let me assume for a second that you lay because the model underpredicts w.r.t. odds. Is it possible it underpredicts because you have a missing feature? This would not affect 'backs' too much because without the faeature it would not be able to overpredict w.r.t. odds.

*Tags: Feature Engineering, Strategies*

---

## 2022-08-21

**Mo** - *07:20:39*

[@U01C12ZEADQ](@U01C12ZEADQ) I'm not sure what question you're referring to as "very stupid" :sweat_smile: my current line of investigation is indeed that the model needs one or more additional features and/or a structural change

*Tags: Feature Engineering, Strategies*

---

**Unknown** - *08:24:16*

Good morning everyone, I am backtesting a strategy and obtained this profit plot. Do you think this indicates the value opportunity was there but it disappeared when profit goes "flat"? This is ~900 markets, ~2200 orders

*Tags: Strategies*

---

**Mo** - *08:33:38*

This will depend on number of events, average odds, staking, etc. etc. but speaking very generally, it's entirely feasible for a strategy to be flat for a quarter

*Tags: Strategies*

---

**Mo** - *08:36:50*

But it's very good practice to always look at the cumulative return plot as it gives you a much more nuanced feel for the strategy than headline numbers

*Tags: Strategies*

---

**Alessio** - *08:55:23*

:smile: at some point we will all want to know what fixed it.

*Tags: Errors Debugging*

---

**Mo** - *09:39:48*

To be clear I am live trading the strategy just backing



Whether to just move on is an interesting question of how to prioritise the infinite possibilities of what to work on. And perhaps more importantly, whether trying to solve the mystery of the poor laying performance is mainly just an intellectual exercise rather than a profitable one

*Tags: Performance, Deployment, Strategies*

---

**Alessio** - *10:20:41*

Indeed, maybe if you fix the laying it backs even better :slightly_smiling_face:

*Tags: Errors Debugging*

---

**MacrcoPolo** - *16:12:36*

Another thing to look at here from the 'are the prices I'm taking any good?' line is to check the average size of the levels you're trying to take liquidity from on back vs lay in your model. Need to be careful to to compare in the same price buckets since shown sizes vary so much with the price, but worth looking at

*Tags: Strategies*

---

**birchy** - *18:13:37*

I have a similar graph for a live dogs strategy that goes up steeply, then goes flat for a few weeks and then goes up again. So I'd say it _might_ be a goer but you'll need a bigger sample size (i.e. 6+ months).

Just noticed your graph covers 4 months, so I'd say that 2200 bets in that time period is nowhere near enough.

*Tags: Deployment, Strategies*

---

## 2022-08-23

**birchy** - *14:01:21*

To the Flumine users: how do you manage exposure at strategy level? I'm aware of the functions in the `blotter` but am likely not using them efficiently. In particular, `blotter.selection_exposure` is useful but accumulates the total exposure per side, which is great if you're both backing and laying a selection (#nevergreen) but not so good if you have a pure laying strategy that lays multiple runners as it hits my hard coded exposure limit and then stops betting on that particular runner, despite having layed other runners that potentially reduce the exposure on that runner. So I guess my questions are:

1. Do you "balance" exposures across multiple runners?

2. Is there a way to reset the exposures?

3. Should we be using a dynamic exposure limit, i.e. increase it when other runners have been layed, perhaps in conjunction with `blotter.market_exposure`?

4. Or should we loop through the blotter orders and self manage?

This is more of a "best practice" question than anything else...

*Tags: Strategies*

---

## 2022-08-24

**Johnny Boston** - *00:32:22*

don't you set exposure at the strategy level through `max_selection_exposure` ?

*Tags: Strategies*

---

**Newbie99** - *18:17:47*

My approach is probably a bit clunky, but I re-create the Betfair 'if-win' from the GUI, I store this figure in the market.context in a dict unique to each strategy. I can therefore share exposure between strategies (or not) as appropriate.



I set the max_selection_exposure to a really big number I don't expect to hit and then manage exposure separately by performing various validation checks (e.g. current exposure of runner and current worst exposure, so if I am at an exposure limit for runner x and backing runner y would add further exposure to runner x I don't place the order, or rather I have a setting which I can toggle between True or False, depending on whether I want to allow that behaviour on a strategy).

*Tags: Strategies*

---

**Newbie99** - *20:32:39*

[@U016TGY3676](@U016TGY3676) once you get to a certain level then it makes sense that you don’t need to worry about the exposure as you can never get as much matched as your risk appetite. However for the majority of us, who aren’t at that level we have to balance value vs bankroll.

*Tags: General Technical*

---

**liam** - *20:40:34*

Yeah pretty much, I have values for exposure/trade_count but they are very high and more to catch really bad errors/downtime’s etc.



Tbh I really don’t like the idea of stopping based on exposure, it’s almost like stopping when you reach x profit 

*Tags: Errors Debugging*

---

**birchy** - *21:02:53*

I agree that limiting exposure is in the realms of "stop for the day at £N" but, as [@UFTBRB3F1](@UFTBRB3F1) has said, having a £2k exposure on one runner can be a bit daunting although it might also indicate a shit strategy. :joy:

*Tags: Strategies*

---

**Newbie99** - *21:24:31*

Its not the same thing though is it...no one on here for example (I assume) will stop when they hit a random p&amp;l figure, but bankroll management is a legitimate way to stay in business!



To take an extreme example, you could build the perfect model that always pinpoints maximum value and executes perfectly, but there's still no guarantee the runner will win, hence you need to live to fight another day.



The difference is that for the bigger players on here maximum market liquidity is reached well before any risk tolerance becomes a consideration.

*Tags: Deployment, Strategies*

---

## 2022-08-25

**liam** - *08:29:26*

[@U016TGY3676](@U016TGY3676) doesn't [https://github.com/betcode-org/flumine/blob/e21cbebed9003cbc20ae46c8c756c4a0bd182076/flumine/markets/blotter.py#L185|get_exposures](https://github.com/betcode-org/flumine/blob/e21cbebed9003cbc20ae46c8c756c4a0bd182076/flumine/markets/blotter.py#L185|get_exposures) return what you want?

*Tags: General Technical*

---

## 2022-08-31

**Ruben** - *12:56:10*

does it make sense to try football fundamental modelling for pre-off betting if I can't spend thousands of dollars on data feeds?

*Tags: Strategies*

---

**Dave** - *20:04:44*

Question is, on which market does price discovery happen for pre off footie? Asian bookies?

*Tags: General Technical*

---

**VT** - *22:08:36*

[@U0128E7BEHW](@U0128E7BEHW) I didn't understand your question, most of the time for football (and I believe for other sports too) the first bookmaker to publish odds is bet365, then 1xbet (usually copying quotes). In some leagues bet365 has changed its policy and is no longer the first to open, but in general European bookies open first compared to the Asian market.

*Tags: General Technical*

---

## 2022-09-03

**MA** - *09:50:29*

Morning, (perhaps a simple question )but anyone  aware of a Python API betfair code which browses tennis matches and sends a discord alert if player A has lost first set but has more breakpoints opportunities than Player B?

*Tags: General Technical*

---

**MA** - *10:03:36*

Read about in other forums so not a “new secretive” strategy unveiled just curious if anyone was kind to share the code (Discord alert can be ignored)

*Tags: Strategies*

---

**Mo** - *10:05:22*

Wasn't referring to the strategy per se. I mean you're asking for an API that:



1. Is hooked into tennis feeds

2. Can alert on a very specific trigger

3. Is hooked into Discord

Maybe I'm just ignorant but would be surprised if there's something off the shelf that does all of that

*Tags: Strategies*

---

## 2022-09-29

**Jonjonjon** - *09:31:15*

Suppose I wanted to backtest a strategy that needs to consume data from 2 streaming files, where the strategy needs to consider data from both files, and the two markets are trading at the same time... Is there an example that shows me how to do that? I recall reading about it but Google is not being my friend this morning.:disappointed:

*Tags: Strategies*

---

**Aaron Smith** - *13:54:02*

flumine already has this functionality, check docs

*Tags: General Technical*

---

**Aaron Smith** - *13:54:47*

Unless you are speaking of markets not being part of the same event, in which case you would have to merge the streaming files yourself (you can still check flumine code on how to do it)

*Tags: General Technical*

---

## 2022-10-04

**Jonjonjon** - *14:57:21*

I want my strategy to refrain from placing orders if the 5k transaction limit has been reached. What's the recommended way to do that?

*Tags: Strategies*

---

**Newbie99** - *15:48:12*

Doesn't Flumine automatically cap orders at 5k per hour (unless you override it)?

*Tags: General Technical*

---

**ShaunW** - *17:30:16*

I'm not a flumine guy but always curious.... Does your trn limit control have an override, perhaps for cancelling. Seems a shame to leave things messy just to save 0.2p.

*Tags: General Technical*

---

**Jonjonjon** - *19:27:37*

I found the answer to my question. A Strategy instance contains a client object that has the information that I'm after.

*Tags: Strategies*

---

## 2022-10-11

**thambie1** - *10:56:11*

Hard to eye-ball an answer to that question. I like running monte carlo simulations to get an intuition for how much data is needed. One way to run such a simulation would be to sample from your current bets, and assume an expected value based on your current ROI.

*Tags: General Technical*

---

**thambie1** - *10:59:53*

Depending on your strategy you could also use indicators like beating opening line, or beating BSP to get an indication of profitability with less data.

*Tags: Strategies*

---

## 2022-10-25

**Mo** - *17:12:14*

For those of you who do in play, how do you score your models?



Using standard scoring rules has never quite sat right with me given you may be making hundreds (or theoretically infinite numbers) of predictions repeatedly on the same outcome



Backtests is another option but realised profit is slow to converge and all the other things that go into them such as choice of staking make it hard to focus on predictive accuracy of the model

*Tags: Performance, Strategies*

---

**Alessio** - *17:48:52*

Very interesting question. I don't have a good answer for you but the way I do it is that I do two scorings. The first one is by chopping the time of the event in 5-10 mins intervals, and looking at only the ones at the beginning of the window. This is to kind of evaluate the 'baseline' model. The second one, is that i look for changes in the odds OR the model (derivatives) and evaluate a random subset of fixed size per event. For each of these changes, I look X seconds before.

*Tags: Errors Debugging, Strategies*

---

**Mo** - *18:00:47*

Let me see if I understand correctly



For the first method, you make e.g. approximately 10 predictions for a football match spread throughout the match then score them as you would any other predictions? So it's essentially subsampling to reduce but not remove this issue of repeated predictions with the same outcome



For the second method the subsampling is achieved by focusing only on situations where either the odds or model predictions have changed but then with some additional random subsampling. Not sure about the X seconds before part?

*Tags: Strategies*

---

**Alessio** - *18:06:22*

For the second one, i focus on what matters for me, exactly. The X seconds is to account for the fact that you want to see how much your model can see into the future. If the market has already moved and I check if I agree with the market _in that moment_ I look less at my predictive power.

*Tags: Strategies*

---

**D C** - *21:47:01*

Dumb question no doubt, but what exactly do you mean by "scoring" a model?

*Tags: Strategies*

---

**Mo** - *21:53:49*

Literally assign a model a score so you can eg rank different ones and decide which is “better”. For example log loss, Brier score, Ranked Probability Score

*Tags: Strategies*

---

**D C** - *22:24:21*

Ah ok I see. So is it based on results or validation as opposed to model goodness of fit stuff like reduction in deviance on nested linear models for example?

*Tags: Strategies*

---

## 2022-10-26

**Mo** - *11:46:31*

Not sure [@UUE6E1LA1](@UUE6E1LA1). I guess that technically goodness of fit is another way of scoring a model

*Tags: Strategies*

---

**Artiom Giz** - *12:56:37*

Hi!

Question about Stream API events.

After subscribing to Soccer's *over/under 2.5* market I see many events with *BDATB/L* changes only:

```[{"id":"1.204890122","rc":[{"bdatl":[[1,1.51,1789.52]],"id":47973},{"bdatb":[[1,2.98,384.49]],"id":47972}]}]

[{"id":"1.204890122","rc":[{"bdatl":[[2,1.52,559.99]],"id":47973},{"bdatb":[[2,2.96,620.81]],"id":47972}]}]```

(there are no ATB events in between)

I know that "*D*isplayed" means "including virtual prices" (some markets' prices can affectively be converted to other markets prices).



Does somebody know, what can be an equivalent to *over/under 2.5* market (that causes these events) or what is the source of them?

*Tags: General Technical*

---

**jackofallspades** - *13:04:35*

This :arrow_up: may be outdated, but I believe that _Goal Lines_ is the answer to your question.

*Tags: General Technical*

---

**Guy Adini** - *13:34:33*

Thanks Mo - I'm working on this together with Artiom, so this is super helpful to me as well...

I understand the part about batb levels all changing when a level disappears, that's clear.

The other - about virtualization from the other side, is new to me - I didn't realize that they do that!



So if I have a two runner market, say over and under, and I have, for "over" atb [[1.9, LIQ_AMOUNT]], then what is the corresponding virtual price and amount for "under"? Is there maybe some link explaining this?

*Tags: Getting Started*

---

## 2022-11-05

**Ralegh** - *12:46:57*

People experienced with modelling - what sort of R^2 values should I be looking for (versus bsp)? I have two models so far using separate data with R^2 0.3 and 0.15 (tiny p values), I'm going to play about with other methods but are those worth using (i.e. together ensembled somehow with other models) or not?

*Tags: Strategies*

---

**Peter** - *13:42:59*

I agree with [@UBS7QANF3](@UBS7QANF3) that R^2 isn't worth much in this context. EV is useful and I'd also compare the Brier scores for your models to the one for BSP.

*Tags: Strategies*

---

## 2022-11-06

**Unknown** - *11:52:34*

To hedge or not to hedge that is the question :question:

*Tags: General Technical*

---

**mandelbot** - *12:12:29*

doesn't look like a question to me :smile:

*Tags: General Technical*

---

**Jonjonjon** - *14:01:15*

Or if you can find a way to diversify you strategy signal, you might not need to worry about the downside too much

*Tags: Strategies*

---

## 2022-11-07

**ShaunW** - *12:14:18*

It's pretty easy to figure out what difference hedging or not, at bsp, makes any difference to your specific strategy (size permitting). It's not always clearcut. Absolutely understand your position but my situation is that I gain from hedging so I don't like to be prescriptive, in fact when I don't hedge it turns into a loser.

*Tags: Strategies*

---

**AndyL** - *12:56:53*

Variance of no hedge can be very varied from strategy to strategy, the above is mild, and I've done some analysis with signal diversity and some no hedge  and am letting it run this month..

Other strategies/markets can be extreme variance with long period return to mean as it were.. these i think benefit from hedging more

*Tags: Strategies*

---

**Michael** - *16:53:14*

To be fair [@UEA14GBRR](@UEA14GBRR) I think you'll agree that yours is an unusual scenario in which your "closing" bet isn't really what you would normally mean by a "hedge". Taken in the general sense where hedging is done for stability of returns it's not controversial to say that it's negative ev (with a caveat to cover unusual bankroll limited scenarios).

*Tags: General Technical*

---

## 2022-11-10

**R** - *09:49:28*

This might be the wrong thread but I have a question around the liquidity of these markets.  I won't be coy, I'm looking at horses.  Is there a general sense of how much money we can get on for each bet type (~ 1-2 minutes before the event starts)?  e.g. win, place, forecast.  Even an order of magnitude estimate would be appreciated!  Will help me greatly in backtesting (I have built a custom system that I hope to port over to flumine soon)

*Tags: General Technical*

---

**liam** - *10:26:03*

you should split the problem in two, finding value and getting value, the latter is the hard part

*Tags: General Technical*

---

**R** - *10:33:19*

Thanks guys, for sure I can see that the majority is in the win/place.  I have found value in the sense of I can determine where to place bets; but yes now the issue is getting it (after commission!).



This might be a cheeky question, so feel free to slap me on the wrists.  I am torn between pure market based betting (i.e. just using price history) and fundamental handicapping.  The latter requires data, expensive data.  Any thoughts?

*Tags: Strategies*

---

**D C** - *12:53:40*

Depends on what you mean by expensive. I believe that proform is about £180 for an 8 week sub and I believe that you can export the entire database. £180 is less than I pay in AWS fees a month so I would say it is good value. I am quite interested in doing this myself as my IP returns seem to be getting smaller each month so if you do look into this and find that anything I've mentioned above is not true, please do let me know.

*Tags: Deployment*

---

**liam** - *13:27:49*

(in flumine)

*Tags: General Technical*

---

**Jonjonjon** - *13:47:58*

One of them did a good presentation on Greyhound Modelling. It's online somewhere.

*Tags: Strategies*

---

## 2022-11-18

**Craig Welch** - *09:56:24*

[@UPMUFSGCR](@UPMUFSGCR): “_One of them did a good presentation on Greyhound Modelling. It’s online somewhere_”



I’d be very interested to read that. Do you recall any clues that might help me find it ?



TIA

*Tags: Strategies*

---

## 2022-11-19

**Liam Querido** - *13:29:47*

What is the best way to mitigate execution risk? I have an arbitrage trading strategy that places three bets anytime an arbitrage opportunity exists. The problem comes when one of the bets cannot be matched, but the other two can be.



I have considered using the MatchMe feature and setting a liquidity filter to ensure that all my bets are matched, but is there a better method?

*Tags: Feature Engineering, Strategies*

---

**river_shah** - *13:34:46*

If your missed bets are approx 0 EV then don't worry about it and just think of it as added variance / part of normal operation of strategy

*Tags: Strategies*

---

**AP** - *19:35:56*

What's the most efficient way to retrieve matched orders on the market within the strategy class?

*Tags: Strategies*

---

**Liam Querido** - *20:44:01*

So there is no set strategy for dealing with this problem? Perhaps just constrain all bets to at least 0 EV in case one bet is unmatched. Again, however, this will require calculation of a probability of success p.

*Tags: Strategies*

---

## 2022-11-20

**liam** - *06:38:00*

Probably `blotter.strategy_orders(matched_only=True)`

*Tags: Strategies*

---

**James** - *14:56:00*

@R your comment reminded me of a recent interview with Patrick Veitch. To paraphrase, his favourite strategy is TFW - Timeform Wrong 

*Tags: Strategies*

---

## 2022-11-21

**Paul** - *22:57:06*

Prices definitely sometimes move in ways that tech analysis can help with. There are resistance levels (especially when hitting a level where the size of gaps between odds change), and some prices move in bands pre-off for periods before moving. I wouldn’t put you off exploring it on highly liquid markets, as I’ve seen things there that look promising. I don’t think some of the more exotic TA patterns are present, but then I don’t think they’re real in most financial markets either.

*Tags: General Technical*

---

## 2022-11-22

**river_shah** - *14:03:04*

Sorry if I am being very dense but why is it called Brier score? isn't it `mean_squared_error` applied to random variables bounded [0, 1]. Unrelated to the actual post, which I will go through. Thanks for posting

*Tags: Errors Debugging*

---

**R** - *14:04:30*

you might be right, the word "eigenvector" is involved so that tends to make people run screaming in terror :joy:

*Tags: Errors Debugging*

---

**Alessio** - *17:58:17*

I wonder if the right way to do this is to take a "good enough" model, start perturbing it, and then see which metrics are sensitive to it.

*Tags: Strategies*

---

**Dave** - *19:21:48*

Question for those running strategies with high passive top-of-book presence. Do you make any effort to remove your orders in historical data? Seems highly non-trivial but also highly important if you want to then launch your backtest on a period over which you ran in prod....

*Tags: Data Quality*

---

## 2022-11-23

**liam** - *08:55:26*

I was thinking about this yesterday when trying to test latencies, I couldn't work out how to even prove that any changes actually improved the accuracy so gave up

*Tags: General Technical*

---

## 2022-11-24

**R** - *21:00:01*

[@U040MT4RVTK](@U040MT4RVTK) you're my guy.  This is a lot of what I am leaning on.  Tools that work in the larger market should work here.  Just a matter of figuring out how to map the toolkit over.



On mean reversion, I can see that the first differences of returns is stationary time series... so momentum is on the cards.

*Tags: General Technical*

---

## 2022-11-25

**Dave** - *21:08:11*

I have to say, flumine backtest with passive orders is extremely accurate. nice one.

*Tags: General Technical*

---

## 2022-12-08

**Mo** - *09:17:12*

Yeah same with tennis. But I wonder if you have tried looking at the EV of these bets based on market implied probabilities? This is something I’m currently working on for something which is pretty similar which is trying to assess if a new strategy is long term profitable based on limited data

*Tags: Strategies*

---

**liam** - *09:18:22*

But what is the market implied probability?? (this strategy takes advantage of market volatility)

*Tags: Strategies*

---

**Unknown** - *09:24:55*

Here is 100% match, this is telling me that maybe I am the problem / the market doesn't have the capacity it once did

*Tags: General Technical*

---

**D C** - *09:45:59*

Pretty much all my inplay GPS based strats have been down significantly on last year performance. Still making enough to dodge returning to employment but its getting close enough that I have to keep thinking about it. I don't know how I would go about testing it but it 'feels' like the inplay market is sharpening up a bit with fewer opportunities.

*Tags: Performance*

---

**D C** - *09:55:46*

After some analysis I worked out that for 2 months I was causing self-inflicted damage so its hard to work out what profit would be without that. I've a small sample size to play with though as I only have 2.5 years of profitability to go on. Was last year very good, was this year very bad or is the market just getting more efficient? I've no idea where to start to answer that question.

*Tags: General Technical*

---

**liam** - *09:58:37*

Last year was insanely good, this year things have got tougher, feel free to start a new thread on this as I will continue my analysis on this strategy on this one and might get messy

*Tags: Strategies*

---

**Jorge** - *11:20:21*

[@UBS7QANF3](@UBS7QANF3) what price do you use to calculate implied_probability, knowing the market_book? This is a problem I am facing if the spread is big..

*Tags: General Technical*

---

**Newbie99** - *11:30:29*

Just going back to your earlier point about less size being matched, could it be anything more complex than a drop in discretionary spending due to increased fixed living costs (hence betfair volumes are down amongst recreational punters and a higher proportion of those remaining are more likely to only take/offer value prices)?

*Tags: Errors Debugging*

---

**PeterLe** - *11:45:49*

Yes what I mean by the reference to betangel was that there would be a number of uses using it that needed to get up to speed so there may have been som easier profits available initially. I have the TPD feed via betfair (using c#) and i also tested using betangel. I got different results using the same strategy criteria using my own program and betangel. Your correct about the frequency of the updates by the way.

*Tags: Performance, Strategies*

---

**liam** - *12:51:25*

[@UUE6E1LA1](@UUE6E1LA1) blue line is both, not sure what you mean tbh? This strategy works purely off market data with no TPD info

*Tags: Strategies*

---

**liam** - *13:02:28*

the idea of others using bflw/flumine to make money is what keeps the open source going for me

*Tags: General Technical*

---

**ShaunW** - *13:08:51*

100% understand. Helping others is rewarding in itself and often triggers new ideas or at the very least the need to prove what you do is in fact sound.   



Sorry for the thread drift there, but efficiency and competition are always a problem as exchanges mature.  Stark and sudden changes are a mystery though.

*Tags: General Technical*

---

## 2022-12-09

**Unknown** - *11:45:09*

I was just looking at my results from a strategy, graph looks similar to the one you posted this morning (although I didn't trade Clon)

Anyway it will be interesting to see how you get on with this Liam, it gets the old grey matter working if nothing else :grinning:

*Tags: Strategies*

---

## 2022-12-10

**river_shah** - *14:36:11*

[@U4H19D1D2](@U4H19D1D2) not directly related but when you calibrate / backtest with 100% simulated hit rates, do you get plausible / intuitive strategy parameters or does it tend to produce garbage?

*Tags: Strategies*

---

## 2022-12-11

**thambie1** - *15:01:09*

Both of the following options are strategy dependent. Won't always work.

1. Try to get actual results to line up with backtest results, then backtest on a larger sample.

2. Calculate EV in some manner. Based on bsp or closing odds.

*Tags: Strategies*

---

## 2022-12-16

**Unknown** - *09:09:31*

Day 7, graph showing the difference in ev based on the changes made, step change in the median ev I am now getting which is having a very positive impact on ROI/pnl.



Going back through git (pycharm makes this very easy) I can see that I did some AvB testing on adding the new strategies (now removed) and saw no impact, it seems the problem was that this was during the summer when liquidity was good.



Calculating the ev has seen me go full circle and started feeding it back into the model (ifs) that it uses that has opened up a few ideas...

*Tags: Strategies*

---

## 2022-12-23

**Mo** - *09:03:36*

Personally I think provided you are using an appropriate figure for your bet latency (and including the bet delay if appropriate) then you can assume if what you want is still on the order book then you get matched

*Tags: Performance*

---

**Liam Querido** - *09:04:50*

What is an appropriate figure for bet latency?

*Tags: Performance*

---

**Mo** - *09:06:12*

I believe these are the figures that flumine uses: [https://github.com/betcode-org/flumine/blob/9307dd5a0084c3583207921a521a1c39e430220f/flumine/config.py#L26-L29](https://github.com/betcode-org/flumine/blob/9307dd5a0084c3583207921a521a1c39e430220f/flumine/config.py#L26-L29)



Personally I think they're a little conservative but that's better than being overly optimistic

*Tags: General Technical*

---

**Liam Querido** - *09:20:56*

One other general question, what are some of the ways that a bet can be cancelled? Is it simply by someone else taking that price?

*Tags: General Technical*

---

**Michael** - *11:52:48*

Whats the best strategy to use to be able to collect data without losing too much (lowest lay?)

*Tags: Strategies*

---

**Peter** - *15:29:34*

Use the [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|market recorder](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|market recorder) to record and save the raw stream data.

*Tags: Data Quality*

---

## 2022-12-24

**Michael** - *01:01:42*

Thanks Peter, the guys have mentioned you need to bet a minimum amount of volume to avoid getting stopped from streaming raw data. So was hoping for any strategies which I can run which whilst may not make money, don't lose me a huge amount either as I collect data

*Tags: General Technical*

---

**D C** - *10:38:20*

I don't pay PC as I spent years losing money but I guess this means that even just a break even strategy on AUS racing is very useful to reduce it for those who do pay as well as keeping it at bay for those that don't?

*Tags: Strategies*

---

**Dave** - *11:47:54*

How to ensure your markets have extremely wide spreads 101: Impose terrible charges and no incentives for makers

*Tags: General Technical*

---

**Mo** - *11:48:34*

It’s not Betfair’s fault though is it? The Australian tax regime doesn’t understand the exchange model

*Tags: Strategies*

---

## 2022-12-30

**birchy** - *15:59:56*

[@U04380WMKL2](@U04380WMKL2) just FYI, Flumine simulation doesn't (afaik) handle double counting of liquidity but DOES give excellent results vs live for passive bets (i.e. not taking the best available prices).

*Tags: Deployment*

---

## 2023-01-03

**PeterLe** - *22:56:37*

Quick question please on using the 100% match (simulate full match)...when back testing...

If I know my true match rate (based on 30K real bets)...eg 36%..

I then run the back test on 13K markets and get £x on 100% match value

Is it a good measure to take 36% of X in the above scenario?

or is that too simple? thanks

*Tags: General Technical*

---

**Alessio** - *22:59:07*

Why wouldn't the backtest also implicity get a 36%, hence giving you the correct number? Mostly a maker strategy?

*Tags: Strategies*

---

**birchy** - *23:31:05*

[@UQL0QDEKA](@UQL0QDEKA) Personally, I use the "full match" P&amp;L purely as an indicator of whether the strategy is sensible, i.e. the best possible outcome, which is not achievable in the real world. The more interesting number is the actual simulated match P&amp;L as it can highlight adverse selection. For example, we can set the cancellation time to a bigger number to increase the match rate but that may decrease P&amp;L due to adverse selection.

For reference, I've just looked at a recent simulation which shows a 58% match rate (passive bets) but actual "matched" profit is equal to 46% of the "full match" P&amp;L after commission at 2%.

*Tags: Strategies*

---

**PeterLe** - *23:50:23*

hi Birchy, Thanks for that. Adverse selection is a big factor, so Im always cautious (maybe too much sometimes)...

For example, if I run the back test with a stake of x, I get 100% Match of £98K, actual sim match = £9.3k

If I use 4x then the resp values are £384K and £10.2K

So given that, I guess that the answer to my opening question is, maybe the 36% of 100% is a guide subject to 'realistic stakes'

I know from past experience that i should be fine up to 2x stakes, given where the odds are (ie they are not at 1000 or 1.01 etc) they are right in the thick of the action

Just  trying to move up one category on Mo's poll :grinning:

*Tags: General Technical*

---

**birchy** - *23:54:23*

Personally I like to keep the stakes low (£1 - £10 MAX) for simulations as that's more real world and then reassess stakes once I've got some live data.

*Tags: Deployment*

---

## 2023-01-12

**Unknown** - *10:34:41*

This doesn't look good, TPD has been a step change in making inplay more efficient..



To put in context the loss of 0.004 of ev results in a halving of profits for this particular strategy

*Tags: Strategies*

---

**liam** - *10:43:54*

Yeah I was contemplating rerunning my model code to try and improve it but I doubt it will make a difference, just seems to come down to more competition / more efficient markets. I was 25% down in 2022 compared to 2021 and tbh I would be happy to match 2022 in 23 but its not going to be easy / not sure I can be bothered :joy:

*Tags: Strategies*

---

**D C** - *10:49:02*

I don't know how to test it but it feels like the GPS markets are a lot more "static" now - especially in the early race stages. I find it hard to believe that this is down to BA users via manual trading - it feels like someone has a half decent model for ALL stages of the race now which it never felt like before. I've stopped trying to refine my GPS stuff now really just try to be more selective on entry points.

*Tags: Strategies*

---

**D C** - *10:56:15*

I suppose if you are a large player from a volume perspective thats a great tool at your disposal. It feels like a mystery to me as I will probably replay at least half of the GPS markets visually in sync with the price feed and the GPS still has a LOT of measurement error - particularly with motion perpendicular to the racing line. I've reached a point now where I spend my time looking at non-GPS IP strats and PE stuff now. Feels like the ROI (from a time / productivity perspective) on GPS stuff has dropped off a cliff for me.

*Tags: Errors Debugging*

---

**liam** - *13:10:23*

I have got to the point now that my GPS model can be used to calculate the accuracy / ev of my market based models

*Tags: Strategies*

---

**PeterLe** - *13:26:50*

This best describes what I found with TPD :

[https://en.wikipedia.org/wiki/Conjunction_fallacy](https://en.wikipedia.org/wiki/Conjunction_fallacy)

(Daniel Kahneman), The Linda problem. I found i was doing OK with it, but better without including it in the mix

*Tags: General Technical*

---

**D C** - *13:45:55*

Yeah but I think that is also dependent on exactly how you are using it. Sometimes these errors can be "removed" but we will all be biased from our own perspectives and how we are dealing with it.

*Tags: Errors Debugging*

---

**D C** - *16:06:19*

I'm not a flumine user but interested to know how many threads you have spinning in the background that are just to place bets. You must be spitting out a lot of bets if max stake is £4

*Tags: General Technical*

---

**D C** - *16:57:16*

Me too. It's really interesting to see difference in the scale of operating - in terms of bets and deployment. Still getting my head around 128 threads to be honest.

*Tags: Deployment*

---

**liam** - *17:10:39*

I might try and do a thread each day I am working on something, it has oddly helped me stay focussed today (it’s rare I am still being productive past 11am)

*Tags: General Technical*

---

## 2023-01-13

**R** - *17:33:58*

For the modelling / ML people in the crowd: curious if you guys use online / incremental learning?  I've been toying around with River ML ([https://riverml.xyz/0.14.0/](https://riverml.xyz/0.14.0/)) and find it kind of fun to play with models in an online / streaming setting.



Current toy project is modeling expected value using an online ML.  Very basic, linear models kind of stuff but interested to see who else is out there!

*Tags: Strategies*

---

**Alessio** - *18:42:13*

The main problem I see in online is understanding how to validate the model and see if it is drifting badly. It can end up with a lot of overhead.

*Tags: Strategies*

---

**Alessio** - *18:49:48*

Online is always non-better in a strict learning sense, because you see each item once, you cannot do multiple batch passes. What you get is the ability to react to something happening that changes what the model learnt so far. But the price you pay is that you cannot control if the model is royally screwed up (bad data, bad learning, etc..) manually, you get to do that with alerts and automation, which is where I see the overhead

*Tags: Strategies*

---

**R** - *18:56:36*

Agreed, the whole "catastrophic forgetting" concept is very relevant.  Funnily I've just found that a simple static rule is on-par for performance, maybe it eeks out a small edge but it's practically irrelevant.



I'll toy with it some more for shits 'n giggles though!

*Tags: Performance*

---

## 2023-01-14

**R** - *11:49:40*

Curve is pretty shallow imho.  Although I come from an AI background so maybe I'm biased!



Took me an hour or so to get a basic model running.

*Tags: Strategies*

---

## 2023-01-17

**richard** - *22:06:20*

Are there any resources that you'd recommend for someone interested in learning about how to trade sports markets?

*Tags: General Technical*

---

## 2023-01-18

**R** - *09:01:24*

Any general thread on odds slippage and how to mitigate?  I'm not seeing much of it in my live system right now but have some ideas that may cause it to become a problem.  Or is this a cheeky "don't leak your alpha" question :wink:

*Tags: Deployment*

---

**Newbie99** - *10:59:12*

There are a few academic papers that have been posted on here in the past and in all honesty this Slack is probably the best place to learn. If you have any specific questions (e.g. terminology and/or how to get started with Flumine etc. ask away and people are generally very helpful).



If you're completely new to this then be wary of some of the youtube content, a lot of it uses questionable logic (to put it politely) and is somewhat misleading with regards to the concept of 'value' and how it can be achieved (for example, videos might say bet early in a football match to maximise the chance of a goal being scored...whilst its true to say the probability of a goal being scored is greater over a longer timeframe in any given match, without data its impossible to know whether the odds correctly reflect the probability of a goal being scored, just because they are longer in match a than in match b, it does not guarantee value is on offer).

*Tags: Getting Started*

---

**D C** - *13:33:43*

It's a massive question and depends on what sport and whether it is inplay or not etc (although fundamentals will be similar). Also depends on what you know already. I'd echo what [@UFTBRB3F1](@UFTBRB3F1) says though - you'll learn nothing from 99.9% of youtube/social media and (possibly) a few useful bits and pieces from the other 0.1% but nothing game changing. Most online content is based around people selling something which will undoubtedly end up being (at best) so basic it is worthless or (at worst) so bad that you will lose money by trying to recreate it.

You'll find nothing better than this slack even if you have no desire to automate your betting.

*Tags: Strategies*

---

## 2023-01-20

**Niklas** - *09:12:58*

Hey [@U83SXMVN2](@U83SXMVN2), depends on what area you wanna learn in. There is this really good book from the MIT which talks about algorithms for decision making and multi agent problems. However it is quite technical and requires some basic understandings of machine learning and statistics

[https://algorithmsbook.com/#](https://algorithmsbook.com/#)

*Tags: General Technical*

---

## 2023-01-24

**Nicholas vizard** - *22:55:49*

Hey Dave,

I have the same question and was wondering if you had confirmed if the total_matched attributes represented the cumulative sum of back stakes only placed up until that point?



Cheers for any help you can provide!

*Tags: General Technical*

---

## 2023-02-08

**PeterLe** - *09:32:06*

Morning. If you have a number of strategies running on the same system and the same betfair account...How do you set the exposures for each strategy independently.

I currently have this code in a single strategy:

```strategy = Sub6(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"], # was 7 for horses, 4339 for greyhounds, 15 = todays card greyhounds

        country_codes=["GB","IE"],

        market_types=["WIN"],

    ),```

Would I change it to something like this:

```strategy = Sub6(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"], # was 7 for horses, 4339 for greyhounds, 15 = todays card greyhounds

        country_codes=["GB","IE"],

        market_types=["WIN"],

        max_order_exposure=5,

        max_selection_exposure=20,

        max_trade_count=5,

        max_live_trade_count=1

    ),```

So can the exposures be set in the strategy itself rather than in Strategy.py Sorry only basic python skills so i might be talking $%^&amp; Thanks :grinning:

*Tags: Deployment, Strategies*

---

**Peter** - *10:04:14*

A little confused by the question. Is your strategy not inside your Strategy.py file, or were you referring to the Fumine strategy.py file where BaseStrategy is defined (and which you shouldn't touch). Either way, what you're doing is the right way to go about it.



It's worth playing about a little with these parameters to get a feel for how they work, individually and together, It's super easy to nix a strategy by accidentally setting them too tight, especially when laying. E.g. after recognised prices have been set and stakes have been rounded to 2 decimal places, is there any risk of the actual order exposure could be 5.01?

*Tags: Strategies*

---

**PeterLe** - *12:27:51*

Thanks for the reply Peter. My strategies are standalone ie not inside strategy.py (where BaseStrategy is defined). OK I think I understand what you are saying here. Thanks for clarifying that.

Just as a belt/braces approach, Ive also set the individual betfair account exposures via the master account too, just to be on the safe side. Appreciate the reply thanks

*Tags: Strategies*

---

**Peter** - *17:56:57*

Oops [@UQL0QDEKA](@UQL0QDEKA). Just spotted that you have the parameters in the wrong place. They're parameters for the strategy rather than the market_filter, so your code should be:



```strategy = Sub6(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"], # was 7 for horses, 4339 for greyhounds, 15 = todays card greyhounds

        country_codes=["GB","IE"],

        market_types=["WIN"],

    ),

    max_order_exposure=5,

    max_selection_exposure=20,

    max_trade_count=5,

    max_live_trade_count=1

)```

Apologies for not picking this up first time around.

*Tags: Getting Started, Deployment, Strategies*

---

## 2023-02-12

**PeterLe** - *12:58:54*

OK Just found this...As I say it may well be out of date as I only tend to use 7: (It looks like  Horseracing Todays Card use to be 13?



public enum EventTypeIdEnum

    {

        [Description("Soccer")]

        Soccer = 1,

        [Description("Tennis")]

        Tennis = 2,

        [Description("Golf")]

        Golf = 3,

        [Description("Cricket")]

        Cricket = 4,

        [Description("Rugby Union")]

        RugbyUnion = 5,

        [Description("Boxing")]

        Boxing = 6,

        [Description("Horse Racing")]

        HorseRacing = 7,

        [Description("Motor Sport")]

        MotorSport = 8,

        [Description("Special Bets")]

        SpecialBets = 10,

        [Description("Cycling")]

        Cycling = 11,

        [Description("Rowing")]

        Rowing = 12,

        [Description("HorseRacing Todays Card")]

        HorseRacingTodaysCard = 13,

        [Description("Soccer Fixtures")]

        SoccerFixtures = 14,

        [Description("Greyhound Todays Card")]

        GreyhoundTodaysCard = 15,

        [Description("Rugby League")]

        RugbyLeague = 1477,

        [Description("Darts")]

        Darts = 3503,

        [Description("Athletics")]

        Athletics = 3988,

        [Description("Greyhound Racing")]

        GreyhoundRacing = 4339,

        [Description("Financial Bets")]

        FinancialBets = 6231,

        [Description("Snooker")]

        Snooker = 6422,

        [Description("American Football")]

        AmericanFootball = 6423,

        [Description("Baseball")]

        Baseball = 7511,

        [Description("Basketball")]

        Basketball = 7522,

        [Description("Hockey")]

        Hockey = 7523,

        [Description("Ice Hockey")]

        IceHockey = 7524,

        [Description("Sumo Wrestling")]

        SumoWrestling = 7525,

        [Description("Australian Rules")]

        AustralianRules = 61420,

        [Description("Gaelic Football")]

        GaelicFootball = 66598,

        [Description("Hurling")]

        Hurling = 66599,

        [Description("Pool")]

        Pool = 72382,

        [Description("Chess")]

        Chess = 136332,

        [Description("Trotting")]

        Trotting = 256284,

        [Description("Commonwealth Games")]

        CommonwealthGames = 300000,

        [Description("Poker")]

        Poker = 315220,

        [Description("Winter Sports")]

        WinterSports = 451485,

        [Description("Handball")]

        Handball = 468328,

        [Description("Badminton")]

        Badminton = 627555,

        [Description("International Rules")]

        InternationalRules = 678378,

        [Description("Bridge")]

        Bridge = 982477,

        [Description("Volleyball")]

        Volleyball = 998917,

        [Description("Bowls")]

        Bowls = 998919,

        [Description("Floorball")]

        Floorball = 998920,

        [Description("Netball")]

        Netball = 606611,

        [Description("Yachting")]

        Yachting = 998916,

        [Description("Swimming")]

        Swimming = 620576,

        [Description("Exchange Poker")]

        ExchangePoker = 1444073,

        [Description("Backgammon")]

        Backgammon = 1938544,

        [Description("GAA Sports")]

        GAASports = 2030972,

        [Description("Gaelic Games")]

        GaelicGames = 2152880,

        [Description("International Markets")]

        InternationalMarkets = 2264869,

        [Description("Politics")]

        Politics = 2378961

*Tags: Errors Debugging*

---

## 2023-02-20

**D C** - *15:39:40*

In general I mean, not in this specific example (I've never tried flumine out although I probably should)

*Tags: General Technical*

---

**birchy** - *20:07:41*

I've found the same as [@UQL0QDEKA](@UQL0QDEKA) and no longer bin strategies that are only slightly +EV in simulation. [@U4H19D1D2](@U4H19D1D2) I've had quite a few similar graphs where I've analysed bet logs and then trimmed out some "poor" bets which simulation shows an improvement but real-world shows a downturn. My conclusions are:

1. The strategy is already optimal for the available £

2. I've inadvertently overfitted

*Tags: Strategies*

---

## 2023-02-21

**D C** - *08:56:22*

So for those of you who do in depth simulations, how far do you go? Specifically, in the case of multiple strategies, how do you simulate the interaction between them if you have multiple strategies performing on the same markets? Does the flumine framework just deal with all of this?

*Tags: General Technical*

---

**liam** - *08:57:39*

It can do it on passive orders with [https://github.com/betcode-org/flumine/blob/9dc0bf01ac211f925c49fcc05a8617e0c2a61b5b/flumine/config.py#L5|this](https://github.com/betcode-org/flumine/blob/9dc0bf01ac211f925c49fcc05a8617e0c2a61b5b/flumine/config.py#L5|this) however its accuracy is very questionable on low liquid markets or when you really start pushing things it all falls apart

*Tags: General Technical*

---

**D C** - *09:00:29*

So you can line up multiple strategies to simulate and it will attempt to run them all concurrently? I might have to seriously look at flumine as I am sure that I am getting into a muddle at times in the market when multiple strats are trying to place orders on the same runners at the same times.

*Tags: General Technical*

---

**liam** - *09:01:32*

So probably doesn't solve your problem

*Tags: General Technical*

---

## 2023-03-05

**Dave** - *15:44:38*

Question regarding strategy calibration: how often do you tend to recalibrate your strategy parameters, e.g. refresh the thresholds of your triggers etc? For my football betting I have always been able to keep my thresholds the same, but retrain my ML models periodically, for a few years now. Never really been an issue. For racing (which I've only been doing at scale since last Nov), I have no model, just reacting to observations in market data (so the model ends up being a bunch of handcrafted thresholds). I have no sense yet as to how quickly my calibrated values may become too stale. Does anyone do anything as basic as this, and if so, how do you keep your thresholds/parameters up to date? Run some periodic optimization over a grid of possible thresholds?

*Tags: Performance, Strategies*

---

## 2023-03-08

**birchy** - *10:22:14*

[@U0128E7BEHW](@U0128E7BEHW) I use market data only which is essentially based on "abnormal" activity, so other than occasionally rechecking what a "normal" market is, I don't change parameters/signals/etc. In fact, I have one particular strategy that has been ticking along for 10+ years and every time I've attempted to optimise it has a detrimental effect. :grinning:

*Tags: Strategies*

---

**Dave** - *19:01:22*

Fair enough - sounds reasonable. Obviously "how quickly thresholds go stale" is a very "how long is a piece of string" question but I was curious to see what recalibration regimes people tend to use. Easiest way to answer this question though is to just backtest over years and look for staleness, but just lazy to backtest that far :smile:

*Tags: General Technical*

---

**liam** - *19:48:42*

I think it’s more important to try and understand what impacts a strategy pnl.



I find that optimising model variables / thresholds in an ideal world to be be the easy part, the problem is the market is not ideal. You can see this by comparing a model with 100% matching vs real world, the optimum will be different. 



To answer your question I think it can certainly vary per market type, racing is a nightmare as you have the slow decay mixed with the change in ‘seasons’. I don’t think I have ever had a strategy increase it’s profit/ROI without an iterative AvB type development to see where the limits are. You can’t do this purely through simulation, sometimes involves bleeding some cash. I have mentioned this before and I think there was a poll but I spend 80% of my time on execution/matching (which feeds into threshold and model optimization) 

*Tags: Performance, Strategies*

---

**Dave** - *19:54:16*

Sure, and tbh I have experienced just that. For racing, my initial thresholds were picked from backtest but I've had to tune down the aggressiveness based on real trading. I was concerned if the tuning based on real trades was me adapting to seasonality (and therefore might have to tune again in a month or two to pick up spring racing etc) OR just me adapting to the differences in simulation Vs reality (and therefore I've now hit my stable state which shouldn't go stale). For football on the other hand, for years I've kept my execution the same (minus some moderate tweaking), but my black-box models get retrained every few months to pick up changes in league stage etc.

*Tags: Strategies*

---

**Dave** - *19:55:57*

I wish I had a model for racing instead and just said "always trade at my modeled price + some markup", which is way easier to manage. But alas, my triggers are just based on observed values in market data for now + some hypothesis as to how the impact future price.

*Tags: Strategies*

---

**Dave** - *19:57:20*

With that being said...flumine simulation is fab :clap:

*Tags: General Technical*

---

**Dave** - *20:06:24*

If I were you, I'd ask BF for a cut from the comission paid for every winning aggressive bet submitted by a flumine instance.

*Tags: General Technical*

---

## 2023-03-16

**birchy** - *07:48:08*

Just to clarify I'm using this correctly, does the following create a new market stream for each strategy or does/can it share one stream because the filters are the same?

`strategy1 = Horses1(`

    `market_filter=filters.streaming_market_filter(

        event_type_ids=['7'],

        market_types=['WIN'],

        country_codes=['GB', 'IE'],

    ),

)

framework.add_strategy(strategy1)`



`strategy2 = Horses2(

    market_filter=filters.streaming_market_filter(

        event_type_ids=['7'],

        market_types=['WIN'],

        country_codes=['GB', 'IE'],

    ),

)

framework.add_strategy(strategy2)`

*Tags: Strategies*

---

**Andrew** - *08:54:41*

A somewhat related question. Is market context shared between strategies? I can see from the code that strategy context is unique per strategy. Is the same market instance passed to both strategies in the above example?

*Tags: Strategies*

---

**birchy** - *13:24:13*

So that's one market stream per framework? If we want to add more event types (e.g. greyhounds), I presume it's preferable to set `event_type_ids=['7', '4339']` and then filter at strategy level? Also is it best practice to reuse a filter instance? e.g:



 `mf=filters.streaming_market_filter(

    event_type_ids=['7', '4339'],

    market_types=['WIN'],

    country_codes=['GB', 'IE'],

)`

`strategy1 = Horses1(market_filter=mf)

strategy2 = Horses2(market_filter=mf)`

*Tags: Strategies*

---

## 2023-03-20

**George** - *17:49:09*

is anyone using flumine for market-making strategies where you are sitting on both the back and lay continously? or would it be fair to say that flumine is really more built for implementing discrete trigger-based strategies.

*Tags: General Technical*

---

**George** - *18:21:29*

do you find that there is enough in flumine for a market making strategy that relies a lot on tracking existing orders and liabilities, both live and executed? or did you have to add to flumine to get what you needed in that respect?

*Tags: Deployment, Strategies*

---

## 2023-03-21

**Paul** - *18:41:29*

You’ll need to build a market making model. Just constantly adding to either side of the book without considering how a limit order book works, could land you in some hot water. At the last meetup, Matthew Trenhaile suggested market making on BF meant you were in a constant losing battle to 2-3 people who know more than you, but I’m not sure that’s true.

*Tags: Strategies*

---

**George** - *19:06:10*

It goes back to my question, really. In my view it's all about being extremely vigilant about your existing orders and liabilities, and using that to influence which orders you want to send to the market. Hence why I was wondering whether flumine is really set up for that or not. I am not sure that it is.

*Tags: General Technical*

---

**Paul** - *19:09:21*

The blotter is there, and there is example middleware to populate on restart. There are methods for showing your current p&amp;l on a selection. Processing the existing orders and the market book are explicit methods. I am concerned about how rapidly market book updates are processed but that’s likely a problem in my code - api limits seem to prevent HFT strategies anyway - but if you can explain what you’re after without leaking alpha, perhaps people can help with ideas or explain corners of flumine you haven’t seen

*Tags: General Technical*

---

**George** - *19:12:16*

that's easy as I don't have any alpha to leak yet :smile: just some ideas!

i'm going to see if I can get a skeleton strategy coded up this week and play around with flumine to see if it has what I am looking for. if I can't find it probably someone helpful will point me in the right direction. what a great group!

*Tags: Strategies*

---

## 2023-03-22

**George** - *17:27:55*

In flumine, is there a simple way to look at the market book but with my own executable orders deducted from it? Or is this something that the 'user' has to build themself?

*Tags: General Technical*

---

## 2023-03-25

**R** - *19:21:58*

The general idea behind Benter's work still stands today.  Financial (quant) literature talks of two types of probabilities: P &amp; Q.



P is generated from fundamental models (e.g. logit models based on data)

Q is generated from market information (e.g. implied probabilities)



In general, a final probability is generated as a weighted combination of P&amp;Q.



Benter implemented this and attached it to a kelly system.   It worked.  Very effectively.



The beauty of this approach is that it rests on one single question: how well can you model the price of a horse?



If you can do that well, kelly is your man.  Otherwise you better figure out something else.

*Tags: Strategies*

---

## 2023-03-26

**birchy** - *14:25:44*

As a complete noob to ML, etc, I'm playing around with sklearn and LogisticRegression with a target variable of runner win/lose based on market data generated signals. I'm trying to understand the metrics, which are as follows:

`Accuracy: 0.8346274074821014

Precision: 0.8353535353535354

Recall: 0.998792270531401

F1-score: 0.9097909790979097

MSE: 0.16537259251789857`



On paper, it looks like a reasonable model, however the implied probability of my matched prices in the dataset is `~0.84`. If I understand correctly, I would need to get to an `accuracy` score &gt; matched implied probability?

Edit: to add some further context, my dataset is a betting history with ~50K bets on 1000 markets and i'm using an 80/20 split.

*Tags: Strategies*

---

**R** - *14:51:43*

also, in case you didn't spot it there is a class called SGDClassifier ([https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html))



This is a nice gradient based method for various things, logistic regression being one of them.  Allows incremental learning too.



I've found it to be a bit better than the LogisticRegression class (faster too IMO)

*Tags: Strategies*

---

**birchy** - *16:09:23*

Thanks [@U04AD6RAGGH](@U04AD6RAGGH). So my features are signals generated from changes in price, volume, etc. The strategy is inplay horses, so my main concern is that it may be identifying that 1.01=winner, even though that's not always the case...

Will look at SGD. I also have plans to use a similar model for predicting price movement, i.e. the binary target being `price_now &lt; price_previously`. Any advice on that?

*Tags: Feature Engineering, Strategies*

---

**Peter** - *16:48:22*

I'm rather worried by your recall stat. It implies that you have have hardly any false negatives, which isn't really realistic with the sort of data we work with. I'm wondering whether you have an imbalanced class issue. Have you built a [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html|confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html|confusion matrix) for your model's predictions?

*Tags: Strategies*

---

**R** - *17:40:24*

as a quick way to get started, try something like PyCaret

 [https://pycaret.readthedocs.io/en/stable/index.html](https://pycaret.readthedocs.io/en/stable/index.html)



It'll give you exposure to a lot of different models, preprocessing steps, and visualizations like confusion matrices.  Great for quick model experimentation

*Tags: Strategies*

---

**R** - *17:43:47*

One thing though, aside from all of these metrics: [http://nicolas.kruchten.com/content/2016/01/ml-meets-economics/](http://nicolas.kruchten.com/content/2016/01/ml-meets-economics/)



Pay attention to your metrics, but also pay attention to what constitutes "good".  Above is a great article on when a higher number isn't always best.



This is why measuring the Expected Value of your model is crucial.  Accuracy is great, Value is better :slightly_smiling_face:

*Tags: Strategies*

---

**birchy** - *17:46:52*

Just for fun, chat GPT came up with this:

`import pandas as pd

from hmmlearn import hmm



# Load data into pandas dataframe

df = pd.read_csv("market_data.csv")



# Define the state space

states = ["increase", "decrease", "same"]



# Define the observation space

observations = df[["bid_price", "ask_price"]].values



# Define the transition probabilities

trans_mat = [[0.7, 0.2, 0.1],

             [0.3, 0.5, 0.2],

             [0.2, 0.3, 0.5]]



# Define the emission probabilities

emissionprob_mat = [[0.2, 0.4, 0.4],

                    [0.4, 0.3, 0.3],

                    [0.3, 0.3, 0.4],

                    [0.4, 0.4, 0.2]]



# Create the HMM model

model = hmm.MultinomialHMM(n_components=3)

model.startprob_ = [0.3, 0.4, 0.3]

model.transmat_ = trans_mat

model.emissionprob_ = emissionprob_mat



# Fit the model to the data

model.fit(observations)



# Make predictions

prediction = model.predict(df[["bid_price", "ask_price"]].values)

predicted_states = [states[state] for state in prediction]

df["predicted_states"] = predicted_states



# Print the predictions

print(df["predicted_states"])`



*Tags: Feature Engineering, Strategies*

---

**Peter** - *17:57:37*

[@U016TGY3676](@U016TGY3676) We need to be a bit careful about looking for expected or "normal" values in the stats. They will depend a lot on how noisy the data is. Moreover, they're just one part of some subtle evaluations.



To illustrate, imagine a UK greyhound model that predicts that every dog in a six dog race will lose (there are some poor tutorials out there that actually build models that do this!). the model is pants, but it's also accurate 83.33% of the time. On the other hand a model with some genuine predictive power may have an accuracy down in the 70s but yield better results when used to place actual bets. So I agree with [@U04AD6RAGGH](@U04AD6RAGGH) that calculating expected returns can be a useful metric for deciding whether you have a viable model.

*Tags: Strategies*

---

**birchy** - *19:17:50*

Going back to my opening post, the model I was using:



`from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error`



`# Convert the target variable to binary (0 for lose, 1 for win)

df['winner'] = df['profit'].apply(lambda x: 1 if x &gt; 0 else 0)`



`# Split data into training and test sets

features = ['sig1', 'sig2', 'sig3']`

`X_train, X_test, y_train, y_test = train_test_split(df[features], df['winner'], test_size=0.2)`



`# Instantiate a logistic regression model

model = LogisticRegression()`



`# Fit the model to the training data

model.fit(X_train, y_train)`



`# Use the model to make predictions on the test data

y_pred = model.predict(X_test)`



`# Evaluate the model's performance using appropriate metrics

accuracy = accuracy_score(y_test, y_pred)

precision = precision_score(y_test, y_pred)

recall = recall_score(y_test, y_pred)

f1 = f1_score(y_test, y_pred)

mse = mean_squared_error(y_test, y_pred)

print(f"Accuracy: {accuracy}")

print(f"Precision: {precision}")

print(f"Recall: {recall}")

print(f"F1-score: {f1}")

print(f"MSE: {mse}")`



Bearing in mind that the dataset is a bet history (BACK bets only) where all unmatched bets have been removed and 'profit' is a real-world number, would this not achieve the same result as calculating EV for training? My gut feeling is that the model is shyte, or at least the data I'm using is too optimised due to being pre-filtered by the strategy that generated it. Thoughts?

*Tags: Errors Debugging, Feature Engineering, Performance, Strategies*

---

**ShaunW** - *19:34:21*

Have you tried asking ChatGPT?  I gave it your original question and got...

*Tags: General Technical*

---

**ShaunW** - *19:34:25*

Your understanding is correct. The implied probability of the matched prices in your dataset is approximately 0.84, which means that, on average, the market believes there is an 84% chance of the runner winning. To beat the market, your model's accuracy should be higher than this implied probability.

In your case, the accuracy of the LogisticRegression model is 0.8346, which is close to the market's implied probability but not quite above it. This means that, although your model appears to be performing reasonably well, it is not yet outperforming the market.

To improve your model's performance, you can try several approaches:

1. Feature Engineering: Create new features or transform existing features to better capture the underlying patterns in the data.

2. Feature Selection: Remove irrelevant or noisy features to reduce overfitting and improve model generalization.

3. Hyperparameter Tuning: Optimize the hyperparameters of your Logistic Regression model, such as regularization strength (C) or using different solvers.

4. Ensemble Methods: Combine multiple models, like decision trees or other classifiers, to improve overall performance.

5. Experiment with different algorithms: Try other machine learning algorithms that might better capture the patterns in your dataset.

Remember to also take into account the Precision, Recall, and F1-score of your model, as they provide additional insights into its performance. For instance, your model's high Recall (0.9988) and lower Precision (0.8354) indicate that it is good at identifying winning bets but may also produce many false positives, which can lead to losses. Balancing these metrics can help you create a more effective betting strategy.

*Tags: Feature Engineering, Performance, Strategies*

---

**R** - *19:41:37*

also yes, hyperparameter optimization and cross-validate the shit out of everything.

*Tags: Performance, Strategies*

---

**R** - *21:06:45*

So feature scaling applies to logistic regression only when you are interested in regularizing the model somehow (i.e. L1 and L2 terms are non-zero).  In most cases we want to regularize to improve generalization (reduce overfitting).



I would try to just convert everything to implied probabilities and that will act as a scaling transform.  If there are features that you can't do this to (like volume perhaps) then standardize them or play around with some scaling methods to see what works best (e.g. normalization and minmax scaling can also work)



quick and dirty method is to minmax scale things and go from there.

*Tags: Feature Engineering, Strategies*

---

## 2023-03-27

**birchy** - *13:03:31*

I now have some quick n dirty signals as a proof of concept and running against a complete dataset generated by Flumine simulation where the strategy backs every runner for £1 at LTP on every update. Results appear to be more realistic, particularly the recall stat [@U9JHLMZB4](@U9JHLMZB4) mentioned:



`Accuracy: 0.8649507394792786

Precision: 0.6514084507042254

Recall: 0.34230002846569885

F1-score: 0.4487777570442247

MSE: 0.13504926052072141

LogLoss: 4.867668736665466`



Using irrelevant features such as market_id and selection_id give poor results as expected. Surprised that the Accuracy is still quite high though. Thoughts?

*Tags: Feature Engineering, Strategies*

---

**R** - *15:54:50*

Per [@U9JHLMZB4](@U9JHLMZB4)’s comment above, your accuracy can be high when predicting every runner will lose.  In a 10 runner race, you'd have 90% accuracy!  So we have to take a deeper look at things.  Usually you can start at precision and recall:



Precision is related to False Positives

Recall is related to False Negatives



In this case, False Positives can lose you money while False Negatives will miss trades.  So:



low Recall = Lots of missed trades

low Precision = lots of bad trades



From your numbers, you are missing more trades than you are losing; which is better than the other way around.



Not surprised that those ID features don't work, they are not "numbers" in the sense that a logistic model can deal with them.  They are more like categorical variables.  It's like trying to numerically model a word, it can be done, but needs some preprocessing.



I'm curious: is there one trade per event you are looking for or multiple signals simultaneously?  Trying to understand if you have a class imbalance problem.



Also, use cross-validation and calibration : [https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html)

*Tags: Feature Engineering, Strategies*

---

**birchy** - *17:22:21*

[@U04AD6RAGGH](@U04AD6RAGGH) good point on the IDs, I overlooked that. As for the signals, they're calculated on every market update and in the above example, only saved to the CSV for analysis &amp; modelling. As such, the "bets" are only placed to populate the CSV. I have a handful of signals at present and trying various combinations to see which ones are good/bad. This seems like a sensible approach with my current understanding?



Will look at validation and calibration.

*Tags: Strategies*

---

**birchy** - *19:35:40*

Thanks for your help [@U04AD6RAGGH](@U04AD6RAGGH), it's very much appreciated. I had a quick play with CalibratedClassifierCV but it seems to produce very similar results to my basic LogisticRegression. Is that to be expected? My implementation is:



`#model = LogisticRegression()

model = CalibratedClassifierCV(

    estimator=LogisticRegression(),

    method='sigmoid'

#    method='isotonic'

)`

I believe it to be correct? At this stage, I'm starting to steer towards one of the auto ml libraries rather than manually trying the infinite number of combinations.

*Tags: Strategies*

---

**R** - *20:21:25*

Usually automl wont help all that much until there is a clear hypothesis to be tested.  E.G. "I think that features &lt;x, y, z&gt; will have effect &lt;e&gt;" and then I can go and test for that.  Otherwise it's like throwing shit at a wall to see what sticks... a method I am also familiar with XD

*Tags: Feature Engineering*

---

## 2023-04-11

**Andy B** - *22:35:18*

I have 14 months worth of Aus greyhound markets in a folder for back testing, but I was wondering if there was a way to tell flumine to only use a specific date range when you want to only do a quick test of a smaller subset?  Or, is the standard approach to simply put the markets you need into a folder and test against that?

*Tags: General Technical*

---

## 2023-04-12

**liam** - *06:47:54*

Lookup table in database -&gt; filter -&gt; list of id’s -&gt; process 

*Tags: General Technical*

---

## 2023-05-06

**Jonjonjon** - *21:41:24*

From within Flumine, I would like to cancel an order, and then place another order if the cancellation is successful.



What patterns do people use to implement this?



Market.cancel_orders only returns a boolean,.so my current implementation involves waiting for subsequent calls process_market_book and then checking pending cancellations for updates.



I've been reading about Asyncio but can't work out if that will help or not.

*Tags: General Technical*

---

## 2023-05-13

**river_shah** - *07:31:20*

Much more depending on what your strategy is doing. 10K races is a minimum to avoid all kinds of data and backtest snooping issues.

*Tags: Strategies*

---

**river_shah** - *07:44:19*

This is very much dependent on what your strategy is doing and what your modelling process is. If you want your strategy to pick up on weak long shot signals, need to throw more data at the problem. 10K is a heuristic that is very feasible computationally even on a laptop with flumine

*Tags: Strategies*

---

## 2023-05-15

**Paul** - *17:04:13*

I think if your models are focused on fundamentals, smaller with reference to BSP like Mo suggested is a great indicator and you can have strong confidence on a smaller sample size. If price action based, you might need 10k+ examples, but you could have tens or hundreds of examples per event. However, I think it’s worth pointing out that regardless there are not many 2yo 5f events, or 4m Hcap Chase events, or races with pre-off fav below 1.2 or higher than 7.0, and the fundamentals and price action - even pre-event - are going change over all the spaces in between. I think I’d want a large enough sample space across all those dimensions, and so you get closer to 10k events - about 2.5 years of racing.

*Tags: Strategies*

---

**Paul** - *17:05:18*

However I might just be talking through my pocket - I’ve not got that much data yet, so that’s the reason my models aren’t profitable, it can’t be my fault, surely? :joy: 

*Tags: Strategies*

---

## 2023-05-17

**Alex** - *02:27:32*

It also depends on the number of bets placed, i.e. are you closing out bets half way through the race, then placing new bets? With 1000 datapoints your estimated strategy performance will have larger error than with 10000. Overfitting might be an issued based on how many parameters your model has (you'll likely be fine if you just have a few). You can try to bootstrap standard errors of your strategy to get an idea of how significant your results are.

*Tags: Errors Debugging, Performance, Strategies*

---

## 2023-05-22

**James** - *12:25:15*

Is there a “best” way of mitigating risk when using highly correlated strategies, ie those whose selections overlap? Eg  80% of selections in strategy B are also selections in strategy A (albeit placed by auto instead of manually and at a difference time). Likewise 60% of selections in strategy C are also picked in strategy A. Capping the max exposure per selection to a fixed amount is one option but I wondered whether there is a better way. Thanks

*Tags: Errors Debugging, Strategies*

---

**Andrew** - *13:34:21*

A simple method could be to split bank across strategies in a fixed but not necessarily equal proportion. Borrowing from security selection and position sizing in finance you could modify stake size by confidence of selection. I don’t apply anything like this, rather thinking from my day job in investment management.

*Tags: Errors Debugging*

---

**Michael** - *17:42:55*

Sorry - horseracing. I didn’t know about rpscrape actually so thats massively helpful vs writing my own! Thanks!

*Tags: General Technical*

---

## 2023-05-23

**river_shah** - *12:09:19*

Sorry if this is a digression and over engineered for your use case but as you say "best", the answer is in mean variance optimization. The mathematically grounded way is that you come up with strategy expectations of returns and covariances and then blend all individual strategies into a meta strategy and trade that. The meta strategy assigns positions limits to ensure you don't blow through market capacity.

*Tags: Performance, Strategies*

---

## 2023-05-30

**Unknown** - *15:09:46*

I have this screenshot from last year which might help

*Tags: General Technical*

---

**AndyL** - *19:55:12*

So let me see, that's telling me the most common latency in executing a cancel is about 50ms ?

*Tags: Performance*

---

## 2023-06-06

**liam** - *13:43:59*

I have found a higher EV can equal higher ROI (as expected) but as you are implying only when the EV is accurate, it tends to highlight model inefficiencies as well

*Tags: Strategies*

---

**Aaron Smith** - *13:58:00*

I think we maybe need to differentiate between 2 ways an EV can be inaccurate:

1. pnl/EV converges towards 1 over time, but slowly.

2. pnl/EV does not converge towards 1

For example, if i have a strategy that is equally likely to back or lay a runner, independent on the odds, then i could calculate an EV by just comparing the odds i took to 1/n (basically using 1/n where n = number of runners instead of some kind of final price). This EV would obviously be super inaccurate, but still correct in the sense that pnl/EV would over time still converge to 1, just super slowly.



I m more interested in case 2. Suppose we take final prices (at schedueled off) to calc our EV, can you come up with any reason pnl/EV would not converge towards 1 over time? Or have you had strategies that showed amazing EV (calculated from actual bets, not simulation), but you disregarded them based on actual pnl? If so, did you find any reason on how the EV came to be inflated?

*Tags: Performance, Strategies*

---

**D C** - *14:21:37*

Surely you're never going to be able to identify inaccurate prices on individual case basis. You can only really assess performance of probability estimates on a large scale (regardless of whether BSP / odds at off etc). We can get away with it on theoretical idealisation of things like fair coins/dice etc, but you can surely never know that an even money closing price favourite should really have gone off at 10/11 ??? And then you've got stuff like race/match fixing and inside info to contend with. Surely its all a lost cause trying to work out when a particilar off price is "bad" ?

*Tags: Errors Debugging, Performance*

---

**Aaron Smith** - *14:59:53*

nothing of the kind, its about discussing reasons for why pnl/EV (pnl divided by EV) may not converge to 1 (if that is possible), where pnl is regarding a strategy, not a single bet, and EV is calculated based on placed bets in the past by that same strategy and calculated by using final market prices

*Tags: Strategies*

---

## 2023-06-07

**liam** - *08:12:19*

To answer your question proof is in the profit for me, EV is simply a signal to how I work

*Tags: General Technical*

---

**Mo** - *12:20:51*

I'm not (yet) convinced by your argument. You seem to be saying that provided there is no systematic bias in your probabilities compared to the market then you can use whatever probability you want for the EV calculation. It seems contrived to think there is no systematic bias in your probabilities compared to the market and, in fact, in all likelihood there must be some for you to identify a mispricing



Example situation where the EV could be misleading:



1. Your strategy backs long price horses

2. You calculate EV using market implied probabilities calculated from BSP using the standard overround removal method

3. Your overround removal method has failed to account for favourite longshot bias, the market implied probabilities are too high and your EV is an overestimate

*Tags: Errors Debugging, Strategies*

---

**Aaron Smith** - *12:44:11*

[@UBS7QANF3](@UBS7QANF3) you are correct that assuming that there is no systematic bias in the bet selection of any strategy would be a questionable claim to make without any further proof given. However, i didnt claim this was true for any of my strategies. I merely constructed such a strategy for the purpose of showing an example of an EV calculation that may (for this particular strategy) be accurate in the sense that pnl/EV would eventually converge towards 1 over time, but would still be inaccurate in the sense that this could be a very slow process and there are obviously better ways to calculate an EV.

*Tags: Performance, Strategies*

---

## 2023-06-08

**R** - *14:15:37*

n00b question here.



I've built and maintained my own backtesting / simulation framework but it's not as well refined as Flumine, so I'm in the process of figuring out how to work with flumine.



One thing I have is a bunch of historical market data (PRO data from betfair exchange).  I'm running the `SimulatedClient` along with the `FlumineSimulation` framework.  Things are going well except that I get `Market is not open` when running things.  Is there a flag to be set to ignore this and just run the strategy with simulated returns / execution or am I completely misunderstanding things?

*Tags: Strategies*

---

**George** - *14:21:59*

I had this yesterday and resolved it by using

`if market.status == 'OPEN':`

    `do stuff`

*Tags: General Technical*

---

**R** - *14:23:25*

sweet, thanks George.  I was about to code this exact thing but thought the flumine framework might have a "flumine-ish" way to handle it

*Tags: General Technical*

---

**George** - *16:40:33*

I can't believe I'm asking this but here goes - how do you cancel an executable order in Flumine?

The code for `order.cancel()` sets the order's status to `OrderStatus.CANCELLING` but doesn't actually do anything else?!

*Tags: General Technical*

---

**Lee** - *16:44:50*

I assume you’re looking at the `Transaction` class? Look at the following line after that is called and follow that through [https://github.com/betcode-org/flumine/blob/d2e7d8e6cbc65687d2a0fb41e8f24bf133081f59/flumine/execution/transaction.py#L102](https://github.com/betcode-org/flumine/blob/d2e7d8e6cbc65687d2a0fb41e8f24bf133081f59/flumine/execution/transaction.py#L102)

*Tags: General Technical*

---

**George** - *16:45:52*

I was looking at line 335 of flumine.order.order.BetfairOrder which is what gets called when you run order.cancel() (I think!)

*Tags: General Technical*

---

**Lee** - *16:47:34*

in the strategy you should be calling `market.cancel_order(order)`

*Tags: Strategies*

---

## 2023-06-09

**JazzMan** - *16:16:29*

How old is your historical data?

*Tags: Data Quality*

---

**AndyL** - *21:26:13*

I've being working on some racing inplay strategies recently, and notice the Flumine simulation has a hard time accurately simulating matching "inplay". Backtest vs Live seems a wider gap compared to pre-race. Trying to understand why, I am guessing order flow and volatility is high inplay, so hard to simulate accurately? The best way to approach "inplay" seems to be to put a toe in the water and see how it goes!

*Tags: Deployment*

---

## 2023-06-10

**liam** - *07:27:53*

Any context to the strategy in terms of order type (passive)/ size/quantity? How off is it?



I find it accurate enough to trust simulations when used correctly, volatility certainly makes things tricky 

*Tags: Strategies*

---

**AndyL** - *10:24:21*

Say for example trying to "take" £5 low odds back bets in the last 20seconds..., I found Simulated I was getting 80% match rate, live only 25%, and thus adverse selection was a lot higher... My theory there is Flumine can't judge the actual "demand" maybe? so match rate low..

*Tags: Deployment*

---

**AndyL** - *10:47:31*

```def run_process(markets):

    uid = str(os.getpid())



    client = clients.SimulatedClient()

    framework = FlumineSimulation(client=client)

    bsp_file = "hrbsp/hrbsp_"+uid

    strategy = UkHrBot(framework, bsp_file,

        market_filter={"markets": markets,

                   "listener_kwargs": {"seconds_to_start": 20}

                  },

        max_order_exposure=500,

        max_selection_exposure=500,

        max_live_trade_count=4

    )



    with mock_patch("builtins.open", smart_open.open):

        control = BacktestLoggingControl("results/hrorders_"+uid+".txt")

        framework.add_logging_control(control)

        framework.add_strategy(strategy)

        framework.run()```

*Tags: Deployment, Strategies*

---

**liam** - *10:51:02*

Yeah that looks fine assuming live is the same, if you can get some strategy code that replicates the issue I can run on my recorded market.



Btw any reason you need 20s of pre play rather than filter just in play on the kwargs?

*Tags: Deployment, Strategies*

---

## 2023-06-13

**George** - *17:17:14*

If I was to develop a 24/7 fully automated market-making strategy, I imagine there are a number of things that could go wrong because of Betfair outages and similar things.

• Is anyone able to point me to the code where Flumine deals with these sorts of issues, so that I can be aware of what kinds of manual intervention might be required?

• I imagine several people in this slack are running strategies of this kind, do you have any kind of advice for someone starting out on this path?

*Tags: Strategies*

---

**AndyL** - *17:57:08*

Depends if you hedge or not... any win/losses average out over time... Last Sunday I lost -£120 on my hedging strategy :-)

*Tags: Strategies*

---

**Jonjonjon** - *18:12:34*

Good question. when creating a Strategy you can limit your selection exposure, order exposure, number of live orders. I think it's in the BaseStrategy class

*Tags: Deployment, Strategies*

---

**George** - *18:18:06*

Indeed - thanks for pointing out. I have set these limits and I am happy with them. However, I am more concerned around the fact that my supposedly fully-automated strategy might have terminated (either gracefully or not) due to a Betfair outage (or similar), and that - as a result - I will need to manually intervene in order to restart it.

Does anyone have any experience with needing to do that, and in what cases does it occur / how does Flumine respond?

*Tags: Strategies*

---

**liam** - *18:20:33*

flumine deals with it as gracefully as it can, have a look at the [https://github.com/betcode-org/flumine/blob/master/flumine/streams/marketstream.py|marketstream](https://github.com/betcode-org/flumine/blob/master/flumine/streams/marketstream.py|marketstream) code to see how it retries to connect with exponential backoff. I think there has only been one occasion in the past year where I have had to restart but I think we have fixed this particular issue (memory leak)

*Tags: Errors Debugging, Performance*

---

**George** - *18:21:28*

perfect thanks. so you are essentially able to run your strategy 24/7 without ever manually intervening?

*Tags: Strategies*

---

**Peter** - *21:18:37*

My approach is to use the terminate middleware to close my strategies down at the end of each day and cron to re-start them the next day, so they're always close to a fresh start, and then rely on Flumine to bridge any intraday outages.



Like others I don't worry about the odd exceptional event, anticipating that the positives and negatives will even out over the long-term, and the net effect will therefore be too small to justify the added complexity of trying to mitigate or handle them.

*Tags: Errors Debugging*

---

**Michael** - *23:21:22*

On my market recorder I’m trying to collect IRE and GB racing across several markets. I keep running into the max subscribers of 200, so I’ve split GB and IRE into separate strategies, seems to be working currently but am I going to run into issues with this approach / how do others approach this?

*Tags: Data Quality*

---

## 2023-06-14

**Jonjonjon** - *00:06:51*

I've spent very little time out of the market. Flumine and modern cloud services have been kind to me.

*Tags: General Technical*

---

**liam** - *05:33:57*

As above but market filter also accepts a list so you can split but with a single strategy 

*Tags: Strategies*

---

**Lee** - *11:51:27*

I just let flumine run, have instances that been up for months without any issues

*Tags: General Technical*

---

## 2023-06-18

**AndyL** - *19:25:47*

I understand the :face_vomiting: from [@U4H19D1D2](@U4H19D1D2) and I think for strategies that are showing 1-2% or more ROI, why would you bother adding 1000s of potentially -EV bets..? However, from recent experience in comparing a rather weak market-maker strategy I had, which was a mere 0.2% roi, I actually found that over the long run hedging vs not-hedging made the same amount of money, but the difference was the hedge method had a max drawdown of about -£50, whereas the non-hedge method a drawdown of -£2000 !!

*Tags: Strategies*

---

## 2023-06-19

**James** - *14:05:50*

Hey, everyone I’m new to Flumine,

I’ve been using an JS based closed source framework for a while doing mostly in play trading.

The framework I have been using  handles a few things flumine doesn’t seem to (Flumine handles a LOT that the js framework doesn’t).

I’m wondering if theres an easy way to get your current matched win/lose positions on a runner?

Also does any one have any nice util functions for hedging/ledging that they’d be willing to share?

*Tags: Getting Started, Strategies*

---

**liam** - *14:14:06*

Welcome, `market.blotter` is what you want, for example [https://github.com/betcode-org/flumine/blob/d2e7d8e6cbc65687d2a0fb41e8f24bf133081f59/flumine/markets/blotter.py#L172|selection_exposure](https://github.com/betcode-org/flumine/blob/d2e7d8e6cbc65687d2a0fb41e8f24bf133081f59/flumine/markets/blotter.py#L172|selection_exposure)



There is a reason flumine doesn't offer hedging functions :face_vomiting:

*Tags: General Technical*

---

## 2023-06-20

**thambie1** - *17:38:40*

Anyone have suggestions for how to approach pricing/sizing for market making using a fundamental model? Ideally I'd like to combine something like the Avellaneda &amp; Stoikov paper with the kelly criterion. The issue being, the Avellaneda &amp; Stoikov approach is suitable to a single product, whereas on Betfair you are often market making several runners. Furthermore, the paper's concept of inventory management doesn't completely apply given that you don't need to sell back your positions in sports betting. The kelly criterion on the other hand is great, but it can only tell you the amount to offer given a price, or a price given the amount you want to offer, but not both at the same time.

*Tags: Strategies*

---

**thambie1** - *18:16:41*

Tried it, the risk profile was worse than either of the approaches I described. But I'm certain I'm still leaving a lot of opportunity on the table, hence the question

*Tags: General Technical*

---

**thambie1** - *18:29:34*

Visually by looking at a graph of returns. Profiling against alternatives. Definition doesn't really matter, as I'm not being nuanced about risk. I found fixed liability given a margin was worse against any risk profile metric you could reasonably come up with

*Tags: Errors Debugging*

---

**AndyL** - *21:44:05*

So if you come up with the best "model" you can for fair prices... it's been said that Betfair BSP is always going to be more accurate. But am I right in thinking that's an obvious statement, because BSP is simply the "starting price" once everyone and his dog has decided on their own fair price and bet in the market, so of course BSP is going to be the "best" as it's an average of everyone's views.....?

The key is coming up with a decent estimate of BSP, before it happens ?

*Tags: Strategies*

---

**Mo** - *21:46:42*

I don’t think it’s been said BSP is more accurate. BSP is very accurate for the amount of work you have to put into making the prediction i.e. zero. It is entirely possible to have a model that is better than BSP

*Tags: Strategies*

---

## 2023-06-21

**Jonjonjon** - *09:06:32*

I'm happy with the performance at £2 stakes. No need to reduce my size.

*Tags: Performance*

---

**Jonjonjon** - *09:12:31*

Performance doesn't seem to improve if I increase the size.

*Tags: Performance*

---

**Mo** - *09:13:19*

By performance you mean absolute returns?

*Tags: Performance*

---

**liam** - *09:19:46*

[@U01DCR5PXDY](@U01DCR5PXDY) does your model return higher ROI (real) based on your signal?

*Tags: Strategies*

---

**thambie1** - *09:21:04*

...I don't understand the question? What signal are we talking about here?

*Tags: General Technical*

---

**liam** - *09:28:44*

I assume your model gives you a signal?

*Tags: Strategies*

---

**liam** - *09:30:55*

Have you experimented with taking advantage of this along the lines of kelly?

*Tags: Strategies*

---

**thambie1** - *09:31:13*

Yeah, I have taking strategies that incorporate kelly, they work well

*Tags: Strategies*

---

**thambie1** - *09:40:38*

If the question was in regard to market making, like I mentioned in the question kelly can't provide both the price and size at the same time, but only one given the other.

*Tags: Strategies*

---

**Mo** - *09:56:19*

How about Kelly with fixed margin?

*Tags: Errors Debugging, Strategies*

---

**thambie1** - *09:57:38*

Doesn't account for existing positions. Kelly will eventually say to stop offering at the price if you build up a large enough position, where what you would really like to do is offer a worse price

*Tags: Strategies*

---

**Mo** - *09:58:37*

So if Kelly tells you 0 stake, increase margin and try again?

*Tags: Strategies*

---

## 2023-06-23

**Tom** - *01:26:58*

I think about BSP something like this - odds chosen at random at 100% markets would eventually return 0 if you chose horses at random.

From there, there is information implied in decision-making that maximises value, shaping models of probabilistic outcomes.

So (optimised) decisions assert information or that other decisions do not assert information.

*Tags: Strategies*

---

**Nelsen** - *08:41:52*

Good Morning guys, have you ever heard about Harville probabilities for exotic pools? What do you think about it? Is it old stuff or can help even today predicting exotic pools?

*Tags: General Technical*

---

**Peter** - *10:44:10*

[https://www.researchgate.net/publication/4748916_Probability_and_Statistical_Models_for_Racing|This paper might help you](https://www.researchgate.net/publication/4748916_Probability_and_Statistical_Models_for_Racing|This paper might help you) but as [@UBS7QANF3](@UBS7QANF3) implies, if these models gave you an edge, people would be very quick to exploit it and the edge would disappear. In the case of Harville, there's a clear bias in the results that it delivers towards shorter priced runners, making it a poor estimator for place odds.

*Tags: Deployment, Strategies*

---

**James** - *10:45:17*

I want to run a strategy against a set of football markets based on competition (and maybe a few other params), but I don’t want to maintain the stream for every football market in a given country, but market filters can’t let me filter by competition.

Has anyone used a worker to push market_ids into a strategy’s market filter so I don’t have to manually input a set of market ids every few days? Or is there an easier way to do this that i’m missing..

*Tags: Strategies*

---

**Peter** - *10:47:54*

There isn't an easier way. Apparently adding competition to the filter is a bigger deal for Betfair than you might think (the question's been asked before!).



However, be aware that once you're subscribed to a stream you can't change the filter, so I doubt your worker will have much effect, unless it closes and re-opens the stream.

*Tags: General Technical*

---

**Peter** - *10:49:30*

However, why are you worried about streaming markets in which you're not interested? Just filter them out in your strategy's `check_market` method.

*Tags: Strategies*

---

**George** - *16:10:44*

In my strategy i would like to ensure that `check_market_book` returns `False` up to 60 seconds after the market status changes from SUSPENDED to OPEN. However, when I implement this using `time.time()` , the simulation fails, because the `time.time()` counter is aligned with "real life" and not with the flumine simulation.

What have I done wrong?

*Tags: Strategies*

---

**Jonjonjon** - *20:44:44*

I prefer market_book.publish_time. the above will behave differently based on your latency

*Tags: Performance*

---

## 2023-06-24

**thambie1** - *08:20:28*

Thanks for all the suggestions. The approach that that I'm experimenting with now is to use a fixed margin, then use kelly (ignoring existing positions) to determine the size given the margin, and finally use kelly (considering existing positions) to determine what price to offer given the size computed in the previous step. Theoretically I'm pretty comfortable with it :crossed_fingers:

*Tags: Errors Debugging, Strategies*

---

**thambie1** - *08:25:03*

Anyone have success handling football VAR events in backtests? Is there any way to identify such events in the historical data provided by Betfair, or in recorded data?

*Tags: Data Quality*

---

## 2023-06-29

**Aaron Smith** - *16:06:18*

I want to make a selection_id_to_name dict to use in process_market_book. As this doesnt change on each market_book update, i would like to only generate this once. Where would be the right place for this in a flumine strategy? Also, what happens with the names when a replacement runner gets introduced? Normally a name goes like "x. name", where x is the starting position. So if a runner comes in as a replacement, i guess the name would change, so i need to update this dict with every new market_catalogue?

*Tags: Strategies*

---

**Peter** - *17:32:43*

Personally I'd turn this around by holding the lookup dict in the market context and if/when the strategy spotted a selection_id that's not in the lookup, pass the market object to a custom event to query the market_catalogue and update the market.

*Tags: Strategies*

---

## 2023-06-30

**Lee** - *11:28:30*

I’m working on improving a strategy with a simple (and logical) optimisation that I tested on a single month of real bet data. This also shows good results for the previous 12 months but eats into profit in the previous years. At what point am I optimising for the evolving market conditions vs overfitting?

*Tags: Strategies*

---

**James** - *12:23:37*

Ah, i think I answered by own question. I think its to to link two orders you create and place yourself? It doesn’t automatically place the offset when the first order is placed like I first thought.

*Tags: General Technical*

---

**thambie1** - *12:26:27*

I've had this problem. I don't think there's one clear answer. If the amount of data over the 12 months is large enough of a sample size, that may be enough evidence. Understanding exactly what changed and why can be very helpful as well. Is your strategy more profitable because spreads have changed, or because the market is more erratic? If you can explain the reason why profitability has changed, then there's less reason to pin it on nebulous overfitting.

*Tags: Strategies*

---

**Dario Scardina** - *17:01:30*

Hello everyone :slightly_smiling_face:



I have a question for you: is there any way to place a lay order to match the exact price I set?

I mean: i.e. I wanna lay a selection at 1.59. If a place the order there are simply three cases:

1. current_lay_price &lt; lay_price_requested -&gt; my bet will be matched at lower price (i.e. 1.52)

2. current_lay_price = lay_price_requested -&gt; OK (my order will match at exact price 1.59)

3. current_lay_price &gt; lay_price_requested -&gt; my bet will be not matched (until the price decrease equal or lower than 1.59)

I know that it seems a stupid question because case 1 for example is more profitable, but I want in each case to match only exact price. "Who are you to judge how people want to lose their money?" :smile:



Thanks in advance :slightly_smiling_face:

*Tags: General Technical*

---

## 2023-07-03

**Dario Scardina** - *15:43:47*

I mean using betfairlightweight placeorder function

*Tags: General Technical*

---

**George** - *16:13:33*

In simulation, I can see my strategy is matching against the exact same order hundreds of times until the risk controls kick in.

How can I stop this happening? Surely it is not expected behaviour?

*Tags: Strategies*

---

**Lee** - *16:18:41*

It’s expected, have you read the docs? [https://betcode-org.github.io/flumine/quickstart/#simulation_1](https://betcode-org.github.io/flumine/quickstart/#simulation_1)

*Tags: General Technical*

---

**Mo** - *16:50:20*

I’m hoping to put something more sophisticated than that into betfairutil but this is not an easy problem to solve

*Tags: General Technical*

---

**George** - *21:44:26*

It's not totally straightforward but I think flumine has solved much harder problems than this successfully.

*Tags: General Technical*

---

## 2023-07-04

**R** - *13:23:18*

general question on using flumine with multiple strategies: is there a convenient way to plot the returns for each strategy separately rather than just using something like (pseudo code)



`for market in framework.markets:`

`... aggregate returns`

`plot`

*Tags: Strategies*

---

**R** - *13:27:07*

reason i'm asking is I want to use flumine to simulate inside of a monte carlo tester I have built.



So I was hoping to effectively add a strategy for each MC run, and then access the returns of each strategy separately so I can compute statistics

*Tags: Strategies*

---

**Peter** - *14:06:48*

Logging control gives you hooks to access the data as it becomes available. What you do with it, e.g. write to a file, persist it to a database, stream to another app, etc. is up to you. So it's perfectly feasible to consume and plot it in real-time, or to store it for analysis later.

*Tags: General Technical*

---

**R** - *14:09:31*

nice, thanks guys.  I'll dig into understanding the control classes now.  Slowly internalizing the philosophy of flumine.



much appreciated to you both!

*Tags: Performance*

---

**George** - *16:10:49*

if `process_market_book` can take up to 20ms, is that slow enough to cause 'problems'?

*Tags: Performance*

---

**foxwood** - *16:21:28*

I tend to deal with outcomes in `process_closed_market` since there is no particular time issue once the market is over. To make it easy to identify things I also patch over the flumine hash which is used for `order.customer_order_ref` so that it is easy to pick out multiple orders by specific strategy eg the order ref for bets placed by this strategy would always start "strat999" (max 12 characters)  ...



`class strat999(BaseStrategy) :`



	`def add(self) -&gt; None:`

		`...`

	    `self.name_hash = self.__class__.__name__\`

			`+ "x"*(STRATEGY_NAME_HASH_LENGTH-len(self.__class__.__name__))`



	`def somefunc(self,market,market_book)`

		`for order in market.blotter:`

	            `# check order belongs to this strat`

        	    `if order.customer_order_ref[:len(self.name_hash)] != self.name_hash :`

                	`continue`

		

*Tags: Strategies*

---

**liam** - *17:00:05*

Mo will correct me if I am wrong but I believe the matching cycle is 50ms so 20ms is fine, assuming you have one strategy etc 

*Tags: Strategies*

---

**Mo** - *17:09:55*

Two comments, neither of which is a correction:



1. Indeed, according to the documentation the matching cycle is 50 milliseconds. However, anecdotally the frequency of stream updates can vary significantly from this. Take a look at some data to get a feel for this

2. You might need to consider the distribution of times your `process_market_book` takes. For example, average of 20 milliseconds but 99th percentile of 1 second might give you problems

*Tags: General Technical*

---

## 2023-07-05

**Unknown** - *16:05:52*

regarding logging controls:

the examples I have seen in the flumine code all follow the principle of having a single large log file and everything gets logged in there.

but, in practice, I suspect this is quite inconvenient, especially for people who are active on hundreds of markets every day.

the problem I see is that each `event` does not necessarily have access to the `market_id` , and so it's not possible to have one log-file per market-id.

can I ask: what do people actually do in practice?

*Tags: General Technical*

---

## 2023-07-06

**Charaka A** - *18:17:00*

I'm new to using flumine and was wondering if anyone has an idea how to go about maintaining the state of the ladder as part of the strategy. I'd like to later visualise the ladder but initially would like to maintain my own ladder for each runner which would be updated based on the mc packets received

*Tags: Getting Started, Strategies*

---

**Mo** - *19:35:03*

flumine (betfairlightweight) does this for you

*Tags: General Technical*

---

## 2023-07-09

**jackofallspades** - *15:52:44*

&gt; according to the documentation the matching cycle is 50 milliseconds

I can't find this info (and generally matching cycle description) in the documentation. [@UBS7QANF3](@UBS7QANF3), can you please direct me to where it can be found?

*Tags: General Technical*

---

**Mo** - *16:30:03*

Good point [@U02K1MG7YCA](@U02K1MG7YCA), I cannot find this in the documentation now. Perhaps one of the other old guard can recall where it originated

*Tags: General Technical*

---

## 2023-07-13

**R** - *13:38:39*

For those that like good ole fashioned monte carlo statistics on their strategies, do you find flumine is fast enough to run simulations?  I'm finding that my strategies are just taking too long to run let's say 10,000 samples (simulated markets), but perhaps I am just quite terrible at coding them up / using flumine.

*Tags: General Technical*

---

**Peter** - *17:27:56*

That's really not a surprise. It's not that Flumine is slow, it's that it's receiving and processing updates for every market to which you're subscribed as frequently as every 50 milliseconds, so there's really not enough space in there to run 10k sample monte carlo simulations as well.



If the simulations are important to you, you have three things you could do:

1. in your subscription request, ask for conflated data e.g. only receive the data every second, opening a bigger window into which to run your simulations

2. offload the simulations to a worker (to stop them blocking your main process) and write the results back to the relevant market.context for ingestion into market update processing

3. 1 and 2 above.

*Tags: Performance*

---

## 2023-07-14

**Peter** - *07:22:52*

Stepping back, I use Monte Carlo simulation too, but (almost) always offline as part of my analysis to develop and evaluate strategies. I only have one strategy currently where it's integrated into the strategy itself. For that strategy, what's changing and warrants re-running the simulation is the state of the event itself, i.e. fundamental rather than market data. Updates to that are way slower than every 50 milliseconds. So I have a process that runs in parallel to my Flumine strategy to ingest external data about the state of the event, run the simulation and store the latest result, which can be read by my Flumine strategy as and when it needs it.



In my case it's an older home grown process (it's been around for a while). But Flumine provides [https://betcode-org.github.io/flumine/workers/|workers](https://betcode-org.github.io/flumine/workers/|workers) that are how I could do this job if I were writing this today. A worker runs in parallel to your Flumine strategy so (normally) won't slow it down (though if you have a single processor available to your Flumine instance it might). Thus you could run your simulations continuously in a worker function and, using the flumine object available to each worker, write the results back to your strategy or individual markets objects each time a simulation completes, from which your strategy can pick them up when processing market updates.

*Tags: Performance, Strategies*

---

**R** - *12:39:57*

very nice, I didn't know about flumine workers.  Cheers Peter.



I'm in a similar position RE offline evaluation.  What I'm wanting for is basically using flumine as part of the evaluation so the code I use to test and evaluate is as close as possible to the code that will be deployed.   My guess is that I need to just reduce the frequency of market updates in that case.  Or adapt flumine a bit to get it faster for the backtest case.

*Tags: Deployment*

---

**R** - *15:28:21*

I think this might also be helpful too in terms of parallelizing independent monte carlo runs



[https://betcode-org.github.io/flumine/performance/#multiprocessing](https://betcode-org.github.io/flumine/performance/#multiprocessing)

*Tags: Performance*

---

## 2023-07-16

**James** - *09:57:54*

I’m using a  modified version of the backtest logging control from here to log my results to a db on market closure [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py)



It\s successfully pushing to the DB, but order.profit is always 0 for some reason. Has anyone run into this issue at all? The bets are placed and I can see different numbers for profit on the betfair site,  but for whatever reason its always coming as 0 on the order.

*Tags: General Technical*

---

## 2023-07-26

**JazzMan** - *11:13:01*

UK Horse Racing, 30 minutes before start.

I had a problem last night where someone had reverse engineered my strategies.

Race 1 -£18

Race 2 -£25

Race 3 -£45

Race 4 -£83

This was after a very successful week, and never in 15 years have I lost £83 in 1 race. 

Complete waste of time on their part because I just pulled the plug on them.

That’s the problem when you manipulate the market, you’ll get short term gains.



*Tags: General Technical*

---

**Mo** - *11:23:31*

1. What was your maximum loss prior to this?

2. Are you basing `I had a problem last night where someone had reverse engineered my strategies.` on any observations other than your realised performance?

3. What was your EV according to BSP?

*Tags: Performance*

---

**Aaron Smith** - *11:50:02*

reverse engineering a strategy with that low volume seems incredibly difficult and i dont think anyone who is capable of doing so (and i m not sure someone like that exists) would bother with 100 bucks from your strat. Honestly 4 races with a loss in a row seems like normal fluctuation to me? If -83 is far beyond your previous max loss per market, i d rather check what caused you to put in more volume than usual. That you lost that single race where you put in the volume doesnt really say anything,

*Tags: Strategies*

---

**mandelbot** - *13:35:32*

I think what's more likely than someone re-engineering your strategy is someone is manipulating the market to catch strategies like yours

*Tags: Strategies*

---

## 2023-07-31

**Mo** - *18:53:13*

So what’s the update? Did you restart the strategy? Did you do any of the analysis we suggested?

*Tags: Strategies*

---

## 2023-08-04

**liam** - *10:57:15*

After 10 years of ignoring it I think next week I am going to try applying some simple ML to a strategy which is currently using my trusty `if` statements.



a) Are people interested in me going through this process (data/model/predictions/flumine integration/execution etc) publicly on here?

b) Can I have some help from those that know what they are doing?



I see it being part II of my [https://medium.com/@paulingliam/using-ml-to-predict-horse-race-time-duration-6340776536e4|first attempt](https://medium.com/@paulingliam/using-ml-to-predict-horse-race-time-duration-6340776536e4|first attempt) (6 years on and still working / used daily), if so where do I start?

*Tags: Strategies*

---

**Mo** - *11:03:15*

a) Of course, anything to breath life into this channel



By part II are you saying that specifically you want to start by improving the model in part I?

*Tags: Strategies*

---

**liam** - *11:06:08*

No this would be using ML to create strategy predictions that are better than my current if statements using the same data.



Mentioned it being part II as part I is the only experience / skills I currently have when it comes to ML and is likely where I will start (RF) but want people to point me in the right/correct direction.

*Tags: Strategies*

---

**Mo** - *11:25:26*

Sounds good. It touches on something I have been thinking about recently. How much value add will you find from using the same data but with a more sophisticated model (i.e. ML vs if statements) vs value add from additional (orthogonal) data?

*Tags: Strategies*

---

**AndyL** - *11:25:40*

Definitely interested in this, i played briefly with pandas in my early goat days  with advice from [@UBS7QANF3](@UBS7QANF3). But didn't really see it's full potential.

Wondering, are all ML straight line fits? For example could you make it fit a quadratic for ax2+bx+c ? Ie.polynomial regression?

*Tags: Feature Engineering*

---

**Leo** - *12:44:12*

Also lightgbm is the way to go for almost all structured data problems these days 

*Tags: General Technical*

---

**Andrew** - *13:40:00*

Do you have an idea what model type (algorithm) you’d try? Random Forest could be a place to start being just a complex array of ifs.

*Tags: Strategies*

---

**foxwood** - *15:30:59*

Did a lot of low-level experimenting with Encog about 10 years ago and later with Tensorflow, Keras etc and most recently [http://ML.NET|ML.NET](http://ML.NET|ML.NET). Have used recorded data and bought in data. To avoid complexities of integrating different packages for live I tend to use a csv file to handshake with the ML lol. Processing time was not generally important so easy solution. Sadly none of the ventures has proved better than "trad" methods of "if's" regardless of fields selected for training or type of model used. An itch I still scratch - so yes - very interested in this area. If you make the work as public as flumine though I think others will copy and edges will vanish.

*Tags: Deployment, Strategies*

---

**liam** - *15:38:08*

I wont be sharing any data / models, just the process / results etc

*Tags: Strategies*

---

**Leo** - *17:44:02*

That said, the specific model is far less important than the quality of your features 

*Tags: Feature Engineering, Strategies*

---

**Adam** - *18:15:25*

Will this be an in-play strategy? Another thing to consider will be model complexity and inference time. If using ensemble approaches (multiple, different types of algorithms), then by the time the prediction is made, you may miss the window

*Tags: Strategies*

---

**Leo** - *18:33:27*

[@UPMUFSGCR](@UPMUFSGCR) it's generally faster and more memory efficient, so can iterate faster during development. Performance does vary from problem to problem, and lightgbm takes more tuning, there probably isn't much in it but lightgbm seems to come out on top more often than not and seems to be the new standard in kaggle competitions 

*Tags: Performance*

---

**Leo** - *18:36:27*

Either a good choice imo, both blow rf out the water on the majority of problems 

*Tags: General Technical*

---

**liam** - *19:52:28*

[@U05LDB7TK8B](@U05LDB7TK8B) yeah expecting that to be a problem to solve, it’s a question that comes up a lot when it comes to latency / blocking the main queue etc. 

*Tags: Performance*

---

## 2023-08-05

**Unknown** - *12:03:59*

[@U02RN7YDRQ9](@U02RN7YDRQ9) I use .net ML as well as all other ML algorithms from python libraries. Here is the way I do it, comparing different ML strategies and using just profitable one/s.

*Tags: General Technical*

---

## 2023-08-07

**Unknown** - *08:46:04*

*Day 1*



Setting the scene, the plan is to improve a live (profitable) strategy by optimising its current parameters (features) using ML.  This is going to be a learning exercise so if you have any questions / advice / criticism please get involved :wave:



The strategy is for TPD inplay racing, low stakes and taking Lay prices, market impact can be ignored for now. To keep things simple I am going to limit to Flat racing for the past 6 months, Jan-June inclusive, which gives around 1500 markets.



One of the features is what I would call the model and gives a % chance of the horse winning/losing and is what I have used for the past 4 yrs as the most important feature in the `ifs`. However I know that with the inclusion of other features it can be improved but I have never been able to optimise manually and get results.



Stats: £6.5 profit per market, 11.6% roi, £15.8 matched per selection

*Tags: Feature Engineering, Deployment, Strategies*

---

**liam** - *08:48:58*

First step is to get all the data/features into a dataset I can process, this might be the most painful part.



I have the 'base' strategy analysed in terms of profit/roi but I am wondering if I should run typical ML analysis on my predictions, accuracy/precision/recall to then compare with future models?

*Tags: Feature Engineering, Strategies*

---

**Joe** - *09:07:55*

Are you looking to optimise a set of thresholds in your IFs or are you looking to recombine your features in a non-linear model? i.e. are you trying to find C to optimise:

IF ( f1 &gt; c1 &amp;&amp; f2 &gt; c2 &amp;&amp; f3 &gt; c3 ) THEN placeBet()

or are you trying to learn a new function F:

IF (F(f1, f2, f3) &gt; 0) THEN placeBet()

*Tags: Feature Engineering, Strategies*

---

**liam** - *09:14:46*

[@UPMUFSGCR](@UPMUFSGCR) yes a lot more than 1



[@U05L8PZD2FM](@U05L8PZD2FM) currently the former



[@UBS7QANF3](@UBS7QANF3) more the former but I think its both, I hope this will help me learn where the value is coming from and probably form the basis of model v2 or even combine into a single model

*Tags: Strategies*

---

**Joe** - *09:25:56*

[@U4H19D1D2](@U4H19D1D2)ok so unlabeled unsupervised derivative free reinforcement learning with the goal of maximising profit or ROI. You need to get your data into a memory structure that can be executed at very high frequency then use a derivative free / metaheuristic search such as FF, PSO, etc. Your conditional betting logic would need to be rephrased to operate on the new memory model and you would need some type of environment that can minimally mimic the market if your logic takes actions based on its previous actions.

*Tags: Performance, Strategies*

---

**liam** - *09:54:00*

Ok, I follow, I think, this sounds like level 10 vs my current status of level 0.



It is already sounding like my current process of using a model and then filtering after with other features is less than ideal. However I feel starting with this using some simple models is probably going to see some improvements / move me onto level 1.

*Tags: Feature Engineering, Strategies*

---

**D C** - *09:57:14*

Are your C parameters in your if filter also inputs to the model that generates the initial probability estimate or completely independent of it?

*Tags: Strategies*

---

**Joe** - *09:58:47*

Without knowing how long it takes you to run a back test with a set of parameters it is difficult to advise. But say you can already do this reasonably quickly then simply wrapping your existing back tester in a simple parameter walk (fix N-1 params vary 1 until optimal, then vary the next one) using newtons method might help.

*Tags: Errors Debugging*

---

**liam** - *10:05:00*

[@UUE6E1LA1](@UUE6E1LA1) they are independent but likely related, i guess the end goal would be that they would all be fed into a model to get a single prediction 

*Tags: Strategies*

---

**D C** - *10:07:17*

I was just wondering about what a lot of people have said about TPD on here - they have a model that works well without any TPD and use the TPD fields to improve on it. I only ask because I have a model that takes TPD inputs but I subsequently filter on some parameter ranges of those same inputs because it performs poorly on certain ranges and I want to exclude bet placement in those cases.

*Tags: Strategies*

---

**liam** - *10:19:04*

Interestingly when I look at feature importance my model is at the bottom with 0.06% importance :joy:

*Tags: Feature Engineering, Strategies*

---

**Unknown** - *13:05:50*

So I have a rough pipeline setup to allow model -&gt; flumine simulation, it works and I have results which point to some potential.



Stats: £30.0 profit per market, 14.0% roi, £59.4 matched per selection



A huge increase in matched amount which is not viable but the increase in ROI is most welcome, going to play around with some features and try and reduce the amount matched (probably a ML term for this when it comes to the model)

*Tags: Getting Started, Feature Engineering, Strategies*

---

**Lee** - *13:08:08*

Is this model using the random forest classifier?

*Tags: Strategies*

---

**liam** - *13:50:20*

Confirmed when I remove the shuffle from `train_test_split` I get a big drop in accuracy



```Model accuracy score: 0.8789```

*Tags: Strategies*

---

**river_shah** - *14:16:50*

[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html|TimeSeriesSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html|TimeSeriesSplit) is your friend (grouped by market_id)

*Tags: Strategies*

---

**liam** - *14:25:48*

Ok, I understand the problem but i don't understand the solution, what is a fold?

*Tags: General Technical*

---

**river_shah** - *14:27:07*

Copy pasted from [https://machinelearningmastery.com/k-fold-cross-validation/|machinelearningmastery](https://machinelearningmastery.com/k-fold-cross-validation/|machinelearningmastery):



```k-Fold Cross-Validation

Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.

The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.

Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.

It is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train/test split.

The general procedure is as follows:

Shuffle the dataset randomly.

Split the dataset into k groups

For each unique group:

Take the group as a hold out or test data set

Take the remaining groups as a training data set

Fit a model on the training set and evaluate it on the test set

Retain the evaluation score and discard the model

Summarize the skill of the model using the sample of model evaluation scores

Importantly, each observation in the data sample is assigned to an individual group and stays in that group for the duration of the procedure. This means that each sample is given the opportunity to be used in the hold out set 1 time and used to train the model k-1 times.```

*Tags: Strategies*

---

**liam** - *17:57:56*

[@U02RN7YDRQ9](@U02RN7YDRQ9) would that not be mega slow?



I think the only thing I have achieved today is a headache, certainly fallen into the overfitting trap, thanks [@U01B8031PM1](@U01B8031PM1) for sharing `GroupTimeSeriesSplit` makes sense and works well.



I have a pipeline that is working well and likely something that I will share as its very cool to be able to create a model and then run it within a flumine simulation with a few lines.



Really starting to question the suitability of using ML at this stage, maybe its the classifiers I am using but they just want to overfit. A few could have predicted this but I am going to go back to the beginning with my model predictions and see if that can be improved using regression.

*Tags: Performance, Strategies*

---

**D C** - *18:12:33*

Nowt wrong with linear models [@U4H19D1D2](@U4H19D1D2) . Maybe you end up with something less optimal, but at least using classical inference you are not left with a black box type predictor. I'd love to get into ML but this thread has put me off (even thought I have found it really really interesting).

*Tags: Strategies*

---

**liam** - *18:17:22*

Not giving up yet, I think the underlying problem is 'imbalanced classification' on winners/losers

*Tags: General Technical*

---

**John Foley** - *18:19:03*

if you’re using predictions from another “model” as one of the input variables to your ML model - why not just use the input variables for your first model + input variables for your new model and make a training set from both? (i may not fully understand what you’re doing for the first model) 

*Tags: Strategies*

---

**liam** - *18:20:22*

The first model is very good at spitting out a prediction but the way it is formed is very unorthodox, no ML involved

*Tags: Strategies*

---

**John Foley** - *18:20:39*

Also more generally - would be interested to hear what people have used as a target variable for supervised learning models that use market data only? Win/lose for a selection  I don’t think works well in most cases 

*Tags: Strategies*

---

**Lee** - *18:21:28*

by imbalanced classification do you mean the markets have more losers than winners so the model ends up with a bias?

*Tags: Strategies*

---

**Lee** - *18:23:41*

is the first model also a win/lose or a pricing model?

*Tags: Strategies*

---

**liam** - *18:25:05*

I don't even know how to describe it tbh and don't want to give too much away but it will give a percentage chance of winning

*Tags: General Technical*

---

**foxwood** - *19:02:50*

Training per race would be slower but that is only while training - applying the model to live data would be no different. To add to the headache have a look at a regression model instead of classification - that should give you a score instead of a binary win/lose - effectively a probability.

*Tags: Performance, Deployment, Strategies*

---

**Jonjonjon** - *20:54:42*

Are all the top guys using classification models?

*Tags: Strategies*

---

**Joe** - *21:22:43*

[@U4H19D1D2](@U4H19D1D2) I assumed you are trying to find parameters for your model that maximize the accuracy of your predictions, one way to do this in supervised learning is to use brier as your cost function.

*Tags: Strategies*

---

## 2023-08-08

**liam** - *08:34:45*

*Day 2*



Back to basics for me, the plan is to get a better understanding of how ML works when it comes to applying models to problems. Looking at going through the google ML unless anyone knows of anything better?



[https://developers.google.com/machine-learning](https://developers.google.com/machine-learning)

*Tags: Strategies*

---

**Joe** - *08:54:26*

Try to get an understanding of whether you need simple supervised learning, i.e. you have labelled data, or whether you are going full in on unsupervised learning using simply your parameterised branching strategy and a cost function (like -1 x profit). There is very little useful text on the latter.

*Tags: Strategies*

---

**Derek C** - *09:14:16*

Another consideration is whether the goal is to learn ML or get a good solution. Using an Auto-ML to generate features and models for you will give a better result than most people could do themselves,

*Tags: Feature Engineering, Strategies*

---

**foxwood** - *10:11:38*

I've used Google's tensorflow with python -  flexible for building / managing features - works well and now the docs appear sorted should be easier getting started. Any learning you do will transfer easily to other packages. If you are familiar with MS Visual Studio then their [http://ML.NET|ML.NET](http://ML.NET|ML.NET) add on is worth looking at - give it a csv then select fields to use and decide if feature or category is all you do - it then tries lots of different models to offer the best one. Seriously lacks ability to fine tune easily as in TF or SK but quick way to assess possibilities of datasets / models.

*Tags: Getting Started, Feature Engineering, Strategies*

---

**D C** - *12:28:27*

ChatGPT not cracket global optimisation problems yet then? What a scam. On a serious not though, as someone who has studied nonlinear optimisation but knows close to nothing about ML, are specific optimisation methods locked into each of these ML approaches? The entire field is a glut of terminology which I find very confusing and probably just relabelling stuff I've studied in the past. Can any of the experts here suggest a good intro to ML tailored to someone with classical inference and nonlinear optimisation background? I feel that if I could muster a bit of enthusiasm for ML via a decent entry text (rather than a python how to guide) it would be beneficial.

*Tags: General Technical*

---

**Joe** - *12:54:56*

[@UUE6E1LA1](@UUE6E1LA1) if you mean non-linear programming as a cousin of linear-programming (simplex) with constraints then there are some similarities between them and back propagation training network weights for labelled data (e.g. for classification). If you have coded and watched simplex pivoting then you'd see the walk looks similar albeit the mechanism is entirely different. These all use the mathematical properties of at least some part of the function being optimised to navigate to local optima (global in the case of a simplex).

If the thing you are trying to optimise is not a differentiable function (say it is a bunch of branching stateful procedural code and data that when run with some parameters produces some cost) then you are into the murky world of metaheuristic search and unsupervised / reinforcement learning. You wont find much in the way of satisfactory justification for why these techniques yield decent results nor how to stabalise them so they don't get stuck in dead ends, or massively overfit (backing brown dogs from trap 3 on Tuesday mornings yields 1000% ROI).

*Tags: General Technical*

---

**D C** - *13:22:23*

[@U05L8PZD2FM](@U05L8PZD2FM) I meant more like does "technique X" (some ML algo that the world is currently excited about - tailored specifically to solve problems of type X) involve some optimisation step that uses a specific algorithm (Newton/Gauss-Newton/Levenberg-Marquardt etc) or is it left down to the user to choose the appropriate optimisation technique out of several choices. I don't want to pollute this thread so maybe if you had a decent suggestion for a good text you could DM it to me?

*Tags: General Technical*

---

**Joe** - *14:03:29*

[@UUE6E1LA1](@UUE6E1LA1) I don't have any papers I'd recommend.

If you are doing NN classification with back propagation then you are just using the chain rule of derivatives to calculate the changes to the network weights to minimise total network error, the derivatives being those of the activation functions in your nodes and the learning rate basically how far down the slope you want to step. Typically you would choose the activation functions only.

If you are doing decision trees (boosted and/or ensemble / RF) then you splitting based on the most effective split point in the most effective dimension, usually assuming a linear gradient wrt that dimension.

If you are doing metaheuristic search then these are usually based on a pseudo biological explanation, like genetic crossing and mutation in populations, or the swarming nature of insects around some energy source, but all really boil down to guided brute force, these algos don't bear any resemblance to anything mathematical in the underlying system being optimised.



I think the problem you are having getting started is that every single non-paper based method of optimising anything has fallen under the umbrella term of ML, mainly because it enables the fading coding/developer tech industry to pivot to a post-coding 'data science' industry, this makes is really difficult to discuss in general terms.



If I was getting started today I would start with simple NN classifier, read up on it and code a network solver.

If you wanted to look at something more unsupervised then someone has decided to name a fairly general method 'Q-Learning', look this up.

*Tags: Getting Started, Errors Debugging*

---

## 2023-08-10

**liam** - *14:58:49*

But I have gone back to my model to improve using regression, it became *much* more accurate but has had zero impact (-ve) on pnl so it’s back to the white board.



Anyone else got anything interesting to share? 

*Tags: Strategies*

---

## 2023-08-11

**James** - *05:20:56*

Can I use a country market filter or alike to filter for the womens world cup/Fifa/uefa comps?

When looking at the navigation it returns an empty string as the country code for these competitions, I haven’t dug deeply enough into flumine to confirm, but I get the impression that I’m not being streamed the market data for these markets when I use a country filter of

```market_filter=streaming_market_filter(

    event_type_ids=["1"],  # Soccer

    country_codes=[''],

    market_types=selected_market_types,

)```

Is there a way to filter to international football? Or do I have to stream all football markets in every country for fifa/uefa comps?

*Tags: General Technical*

---

**James** - *05:23:49*

I find in football markets that the top leagues are more reliably predictable and similar, as top coaches and players tend to behave similarly. There is obvious variance in style, but youth/teenage teams or those in developing football nations tend to be more erratic IMO as they aren’t so highly coached.



It depends on your strategy tho, if you can make money from erratic, maybe they’re your target.



If you want strong backtesting and you see a gap to exploit maybe top leagues are best for you.



Note that liquidity is also higher in top leagues so this too can have an influence on the success in smaller/lower leagues.

*Tags: Strategies*

---

**liam** - *07:31:24*

Missing competitionId is a limitation of the stream, country_codes does work and you can give market_filter a list of filters in flumine 

*Tags: General Technical*

---

**Mo** - *16:21:26*

I did take a good look a while ago due to our previous discussions about having the betfairlightweight classes implement __getitem__ and there were some things Rob was doing that I couldn't get my head around. Might take another look again soon

*Tags: General Technical*

---

**Clive** - *16:32:29*

Does it help it being in Rust? Or is it an educational project?

*Tags: General Technical*

---

**Mo** - *16:43:26*

Absolutely helps compared to pure Python. If you try to do Kelly staking with many runners and have a time constraint on the calculations it would probably be impossible to do it in pure Python. I will add some benchmarks shortly 

*Tags: Strategies*

---

**Clive** - *16:45:19*

Sorry, poorly worded question. It was more do you pay less going over the Python to Rust vs using something like numba? Given you ported a scipy minimisation, couldn't you just write the  heavy bit with numba.

*Tags: General Technical*

---

**Mo** - *16:47:01*

Fair question but no idea. Never used numba. I do have some components that are pure Rust so it is personally helpful for me to practice my Rust

*Tags: General Technical*

---

**Jonjonjon** - *23:49:00*

Numba definitely helps put money in my bank account and the 3.11 update was a huge performance boost.

*Tags: Performance*

---

**Jonjonjon** - *23:57:20*

[@UBS7QANF3](@UBS7QANF3) where can I learn about the mathematical details behind your Kelly algo? I'm only familiar with the standard Kelly, which has a closed form solution and does not need minimisation. Apologies if this is a dumb question.

*Tags: Strategies*

---

## 2023-08-12

**Mo** - *08:40:39*

You arrive at the standard Kelly formula by writing down your log expected wealth assuming you are placing a new back bet on one of the outcomes, then differentiate the log expected wealth with respect to the stake, set it to zero and solve for stake. Which is, in other words, finding the stake that maximises your log expected wealth



I'm doing exactly the same thing in this package. I am maximising the log expected wealth except handling more than 2 outcomes and both backing and laying. Partly for convenience and partly for necessity I'm doing it numerically rather than analytically. As a minor point I am minimising the negative log expected wealth which is obviously the same as maximising the log expected wealth



The intention is to add the closed form calculations for when there are two outcomes. You'll call the same function in the package and it will automatically skip the numerical optimisation in this case



In fact, there is a closed form solution for 3 outcomes which I am already using in Python but I found that numerically optimising in Rust is 3 times faster which blew my mind. Again, I will ultimately add the closed form solution for 3 outcomes to the package



For 3 outcomes, if you differentiate the log expected wealth and set it to zero you end up with a quadratic equation to solve. For 4 outcomes, you would end up with a cubic. For 5 outcomes, a quartic and so on. There are explicit formulas for solving cubics and quartics but no such formula exists for higher degree polynomials. Hence the need for numerical optimisation if you want to handle an arbitrary number of outcomes



I have thought about using a Taylor series approximation in the higher degree cases to avoid the numerical optimisation but I'm not sure if this would work or what the loss in accuracy would be like



All of this is assuming you are placing a bet on only one of the outcomes. Once you start thinking about placing bets on multiple outcomes simultaneously then you pretty much have to use a numerical method although there are specific "solvers" like [https://en.wikipedia.org/wiki/Kelly_criterion#Multiple_outcomes|the method described in the multiple outcomes section of the Wikipedia article](https://en.wikipedia.org/wiki/Kelly_criterion#Multiple_outcomes|the method described in the multiple outcomes section of the Wikipedia article)



I am surprised there is not a lot in the literature about closed form solutions for Kelly staking with "large" numbers of outcomes that you see in almost all horse races. It seems that the general philosophy is to throw a numerical solver at it which I think reflects academics' lack of real world experience. If you aspire to be doing these calculations on every market book update you need to be fast

*Tags: Strategies*

---

**Mo** - *09:30:45*

I found this nice short article on the three outcome case: [https://elliotnoma.wordpress.com/2013/04/21/the-kelly-criterion-for-three-outcome-gambles/](https://elliotnoma.wordpress.com/2013/04/21/the-kelly-criterion-for-three-outcome-gambles/)

*Tags: Strategies*

---

**AndyL** - *12:58:40*

I really need to create myself a "pricing model", seems all you clever guys are into this...?

*Tags: Strategies*

---

**Jonjonjon** - *16:34:56*

It's a broad area. But in general it's predicting future moves using historical data.

*Tags: Data Quality*

---

## 2023-08-13

**Mo** - *11:18:57*

If you're predicting future price moves doesn't that mean you have a pricing model?

*Tags: Strategies*

---

**R** - *12:25:28*

[@UBS7QANF3](@UBS7QANF3) solving kelly for multiple simultaneous outcomes only has analytical solutions for when they are mutually exclusive (i.e. only one horse can win a race and the rest lose - excluding draws).



For when things get more complex (e.g. placing WIN and FORECAST bets for example), don't you need to know the correlation structure between bets in order to determine kelly?

*Tags: Deployment, Strategies*

---

**R** - *12:26:23*

Also, so glad to see people leveraging Kelly and the repo looks fantastic.  I had built my own too, using cvxpy though instead of scipy optimizers

*Tags: Strategies*

---

**AndyL** - *13:57:37*

[@UBS7QANF3](@UBS7QANF3) so yes i have a model i suppose, but my model is based upon the current price of a selection, rather than a model that says perhaps  horse1 fair price is 4.6, horse2 7.8, etc....

*Tags: Strategies*

---

**Mo** - *13:59:22*

Not entirely clear what you mean. What is the output of your model? Is it a probability?

*Tags: Strategies*

---

**Mo** - *14:03:44*

I think you can turn that into a pricing model

*Tags: Strategies*

---

## 2023-08-14

**PeterLe** - *12:52:15*

Question Please: As a relative beginner to Python, Im learning pandas (dataframes etc) and how to model data. I've used the price recorder to create some data in the form a of a CSV file..from my recorded markets..

Then I'm using sklearn, LinerRegression, seaborn etc...

When I compare the relationship between Profit and a particular variable, when that variable is below x, I get :

Test 1

Coefficient: [-0.00026511]

Intercept: 0.8063449241964358

(Sloping downwards left to right) - Not good,but expected



Whereas when I run it when the variable is above x I get :

Test 2

Coefficient: [0.06673513]

Intercept: 0.10630060921962795

(Sloping left to right upwards) - Good and proven in real world bets



(It should be noted that this is a very small data set, Ill be running on bigger dataset when i have found my feet..)

Now I know that from real world bets placed this variable above x definitely increases profits, but in test 2, the coefficient is very small?



So questions please:

Is linear regression a useful tool? Do the pro's on here use it in a similar way?

Given that you only need a slight edge, should I be confident in the signal with such a low coefficient?

Other than ML (Which is way above my head for now :grinning:) and linear regression, what other python tools do you use to model your data?

Thanks in advance

*Tags: Getting Started, Feature Engineering, Strategies*

---

**R** - *16:52:33*

[@UBS7QANF3](@UBS7QANF3) can't say on speed, I know it leverages other libs for certain routines based in C so it's fast enough that I've never thought to profile it against scipy.



Thing for me was just more granular API on optimization problems.

*Tags: Performance, Strategies*

---

## 2023-08-15

**PeterLe** - *14:23:37*

Quick Question Please. Using the price recorder on one month of GB and IE Horses racing produces a CSV of circa 2.65gb...

How big a CSV file can you use before pandas start to struggle with it?

thakns in advance

*Tags: Feature Engineering*

---

**liam** - *14:25:13*

Pandas will be fine, it will be your CPU/ram that becomes the problem, why so big? / do you need everything you are recording?

*Tags: Feature Engineering*

---

**Unknown** - *14:30:49*

No you're right, I don't need everything, just in play. I need to fix that going forward...

Its really interesting looking at this data

I was checking something else as per the attached graph...The blue vertical line is the suggested optimal value of a certain variable...the red line is the value I've been using for years..(through trial and error)

Its only a slight change but I'm sure this will increase profits

*Tags: Errors Debugging*

---

**liam** - *14:53:11*

I imagine the red line probably is optimal due to matching / adverse selection, plotting ROI can help visualise as I imagine it starts high and drops as x increases 

*Tags: General Technical*

---

## 2023-08-17

**George** - *22:15:50*

This evening I have been playing around with this fun tool: [https://www.sportsbookreview.com/betting-calculators/kelly-calculator/](https://www.sportsbookreview.com/betting-calculators/kelly-calculator/), using the Exclusive Outcomes option. I noticed something unexpected (to me, at least!).



If a bettor has two selections with expected positive value, they could calculate the Kelly stakes individually, as if the two bets were independent of each other, and place both bets. It might be a bit naive to do this, but of course it can be done. Let's call the wrongly-assumed-independent-Kelly stakes for the two runners: a1, and b1.



Then instead, if a bettor goes onto that website and uses the Kelly calculator for two bets using Exclusive Outcomes, they will be returned different (correct) Kelly stakes; we can call them a2 and b2.



I have been playing around comparing a1 with a2, and b1 with b2. To my surprise, I noticed that a2 &gt; a1, and of course b2 &gt; b1 also.



It seems that the conclusion is: if you have two selections with positive edge, you should bet MORE on each of them compared with if you had only had one selection. To me, that sounds counter-intuitive. Can anyone explain why this is?

*Tags: Strategies*

---

**George** - *22:33:21*

yes, it makes sense. I suppose as a human I was thinking, well, if I have multiple selections then probably it's more likely my model is wrong so i should be more cautious not more aggressive. but obviously that's not an input into the Kelly optimisation so I shouldn't have expected it to come out like that.

*Tags: Strategies*

---

## 2023-08-19

**thambie1** - *14:54:51*

Not seeing any issues (though not using BFLW/Flumine)

*Tags: General Technical*

---

**D C** - *14:57:18*

Must be my server then (I'm also not using flumine). Having to kill connections due to heartbeat timeouts about 20 times so far this afternoon. Usually its about 2 a week.

*Tags: Deployment*

---

## 2023-08-20

**AI Trader** - *12:45:47*

[@U4H19D1D2](@U4H19D1D2) If I understand correctly you are forecasting the WIN probability. This is a supervised classification problem. You can use any classification algo for the task (logistic regression, random forest, ...). What you said about the fact that there are way more loosers than winners might be very important. This has a name in ML: imbalanced classes. There are multiple ways to address this. One is oversampling, ie, just repeating the minority class (as I think you have done). Another technique is simply putting more importance in the classification of the minority classs. This can be achieved thorugh the  *class_weight = "balanced" parameter in RandomForestClassifier*.



Also, as people have mentionned, be careful with overfitting. The general idea in ML is to split your data in three groups: Train, Validation and Test. You use the train to train the model, the validation to test different hyperparameters, and the test to have an idea of the true accuracy of the model. This is extremely important for hyerparameter tunning in order to avoid overfitting.



Another important thing it to choose well the metric you are optimising. Just accuracy of the model alone is unlikely to be a good metric, especially if you size your bets based on probability. In that case I would suggest other metrics such as f1-score/auc.



I am happy to give more help if I can.

*Tags: Strategies*

---

## 2023-08-21

**liam** - *08:55:58*

Thanks [@U04980ZT6UX](@U04980ZT6UX) that all makes sense, I am going to get stuck back into this soon, just looking for an easier problem to tackle

*Tags: General Technical*

---

**AndyL** - *17:43:04*

Thanks, was expecting that. However, isn't aspects like "Running style", "pace", "race phase", all relevant as a "Variable" in these so called ML "Models" ?

*Tags: Strategies*

---

## 2023-08-23

**George** - *11:01:33*

I would like to run two strategies from two different subaccounts.

Can I do this using one flumine process?

If not - is it ok to have two separate flumine processes running on the same machine?

*Tags: Multi Client*

---

**liam** - *12:36:46*

Yes yes



[https://betcode-org.github.io/flumine/clients/](https://betcode-org.github.io/flumine/clients/)

*Tags: General Technical*

---

**liam** - *12:41:39*

No easy abstraction in the strategy itself but I normally put the client in the `strategy.context`

*Tags: Strategies*

---

**George** - *14:24:14*

in the Kelly package (or elsewhere!) is there a way to do the numerical optimisation for simultaneous multiple mutually-exclusive bets in the same market?

*Tags: Strategies*

---

**R** - *14:30:19*

no need for numerical solvers in explicit mutual exclusive cases, it can be solved analytically

*Tags: General Technical*

---

**Y B** - *22:41:48*

Anybody trades soccer here and doesn't mind sharing what datasources you are using for historical scores / match stats AND what data sources you are using for live data?

*Tags: Deployment*

---

**Mo** - *22:43:20*

For live - [https://github.com/betcode-org/betfair/blob/master/examples/exampleinplayservice.py|Betfair inplayservice](https://github.com/betcode-org/betfair/blob/master/examples/exampleinplayservice.py|Betfair inplayservice)

*Tags: Deployment*

---

**Y B** - *22:51:46*

Thanks, that's helpful.

*Tags: General Technical*

---

## 2023-08-24

**Y B** - *17:40:03*

Question about reference data. Where do you get reference data about football teams? i.e. trying to have a strict mapping between a team name and its ID, league refdata (what teams are playing, etc.)?

*Tags: General Technical*

---

**Riccardo Fresi** - *18:08:33*

i use this self-made function, maybe can help, or improved

```def get_statistics(in_play_only=None):

    for event_type in Calcio_event_type_id:

        # prints id, name and market count

        #print(event_type.event_type.id, event_type.event_type.name, event_type.market_count)

        calcio_id = event_type.event_type.id



        # list of events

        event_list = trading.betting.list_events(

            filter=filters.market_filter(

                event_type_ids=[calcio_id], # filter on just football

                in_play_only=in_play_only,  

                #market_countries=["IT"],  # filter on just GB countries

                # market_type_codes=["WIN"],  # filter on just WIN market types



            ), locale="IT"

        )[:10]  # Seleziona solo i primi 10 elementi della lista

        

        combined_runner_dict = {}  # Dizionario per memorizzare i dati dei runner fusi

        for event in event_list:

            #print(event.event.id, event.event.name)

            inplay = trading.in_play_service.get_event_timeline(event_id=event.event.id)

            

            market_catalogues = trading.betting.list_market_catalogue(

                filter=filters.market_filter(

                    event_type_ids=[calcio_id],  # filter on just football

                    event_ids=[event.event.id],

                    # market_countries=["GB"],  # filter on just GB countries

                    market_type_codes=["MATCH_ODDS"],  # filter on just WIN market types

                ),

                market_projection=[

                    "COMPETITION",

                    "EVENT",

                    "EVENT_TYPE",

                    "MARKET_START_TIME",

                    "MARKET_DESCRIPTION",

                    "RUNNER_DESCRIPTION",

                    "RUNNER_METADATA",

                ],  # runner description required

                max_results=10, locale="IT",

            )

            #print("%s market catalogues returned" % len(market_catalogues))

            for market_catalogue in market_catalogues:

                #print(market_catalogue.market_id,

                    #market_catalogue.market_name,

                    #market_catalogue.market_start_time,)

                

                for runner in market_catalogue.runners:

                    #print(runner.selection_id, runner.runner_name, runner.handicap)

                    if inplay.status:

                        

                        runner_data = combined_runner_dict.setdefault(f"{market_catalogue.market_id}_{runner.selection_id}_{inplay.time_elapsed}",{})

                        runner_data["event_id"] = event.event.id

                        runner_data["event_name"] = event.event.name

                        runner_data["market_id"] = market_catalogue.market_id

                        runner_data["market_start_time"] = market_catalogue.market_start_time

                        runner_data["selection_id"] = runner.selection_id

                        runner_data["runner_name"] = runner.runner_name

                        runner_data["handicap"] = runner.handicap

                        market_books = trading.betting.list_market_book(

                            market_ids=[market_catalogue.market_id],

                            price_projection=filters.price_projection(

                            price_data=filters.price_data(ex_all_offers=True)

                            ), locale="IT",

                        )

                        for market_book in market_books:

                        #print(market_book.market_id,

                            #market_book.inplay,

                            #market_book.status,

                            #market_book.total_matched,)

                            for runner in market_book.runners:

                            #print(runner.selection_id, runner.status, runner.total_matched, runner.last_price_traded)

                                runner_data = combined_runner_dict.setdefault(f"{market_catalogue.market_id}_{runner.selection_id}_{inplay.time_elapsed}", {})

                                runner_data["selection_id"] = runner.selection_id

                                runner_data["total_matched"] = runner.total_matched

                                runner_data["last_price_traded"] = runner.last_price_traded

                                runner_data["Status"] = market_book.status

                                

                                #runner_data = combined_runner_dict.setdefault(f"{market_catalogue.market_id}_{runner.selection_id}_{inplay.time_elapsed}", {})

                                runner_data["Home Score"] = inplay.score.home.score

                                runner_data["Away Score"] = inplay.score.away.score

                                runner_data["Home Corners"] = inplay.score.home.number_of_corners

                                runner_data["Away Corners"] = inplay.score.away.number_of_corners

                                runner_data["Time Elapsed"] = inplay.time_elapsed

                                runner_data["Match Status"] = inplay.in_play_match_status

                                

                                

                                available_to_back = runner.ex.available_to_back

                                available_to_lay = runner.ex.available_to_lay

                            

                            

                            

                                combined_runner_list = list(combined_runner_dict.values())

        if combined_runner_dict:

            df = pd.DataFrame(combined_runner_list)  

            #df.sort_values(by=['event_id'], inplace=True)    

        else:

            df = pd.DataFrame()  # Create an empty DataFrame

            #trading.race_card.login()

            #race_cards = trading.race_card.get_race_card(market_ids=[market_catalogue.market_id])

            #results = trading.race_card.get_race_result(market_ids=[market_catalogue.market_id])

            #print(results)

    

    return df ```

*Tags: Feature Engineering, Strategies*

---

## 2023-09-02

**AndyL** - *22:06:25*

[@UQL0QDEKA](@UQL0QDEKA) [@U05L8PZD2FM](@U05L8PZD2FM) Made some in-roads with basic Pandas, and a simple Lay inplay &lt; 1.5 “BaseLine”:

```Total pnl =  16640.0```

So i’ve turned my market stream data into a CSV merged with winner and a PnL column, but currently “every” market update that matches inplay Lay price&lt;1.5, bets a notional £1, hence total pnl above for just 3 days is huge, could equally be huge negative...

So I could add a random classifier, and say that refines the bet choice, but that could still be 1000s of unrealistic bets in reality, since matching is unknown? In reality it would bet maybe up to a certain liability of matched bets ... I guess at some point you need to run a real Flumine backtest?

*Tags: Feature Engineering*

---

**PeterLe** - *22:40:26*

Andy (given my VERY basic knowledge of ML) I think you need to initially add some more features into the ML, ie so, back, lay and LTP book etc ( you can use pandas to create these for you very easily) then create a simple strategy ( or something you are doing currently). You can the use this to learn about ‘feature importance’ … and that led me to an area I’d not thought about before (in hind sight it makes perfect sense now)

so I used this to explore and investigate things, it’s not a back test though and won’t replace flumine

*Tags: Feature Engineering, Strategies*

---

## 2023-09-03

**Joe** - *09:06:51*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) There are two different things going on.



If you are trying to learn a strategy or parameters for a strategy I would use ML on real back tests running real execution logic with simulated exchanges rather than try and regress a dependent 'win' or 'profit' variable against some independent vars in some tabulated data when doing the type of profit training you are talking about.



If you are trying to create a model that produces a win probability (that will later be used as a strategy input/signal) from some independent vars then simple logistic regression of tabulated data is fine.



If you want to go full deep learning on this then you can use ML to train the lot at once, i.e. all parameters of all signals plus all hyper/execution parameters, with real execution logic in a simulated exchange. But this does require fast software and lots of hardware.

*Tags: Strategies*

---

**Unknown** - *10:19:44*

I was just looking at my code..

I dont know if this helps Andy (or hinders :grinning:) but these are the building blocks I used:



```# Load your dataset



# Histogram and Density Plot for 'X'



# Correlation Matrix considering only numeric columns



# Take a 100% random sample of the dataframe (Or reduced value to save time etc)



# Selecting features and target



# Handling any NaN values



# Splitting the data



# Training Random Forest model



# Making predictions



# Calculating Random Forest Accuracy



# Test set predictions and accuracy



# Feature Importance



# Confusion Matrix



# Initialize profit/loss tracking for all three strategies



# Lists to store profit/loss over bets



# Simple, SimpleSP and ML strategies



    # Simple Strategy



    # SimpleX Strategy



    # ML Strategy



# Plotting the Profit/Loss graph```

Re the matching rate as Liam mentioned, I know from real bets that I have a matching rate of 52% so i factored that in

Hope that helps? (Conscious Im new to this sort of thing, I hope Im not sending you down the wrong path, so Joe is worth listening to rather than me)

(PS I found the Correlation matrix worth looking at, its starts to get the old grey matter moving :grinning:)

*Tags: Getting Started, Feature Engineering, Strategies*

---

**AndyL** - *11:04:54*

Thanks [@UQL0QDEKA](@UQL0QDEKA) [@U05L8PZD2FM](@U05L8PZD2FM) [@U4H19D1D2](@U4H19D1D2)

Yes, I have merged the race winner for W/L, and thus PnL.

So I think where I want to head, is I have a rough "Idea", but don't really know what variables might help make it more profitable (assuming it is to start with!)... So I was doing an initial simple baseline to understand the process... but then not sure I could equate having many many thousands of bets how you would use that in reality or refine it.

Your step by step is useful thanks [@UQL0QDEKA](@UQL0QDEKA) i've done some of the first few...

[@U05L8PZD2FM](@U05L8PZD2FM) I think I probably want to implement the basic Flumine strategy for my "idea", even though it maybe rough and ready and probably make it not too selective, and possibly not very profitable (or -EV)! Then run that through the Flumine simulator capturing CSV data for the bets, and to refine using ML what the dependent features are that would improve it?

I don't think going down the brute force, lots of hardware route sounds right for me.

*Tags: Feature Engineering, Strategies*

---

**Joe** - *11:34:09*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) possibly it depends how simple your strategy is, i've definitely improved things by putting an ML optimised filter on the output of existing strategies which is I think what you are suggesting. The problem comes if the strategy is adaptive / contextual placing multiple bets into each market contingent on previous bets placed / matched. I think most strategies are really like this right? Depend on real world info, market data, AND current position. You can't optimise such a strategy by filtering simple tabulated output data/bets since if you remove one bet then in reality your strategy wouldn't go onto place some others which you may want to keep.

*Tags: Strategies*

---

**AndyL** - *11:39:58*

[@U05L8PZD2FM](@U05L8PZD2FM) yes, that's exactly the problem I was thinking about, trying to equate ML with a real world strategy, as you say it typically adapts to what us matched, and adverse selection is probably hard to model..?

I think I'll head down the optimization route, as i can see through my current backtesting workflow im probably not being very good at optimising 

Thanks

*Tags: Performance, Strategies*

---

## 2023-09-04

**Paul** - *12:26:36*

Are trying to solve for a filter/classification, a prediction, or a control problem? You need to think very, very carefully about the problem you’re solving for before you choose an approach and the appropriate features

*Tags: Feature Engineering*

---

**Paul** - *12:30:14*

Because the approach you’re taking with random forest is classification, but the problem you’re struggling with trying to wrap your head around sounds like a control problem (which is reinforcement learning).

*Tags: General Technical*

---

## 2023-09-05

**liam** - *13:50:07*

Hmm yeah I think that should work but I think my problem is that I have shifted the orange graph by adding 'd' to the x axis

*Tags: General Technical*

---

**Dave** - *21:12:23*

Does anyone measure their trading latency? If anyone is willing to share, I'm particularly interested in (order_submit_time - book_publish_time), separately for pre-play and in-play. :slightly_smiling_face:

*Tags: Performance, Strategies*

---

## 2023-09-07

**Unknown** - *16:49:11*

Say you have a model that you are happy with for the most part but falls off at the extremes....do you

*Tags: Strategies*

---

**thambie1** - *17:01:43*

I have all three strategies currently deployed for various models/situations. #3 is ideal but takes more effort, #1 is second best and is super easy, #2 for when the loss is too little to be worth adapting your strategy for.

*Tags: Deployment, Strategies*

---

## 2023-09-08

**D C** - *09:40:35*

8-10 ms latency? Wow - I just looked at august and the lowest latency I had was 1045 milliseconds (inplay horses). However, I measure mine from time the order is created by a strategy, to the time that I receive the bet on the order stream. I place with the async flag and the lowest latency I have on the placement is 20ms. My SQL is quite basic and I am not familiar with how to extract the median.

*Tags: Performance, Strategies*

---

**D C** - *09:58:32*

Yeah I probably have lower latency in reality as I use an async component so I log transmission time as the time that the non-blocking write call returns - it doesn't mean that is when the write actually happened. Although I log the publish time I don't actually use that in the database but I can see that does reduce it further. Even with all that taken intou account though I am nowhere near single figure latencies.

*Tags: Performance*

---

**Dave** - *20:55:40*

That's correct Liam, I am computing as (time.time()*1e3 - book.publish_time_epoch). I'll double check my clock and make sure it's not behind for some reason. I am sure I am trading on fewer events though which could explain it (and possibly simpler logic). Possible dumb question: I assume you have n flumine instances running a number of strategies each - even though your machine's CPU may not be saturated, have you looked at latency impact of just splitting one of those instances across two boxes (i.e. cheap way to truly parallelize with no GIL in the way)?

*Tags: Performance, Strategies*

---

**D C** - *21:27:42*

Well, I see that as just 45ms latency given 1000 for the inplay delay. I was actually pretty happy with my mean latency times which are around the 1150 mark until I saw your post. I suspect that most of this is down to the asynchronous component I use. I'm moving to a connection pool implementation so will be interesting to review things after that.

*Tags: Performance*

---

**Dave** - *21:33:43*

ahh okay, your 1045ms is total roundtrip, with 1000 being in-play delay, so 45ms includes: (book recv latency + trading logic latency + order submission latency + ack latency), so your actual reaction time to the market book tick may well be single digit ms (which is what I am measuring, i.e. how long between book publish time and triggering an order to Betfair).

*Tags: Performance, Strategies*

---

## 2023-09-12

**Johnny** - *14:38:11*

[@U4H19D1D2](@U4H19D1D2) on your ML medium article you wrote "Implementing random sleeps in between scrapes and using a mix of ec2 instances and serverless I was able to get a years worth of data after a few days (still scraping the rest)".  Assuming these were not live markets, how did you achieve this?

*Tags: Deployment*

---

**Justice** - *14:47:18*

[@U4H19D1D2](@U4H19D1D2) could you link me to the article? I scraped 17 years of data from RacingPost and Timeform into a relational database. Took 2 weeks to run due to rate limiting lol

*Tags: General Technical*

---

## 2023-09-13

**Justice** - *16:58:28*

[@U027P3N2WMQ](@U027P3N2WMQ) It was a huge automated data pipeline project I did for a client. I believe it is still in use. Scrapes data from racing post, timeform, at the races and also the betfair sp data. The data and all the pipelines are hosted on Azure and I don't have access to them anymore. I'll see if I can find a pre-production schema

*Tags: Deployment*

---

**Pramod S** - *17:19:34*

So how to get this data.

*Tags: General Technical*

---

## 2023-09-16

**mandelbot** - *12:08:22*

I have a strategy that I've run a monte carlo sim on (thanks to [@U016TGY3676](@U016TGY3676) for sharing the code). It gets beaten by monte carlo around 6% of the time. (~1700 low odds lay bets). Does this sound like a viable strategy or not enough info?

*Tags: Strategies*

---

**birchy** - *13:10:32*

I have a couple of strategies that are basically churners which are in the 5% - 10% range when run through the MC sim. My 'best' strategy is &lt; 0.1% and zero if I feed it &gt; 2000 bets. FYI, setting the sim count &gt; 10K doesn't make much difference other than it takes longer to run.

*Tags: Strategies*

---

**birchy** - *15:18:08*

6% is workable live but, in my experience, is generally a rollercoaster that makes a small profit long term.

The 'real' edges will give you 0% as [@UQL0QDEKA](@UQL0QDEKA) suggested.

Also worth noting that I've found that strategies that barely break even in flumine simulation can be much better when run live due to real-world match rates. For me, the key is to find strategies where the simulation ROI is decent (i.e. &gt;5%) when you assume a 100% match rate. I have my own code to calculate the P&amp;L at 100% match rate but I believe [@U4H19D1D2](@U4H19D1D2) has added it to flumine simulation recently.

*Tags: Deployment*

---

## 2023-09-17

**D C** - *15:15:41*

What do you mean by "assume a 100%" match rate [@U016TGY3676](@U016TGY3676)? Surely we could fabricate any strategy that is "profitable" if we assume that? What am I missing?

*Tags: Strategies*

---

**birchy** - *16:37:06*

[@UUE6E1LA1](@UUE6E1LA1) You're absolutely right, a strategy that lays all runners at 1.01 would clearly make a profit with 100% match rate. All of my stuff is very close to the spread, i.e. within a couple of ticks so assuming a higher match rate gives an indication of where my bets are vs the 'true' price.

MC simulation takes a similar approach, where it assumes your bet price was the 'true' price.

*Tags: Strategies*

---

## 2023-09-18

**D C** - *10:53:16*

I still don't quite get it [@U016TGY3676](@U016TGY3676) - if I decided that my strategy was just to lay at the best back price and back at the best lay price, assuming a 100% match rate is going give me large simulated profits. I really feel I am missing something here.

*Tags: Strategies*

---

**D C** - *11:11:48*

Yeah I am trying the linear model type approach using very similar explanatory variables - simply because it is easy to get predicted results (one of the reasons I don't like ML - getting the predicted values out of the black box when you don't use python). Accuracy could be better but I think it will be good enough - particularly given how much variation you get in when betfair decide to throw the market inplay anyway.

*Tags: Strategies*

---

**Unknown** - *12:53:40*

[@UUE6E1LA1](@UUE6E1LA1) I simply use the 100% match rate as a 'guide' as to the potential profit in a system.

By way of example; In image 1 it shows a small profit on a back system, but the 100% match would be a disaster, so I'm performing above the theoretical or potential of that system. Therefore Id park this.

Conversely, the bottom image shows that whilst this strat is profitable, it is no where near as good as it could be. Id therefore try to work out how I might improve matching rates. (there would be no point in trying to increase the matching rate on image 1)

I know this doesn't answer your question specifically, but I thought others may find it useful as how it may be used.

*Tags: General Technical*

---

**liam** - *13:11:15*

You can also use it see if a drop in pnl over time is due to matching / model performance / market

*Tags: Performance, Strategies*

---

**D C** - *13:39:27*

OK thanks for a more detailed explanation. I can see that there must be a lot more to it and maybe how you determine 100% match is specific to each strategy (for example, if I back everything at 1000-1 and assume it matches I will be winning on a simulation).



I can also see how it might be useful for pre-start betting but still struggle with inplay because every back bet placed inplay (that is left to run) will almost certainly be matched for all losers, but the same can't be said of losers in pre-off markets. I guess I need to do some deeper thinking.

*Tags: Strategies*

---

**liam** - *13:47:05*

It's not for strategy creation, think development / monitoring, so your first paragraph isn't relevant

*Tags: Strategies*

---

## 2023-09-20

**James** - *09:52:18*

I've been trying to come up with some punting strategies for Aus racing. I've found something that looks promising but CL2 races seem to be unprofitable. The strategy looks for selections in CL1-CL6 races. It is profitable over a decent sample in all classes except for CL2 which seems odd. It is possible it is just variance or a quirk in the data but unlikely. I know very little about Aus racing so wondered whether there is anything unique about CL2 races?

*Tags: Strategies*

---

## 2023-09-27

**joe taylor** - *11:21:07*

Hi all! Had a question for people who do some pre-inplay trading. Hedging at bsp vs carrying the position and not hedging, which has better EV? (I know from betfair’s research EV should be same, but wanted hear everyone’s experience on on this) 

*Tags: Strategies*

---

**Mo** - *13:04:56*

You are sort of correct about laying at BSP being winning strategy but two points. Number one - because you don’t know the exact price you will end up laying it, sizing is difficult to get right. Number two - you need to consider commission which is arguably the reason why BSP has an overround

*Tags: Strategies*

---

**Mo** - *13:08:24*

As for your second point if you can’t see that placing bets at odds that have a margin over the fair probability is going to cost you money then I can’t help you

*Tags: General Technical*

---

**D C** - *17:54:46*

Ive never placed a BSP bet in my life so this might be a dumb question, but is it susceptible to manipulation (and if so do many people try) ?

*Tags: General Technical*

---

## 2023-09-28

**ShaunW** - *13:08:07*

[@UPMUFSGCR](@UPMUFSGCR) "Hedging is for losers that have been brainwashed by PW's YouTube tutorials.". You're wrong, it's very straightforward to test that hypothesis and in certain situations it's very useful to hedge.... Besides, with bsp being 0ev it's a harmless bet, but it has the effect of reducing the size of the wins and gains which reduces commission (unless you're keen to acrue more).  There's no hard and fast rule, I have one strategy that is better not hedging and another which would lose if I wasn't hedging.   Saying you should never hedge is as daft as saying you always should.

*Tags: Strategies*

---

**liam** - *14:27:20*

[https://robotjames.com/posts/kwant-signal-trade-offs-in-the-real-world/|kwant signal trade-offs in the real world](https://robotjames.com/posts/kwant-signal-trade-offs-in-the-real-world/|kwant signal trade-offs in the real world)



Well worth reading the other posts, he is able to articulate a lot of what I do a lot better than I have ever managed :joy:



&gt; *&lt;https://robotjames.com/posts/how-scalable-is-this-strategy/

&gt; |how scalable is this strategy?&gt;*

&gt; friend asks "how do i know how scalable this strategy is?"

&gt; one very simple thing you can do to get an idea, is to look at the decay in simulated performance when you delay acting upon your signals.

&gt; how much worse does it look if you delay your rebalance trades by 15m, 60m, 4h etc?

&gt; slow decay means you might have a chance of sizing it up.

&gt; if it falls off a cliff, you probably gotta stay small and nimble in that strategy.

&gt; we're always trading at other people's prices and can only eat what we are fed.

&gt; beep....boop

*Tags: Performance, Strategies*

---

**joe taylor** - *14:34:47*

[@UEA14GBRR](@UEA14GBRR)  if we agree to the fact that overall hedging at bsp gives 0 EV, it would also mean that EV of any strategy(pre-inplay)would be equal to pnl of all its trades hedged at bsp. I don’t understand why you say that one of your strategy would be loosing if unhedged

*Tags: Strategies*

---

## 2023-10-15

**Jonjonjon** - *19:40:23*

Have you tried throwing the same data at different algorithmns? I found that for my dataset, Elastic Net CV gave better out-of-sample performance than Random Forest.

*Tags: Performance*

---

## 2023-10-16

**Unknown** - *10:33:40*

*ML Day 70*



Haven't given up, after exploring a few algos I found that using XGBoost seemed to be the most applicable / get the best results in terms of accuracy on the problem I was trying to solve. Using my current prediction I used the algo to predict the error (delta), from my understanding this is a common technique (does it have a name?) and proved to be far more accurate than starting 'fresh'.



Had a few issues with latency when simulating using the sklearn integration as its considerably slower compared to using the low level [https://xgboost.readthedocs.io/en/stable/python/python_api.html|library](https://xgboost.readthedocs.io/en/stable/python/python_api.html|library).



However the improvements just didn't equate to more profit, I have since worked on trying to get the algo to reveal what it was doing. I started reverse engineering / brute force by plugging in values that I believe had the highest importance and graphing the results. Without revealing too much this gave me a regression model for the variables as opposed to an 'if statement' on the min/max, not sure this is how I am supposed to be using machine learning but its working for me right now :joy:

*Tags: Errors Debugging, Performance, Strategies*

---

**PeterLe** - *14:27:19*

The biggest learning for me (so far) is clearly understand the problem to which you are trying to gain an insight/solution.

As a complete newbie to ML I incorrectly thought that you could just supply a large datset and let ML do its thing.

From the thread I started on ML, one thing im keen to delve more into is particle swarm optimisation. Time permitting.

As an aside; Ive been running something live since the 19th Sept based on something I found via ML and the graph looks like the flight path of the first wright bros plane :grinning:

Still optimistic though..I think it will fly

*Tags: Deployment*

---

## 2023-10-17

**Jonjonjon** - *19:09:16*

Thanks for sharing Liam. I've tried predicting the error too, but with mixed results.

*Tags: Errors Debugging*

---

## 2023-10-22

**Unknown** - *12:17:52*

[@U01DVUAE2G1](@U01DVUAE2G1)  I just came here to ask the same question. Something has definitely changed on the WIN markets for both UK and Aus. The TVs are trading in much tighter ranges across every runner...

*Tags: Strategies*

---

**Adam** - *13:07:53*

Apology in advance if this is the incorrect channel to ask this question. If so, please let me know where to go, thank you :slightly_smiling_face:



Hey all, I’m new to flumine and I’m following through this amazing guide: [https://betfair-datascientists.github.io/tutorials/How_to_Automate_1/](https://betfair-datascientists.github.io/tutorials/How_to_Automate_1/). When it comes to streaming horse markets, I was wondering if there is a way to continually add markets to an strategy (that is already streaming events) as the markets become live?

The reason being is I want to set up the strategy at the beginning of the day, and as races become live, I want to add that race to the strategy for processing, and remove it as the race becomes closed or suspended. I can see there’s a `remove_market` method in the `BaseStrategy` but I can’t see an equivalent `add_market`. This suggests that the way it’s supposed to work is that we create the strategy filtering on all the australian horse markets at the beginning of the day and remove them when they’re suspended? There’s also a method called `process_new_market` but is this called when a market becomes live or does this suggest it can be used for what I’m asking?

*Tags: Getting Started, Deployment, Strategies*

---

**Mo** - *13:16:45*

If you are streaming then you will automatically receive data for new races

*Tags: General Technical*

---

**birchy** - *13:20:26*

[@UUE6E1LA1](@UUE6E1LA1) volume looks to be same at a quick glance. I think someone is trying out a new model. :wink:

*Tags: Strategies*

---

**Aaron Smith** - *14:13:04*

[@UUE6E1LA1](@UUE6E1LA1) never checked those, would have to research that, however i m not sure if it makes much sense to use those to calculate EV, i d feel a lot less confident in those than betfair prices, as those are the ones im competing against. Also, if the ones from your site were more accurate, it can only really be by a tiny margin, as otherwise ppl would just arb between the two.



[@UQL0QDEKA](@UQL0QDEKA) interesting idea, however i wouldnt have much to compare with, as i ve never checked it before :smile:



[@U016TGY3676](@U016TGY3676) did this effect your EV or volume? or PNL? what eggs me is that volume and EV have remained stable and solid through out all this. I ve had it before that a strategy wouldnt work anymore after some time, but that would always be reflected in the EV. Not much to wonder there, the market evolved some, but here i m just confused.

*Tags: Strategies*

---

**Adam** - *22:51:55*

Hey Mo, thanks for responding. I am streaming. So just subscribe to all races I care about (e.g. all Australian horse markets) when I init my strategy at the beginning of the day, and remove the markets throughout the day as they become suspended/closed by calling `remove_market`

*Tags: Strategies*

---

**Derek C** - *23:29:10*

There should be no need to call remove_market, and once you are subscribed flumine just keeps running - so need to stop/restart each day.

*Tags: General Technical*

---

## 2023-10-23

**Derek C** - *09:39:37*

_'You can’t just keep a stream open all day every day and just tell it to “monitor every horse market in Australia” indefinitely?'  - T_hat is literally what you do with Flumine/BF streaming. Subscribe to a filter condition and let it run, new markets appear in the stream when they match the filter.

*Tags: General Technical*

---

**Derek C** - *12:05:27*

Thanks to [@U4H19D1D2](@U4H19D1D2) - Flumine can run interrupted for weeks and months and auto-recovers from common  issues.

*Tags: General Technical*

---

## 2023-10-24

**PeterLe** - *14:36:28*

[@U01DVUAE2G1](@U01DVUAE2G1) wrote :

[@UQL0QDEKA](@UQL0QDEKA) interesting idea, however i wouldnt have much to compare with, as i ve never checked it before :smile:

..well i never checked the pipework under the sink, but if i did and there was a leak, Id recognise the problem straight away :grinning:

*Tags: General Technical*

---

## 2023-10-28

**Jonjonjon** - *19:06:43*

What generic, non strategy related techniques, do people use to get more size off?

*Tags: Strategies*

---

**Jonjonjon** - *19:23:50*

that doesn't seem to work for me.



I've used your excellent market_id_to_random function to assign each market to one of two strategies... Strategy A and Strategy B.



Strategy A has a max stake of £100.

Strategy B has a max stake of £200.



They both make about the same amount of money.

*Tags: Strategies*

---

## 2023-10-31

**Riccardo Fresi** - *13:20:48*

Hi guys, looking at lowestlayer is not clear to me:

1. once placed an order for a marketid the strategy add new orders till market is open? or it make just 1 order for 1 market once the logic match?

2. if n°1 is true (more orders for 1 market) is there any way to place just one?

3. i don't get how to simulate it, i mean, i use simulation script as template, but i don't get how to use it on live event?

*Tags: Deployment, Strategies*

---

**liam** - *13:23:51*

1. The [https://github.com/betcode-org/flumine/blob/06451c28252b87a94132dc1f29777cc696937f98/examples/strategies/lowestlayer.py#L46|code](https://github.com/betcode-org/flumine/blob/06451c28252b87a94132dc1f29777cc696937f98/examples/strategies/lowestlayer.py#L46|code) currently just checks for live orders on a market and places if zero, but you could change this to look at `runner_context.trade_count`

2. As above 

3. To simulate you just run the script? To run live you would just copy/paste the [https://github.com/betcode-org/flumine/blob/master/examples/example.py|example](https://github.com/betcode-org/flumine/blob/master/examples/example.py|example) but change the strategy 

*Tags: Deployment, Strategies*

---

## 2023-11-02

**foxwood** - *14:22:25*

Thought I'd seen something on here about how to skip to market closure on backtest to save time. Tried search which had "backtest skip to closed" in it's suggestion list but then found nothing and hit the 90 day archive message.  Anyone know the answer to this - if there is one ?

*Tags: General Technical*

---

**liam** - *14:25:50*

Have you read [https://betcode-org.github.io/flumine/performance/|this](https://betcode-org.github.io/flumine/performance/|this)?

*Tags: Performance*

---

## 2023-11-04

**liam** - *13:24:39*

What the hell is a [https://x.com/quantbeckman/status/1720717180029768110?s=46&t=MabdsEbyfbpsJHPcnVHwaQ|metamodel](https://x.com/quantbeckman/status/1720717180029768110?s=46&t=MabdsEbyfbpsJHPcnVHwaQ|metamodel)? 

*Tags: Strategies*

---

## 2023-11-06

**mandelbot** - *05:45:38*

I'm trying to estimate how much a BSP bet will affect the final BSP price. Can anyone shed some light on how to estimate slippage on BSP bets?

*Tags: General Technical*

---

**mandelbot** - *10:19:40*

Thanks for your thoughtful respons, you've confirmed what I've suspected, which is very helpful. I've also made several attempts to estimate it and have figured that logging the pre-off BSP liabilities and stakes gets very close. Also it seems doubtful that these are the final amounts matched at BSP because then you could move BSP quite significantly if that were the case. At least now I know my 20 pound bets won't destroy the market :joy:

*Tags: General Technical*

---

## 2023-11-16

**Peter** - *14:41:05*

Hi All, I'm confused by some flumine backtest results I'm experiencing with a new strategy. When I place a limitOrder, I sometimes see an average price matched that is lower than my submission price - I'm confused by this as the Betfair API docs say that with a limitOrder the price submitted is the lowest price at which any fraction of the bet may be matched. If anyone could help me understand why this occurs I'd really appreciate it

*Tags: Strategies*

---

**Unknown** - *15:30:22*

I used the priceBlockage example from the flumine GitHub and reproduced the problem - I see orders matched at prices lower than the submission price when I run this. Hopefully it's just something I'm doing with the logging.

*Tags: General Technical*

---

## 2023-11-23

**liam** - *08:31:10*

Is weighted average the only answer when combining probabilities? For context I am trying to form a more accurate price by including the starting price with my own predictions when inplay, theoretical example:



Starting price: 4.0

*Half Time*

Prediction A: 2.0

Prediction B: 3.0

Current price: 3.2

```Combined Probability = (ω1 * 4.0) + (ω2 * 2.0) + (ω3 * 3.0)```

Using a default weight I get:

```Combined Probability = (0.33 * (1/4)) + (0.33 * (1/2)) + (0.33 * (1/3)) ≈ 2.80```

However the starting price is obviously having a large impact in this calculation where I imagine it decays. The predictions themselves are based on historical data with no current knowledge of the starting price.

*Tags: Data Quality*

---

**Mo** - *08:38:13*

You could use a meta model (not sure if this is technically the correct term) which has the individual model outputs as features. If you use a non linear modelling technique this allows you to combine the predictions in a non linear way

*Tags: Feature Engineering, Strategies*

---

**liam** - *09:03:01*

[@UCQB6S222](@UCQB6S222) good point, updated



Meta model sounds interesting, I am then not sure what my target becomes / I am well out of my depth already tbh.



Regarding the decay is it stupid to assume a linear 'weight decay' of the SP from 1 to 0 at full time?

*Tags: Strategies*

---

**Mo** - *09:04:32*

Target would be the actual result but then you arrive at the usual in running problem of making many predictions on the same outcome over and over again

*Tags: General Technical*

---

**Mo** - *09:11:06*

It doesn't seem crazy to start with a linear decay. I think probably using backtest performance to compare different weighting schemes is a good way forward

*Tags: Performance*

---

**liam** - *09:36:18*

Its an interesting problem, its something I have been battling with since I started but there doesn't seem to be anything out there to solve it.



I find racing is simpler and tbh I have never got SP to work as a feature, ignoring it and let the market tell me has always worked well for me. Maybe including the current price can do this for other sports as well..

*Tags: Feature Engineering*

---

**D C** - *09:39:53*

With the GPS, my better performing models contains the BFSP. If I include things like LTP or current back/lay on offer, the effect of that in the model seems to overpower some of the other parameters - which makes perfect sense in most situations. Not sure about how to deal with that but I've found that the SP is significant - until it is not

*Tags: Strategies*

---

**liam** - *09:45:18*

:rolling_on_the_floor_laughing: I think the dark arts are causing my problems as I try and do things with ML :exploding_head:



Started coming up with some mad ideas involving meta model and looking at feature importance at different stages of the game

*Tags: Feature Engineering, Strategies*

---

**Ralegh** - *10:00:38*

If you create a formula with only a few parameters you could just do a grid search of sims for max pnl/max risk adjusted pnl and fix them

*Tags: Errors Debugging*

---

## 2023-11-26

**R** - *13:53:52*

Just a comment on meta stuff.



A meta model is typically a model that predicts another model.  E.G. model-A predicts the coefficients of your equation (w1, w2, w3) while model-B predicts the parameters of model A itself.  Model-B is then known as the meta-model.



This is getting into advanced ML though and I would first look at how well a logistic (maxEnt) classifier does with these price inputs.  The results are quite literally a linear combination as you describe with weights learned to satisfy a probabilistic loss on a binary outcome.  You could extend it to multi-class easily too.



I'd be happy to lend a hand if you wanted to demo it as part of your ML series too!

*Tags: Strategies*

---

**liam** - *19:25:06*

Ok, lots more things I don’t understand. I need to do some work to match up my data / models / prices and simulation which I am going to work on tomorrow so should have an update.



Assuming others are still interested in my struggles :joy: 

*Tags: Strategies*

---

## 2023-11-27

**D C** - *09:47:29*

I assume ML methods don't preclude you doing this anyway though [@U030GKBMCBF](@U030GKBMCBF)? I mean we can fit a quadratic to cubic data - the fit will be poor but you'll get a set of parameters as a solution. Do you get similar things in ML like you do for classical stats (other than generalisations of R^2).

I mean things for checking model assumptions like normality of residuals, uncorrelated residuals etc or is the onus on the user to check for a linear relationship prior to conducting an analysis?

I think part of the reason that I avoid ML stuff is because what I've read seems to just suggest that you can throw generic tools/algorithms at any old shit and you'll get a "solution" - but the adequacy of that solution always seems to be based simply on come cross validation on test data. Maybe I've just reached that age where enthusiasm for learning new things has dried up maybe its because I just prefer a type of analysis where parameters are "meaningful" when it comes to interpreting your results.

*Tags: Strategies*

---

**D C** - *10:37:56*

If you are placing lay bets in a field with more than 2 participants, when it comes to calculating kelly staking, do you just view each bet as a back bet at reciprocal odds on the remainder of the field? Or does the fact that your "win" event now is not from a set of mutually exclusive outcomes in some way invalidate the theory behind Kelly based staking. I am hoping that the answer is "yes" but if not can anyone point me to something that explains calculation of kelly stake for laying?

*Tags: Strategies*

---

**foxwood** - *10:51:10*

Idle thought - if you sum the probabilities (average if multiple bets on same selection) of your bets for each selection and convert that to odds then that could be used as the "single" effective odds to use on Kelly ?

This link has lots of official looking formulae to confuse things [https://www.aussportsbetting.com/2010/07/18/kelly-criterion-backing-andlaying-bets-with-betfair/#:~:text=The%20Kelly%20criterion%20for%20laying%20a%20bet&amp;text=In%20this%20case%2C%20you%20will,f*(d-1)](https://www.aussportsbetting.com/2010/07/18/kelly-criterion-backing-andlaying-bets-with-betfair/#:~:text=The%20Kelly%20criterion%20for%20laying%20a%20bet&amp;text=In%20this%20case%2C%20you%20will,f*(d-1))).

*Tags: Strategies*

---

**Leo** - *16:27:17*

[@UUE6E1LA1](@UUE6E1LA1)  depends what your goal is. Not always the case but often there is a trade off between predictive power and explainability, that's the nature of ML. I might not care about how a model came to a prediction if I'm confident though empirical tests that those predictions are useful for my intended goal. 

*Tags: Strategies*

---

**Adrian** - *19:52:28*

those probs look a bit wild to me. but it depends what you're feeding the model. and when is the prediction being made?

*Tags: Strategies*

---

## 2023-11-29

**Unknown** - *08:51:03*

I have a model and starting to use it in anger, out of interest is this the best way to be calling it in a production env, seems mega hacky..

*Tags: Deployment, Strategies*

---

**liam** - *09:08:26*

Its an xgb model, I have used the xgboost library rather than sklearn as I found it to be considerably faster, hence the matrix conversion

*Tags: Strategies*

---

**John Foley** - *09:12:52*

newer versions of that should definitely accept pandas and other objects without needing to convert. You using an older version maybe? 

*Tags: Feature Engineering*

---

**liam** - *09:29:50*

It seems that what you put in to train it wants when you predict (which makes sense), as far as I could work out when training it wanted something like this:



```dtrain = xgb.DMatrix(X_train.values, label=y_train.values)

dtest  = xgb.DMatrix(X_test.values, label=y_test.values)



# xgboost

param = {

    'objective': 'reg:squarederror'

}

n_estimators = 100

reg = xgb.train(param, dtrain, n_estimators, evals=[(dtest, 'eval'), (dtrain, 'train')], verbose_eval=False)



y_pred = reg.predict(dtest)```

*Tags: Errors Debugging*

---

**liam** - *09:52:04*

`numpy.float64` converting to keep things simple

*Tags: General Technical*

---

## 2023-12-01

**JFP** - *09:16:32*

Hi all, just wondering where I can find the documentation for placing bets on sports (cricket, soccer). Looking for the equivalent of  "runner in market_book.runners" for sport markets?

*Tags: General Technical*

---

**JFP** - *09:26:22*

Thanks, no log errors, must be something in the conditions. Will keep digging, thanks for your response.

*Tags: Errors Debugging*

---

## 2023-12-02

**James** - *23:22:56*

Hey, I’m looking at migrating a strategy to Flumine where I have some bets in place pre-play. and the kill them at a certain point in time in play.

I see fill and/or kill plastered all over the place in flumine and the betfair docs, and it seems to be what I want, but i can’t seem to find how to configure the duration before the kill. Can someone point me to the obvious thing I’m missing or do I need to iterate the strategy orders myself at the point in time and cancel any manually?

*Tags: Strategies*

---

## 2023-12-03

**James** - *00:13:43*

Also whilst on the potentially stupid questions. how can I set my strategy ref per strategy? I’m running multiple strategies in the same markets so running them in the same Flumine instance, the only place I can see it set is globally in config.

*Tags: Strategies*

---

**liam** - *07:06:31*

Sadly betfair fill or kill only executes during execution based on price rather than time. You can see in the flumine examples you can do this manually instead by cancelling after x seconds etc.

*Tags: General Technical*

---

**liam** - *07:08:21*

Flumine uses strategy ref to isolate different running instances, instead logging control should be used to record/log all trades/orders/markets etc.

*Tags: Strategies*

---

## 2023-12-04

**Unknown** - *09:37:19*

Morning All,

I thought Id take a look at one of my existing profitable long term strats to see if i could improve it with ML

These are the basic steps that the ML took ;



loading data Set into dataframe....

Creating Histogram and Density plot....

Creating Correlation Matrix considering only numeric columns....

Selecting features and target....

Split the data....

Train Random Forest....

Making Predictions....

Calculating Random Forest Accuracy....

Validation Accuracy: 0.7990462572417981

Test Set predictions accuracy....

Test Accuracy: 0.7989358028340348

Calculating the feature importance....

Calculating SHAP values for model interpretability...

Analyzing overall feature contributions...



Based on 3 months of inplay data (Circa 35m rows of CSV)



The ML model was significantly better than my simple model and I though it was a good idea to use SHAP to try and understand what the model was doing ...(Now I know next to nothing about ML so not sure if those who work in this field use SHAP or whether it is any good??)



The output of SHAP is shown in the attached illustration.

(Ive removed the descriptions of the features for now)



From Top down, Im only using features 1, 4 and 5 in my day to day strat, but it seems to suggest that these three features have a negative impact on the P/L? So that has confused me :grinning:

Should I really be focusing on features 6 and 7 that have a high positive impact?  (Incidentally use feature 6 in a separate strat successfully)



How would you ML guys delve down deeper? (I though that maybe I could use Pandas to look at each feature independently, although I realise that it wouldn't give a complete picture as it maybe dependent on the values of other features, if that makes sense)

Or maybe I could run SHAP again but on selected features?

My head thinks just use pandas (as the ML stuff takes ages to run). The more I play around with ML, the more I think the simple methods of data interpretation are much quicker and provides good enough insight/indicative ie good enough == good enough

I just thought it may be an interesting topic for discussion, thanks for any thoughts

*Tags: Feature Engineering, Strategies*

---

**liam** - *10:19:32*

For us mere mortals I think your last paragraph sums it up for me. So far for me the biggest insight has been its ability to quantify / visualise relationships between pnl and variables which aren't always linear.



However I have then taken the value out and updated my models / if statements to cater for the non linear relationship. In my case I have been keeping some features static and literally plugging in values to graph the change in outcome from the model, its dirty and could probably be replicated using regression but it works. Simply using the ML model was just matching or falling short of my previous strategies but using ML to advance things is a process that works well on my current strategies.

*Tags: Feature Engineering, Strategies*

---

**Derek C** - *17:08:57*

Hi [@UQL0QDEKA](@UQL0QDEKA), thanks for posting this. Is the model predicting win/lose/(draw) or is it predicting a price?

*Tags: Strategies*

---

**Derek C** - *17:27:42*

Does the 80% accuracy mean it was correct for 80% of the rows in the input data set, or correct for 80% of the markets? Reason I ask is that I found the precision measure more useful in my modelling - predicting the wrong result when LAYing is more expensive than missed opportunities :grimacing:

*Tags: Strategies*

---

**Derek C** - *17:40:26*

As for the SHAP results, I had to look it up to refresh my memory but I think this graph is showing most useful features from top to bottom. Feature 1 predicts a winner when it has a low value, as does 2 &amp; 3 (all the red dots are clustered together). I think feature 7 isn't very predictive - both high and low values are mostly red. That is my understanding but happy to be corrected. [https://datascience.stackexchange.com/questions/65307/how-to-interpret-shapley-value-plot-for-a-model](https://datascience.stackexchange.com/questions/65307/how-to-interpret-shapley-value-plot-for-a-model)

*Tags: Feature Engineering, Performance, Strategies*

---

**PeterLe** - *17:44:33*

Yes the 80% Accuracy, is a measure of how accurate the model is making predictions on both the validation and test data (the fact that they are very similar  is a good indicator

Thanks for the link, Ill have a read of that

*Tags: Strategies*

---

**birchy** - *20:44:37*

[@UQL0QDEKA](@UQL0QDEKA) have you looked at the feature importances? They can sometimes be useful to view good/bad features of your model

*Tags: Feature Engineering, Strategies*

---

## 2023-12-05

**PeterLe** - *08:46:31*

Hi [@U016TGY3676](@U016TGY3676), yes it did:

```# Feature Importance

print("Calculating the feature importance....")

importances = random_forest_model.feature_importances_

sorted_indices = np.argsort(importances)[::-1]

print("Feature Importance:")

for index in sorted_indices:

    print(f"{features[index]}: {importances[index]}"```

But it doesn't tell you much detail, that's why I opted for SHAP as i thought it would provide more detailed info

By the way; its been pointed out to me that whilst my initial thoughts were to see if ML could enhance a current system, the ML strat maybe a completely different strat to what my live one is

So now i know there is a better strat that the one im running live, I just have to work out what it is :grinning: :exploding_head:

*Tags: Feature Engineering, Deployment, Strategies*

---

## 2023-12-12

**Adam** - *09:15:56*

Hey quick question: does Flumine manage streams at the framework level or the strategy level? e.g. If I start two seperate strategies within the same framework that stream events from the *exact same markets (i.e* `framework.add_strategy(strategy1)` and `framework.add_strategy(strategy2)`), would it open one stream that both strategies ingest from, or two separate streams? If it’s the latter, would that increase latency since the network request is doubled?

*Tags: Performance, Strategies*

---

**liam** - *09:18:01*

[https://github.com/betcode-org/flumine/blob/125c18a11881359eb795b248b74838e858d14fc6/flumine/streams/streams.py#L94|logic](https://github.com/betcode-org/flumine/blob/125c18a11881359eb795b248b74838e858d14fc6/flumine/streams/streams.py#L94|logic)

*Tags: General Technical*

---

**PeterLe** - *21:12:25*

Evening. All this chat about TPD :grinning:, although I've never made it pay in all honesty.

I thought that id start to record it anyway by adding a second strategy to my current recorder



Would someone kindly check I've added it in correctly please?

I can see its recording the non TPD data, but i don't think there are anymore TPD races this evening in the US?

If someone can just cast their eye over it for comment ?

thanks



```import time

import logging

import os

import json

import betfairlightweight

from pythonjsonlogger import jsonlogger



from flumine import Flumine, clients, BaseStrategy

from flumine.streams.datastream import DataStream

from marketrecorder import MarketRecorder



logger = logging.getLogger()



custom_format = "%(asctime) %(levelname) %(message)"

log_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_handler.setFormatter(formatter)

logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



myacc = str(input("TPD uk horses please enter account name Test1: "))

app_key = str(input("Please enter key: "))

mypass = str(input("Please enter Password and 2FA: "))

trading = betfairlightweight.APIClient(myacc, mypass, app_key, certs=r"C:\certs")



client = clients.BetfairClient(trading, order_stream=False)



framework = Flumine(client=client)



# First strategy

strategy = MarketRecorder(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB", "IE"],

        market_types=["WIN"],

    ),

    stream_class=DataStream,

    context={

        "local_dir": "E:/ukhorses",

        "force_update": False,

        "remove_file": True,

    },

)



framework.add_strategy(strategy)



# Second strategy tpd

class TPDStrategy(BaseStrategy):

    def __init__(self, *args, **kwargs):

        super().__init__(*args, **kwargs)

        self.data_directory = "E:/ukhorses/tpd"

        os.makedirs(self.data_directory, exist_ok=True)



    def process_sports_data(self, market, sports_data) -&gt; None:

        filename = os.path.join(self.data_directory, f"{market.market_id}.json")

        with open(filename, 'a') as file:

            json.dump({"market": str(market), "data": sports_data}, file)

            file.write('\n')



strategy_tpd = TPDStrategy(

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB", "IE", "US"],

        market_types=["WIN"]

    ),

    sports_data_filter=[

        "raceSubscription"

    ],

)



framework.add_strategy(strategy_tpd)



framework.run()```

Thank you

*Tags: Strategies*

---

**Unknown** - *21:25:39*

```strategy_gps = S3MarketRecorderGPS(

        name="GPSWIN",

        market_filter=None,

        stream_class=RaceDataStream,

        sports_data_filter=['raceSubscription'],

        context={

            "bucket": BUCKET_NAME,

            "local_dir": data_dir_gps,

            "force_update": False,

            "remove_file": True,

            "remove_gz_file": True,

        }

)



framework.add_strategy(strategy_gps)```

One above works, have you missed out the stream_class?

*Tags: Strategies*

---

**PeterLe** - *21:32:26*

sorry dont know Derek. (My python knowledge is basic) :confused:

*Tags: General Technical*

---

**Derek C** - *21:39:29*

I think you will need this import at the top of the file:



```from flumine.streams.datastream import RaceDataStream```

and then add this to the TPDStrategy params:



```stream_class=RaceDataStream,```



*Tags: Strategies*

---

## 2023-12-13

**liam** - *08:18:43*

Its all in the [https://betcode-org.github.io/flumine/sportsdata/#data-recorder|docs](https://betcode-org.github.io/flumine/sportsdata/#data-recorder|docs), the recorder strategy code needs to be modified in regards to the marketId lookup

*Tags: Strategies*

---

**PeterLe** - *08:24:32*

Great thanks both for your help :+1: 

*Tags: General Technical*

---

**Adam** - *09:48:38*

Follow up question [@U4H19D1D2](@U4H19D1D2) (or anyone): are strategies run in their own threads (in parallel) or are they run sequentially, in the order they’re added to the framework (e.g. if an event comes in relevant to both strategies, strategy1 will process it first, then strategy2).

*Tags: Strategies*

---

**liam** - *09:49:51*

Whats the problem?

*Tags: General Technical*

---

**Adam** - *09:52:13*

long story short - they’re slightly different strategies that I want to test on live data rather than on historical data and if they’re sequential, the latter will be effected by the delay to processing

*Tags: Data Quality, Deployment*

---

**liam** - *10:00:22*

Remove the delay is my answer, or create a third strategy `OrderSender` push all your orders into `Market.context["pending_orders"]` and then just send them all at once

*Tags: Strategies*

---

## 2023-12-21

**JazzMan** - *16:54:02*

Has anybody ever tried manipuling the weight of market by placing large amounts, (&gt;£2,000) then cancelling after a few seconds. This could confuse Weight of Money type bots. Would Betfair have a problem with this?

*Tags: General Technical*

---

## 2023-12-24

**Paul** - *09:33:54*

Yeah. Somebody here once claimed it was a syndicate that he was once did some work for. Guaranteed, timed price volatility is helpful.

*Tags: General Technical*

---

## 2023-12-29

**Andrey Luiz Malheiros** - *01:32:02*

Hey guys.

I believe I'm having some issue with my market records. The first line of recorded file should be something containing 'marketDefinition' within 'mc', but that's not happening, so ultimately my market records are missing the initial 'marketDefinition' information.

Has anyone encountered this or knows how to resolve it?

*Tags: General Technical*

---

**Derek C** - *09:25:11*

I had a similar issue with just a very few files - no 'marketdefinition' in the first line of json, which breaks Flumine backtesting. Never got to the bottom of what caused it.

*Tags: General Technical*

---

**Derek C** - *11:08:11*

My best guess is that some issue caused the gz files to get corrupted, so that when my Flumine backtester reads them some initial data is lost:

• My usual archive utility cannot open the 'bad' files, I have to use a different one.

• The bad files are all from the same day, all Tennis, all recorded within a few hours of each other (Feb 2023)

*Tags: General Technical*

---

## 2023-12-31

**Andrey Luiz Malheiros** - *02:42:14*

I changed the reference code of the market recorder, but I kept the same logic. I believe the issue might not be with the changes I made because some files are being saved correctly, starting with 'marketDefinition.' I suspect that somehow, when the code is attempting to retrieve the market_id through 'MARKET_ID_LOOKUP', it might not be obtaining it initially, causing the market_id to remain 'None'.

*Tags: Data Quality*

---

## 2024-01-03

**Unknown** - *10:28:46*

Sorry for the delay, Liam. I'll send the code in a file just to try to make it easier. But basically, instead of generating a file and saving it, I'm storing all the updates in a string and, in the end, uploading it to a database. I use Django, so the StrategyManagementMarketRecorder is the model responsible for inserting into the database. The structure is based on your market recorder code.

*Tags: Data Quality, Strategies*

---

## 2024-01-06

**birchy** - *22:07:05*

Given that ML is popular, does anyone else here randomise execution to avoid becoming predictable? I've been testing it recently and it does seem to have a positive impact. Or random is better than my strategy. :grinning:

*Tags: Strategies*

---

## 2024-01-07

**JazzMan** - *16:25:48*

This is very interesting because when I roll out a new strategy it's profitability decreases after the first week. Continues to go down before stableising

*Tags: Strategies*

---

**Unknown** - *19:30:26*

Spoken a lot about latency vs profit recently but not sure I have shared an example. Here is how one of my strategies performs when simulated, for example there is a 50% drop in profit with just a 0.25s delay!

*Tags: Performance*

---

**liam** - *19:39:43*

Very strategy dependant 

*Tags: Strategies*

---

**foxwood** - *22:53:37*

Guessing he tweaks the latency factors in config.py ...

```# latencies used for simulation

place_latency = 0.120

cancel_latency = 0.170

update_latency = 0.150

replace_latency = 0.280```

Might be interesting to find out my live latency and mod those variables to match so backtests closer to (my) real world timings - yet another thing for the puzzles board.

*Tags: Performance, Deployment*

---

## 2024-01-08

**liam** - *06:28:10*

It’s all simulated, I do something similar to above but needs different code to go negative on the signal latency 

*Tags: Performance*

---

**mandelbot** - *18:20:04*

How do you record latency along with your orders?

*Tags: Performance*

---

**mandelbot** - *19:36:18*

And how do you calculate your latency?

*Tags: Performance*

---

## 2024-01-19

**birchy** - *13:30:11*

Curious, how are others calculating ROI/ROT, particularly for LAY bets? I do the following but not sure if it's the best way to get a ballpark figure for strategy comparison...

`back_roi = sum(profit) / sum(matched_stakes)

lay_roi = sum(profit) / sum(liabilities)`



I treat every bet as an individual but there's also an argument for calculating at market level which is more complicated. There's also an argument for calculating lay bets using the matched stakes only which gives like for like comparison between back and lay bets, however that doesn't fully reflect the liability/risk?

*Tags: Strategies*

---

**birchy** - *13:46:33*

I suppose I'm just trying to get a number to use for comparing strategy A vs B and thinking about it like an interest rate or investment return. I've also seen people use their bankroll growth to calculate, i.e. deposit £500, bankroll now = £750 = 250 / 500 = 50%. 



[@U4H19D1D2](@U4H19D1D2) I am inclined to agree but laying £1 @ 100 is £99 risk rather than £1, so what are your thoughts/findings on just using matched sizes?

*Tags: Strategies*

---

**Mo** - *14:06:32*

In my apparently wrong opinion it should be `sum(profit) / sum(liability)` where for back bets the liability is equal to the matched stake. I also use ROI as the headline metric in comparing strategies, monitoring for degrading strategy performance, identifying areas where strategies need improvement, and evaluating the impact of said improvements

*Tags: Performance, Strategies*

---

**Mo** - *14:15:59*

Obvious reasons? If strategy A is risking 10 times the amount per bet to strategy B then you need some kind of standardisation. If strategy C has the same profit as strategy D but C had to risk a lot to get there then it might not be worth tying up capital on strategy C, or you might infer that strategy C is much closer to liquidity limits

*Tags: Strategies*

---

**thambie1** - *14:29:35*

[@UBS7QANF3](@UBS7QANF3) variance in returns or some similar metric solves the first problem way better. ROI (calculated as discussed) is a poor proxy for figuring out how much capital a strategy is tying up. Best to track that directly

*Tags: Strategies*

---

**Joe** - *16:38:01*

probably to answer [@U016TGY3676](@U016TGY3676) question a sensible measure that can be used to compare two strats irrespective of what sides you bet on is

```sum(net_profit)/sum(matched_payout)```

*Tags: General Technical*

---

**birchy** - *22:01:19*

Oh no, what have I started? :man-facepalming: :grinning: 

Looks like [@UBS7QANF3](@UBS7QANF3) does it the same way that I do, although [@U05L8PZD2FM](@U05L8PZD2FM)'s method also seems sensible. My thinking was to treat it like a financial investment where I've risked X and have return Y, therefore return is `profit / risk`. Ultimately bankroll growth is the bottom line.

*Tags: General Technical*

---

## 2024-01-23

**Unknown** - *08:58:09*

I have been continuing my machine learning strategy from fundamentals and having an issue with EV and SP. When converting my predictions to probability the EV is shifted depending on the SP. I can see that this would be expected but I am intrigued to how others handle this, current ideas:



&gt; Dynamic prediction -&gt; probability conversion based on SP

&gt; Bucket SP's and fit 



*Tags: Strategies*

---

**Bert Raven** - *10:25:17*

I'm probably not adding any value here and don't understand what the axes on the chart are, but it sounds like you are trying to calibrate model probabilities to implied market probabilities. There is quite a bit of literature on that. Personally I find a relatively straightforward scaling (before looking at ev and profitability) to be robust. The important step imho is effectively adjusting the confidence and appropriate bet size afterwards

*Tags: Strategies*

---

**Unknown** - *10:47:42*

Yeah, sorry, so I have taken my model predictions and created a probability (no reference/knowledge of market sp or price) using a normal dist, I have then compared this prediction to the current price to get an EV.



```ev = model_probability - (1 / price)```

Graph below shows this EV against cumulative profit, which is great because it is showing that my model is correctly showing where the value is (ev &gt; 0). However I know that this model/prediction is very naive due to it not taking into account the prior probability (SP) which is highlighted in the original graph when I bucket by SP.

*Tags: Strategies*

---

**D C** - *10:53:46*

So is it a bit like your model output gives a rating (for want of better word) per runner and you want to map those to probabilities for betting purposes - but runner SP is not one of your model inputs?

*Tags: Strategies*

---

**liam** - *10:57:32*

I think [https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/|this](https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/|this) solves my problem but I need it in english

*Tags: Strategies*

---

**Bert Raven** - *11:05:53*

a simple rescaling would be to say the x-percentile of your model probability maps to the y-percentile of the SP distribution, or you could use a simple regression (though imo that doesn't work well), or you can use a classifier as you've linked to

*Tags: Strategies*

---

**Bert Raven** - *11:08:22*

stupid question, but is your cumulative profit laying when ev&lt;0 and does it look dramatically different when you look at average per bet, ie you're likely to have a lot fewer bets in the extremes than in the middle?

*Tags: General Technical*

---

**D C** - *11:21:59*

Are your model probabilities well calibrated to start with or does it perform badly for certain ranges?

*Tags: Strategies*

---

**liam** - *11:31:15*

[@U01QCEQD9MF](@U01QCEQD9MF) this is showing backing only, I might need this explaining to me like I am 5



&gt; a simple rescaling would be to say the x-percentile of your model probability maps to the y-percentile of the SP distribution

[@UUE6E1LA1](@UUE6E1LA1) no idea, let me try and visualise that

*Tags: Strategies*

---

**Bert Raven** - *16:07:44*

sorry my language wasn't clear, but the idea is that you bucket your model probabilities into say 100 buckets, do the same with the SP probabilities, then you match them. So the highest 1% of model probabilities then have a market price of the highest 1% of SPs.



just to be clear wrt chart, small -ve ev back bets show -ve profit, but high -ve ev bets show +ve backing profits?

*Tags: Strategies*

---

**liam** - *19:46:53*

Opposite I think, ie a high ev is showing back profit, the price available is better than my models probability.



After some discussion and getting myself very confused I am going back to my model to try a simulation technique to see if I can improve my prediction.

*Tags: Strategies*

---

## 2024-01-24

**Jared King** - *14:44:46*

You helped me yesterday so maybe I can help back :stuck_out_tongue:

You could use CalibratedClassifierCV from sklearn.calibration, You use your original model eg XGBoost as the base classifier. Plot your calibration curve before and after.

```def show_calib_curve(x, y, bins=20):

    cal_y, cal_x = calibration_curve(x, y, n_bins=bins)

    plt.figure(figsize=(12, 9))

    plt.plot(cal_x, cal_y, marker='o', linewidth=1)

    plt.plot([0, 1], [0, 1], '--', color='gray')

    plt.title("Calibration curve");```

I also like to plot my strike rate against SP strike rate to see how close it looks.

`show_strike_rate(test_data['prob_LogisticRegression'], test_data['StartPrice_probability'])`

```def show_strike_rate(x, y, bins=100):

    sns.displot(data=[x,y], kind="hist",bins=bins, height=7, aspect=2)

    plt.title('StartPrice vs LogisticRegression probabilities distribution')

    plt.xlabel('Probability')

    plt.show()```

Apologies if i misunderstood what you are after :confused:

*Tags: Strategies*

---

## 2024-01-26

**birchy** - *13:34:55*

When using Flumine simulation, what's the best practice for _taking_ prices where the strategy could be potentially taking the same size multiple times because the available size hasn't been reduced accordingly?

*Tags: Strategies*

---

**Mo** - *13:43:04*

Not flumine specific but use a cooldown per selection per side during which time you're not allowed to place another bet. In the extreme case, you only allow one bet per selection per side per market. i.e. infinite cooldown



If you are OK with simulating with best price execution turned off then you could further key the cooldown by price to allow you to take different prices on the same selection/side combo

*Tags: General Technical*

---

**D C** - *13:58:45*

Cheers [@U016TGY3676](@U016TGY3676) . With this question you've just made me realise I've a massive bug in my own simulator code.

*Tags: Errors Debugging*

---

**D C** - *14:38:29*

OK, so let's say your pending bet is asking to back a runner at 3.0 for £20. At time you simulate the bet hitting the market, there is £25 available at 3.0 so you have a full match and £5 remains at 3.0.  In reality though,  nothing WAS matched. 2 seconds later, the volume available comes in at £30 at 3.0. This overwrites the simulated adjustment of £5 available to £30. So for later bets, you could be recounting that as excess money available. You've only removed it in the sense of matching pending bets at the time they enter the market.

Is there actually a way to accurately counter this problem?

*Tags: General Technical*

---

**ShaunW** - *14:41:26*

Can't remember [@UUE6E1LA1](@UUE6E1LA1) I haven't looked at that code for years. It seems pretty fundamental so (not being a flumine user) I'd imagine it's functionality that's already catered for somehow. You couldn't backtest without it.

*Tags: General Technical*

---

## 2024-01-27

**birchy** - *00:28:23*

[@UUE6E1LA1](@UUE6E1LA1) you described the issue perfectly. [@UBS7QANF3](@UBS7QANF3) so all of my simulation stuff assumes bpe is off as I've not (yet) found a single strategy that is better with it enabled. I used to keep track of the previous price and only checked my signals if the price had changed in the current update. That has an issue where: if the price was adjusted but the size available remains the same, then I'm still taking more money than is available.

I also tried various delays and also forcing my bets to be passive, where I enter at 1 tick away from where I want to be but then the strategy is no longer executing at the desired price.

The main issue is simulating strategies which are operating on market movement where missing the target price affects profit. I've currently settled for one bet per price/side/selection but that significantly reduces the hit rate. Testing these strategies live, I'm seeing different matches vs simulation of the same markets. My best live strategy (which has been running for several years) would of been binned if I had simulated it before going live and still performs poorly in simulation, so I guess I'm operating within XM/market impact territory and live testing with small stakes is probably the way forth.

*Tags: Deployment, Strategies*

---

**D C** - *10:08:34*

I was thinking about it all day after your initial message made the penny drop for me. I suppose it might only be a problem if you are placing large numbers of bets within a smal time frame. I thought I had a dirty "solution" but I didn't bother implementing it because I then thought of another issue. Not sure if flumine caters for this but  my own simulator certainly doesn't: if your strat uses LTP or TV as a signal when you place an actual bet, you'd receive an API update with the LTP and your own traded volume. In simulation you'd not receive these values - to properly simulate you'd need to artificially insert a new stream update containing your own simulated match (or add to an existing update if the times matched - does flumine actually do this??).

So in reality, any strategy (and other concurrently run strategies) running live, is potentially acting on market signals that they themselves create - but my simulator doesn't add these extra stream updates.

This made me start thinking about running simulations on recorded market data that your own strategies ran on historically - so you could be simulating matches of your own offered money, or competing with your own historical orders.

So basically the whole quantum mechanical nature of simulation made me slightly depressed and ask if it is even worth trying to get more accurate.

Don't get me wrong though [@U016TGY3676](@U016TGY3676) -  deep down I am glad you posted your original post. It's always good to try and find holes in your own code.

*Tags: Deployment, Strategies*

---

**liam** - *10:14:41*

[@UUE6E1LA1](@UUE6E1LA1) it doesn’t, I have tried to keep a a balance of accuracy and complication, as I have found adding things such as simulated updates or removing amounts available just adds more noise. I would rather it have a decent benchmark on matching/profit over a wide range of scenarios and use the market itself for fine tuning a strategy.

*Tags: Strategies*

---

**D C** - *10:19:52*

Yeah I don't blame you [@U4H19D1D2](@U4H19D1D2) just thinking about all of the interactions and complexities makes my head hurt. I'm going to just leave mine as it is and use it as a rough indicator of a strategy being worth moving to live or not. It might make me design strategies in a way that minimises these issues from now on though.

*Tags: Deployment, Strategies*

---

**birchy** - *11:20:26*

I feel your pain [@UUE6E1LA1](@UUE6E1LA1), mainly because I've built a heavy reliance on simulation and now know that I've binned quite a few potentially good strategies because they under/over performed in simulation. For passive bets, Flumine simulation is pretty accurate, although I have found it tends to under match vs live, but that's ok as long as you account for it. So in conclusion, simulation is probably best used for development, bug fixing and general analysis but going live is the only true way to find out how our bets affect XM, market impact, match rates, etc.

*Tags: Errors Debugging, Deployment*

---

**D C** - *11:25:35*

Yeah I would have used flumine but I've so much of my own code base and largely unfamiliar with python so i thought would be quicker to roll out my own. Not sure I made the right decision there though....

*Tags: General Technical*

---

**birchy** - *11:37:17*

It makes sense to use a language you're familiar with rather than learning a new one. The only caveat is that you have to find and fix your own bugs vs using open sourced code where multiple users are contributing.

*Tags: Errors Debugging*

---

## 2024-01-28

**James** - *10:13:11*

Are many here making money off predictable market dynamics/behavioural trends as opposed to modelling and fundamentals?

I may be wildly off but It seems the majority of the big fish here seem to do it on modelling/ml/statistical analysis rather than behavioural things is this true or am I off the mark?

*Tags: Strategies*

---

**liam** - *14:31:16*

I would say more recently my signals come from fundamentals or live data and execution is driven by price action 

*Tags: Deployment*

---

**D C** - *19:19:12*

I guess this a bigger problem for those who have BPE turned off?

*Tags: General Technical*

---

**thambie1** - *19:23:46*

For those going based of price action, I can see why they might. If based on fundamentals, the strategy should retry on its own without being explicitly told to do so

*Tags: Strategies*

---

**Ralegh** - *20:40:19*

Does it? I’d assume if you’ve already done the round trip to hear a place order failed then super latency sensitive stuff would have gone already, otherwise unless you’re taking a lot of time to process it should be similar latency wise to just resubmitting a failed order, but you wouldn’t resubmit if the condition had changed. If you’re doing heavy calculations with latency sensitivity I’d assume you’d need to offload to another thread and cache the results anyway, and just do a quick check of e.g price &gt; X, so don’t see how it’d be different. Not sure what context you’re talking about though

*Tags: Errors Debugging, Performance*

---

**liam** - *21:05:44*

My initial point was regarding latency sensitive strategies and it’s something I have never really though about / ignored.



Regarding the cost I guess it’s a case of handling the retry and it’s logic, it’s just another complication and if it’s not latency sensitive surely just a retry is the simple option as it’s only been ~500-1000ms 

*Tags: Performance*

---

**Ralegh** - *21:17:15*

Makes sense, and yeah shouldn’t matter either way if not latency sensitive.



I think generally I try to keep trading code stateless so the logic by default would just be to ignore failed orders and only retry if conditions are still true. Target position = (calculation)… trade while position &lt; target position. Or target position at price level = (calculation) and the same

*Tags: Errors Debugging, Performance, Strategies*

---

## 2024-01-29

**ShaunW** - *14:03:49*

Struggling to see how people aren't retrying anyway if their trigger persists and they don't have a corresponding order. Admittedly that might only occur on the next mktchage rather than iro the same update as price change frequency is typically faster than the failed order notification. But if it's a relatively inactive market it could help. But tbh I don't think I have many that fail or at least fail fast enough to process another during the same update.

*Tags: Errors Debugging*

---

## 2024-02-04

**Leo** - *09:28:10*

Looking for some suggestions on reading materials (books/blogs/papers) on price action based strategies in this space. Been working on fundamentals based stuff for a while and would like to start diving into more price action/in play stuff but honestly not even sure where to start in finding an edge. Not expecting anyone to give away their secret sauce but any resources appreciated, the more technical and specific the better! Google isn't much help with these things with their SEO optimised rubbish 

*Tags: General Technical*

---

**JL** - *10:07:14*

I had an idea of trying to find arbitrage using graph theory, shortest paths with negative weights but applied to sports. You would need multiple bookies though with automated betting

[https://blog.skz.dev/arbitrage-as-a-shortest-path-problem](https://blog.skz.dev/arbitrage-as-a-shortest-path-problem)

*Tags: Strategies*

---

## 2024-02-05

**AndyL** - *19:36:48*

run with [http://logger.INFO|logger.INFO](http://logger.INFO|logger.INFO), you maybe getting a placement error...?

*Tags: Errors Debugging*

---

**Unknown** - *21:29:07*

So naturally after posting about it I managed to catch it in the act this morning.



Here are 3 logs, two right after one another, and a third one minutes later.



The bet moves from 0.0 matched, to 0.59 matched, but the order stays live/pending. Even minutes later. Because I’m not hedging until orders are execution complete I think this is likely the cause.



I’m running a slightly outdated version at 2.5.4 of flumine. I’ll try bump that again but is anyone else seeing this behaviour? Or is there something more reliable I can rely on for execution complete?

*Tags: Deployment*

---

**Lee** - *21:58:41*

Probably gonna need to see flumine logs

*Tags: General Technical*

---

**James** - *22:43:06*

Any idea which ones specifically/a prefix [@UUCD6P13J](@UUCD6P13J)?  Lots of spam in my logs rn for debugging reasons.



[@U01PJ5YMFBJ](@U01PJ5YMFBJ) yeah, my LAY is placed pre play if I get the price I want, and I hedge when its fully matched or when market turns in play I hedge the partial match.

These the logs are all for the same bet though, The lay was placed, and appears to be fully matched but it never moved to execution_complete. Even if it wasn’t matched, it should lapse and move to execution complete when the market moved in play. which it didn’t seem to. I may be misunderstanding how the order statuses work though.

*Tags: Errors Debugging*

---

## 2024-02-07

**James** - *06:21:35*

Okay, maybe this is where my mistake is. 2 questions In that situation [@U01PJ5YMFBJ](@U01PJ5YMFBJ) |



When the bet lapses or is cancelled, does it not move to EXECUTION_COMPLETE?



and does size_remaining stay the same even if it lapses? I assumed that when cancelled/lasped size_remaining would be set to 0 at that point in time. Is that assumption wrong? Should I be checking every combination of lapsed, voided, and cancelled to check that my bet is “done”



Also in the example above the bet is a full match.

*Tags: General Technical*

---

## 2024-02-08

**dan** - *16:27:49*

Sure sorry guys [@UBS7QANF3](@UBS7QANF3) [@U012XF5CNPN](@U012XF5CNPN). I was looking into greyhound or horse racing as I felt those would be the easiest to model, Whatever relevent variables I can find like weight, past performance etc. I come from an ML background but not got much experience in this field so no idea how to find good data!

*Tags: Performance, Strategies*

---

## 2024-02-10

**Jonjonjon** - *17:38:12*

How do the top guys analyse the feature importance in their ML models?

*Tags: Feature Engineering, Strategies*

---

## 2024-02-11

**ShaunW** - *15:40:18*

Horses are the hardest to model imo. So many variables. At least on dogs you don't have a jockey, all the tracks are basically the same and the same going, same field size, bigger datasets.     If the bigger money is the appeal on horses then why not tennis, that's just a dog race with only two dogs. Aminals+humans+weather is a tough gig.

*Tags: Strategies*

---

**John Foley** - *17:39:18*

Not a top guy by literally any definition but: 

tree based sklearn models and xgboost python library have a `.feature_importances_` attribute built in which is a handy starting point. for anything more generic or custom i’ve used also shap values which are great but can need a lot of compute 

*Tags: Feature Engineering, Strategies*

---

**Jonjonjon** - *18:02:09*

Thanks. This might be dumb, but suppose I find the most useful 5 features, what would the next step be? If I fit my models on those 5 features, I will have cherry picked the best features and it becomes an in-sample result.

*Tags: Feature Engineering, Strategies*

---

**John Foley** - *18:09:25*

there doesn’t necessarily need to be a “next step.” I wouldn’t feel the need to start removing features unless I was looking to optimise for memory/compute/training time etc. to me it’s for insights and explainability. it’s for questions like:

• which features should I invest time/money in improving?

• why did the model come up with this strange prediction? 

• does the model rely most heavily on the features that I would use if I were making a prediction myself? 

*Tags: Feature Engineering, Performance, Strategies*

---

**John Foley** - *18:51:17*

hard without knowing exactly what feature A is but perhaps:



“hello AGI overlord, please can you: 

• suggest ways I can make feature A more robust 

• suggest transformations on feature A that I could add into my model

• help me figure out what statistical distribution feature A follows

• suggest other ML algorithms for me to try given that feature A is really important and follows a {answer from above} distribution”

*Tags: Feature Engineering, Strategies*

---

## 2024-02-13

**Unknown** - *09:22:39*

[@UPMUFSGCR](@UPMUFSGCR) as you know, i'm only a beginner at this sort of thing..but this is the output of shap when i tried it.

I thought it may help you visualise it.

(For what it is worth; this is in-play horses and the top feature was something that Id never even focused on before. It surprised me that it was even listed (but made total sense after I thought about it ). So worthwhile playing around with it

*Tags: Getting Started, Feature Engineering*

---

## 2024-02-14

**Peter** - *12:50:58*

Answered in the [https://betcode-org.slack.com/archives/C4H05ML2E/p1707912905736399|other thread where you posted this question](https://betcode-org.slack.com/archives/C4H05ML2E/p1707912905736399|other thread where you posted this question).

*Tags: General Technical*

---

## 2024-02-15

**Unknown** - *15:50:05*

The problem is turning data into information... this extract from Bill Benters description of his method was enough to stop me trying to go down that rabbit hole :thinking_face:

*Tags: General Technical*

---

**foxwood** - *17:41:32*

Doing a sanity check on recorded data and streaming. From my recorded data I'm averaging about 9 - 10 calls per race-time second just prior to the off to `process_market_book()` - on dogs and horses. Does that sound reasonable ie just over 100ms per real packet ?

*Tags: General Technical*

---

**Michael** - *19:04:57*

...it's also possible that I've got some sort of throttling turned on and never realised. Back in the days of polling you could poll faster than 100ms and get more updates but as far as I've ever experienced streaming you get 1 update close to 100ms during busy times. If it's less frequent than that then presumably that's because there's no change to post.

*Tags: General Technical*

---

**foxwood** - *23:49:54*

Still get the same number running a minimal strategy so think 10ps is what is in the file. Will look at the raw json tomorrow. Confused because other (paid for) software suggests a fastest refresh rate of 20ms ie 50 updates per second being possible. Something to do with conflate parameter maybe ? Works for me as is but trying to see what is possible.

*Tags: Strategies*

---

## 2024-02-16

**Unknown** - *01:22:27*

This is what i'm getting [@U02RN7YDRQ9](@U02RN7YDRQ9).  Just one random winter day from 20s before the scheduled start. C#/SQL so an impartial comparision.  I'd guess we have the same number of change messages, I can't imagine Flumine letting many slip through the net.  16ms bands like my bet placement:thinking_face:, but two sets 4ms apart.

*Tags: General Technical*

---

**foxwood** - *10:21:27*

[@UEA14GBRR](@UEA14GBRR) stats king :slightly_smiling_face: The bucketed graph does show a rough 100ms average - many thanks. I got there by averaging number of messages available (post flumine pre-processing) in last 120 seconds per market over a random day. Note to self - make more use of visualizations.

*Tags: General Technical*

---

**foxwood** - *12:25:43*

Wow that was useful for all sorts of other stuff that vanishes as well. Betfair discussion from Oct22 Neil advised:

• For performance reasons, all bet matching takes place in a bulk betting matching cycle, each of which takes ~100 m/s.

So it follows that the 9 - 10 updates per second that I am seeing recorded is realistically best performance and not missing anything :slightly_smiling_face:. It's actually slower than I thought - was thinking 50ms for some reason.

Thanks [@U4H19D1D2](@U4H19D1D2)

*Tags: Performance, Strategies*

---

## 2024-02-17

**Justice** - *12:38:09*

Agree with the part about having edge no one else has. TPD is available to anyone and therefore is not an edge on its own- but your sophisticated model or strategy that uses the data might be

*Tags: Strategies*

---

**D C** - *14:17:19*

TPD has been available through Gruss, BA and BM for quite some time now. It is also not free for users getting it through OTS software. The amount of utter horseshit assumptions about TPD I see on twitter from people who have NOT used it is incredible. TPD is great - but it has flaws. If it were that simple, TPD would likely never have made it available for use by paying customers and just milked the markets themselves. There are plenty of clever people on here who have said they've tried and failed to make TPD pay (or at least its not augmented their already profitable mehods). How many have paid X hundred quid for a month or two access only to find that they don't know what to do with it and then given up??? For sure the more people who access it will result in more people finding a way to use it profitably but they will be in the minority. Meanwhile those who try and fail because they think its a "pay to win" solution have given their money to other players and given up when they discover the truth.

*Tags: Errors Debugging, Deployment*

---

## 2024-02-18

**Unknown** - *10:34:36*

Sorry now with winning horse identified correctly :grinning:

If Ive read it and interpreted the data correctly, it looks like selection ID 50724537 (inthewaterside) reached the end of the race first (prog = 0) whereas in fact it got beat greater than 2 lengths

Maybe there is a strategy there after all :grinning:

*Tags: Strategies*

---

## 2024-02-20

**dan** - *01:36:21*

I hear you, feel like it has to be some ML model not multiple regression to be manageable

*Tags: Strategies*

---

**foxwood** - *12:37:57*

Is there any way of reading the RaceStatus API info in flumine and/or an example of using it in betfairlightweight ?

*Tags: General Technical*

---

**liam** - *12:40:48*

Use a [https://github.com/betcode-org/flumine/blob/master/examples/workers/inplayservice.py|worker](https://github.com/betcode-org/flumine/blob/master/examples/workers/inplayservice.py|worker), in [https://github.com/betcode-org/betfair/blob/9dfed6a2acd18550a6c2192c8c67bc6b0d2ce27e/betfairlightweight/endpoints/scores.py#L16|bflw](https://github.com/betcode-org/betfair/blob/9dfed6a2acd18550a6c2192c8c67bc6b0d2ce27e/betfairlightweight/endpoints/scores.py#L16|bflw):



```trading.scores.list_race_details()```

*Tags: Strategies*

---

**Mark Littlewood** - *15:27:16*

[https://markatsmartersig.wordpress.com/2022/07/10/simple-in-running-tennis-model/](https://markatsmartersig.wordpress.com/2022/07/10/simple-in-running-tennis-model/)

*Tags: Strategies*

---

**Mark Littlewood** - *15:33:57*

Maybe of help [https://towardsdatascience.com/which-features-are-harmful-for-your-classification-model-6227859a44a6](https://towardsdatascience.com/which-features-are-harmful-for-your-classification-model-6227859a44a6)

*Tags: Feature Engineering, Strategies*

---

**Mark Littlewood** - *16:09:03*

I have done some work on calibrated classification in Python but in terms of horse racing profit and loss did not find it produced much alpha, perhaps I should revisit this

*Tags: General Technical*

---

**Unknown** - *16:27:58*

Here is a simple model output screen from MySportsAI if you check 3/4 way down you will see a TTDrift figure, this means the average drift across the two input features is 0.532but if I run it with just TRinrace the TTDrift is 0.522 so clearly and expectedly days since the run had the higher drift factor. Numbers closer to 0.5 mean the data distribution is more alike in the train and test sets (good in other words)

*Tags: Feature Engineering, Strategies*

---

## 2024-02-24

**Jared King** - *06:34:58*

meh I tried some different calibration methods but nothing produced better results. A well tuned xgboost model fits well enough for value betting.

*Tags: Strategies*

---

## 2024-02-28

**Andrey Luiz Malheiros** - *11:23:47*

Hey guys, I'm running a strategy in live paper trade mode and recording the markets with a market recorder. When I run a simulation for the same strategy over the same period using recorded files, I'm noticing a significant difference in results. So, I have two questions:

• Is there a big difference in results between the simulation and live paper trade mode?

• If not, can someone tell me the possible reasons why I'm seeing such a significant difference? (I'm observing a 320% difference in net profit)



*Tags: Data Quality, Deployment, Strategies*

---

**Andrey Luiz Malheiros** - *11:41:54*

Both scenarios have a market filter for WIN markets for greyhound racing in GB, and both have the same middlewares. The live strategy is running with no listener_kwargs, while the simulated one is running with listener_kwargs={"seconds_to_start": 600, "inplay": False}

*Tags: Deployment, Strategies*

---

**ShaunW** - *14:10:01*

Shouldn't be any difference, it's just sourcing data from either the stream or the store and from then on presumably it's just the same code?   Live is different though as you're present in the data, live 'you' gets matches that the test instance therfore doesn't.  You've then got to fudge the latency so the test gets there before live did.

*Tags: Performance, Deployment*

---

**Andrey Luiz Malheiros** - *17:22:55*

Can I set a custom latency for simulation?

*Tags: Performance*

---

**AndyL** - *17:48:05*

Yes see the flumine client config to set latency in simulation 

*Tags: Performance*

---

**Andrey Luiz Malheiros** - *23:03:09*

I'm reading about the usage of SimulatedClient, but I'm unable to find where to set the latency. Is it within the execution_cls attribute?

*Tags: Performance*

---

**AndyL** - *23:24:52*

[https://github.com/betcode-org/flumine/blob/d9b729d79af68042e859511be968bf649a938dc8/flumine/config.py#L26](https://github.com/betcode-org/flumine/blob/d9b729d79af68042e859511be968bf649a938dc8/flumine/config.py#L26)

*Tags: General Technical*

---

**AndyL** - *23:25:36*

[https://betcode-org.github.io/flumine/advanced/#config](https://betcode-org.github.io/flumine/advanced/#config)

*Tags: General Technical*

---

**AndyL** - *23:27:49*

```from flumine import config



    config.place_latency = 0.5

    config.cancel_latency = 0.5```

*Tags: Performance*

---

## 2024-02-29

**Lee** - *09:42:09*

You mentioned you’re using listener kwargs for your backtest, are actually you filtering in a similar way within the strategy when running live?

It should be quite easy to spot what’s going on just comparing bets of the two.

*Tags: Deployment, Strategies*

---

## 2024-03-17

**Paul** - *09:45:55*

I think if you treat a Grade 1 race the same way as a sellers plate, and don’t expect any change in market structure, liquidity, volatility, or the accuracy of any fundamentals you might be considering, that that would be more of an amateur trait. And my original question was not about excitement, but whether strategies worked better, worse, or about the same. If you have something that works the same, fair enough.

*Tags: General Technical*

---

**Paul** - *10:22:37*

If things are working for you, maybe your strategy is indifferent. I’d definitely take a look at whether the event has “Hcap” in the description and see if there is an optimisation there. :-) 

*Tags: Strategies*

---

## 2024-04-04

**liam** - *12:21:21*

Simulating using streaming data

*Tags: General Technical*

---

**tone** - *15:02:38*

Does anyone know a good way to exit a program/strategy when the live market is closes? This happens automatically in simulation but the program just hangs idly when running live!

*Tags: Deployment, Strategies*

---

**tone** - *15:20:17*

Purely for tidiness really. I've organised the code so that it runs a single strategy/program instance per market and when the market completes it saves the results, and hopefully closes down. A scheduler starts the instances before the scheduled start time. I assume from your question that there's a better way to do this?

*Tags: Strategies*

---

**liam** - *17:54:59*

flumine is designed to handle hundreds of strategies and unlimited markets, why complicate things? 

*Tags: General Technical*

---

## 2024-04-12

**James Scott** - *13:52:27*

Trying to find a profitable strategy, was using £15 on horses pre race 

*Tags: Strategies*

---

**river_shah** - *13:55:22*

Use min stakes. Also read up on kelly betting and bet proportional to your edge. Use 0.5 kelly. 3/4th returns, half the variance

*Tags: Strategies*

---

**river_shah** - *13:55:48*

Fixed staking is totally wrong and needs to be adjusted for bet price, value etc

*Tags: Errors Debugging*

---

**river_shah** - *13:55:59*

But first, find value. A good staking plan will only help you maximize extractable value, it won't help in finding value

*Tags: General Technical*

---

**liam** - *14:03:35*

Yeah, just used a fixed bankroll/edge so that your size is realistic to the price, £15 is a very large stake, in greyhounds its about 100x too big

*Tags: Errors Debugging*

---

**AndyL** - *19:10:00*

&gt; Trying to find a profitable strategy, was using £15 on horses pre race

took me 2 years to find my first profitable strategy, still looking for my "second"... :joy:

*Tags: Strategies*

---

**liam** - *20:34:17*

No, tested the optimum stakes for your strategy 

*Tags: Strategies*

---

## 2024-04-13

**Chris** - *09:13:13*

How do you get around the order validation error of minimum bet size of 1

*Tags: Errors Debugging*

---

**liam** - *09:40:59*

With betfair or flumine?

*Tags: General Technical*

---

**Chris** - *09:41:24*

Flumine sorry

*Tags: General Technical*

---

**ShaunW** - *14:40:20*

Fixed staking skews the results on the bigger prices. Condiser liability staking, or scale depending on the size of the market.  Yes Andy, alpha is a metric but used here as meaning knowledge.



So what IS the bottom line on minimum staking? Pay enough commision and they'll remove it?

*Tags: Errors Debugging*

---

**Ammar** - *15:12:53*

does Flumine handle closing stream connections on closed markets? I’ve traced the code and can see that BaseFlumine cleans these up in terms of it’s internal objects; but just curious what it’s doing on the stream too?



(still learning the internals and reading as much of the existing / searchable threads on slack too)

*Tags: General Technical*

---

**Ammar** - *19:08:02*

yeah, granted.



my question (curiosity more than anything else) is more about what’s happening on the technical front — is the connection closed; or is it left to betfair to deal with, or is just hanging around still?

*Tags: General Technical*

---

## 2024-04-14

**foxwood** - *10:23:28*

There is only one market data stream per connection which is for all markets that match the filter you supply when adding the strategy to the framework. The subscription limit you mention is only applied each time a new subscription filter is made. Once set up on the connection the market subscription will run "forever" feeding data on current and all new markets as they arise that match your filter. Once the market is closed you would normally receive no more data from BF for it. Sometimes the market is closed in error and is re-opened or has to be opened/closed again to correct WP/winner errors. So, once a market is closed (excl BF issues) that is the end and it never appears on the stream again - all flumine has to do is handle its own objects. Not sure if somewhere in that your question is answered but it's the flow I understand and lets the stream run smoothly 24/7, automatically handling opeing/closing markets.

*Tags: Errors Debugging, Strategies*

---

**Ammar** - *20:53:59*

Thanks Foxwood! That’s super helpful, and yes does give me a better understanding around what I was asking and also to better understand the source code as well.

*Tags: General Technical*

---

## 2024-04-15

**A** - *10:20:43*

Can anyone recommend a cheap place to deploy a flumine bot? Been a while since I’ve done any web stuff. Is Heroku a viable option these days?



My stack is basically just Python + MongoDB.

*Tags: Deployment*

---

**liam** - *10:27:19*

What ever you prefer tbh, most use AWS

*Tags: Deployment*

---

**Fab** - *12:21:53*

If AWS machines are too expensive, have a look at DigitalOcean, which I totally recommend.

*Tags: Deployment*

---

**A** - *12:52:39*

Thanks. I’ve created a Digital Ocean Droplet. Looks a little less overwhelming than AWS for me to get started.

*Tags: Deployment*

---

**ShaunW** - *18:30:42*

AWS Lightsail instances are simple to set up and a reasonable price.  I can't see any reason not to use SQL, free, lightening fast, 50yr+ track record.

*Tags: Deployment*

---

## 2024-04-16

**Andrew** - *00:53:14*

Probably been answered before, but using Flumine what recommended specs of CPU and RAM for just a handful of strategies.

*Tags: General Technical*

---

## 2024-04-19

**Fab** - *07:30:18*

Andrew, it depends on how complex (so CPU-intensive) your strategy calculations are.



Start with a 1CPU 2GB machine then scale up if needed.

*Tags: Strategies*

---

## 2024-04-23

**James** - *12:45:02*

Digital ocean apps containers are $10 per month for a 1gb 1 cpu machine. So cheap.



Betfair servers are in Ireland on AWS, so minimum latency is to host is there, ECS containers or Lightsail tend to be easiest and fast, but not as cheap as DO.

*Tags: Performance, Deployment*

---

**ShaunW** - *12:49:42*

I don't think Betfair use AWS? I thought they ran their own hardware, in Dublin, near AWS. Either way the cost of the service has to be balanced with performance, no point saving a dollar to earn $1000 less.

*Tags: Performance, Deployment*

---

**James** - *12:53:09*

Maybe I was told incorrectly, I was however definitely encouraged to host in Ireland on AWS for the best latency from a cloud point of view.

*Tags: Performance, Deployment*

---

**ShaunW** - *13:08:49*

You might be right James, things change.  I subscribed to half a dozen and ran tests, AWS came out on top. But if AWS is good enough for Netflix and NASA then it's certainly good enough for my a little betting system.

*Tags: Deployment, Strategies*

---

**liam** - *13:09:29*

[@UBS7QANF3](@UBS7QANF3) normally knows the answer to this question

*Tags: General Technical*

---

**Mo** - *13:10:56*

Yeah Betfair aren’t in AWS but they are in Dublin. You’ll have to hire me as a consultant for any more information 

*Tags: Deployment*

---

**ShaunW** - *13:14:15*

If speed is your number one priority then don't use python :wink:

*Tags: Performance*

---

## 2024-04-30

**Ger Gleeson** - *15:16:15*

Ill piggy back on this thread. Also pretty new in this space. Im looking to move running my scripts from running on my local machine to run within the cloud etc. My scripts are pretty straight forward (at the moment). I just to have them hosted in IRE / UK to avoid geoblocking etc, and also run in "background". Im used to running scipts through Anaconda / jupyter lab / chrome etc but this seems vey slow (start up, general navigation etc) on a 1GB RAM setup within AWS lightsail. Anaconda documentation seems to recommend 4BG+ RAM, which looking at the lightsail pricing per month is coming out at 40 USD per month. Can anyone suggest simplier set up i can go with which requires less RAM etc would allow a cheaper rate per month

*Tags: Getting Started, Performance, Deployment*

---

**liam** - *16:07:47*

When deploying you want to run the python script through cli/bash or ideally in a container

*Tags: Deployment*

---

## 2024-05-02

**liam** - *08:17:27*

In bflw



```trading.scores.list_race_details(meeting_ids=['29162687'])



trading.scores.list_race_details(race_ids=['29162687.1430'])```

*Tags: Strategies*

---

## 2024-05-04

**Justice** - *10:20:34*

Does anyone on here have any insights into migrating potentially hundreds of features created on a dataframe into a live production scenario, especially within the context of a flumine strategy? E.g. rolling windows, moving averages etc.

*Tags: Feature Engineering, Deployment, Strategies*

---

**liam** - *12:01:32*

Do you not want to just use pandas? 

*Tags: Feature Engineering*

---

**liam** - *14:23:08*

I always use python lists / dictionary’s for this when live 

*Tags: Deployment*

---

**liam** - *14:23:41*

Need to be careful when things start getting large as everything slows down and causes latency issues 

*Tags: Performance*

---

**Justice** - *14:42:33*

Interesting. A data frame would allow me to re-use a lot of logic from my feature engineering but maybe it's more efficient to use a nested dictionary and a deque for the rolling windows

*Tags: Feature Engineering*

---

**Ralegh** - *15:05:16*

Usually want to preallocate a numpy array per column and fill it with values, can use it as a ring buffer if you don’t have enough memory

*Tags: Performance*

---

## 2024-05-05

**John Foley** - *02:05:54*

I use a feature store ( [https://feast.dev|feast](https://feast.dev|feast)) but it’s definitely  not ideal if latency is important for you 

*Tags: Feature Engineering, Performance*

---

**Rob** - *09:03:50*

Do you have any performance issues to worry about? If not, reusing the feature engineering code you used to build the strategy is a much bigger benefit IMHO, as there are so many edge cases to test if you reimplement processing.



If you need a more performant dataframe, consider [https://pola.rs/](https://pola.rs/)

*Tags: Feature Engineering, Performance, Strategies*

---

**Justice** - *09:06:40*

[@UC70576CB](@UC70576CB) My strategy doesn't seem to be overly sensitive to latency from my testing. All my feature engineering is done with polars, I much prefer it over pandas

*Tags: Feature Engineering, Performance, Strategies*

---

**Rob** - *09:34:42*

If I start again from scratch, I'd use a feature store, but I don't have too big an issue with processing historic data when placing bets, mainly because the historic data appears to be far less powerful then I expected :confused:



polars seems great, I will never need to chunk &amp; parallelise pandas code ever again :wink:

*Tags: Feature Engineering*

---

**Justice** - *09:38:51*

Interesting. I just did some further tests and while latency doesn't seem to affect the % return very much, it does affect the number of bets I am getting matched.



If I was to use polars, it would make things far easier, but I would be appending to the dataframe very frequently. Do you think this would add much overhead?

*Tags: Feature Engineering, Performance*

---

**Rob** - *09:42:11*

For pandas this is very inefficient, but I very much subscribe to worrying about things that materialise. e.g. [https://stackify.com/premature-optimization-evil/](https://stackify.com/premature-optimization-evil/)



Compute is so cheap, we often don't need to worry about optimisation.

*Tags: Feature Engineering, Performance, Strategies*

---

**foxwood** - *09:46:09*

I'm sure lots of people will keep trying that path without regard to the fact of how many subtle errors happen in chatGPT's day job. At the other extreme I get 98% accuracy from my models when I leave the target in the dataset :upside_down_face:

*Tags: Errors Debugging, Strategies*

---

**ShaunW** - *11:45:02*

Why were they asking it for a winner rather than value? Ask a stupid question you get a stupid answer.

*Tags: General Technical*

---

**Justice** - *13:26:55*

The people who develop these models are incredibly smart but know nothing about gambling. The price is by far the most important thing to consider. Also, especially in horse racing, I'd say the hardest aspect to overcome is the quality and richness of the data

*Tags: Strategies*

---

**Dave** - *18:26:49*

has anyone done any benchmarking of flumine's processing time, in particular how much time is spent between a streaming msg being receieved, and process_market_book being called? would be very interested in any results/profiling outputs that can be shared

*Tags: General Technical*

---

**liam** - *18:58:16*

Is this a loaded question?

*Tags: General Technical*

---

**Dave** - *19:13:53*

Not at all, just in absence of me doing any live profiling. I'm mainly curious about what people see for: a) streaming update from Betfair to arrive, b) how long it takes flumine to decode + call process_market_book. I can easily profile the logic after that, I am just wondering how much time is spent at various stages before process_market_book and wondering if there were some numbers already!

*Tags: Deployment*

---

**liam** - *19:50:07*

Just load up a profiler and have a look.



I assume you are using bflw speed? After the network JSON decoding is slow but not much we can do about that, the streaming logic is heavily optimised (for python) but creating the python objects in bflw is the slow part however flumine does some clever stuff to try and limit/remove it. The ‘tick to trade’ code within flumine itself is very minimal and normally rounds to zero relative to what happens within a strategy. 

*Tags: Performance, Strategies*

---

**Dave** - *19:55:41*

I will check out bflw speed, thanks Liam. I ran profiler when backtesting which helped a lot with optimization within the callbacks, but obviously tells you about nothing network related. I'm basically seeing typically around 10millis between the update's publish time and me asking flumine to place an order. I know my logic is no more than around 1.5ms, so I'm wondering how the rest of the 8-9ms is distributed.

*Tags: Performance, Strategies*

---

**liam** - *20:22:07*

If you are worried about that sort of latency then you are using the wrong language tbh 

*Tags: Performance*

---

**D C** - *20:48:52*

10 milliseconds from publish time to your order being placed? I often get bigger latency as the difference between the publish time and the time I've finished parsing the update, nevermind application logic. I always thought this was a reasonable network latency but looks like I need to up my game!

*Tags: Performance*

---

**Dave** - *21:02:09*

We've had a similar conversation before btw so apologies if it sounds similar! Basically I generally see around 10ms from publish time to placement. I added a few extra strategies to my flumine instance and this went up to about 15ms across the strategies, with one showing even 30ms. So I was kinda surprised, when logic in process_market_book is only around 1.5ms. something's getting clogged up somewhere, which might not be unexpected if the additional strategies are ticking at the same time. So I started poking around in flumine internals to see how it spendsnits time, but form what I can see so far, most of the time before me seems to be network. Looks like flumine internals spend negligible part of this 10ms.

*Tags: General Technical*

---

**Dave** - *21:05:23*

So this basically has me wondering if keeping my median latency at 10ms is simply impossible when adding extra strategies, so I should just distribute instances across more cores, or perhaps somehow do computation out of process

*Tags: Performance*

---

**D C** - *21:07:03*

I'd be quite happy with that to be honest. I use C++ and an event based SSL socket and my average from message publish time to message read off the buffer and parsed as JSON is about 13ms. Of course I don't know if mine and Betfair's clocks are synced but I was pretty happy with this. I suspect if I used a specific thread with a blocking socket connection I would shave a little off it but  you can' get rid of network latency completely.

*Tags: Performance*

---

**Dave** - *21:09:34*

DC you need to take inspiration from Flumine's book building logic, you could probably bring that down a lot if using c++ !

*Tags: General Technical*

---

**D C** - *21:16:10*

I'm pretty happy with what I've got I'm just intrigued by other people's perceptions of what they deem as acceptable latency. It seems reasonable to me to be waiting 13ms (average) from when Betfair push the JSON out and my having received and parsed it.

*Tags: Performance*

---

**D C** - *21:17:36*

Is the market book updating in flumine slow then?

*Tags: Performance*

---

**Dave** - *21:18:44*

No, book building is super fast. Like I said, outside of my logic in Flumine's post-book-building stage, it seems all my latency is network. Flumine itself doesn't add anything really, latency wise, from what I can see.

*Tags: Performance*

---

## 2024-05-07

**Ben Coleman** - *07:24:08*

Hi all,

I'm interested in knowing a little bit more about how to go about decreasing latency on trades and getting matched more often. I have a few questions and I wouldn't mind hearing anyones insights:

- What does the force argument do within the market.place_order() function? and would setting it to True potentially make the function run faster?

- Are the streams global or individual specific? Meaning does everyone always get the same update at most every 50ms and it is published at the same instant or is the instant the update is published for individuals only specific to them and hence they might have slightly differing market books due to what happens in the interim?

- How are incoming requests managed on the exchange side? Is it every 50ms the exchange looks at all of the requests in the previous 50ms and executes them in the order that they appeared then publish the updated book every 50ms? Or is there some sort of hierarchy where certain types of requests are executed before others each increment (eg. Process all the fill or kill orders first then the standard limit orders next or all cancel order requests are executed before place order requests?)

Cheers

*Tags: Performance*

---

**liam** - *07:25:51*

First question, you mentioned in your DM that you are aus based, are you hosting in the UK?

*Tags: General Technical*

---

**liam** - *07:28:29*

In the execution logs flumine logs the latency, do you know what you are getting? Median/dist etc?

*Tags: Performance*

---

**Unknown** - *07:51:39*

My cpu utilisation is pretty much always under 2%. I am running it on AWS and this is that metric over the last week

*Tags: Deployment*

---

**liam** - *08:54:10*

Yeah, ok but do you get a lot of those logs? You need to isolate / get some visibility on where the latency is coming from

*Tags: Performance*

---

**Ben Coleman** - *12:49:11*

I would usually get 1 or 2 collections of these logs every day. I am tracking latency between publish time and calling process_market_book which tends to be around 4-5ms and the time from then to placing orders which is around 3-4 ms. I have detailed breakdowns of the latter one, how would you go about breaking down the former one? Also how do you access the execution logs that tracks latency? Does it just appear in the log files because its only that screenshot that I get information on latency.

*Tags: Performance*

---

**liam** - *12:57:53*

This [https://github.com/betcode-org/flumine/blob/bfaa5def6dc66794f088d9e4fed539c3c3043926/flumine/execution/baseexecution.py#L43|one](https://github.com/betcode-org/flumine/blob/bfaa5def6dc66794f088d9e4fed539c3c3043926/flumine/execution/baseexecution.py#L43|one)

*Tags: General Technical*

---

## 2024-05-08

**Ben Coleman** - *01:31:34*

I haven't run simulations on using flumine yet, I'll give that a go.

*Tags: General Technical*

---

**Adrian** - *13:36:51*

When you start a new strategy and see this:

```1. Back	1.99	5.00	-5.00 Lost

1. Back	2.68	5.00	-5.00 Lost

1. Back	1.98	5.00	-5.00 Lost

1. Back	5.90	5.00	-5.00 Lost

4. Back	2.20	5.00	-5.00 Lost

1. Back	1.97	5.00	-5.00 Lost

6. Back	2.59	5.00	-5.00 Lost```

:dotted_line_face:

*Tags: Strategies*

---

## 2024-05-10

**Adrian** - *01:09:00*

Ok so reflecting on your posts, thanks for the input everyone.



> filter out the 'bad' ones when you have enough data

I thought for sure this would be committing the cardinal sin of 'backfitting' - retrospectively selecting only the good bets. Does this have any predictive value?



Actually, this is what's tying me up at the moment. My strategy for discovering how to implement my model involves adjusting the parameters of execution such that the resulting backtest profit is optimised. I feel like this is breaking some kind of statistical rule of thumb?



E.g. scientifically, you propose a hypothesis - 'laying at odds of 2 at t-10 will produce the highest EV'. The backtest shows a marginal profit, then you adjust to t-9, t-8 etc to squeeze the most EV out, moving away from the original hypothesis you chose. Is that acceptable?

*Tags: Strategies*

---

**Unknown** - *09:34:54*

I think you just have to draw a balance between what's theoretically ideal and how things are in the real world. You can't start from scratch with new data every time you have a new idea and there's no data as good as real previous bets, so to some extent you're always "back fitting." The question is how you go about it and whether you stop to check your judgement with sensible tests of reasoning.

*Tags: General Technical*

---

## 2024-05-12

**James** - *14:49:02*

Also depending on which markets you’re in, and win volumes turnover can help with commission/implied commission and offsetting premium charge :slightly_smiling_face:

*Tags: General Technical*

---

## 2024-05-17

**thambie1** - *00:23:11*

I'd rather calculate it myself than pay the latency cost

*Tags: Performance*

---

**James Scott** - *10:10:24*

Does calculating it yourself not cause some latency?

*Tags: Performance*

---

**Justice** - *12:05:08*

I always use EX_BEST_OFFERS. Didn't realise there was a performance penalty?

*Tags: Performance*

---

**Dave** - *20:40:52*

How does the 150ms latency work with the 50ms matching cycle?

*Tags: Performance*

---

**thambie1** - *22:57:30*

[@U05N9773A23](@U05N9773A23) EX_BEST_OFFERS_DISP is the one with the latency penalty

*Tags: Performance*

---

**thambie1** - *22:58:40*

[@U0128E7BEHW](@U0128E7BEHW) According to documentation: "The virtual price stream is updated ~150 m/s after non-virtual prices.". I would take that to mean, EX_BEST_OFFERS is sent out immediately after the matching cycle, and EX_BEST_OFFERS_DISP 150ms after that.

*Tags: General Technical*

---

## 2024-05-19

**foxwood** - *19:17:42*

I'm trying to simulate 2 parallel strategies running on the same market and talking to each using market.context. Seems like they are both created and started ok but then strategy1 goes through all files with no sign of strategy2 getting a look in. From the logs I see each strategy is added but each one gets its own version of the HistoricalStreams created from the market_filter["markets"] file list when the strategy is constructed. Is there a way to make this work in simulation - I assume it works ok live judging by comments in other threads.

*Tags: Deployment, Strategies*

---

**foxwood** - *21:27:08*

I've combined them into 1 strat for now but would be nice to get this working. Code follows docs I think but don't remember anything special on multi markets ? These are the main bits of code

`dataDir = "path to directory"`

`strats = [launchTest1, launchTest2]`

`markets = [globbed list of fully pathed file names]`

`client = clients.SimulatedClient()`

`framework = FlumineSimulation(client=client)`



`with mock_patch("builtins.open", smart_open.open):`

    `for launch in strats :`

        `launch(framework, markets, client)`

    `middleware.marketcatalogue.MARKET_CATALOGUE_PATH = dataDir`

    `framework.add_market_middleware(MarketCatalogueMiddleware())`

    `try:`

        `framework.run()`

    `except Exception as e:`

        `print( traceback.format_exc() )`



`In separate file for each strategy:`



`def launchTestN(framework, markets, client):`

    `launchtest(framework, markets, client)`



`def launchtest(framework, markets, client):`

    `framework.add_strategy(`

        `stratN(`

            `name=stratN.__name__,`

            `market_filter={"markets": markets,`

                           `"listener_kwargs": {}`

                           `},`

            `max_order_exposure=999,`

	         `etc,`

            `context={"client": client, },`

        `)`

    `)`

*Tags: Errors Debugging, Strategies*

---

**foxwood** - *21:35:11*

[https://betcode-org.github.io/flumine](https://betcode-org.github.io/flumine) or something else ?

*Tags: General Technical*

---

**foxwood** - *22:45:27*

So I dug down into this and `add_historical_stream()` in streams.py only returns the stream created by another strategy if the file names are the same (True), the `event_processsing` is the same (True) and the `listener_kwargs` are the same (False).

The two strategies kwargs had different `seconds_to_start` and no other entries. Not sure why that should be counted as a different market feed but does explain what I found. Didn't discover this in any of the "docs" though. Assume the strategies would work in parallel if it did all match but not tested.

*Tags: Strategies*

---

## 2024-05-20

**liam** - *06:20:40*

Your example code doesn't show this? The `listener_kwargs` act at the very lowest level of the stream logic within bflw in order to speed things up so if any are different they cannot be shared

*Tags: Performance*

---

## 2024-05-25

**Troy Edwards** - *01:46:16*

[@U01S1VB9X9P](@U01S1VB9X9P) hi just on min bet, Betfair Australia now allows a min $1 bet which is great for validating models/systems in the real market.



Also I think if you turn over more bets then commission may be lowered.  My view is that turning over more bets for the same return increases your validation of your model/system working

*Tags: Strategies*

---

**Troy Edwards** - *01:55:06*

I'm hearing you :slightly_smiling_face:  I have been playing around with Lasso/Ridge and reduced my data features down to 10.  I then created different models using different data features with one meaning static as in does not rely on odds or volume for Australian Greyhounds Markets.



Train/tested well.  Then after 5 days of validation 4 of those models broke (for that day anyway) but I will continue to validate over a few weeks as to the see the overall effect.



Maybe you should just continue to validate as sometimes one/two bad days is just part of that variance

*Tags: Feature Engineering, Strategies*

---

**Adrian** - *03:25:18*

Thanks Troy! I am enjoying the lower minimum bets. It's helps keeping risk low

*Tags: General Technical*

---

## 2024-05-28

**Andrew** - *06:38:54*

Good point. Misread your question somewhat. Silly me to assume closing out is profitable :blush: 

*Tags: General Technical*

---

## 2024-06-10

**liam** - *20:29:31*

Behind the medium paywall but lots of articles that would help those starting out 



[https://footballdotpy.medium.com/|https://footballdotpy.medium.com/](https://footballdotpy.medium.com/|https://footballdotpy.medium.com/)

*Tags: General Technical*

---

## 2024-06-14

**Adrian** - *11:19:52*

Does anybody have a good method for cashing out using Flumine? I would like the same functionality as the cash out button on betfair. I found this article from Betfair but I dont really understand the API enough to implement it

[https://support.developer.betfair.com/hc/en-us/articles/115003887431--Is-Cashout-available-via-the-Betfair-API](https://support.developer.betfair.com/hc/en-us/articles/115003887431--Is-Cashout-available-via-the-Betfair-API)

*Tags: General Technical*

---

**liam** - *13:49:26*

its not allowed in flumine

*Tags: General Technical*

---

**Rob** - *18:14:19*

I asked this, and whilst I understand the reasons not to, I think there valid reasons why you may want to - it depends what you're doing.



So I wrote a version using betfairlightweight, and whilst it may leave a little EV on the table, it does work, and doesn't need any complex linear programming.



I'd be happy to share the steps in pseudo code, but I don't have a version I can share the code for.

*Tags: General Technical*

---

**Adrian** - *23:04:29*

[@U012XF5CNPN](@U012XF5CNPN) I want to hedge because I'm no longer wanting to back (or lay) the runner. I got my value already- the price swing and I'm sitting on a nice profit. Almost every race. By post, the value isn't there any more. There is no value in holding onto the position. Only variance. If you're betting into 50 different markets, that's no problem. But if you;re just starting out like me and you only have one strategy, you want to put as much money into that strategy as possible without breaking your bank. You can't do that without hedging

*Tags: Strategies*

---

## 2024-06-15

**liam** - *10:41:26*

No / Yes, it’s per strategy / market / runner

*Tags: Strategies*

---

**Adrian** - *23:27:31*

Thanks Peter appreciate your share! Lots to think about. It sucks that your strategy went south after making that change. Were you able to correct it? I haven't made any changes yet. Still weighing up the pros and cons, also would like to backtest it to compare expected profits. There is possibly one thing beside profit that trumps everything else and that's the tax implications of hedging. In Australia that act alone turns you from a tax-free gambler to a taxable trader.

*Tags: Strategies*

---

## 2024-06-16

**Unknown** - *15:19:53*

hey [@U053KV09FKP](@U053KV09FKP), wrote this a while ago but don't use it anymore as I learned pretty quickly that hedging was costing me EV :slightly_smiling_face: so it could probably be done a better way in terms of efficiency etc. its not great/quite hacky. but hopefully might be of some help to you

*Tags: General Technical*

---

**Rob** - *15:42:58*

In terms of the process I coded up, it's effectively:



1. Get all matched orders on the market (and check there are no unmatched orders left)

2. If the back/lay amount for each selection are not even, record the size of the discrepancy (at this point, match price isn't important)

    a. for example, if you've backed runner A for £100 and layed for £90, record a £10 lay bet against this runner

3.  Get your current P&amp;L per runner via `list_market_profit_and_loss` , simulate the effect of adding the bets above to the result at the current prices, and, and get the current prices for the market

4. For runners where you're in profit, record that you want to lay with size `current_profit / current_lp` 

5. For runners where you're losing, record that you want to back with size `( current_loss) / current_bp` 

6. Add the greening bets from steps 4 and 5 to the bets from step 2, and place them at the current back/lay prices and you should have a greened book

The only other complexities are around bet sizes - if you're betting relatively small amounts and include longer odds runners you will be below the minimum bet size. You can place a £1 bet far away from the current prices and then reduce the size, then the price.



At some point you can't do this as you run in to errors from betfair around what's allowed, but at that stage, the amounts of money are small enough not worry about greening

*Tags: Errors Debugging, Strategies*

---

## 2024-06-22

**Adrian** - *11:44:52*

`'Error: strategy.validate_order failed: live_trade_count (1) >= max_live_trade_count (1)'`

Is this limit explicitly defined somewhere?

I haven't changed it. Is it just a default safety measure in flumine or something?

*Tags: Errors Debugging, Deployment, Strategies*

---

**foxwood** - *19:39:42*

To set your own values add `max_live_trade_count=nnn` as argument to strategy constructor - other options in the docs at [https://betcode-org.github.io/flumine/strategies/#parameters](https://betcode-org.github.io/flumine/strategies/#parameters)

*Tags: Deployment, Strategies*

---

**Justice** - *20:25:00*

I am finding that returns for my strategy are highly variable when I back test depending on the subset of markets I hold back to test my model on. How many markets does everyone generally test on? I can spare about 200-250 at the moment for testing

*Tags: Strategies*

---

## 2024-06-23

**Justice** - *08:34:07*

Interesting. Guess I'll have to wait till next year to test my strategy :sweat_smile: It uses TPD data which to my knowledge cannot be purchased unaltered

*Tags: Strategies*

---

**Justice** - *08:50:15*

I have to collect TPD data in a scuffed way because I don't get it through Betfair- so your data might not be the same schema/granularity which may be problematic

*Tags: General Technical*

---

## 2024-07-01

**James** - *00:15:12*

```# latencies used for simulation

place_latency = 0.120

cancel_latency = 0.170

update_latency = 0.150

replace_latency = 0.280```

Sim latencies are in seconds yeah?

*Tags: Performance*

---

## 2024-07-02

**Adrian** - *11:03:41*

Hi i was hoping someone could help me understand `place_reset_seconds` and `reset_seconds`. I am trying to place a bet every 60 seconds on a single runner using

```trade = Trade(market_book.market_id, runner.selection_id, runner.handicap, self)

trade.place_reset_seconds = 60```

In the docs it says `place_reset_seconds` is seconds to wait since runner_context.reset and `reset_seconds` is seconds to wait since runner_context.place

What is the difference between these two and which one should i use if i want to place a single bet every 60 seconds on a single runner?

And do i have to call `runner_context.reset` / `runner_context.place` explicitly in the trade logic or does it happen automatically?

Thank you!

*Tags: General Technical*

---

## 2024-07-05

**Rob** - *12:51:51*

It's part of the BF API,  in betfairlightweight [https://github.com/betcode-org/betfair/blob/8ec3c29d68d35e5aed1e606a5833b53210dc3847/betfairlightweight/endpoints/betting.py#L439|here](https://github.com/betcode-org/betfair/blob/8ec3c29d68d35e5aed1e606a5833b53210dc3847/betfairlightweight/endpoints/betting.py#L439|here)

*Tags: Strategies*

---

## 2024-07-08

**Unknown** - *14:45:33*

Nice example of how important value is, simple flumine middleware to record the lowest price traded 10s after an order is executed (racing inplay)

*Tags: General Technical*

---

**Unknown** - *14:51:07*

And this is a 'real' strategy on the same dataset

*Tags: Strategies*

---

**D C** - *15:33:58*

I realise that this can't be a precise measure of EV, but I was wondering why you suppose that profit is still negative up to (eyeballing it) 0.45 EV ? Is the fact that its around zero on the far left side a consequence of this strategy signal being very strong and you don't actually place many bets that are -EV according to this definition?

*Tags: Strategies*

---

## 2024-07-09

**AndyL** - *09:20:37*

[@U05N9773A23](@U05N9773A23) I don’t think it’s available to everyone unless you use BetMover, BetAngel, or Gruss. To use with Flumine natively I think you would need a Betfair TPD license which they only give to a select few……

*Tags: General Technical*

---

## 2024-07-10

**Jonjonjon** - *14:16:48*

Does VAR mean that we probably can't trust Flumine backrest results for inplay football?

*Tags: General Technical*

---

**Jonjonjon** - *14:22:23*

Isn't it quite frequent that VAR cancels goals, which means that matched bets will be cancelled? But how to know which bets would have been cancelled? This sort of thing doesn't happen too often with horses.

*Tags: General Technical*

---

**liam** - *14:23:09*

I don't understand why that is a problem/blocker either

*Tags: General Technical*

---

**Dave** - *19:49:10*

VAR doesn't make much of a difference IMO. Your bets have +EV or they don't, VAR doesn't change reality. What has been an issue from time to time is untimely suspensions, and therefore matching after a goal has gone in (but market not closed). Can be resolved with a swift message to BF but still annoying it doesn't happen automatically. Nevertheless, quality of suspensions/market closures have improved over last half year or so.

*Tags: General Technical*

---

## 2024-07-11

**thambie1** - *00:04:27*

@James I have done cost experiments for zipped vs unzipped. I found unzipped to be cheaper in my use case. You should have cpu profiling working and be able to do such experiments yourself if you want to make optimizations of this kind. That being said, the financial choice is clear to just buy a better dev machine if that can fix your problem, unless your really not valuing your time to go the cloud route. It really is a time sink, trust me.

*Tags: Errors Debugging, Performance, Strategies*

---

**thambie1** - *00:08:30*

If I were buying a machine for backtests, I'd build a desktop with the most powerful retail CPU on the market. And then in my case, model training is also memory heavy so I'd want minimum 128GB on that. Everything else wouldn't matter as much

*Tags: Performance, Strategies*

---

**thambie1** - *00:14:56*

Been a couple years since I looked at this stuff. I'd just buy whatever has the most cores. I don't use flumine backtesting, but I'd assume it can parallelize well

*Tags: General Technical*

---

**AndyL** - *10:50:33*

I just estimated my recent Flumine backtest speed, I think it's about *10* markets per second :joy:

*Tags: Performance*

---

**Joe** - *11:23:48*

Probably worth pointing out that back testing where you run the markets once to get a result doesn't really matter how fast it is, I'm sure 10mps is fine. Training OTOH where your objective function is a block of procedural stateful strategy code, that is non-differentiable   so you can't use back prop, requires many orders higher run frequency as you are limited to quasi evolutionary / genetic type parameter searches, or even worse some sort of nD grid search. I do the evolutionary thing hence the high frequency requirement of my process.



Or to put it simpler back testing is not the same as training RL albeit both need to run data through your logic and sum up the cost.

*Tags: Strategies*

---

**AndyL** - *13:03:42*

PERFORMANCE

• Single-Precision: 35.58 TFLOPS

• Transistor Count: 28.3 billion



*Tags: Performance*

---

**AndyL** - *13:04:58*

unfortunately I don't have the "model" know-how to use one!

*Tags: Strategies*

---

**Jonjonjon** - *19:49:25*

For people who trade in and out using forecasting models, what techniques do they use to scale bet sizes as a function of forecast magnitude, price etc...? Kelly isn't suitable as I trade in and out (i.e. I'm one of the losers who hedges)

*Tags: Strategies*

---

**birchy** - *20:11:26*

Don't fret [@UPMUFSGCR](@UPMUFSGCR). I run flumine simulation on a $20 AWS Lightsail instance to generate 'features' CSV's and then train models on Google Colab. My strategies are unsophisticated, so 5K markets takes an hour or so to run and model takes &lt; 10 minutes to train with ~30 features. I've set a limit of 5k markets so I can get results fairly quickly and that's working for me. I take the view that if a strategy needs &gt; 5k markets to make a profit, then it's not viable in the real world.

*Tags: Feature Engineering, Deployment, Strategies*

---

**Mo** - *20:28:25*

Why isn't Kelly suitable?

*Tags: Strategies*

---

**birchy** - *20:36:12*

Stakes based on edge (Kelly) is surely the right thing to do within market capacity?

*Tags: Strategies*

---

**Jonjonjon** - *21:25:31*

Suppose I back at 100 and lay at 90, Kelly tells me to bet very little when in reality my model is very reliable so I'd bet quite a lot on it.

*Tags: Strategies*

---

**Jonjonjon** - *21:34:26*

Actually that's a bad example as I made up the numbers.



For back at 100, lay at 90 (fair value) trade, kelly fraction is 12%

For back at 500, lay at 450 (fair value) , kelly fraction is 0.02%.



In reality I often do up to 10% of my bank on bets of 500

*Tags: Strategies*

---

**Rob** - *21:54:44*

I used to use the cloud but it's easy for costs to explode, so I went back to a Ryzen and as much RAM as I can afford.



Mainly posting as I was always curious if RL techniques would work. I previously lacked the sophisticated back testing that flumine brings, so might add this to my list of things to try in future :slightly_smiling_face:

*Tags: General Technical*

---

**Ralegh** - *23:33:23*

If you’re backing and laying then in a sense your bet isn’t the outright bet but you’re betting on your change in EV. Eg you back 500 and (not gonna get the right numbers here but) want to lay at 250, your ev is say double your bet or +1U, whereas if price balloons out then it’s -1U, ie you’d want that trade if probability of hedging at &lt;=250 is &gt; 50%, so that’s what you’d put into Kelly

*Tags: Strategies*

---

## 2024-07-12

**George** - *12:30:28*

It's not a question of EV, or edge, or techniques, it's a question of liquidity. If you don't want to get stuck with an outright punt, then you can only back with the size that you're confident that you'll be able to lay later on.



If there's unlimited liquidity and you're going to trade out anyway (whether the price moves in your favour or not) then you can - and would need to - use big stakes.



Having said all that - it's better to just not trade out, and then you can rely on Kelly staking.

*Tags: Strategies*

---

## 2024-07-16

**Sen** - *08:13:55*

Is this for traditional Machine learning training- or more for backtesting [@U05L8PZD2FM](@U05L8PZD2FM)

*Tags: General Technical*

---

## 2024-07-19

**Unknown** - *09:38:32*

The flask integration is cool, would be easy to add to flumine as a loggingcontrol

*Tags: General Technical*

---

**A** - *11:18:13*

That’s cool, I’ve toyed with the idea of remote control with a little flask app. I have one currently as a separate app that delivers data written from my flumine strats for further analysis.



I’ve also got Telegram reporting going which is extremely distracting but quite fun.



I’m also yet to be profitable, so probably should have focussed on that first :laughing: 

*Tags: Deployment*

---

## 2024-07-25

**liam** - *18:05:26*

I ignore exposure and bet based on a state change, so for TPD it’s an update (with a delay as I don’t want to be every update) and cricket it’s a new ball. 



Basic premise is to reduce risk and try not to have a large exposure on a single ‘state’ that might contain errors / latency that is out of my control 

*Tags: Errors Debugging, Performance*

---

**thambie1** - *18:06:04*

kelly criterion that takes existing positions into account

*Tags: Strategies*

---

## 2024-07-26

**D C** - *10:07:26*

Are there any articles on this kind of thing?  I've read papers on how to adjust Kelly when you are betting on more than one selection in multi selection events but I've never seen anything about kelly criterion adjusted for existing exposure?

Are you deriving these things from first principles?

*Tags: Strategies*

---

**liam** - *11:57:38*

The problem with kelly is that it assumes you know the ev of your bets which I think boils down to problem you are facing? Can you accurately put a figure on your ev?



But then you have the next problem which is market capacity required to fulfil kelly staking which in certain markets (inplay racing) is never going to work

*Tags: Strategies*

---

**Mo** - *12:10:31*

[@U4H19D1D2](@U4H19D1D2), both problems are overstated I think. Obviously you don't know the true probabilities but trivial to protect yourself against this by using fractional Kelly which everyone does. If your probabilities are really so far off then you probably don't have a profitable strategy in the first place. For the market capacity problem, just fire in the Kelly bet and see what happens. You'll take account of what actually got matched when you calculate your next bets. If that opens you up to too much adverse selection then just use a stake size cap

*Tags: Strategies*

---

**Bert Raven** - *12:47:33*

you don't know the true probabilities but you should have a reasonable estimate of what your edge is through backtesting and live realized, in which case I simply scale the kelly ratio by realized (or backtested) ev/ model forecasted ev

*Tags: Deployment, Strategies*

---

**liam** - *14:07:48*

My experience says otherwise, once you start fitting kelly to meet the market capacity you are no longer getting any advantage from it and becomes a hinderance. The bet sizes diverge from the optimal whilst coming at a cost of EV as you move the market, execution is far more crucial than the 'kelly optimal'

*Tags: Strategies*

---

**D C** - *15:45:35*

I'm basing my EV of model price [@U4H19D1D2](@U4H19D1D2) (fundamental, not market signals) but of course that is not going to be accurate all of the time. I see what you are saying though. I actually use in a few strategies a staking based on an old Pinnacle article written by Buchdahl that chooses stake based on the assumption that you've no idea what true EV is. It's basically some kind of logarithmic scaling of ask price. Can't seem to find a link to it now though. Can't remember if it has any theoretical validity in contexts other than those which Buchdahl usually frames his writing.

*Tags: Strategies*

---

**D C** - *16:51:15*

Also, how impactful would you guys say this method of stake sizing is to your profits? I've been rather "rough" around the edges really in certain regards, focusing more on model accuracy than anything but now that I'm having to give BF 40% of my profit every week I feel I really need to tighten up and this could be something useful in future endeavours.

*Tags: Strategies*

---

**thambie1** - *16:54:43*

With human effort, you can get the computational cost down to acceptable levels (by my standards at least). Likely not possible with a python implementation.

*Tags: General Technical*

---

**thambie1** - *16:56:25*

You need stake sizing to manage two problems. Adverse selection, and risk. The approach me and Mo take is exceptionally good at the latter, and only somewhat at the former. If you don't have a risk problem (i.e. massive variance in your profit graph), likely not worth the effort

*Tags: Errors Debugging*

---

## 2024-07-27

**D C** - *12:05:25*

Actually came across a paper that seems to tackle exactly this problem. I had to skim over some of the convex optimisation stuff but might be of interest to others as there are references to placing -EV bets in order to increase the expected wealth gain which is something I've seen recently discussed on here



[https://www.semanticscholar.org/paper/In-game-betting-and-the-Kelly-criterion-Andersen-Hassel/d9be9ec95ca971bd2e45ad2cf9a41f97397b6cbd](https://www.semanticscholar.org/paper/In-game-betting-and-the-Kelly-criterion-Andersen-Hassel/d9be9ec95ca971bd2e45ad2cf9a41f97397b6cbd)

*Tags: Strategies*

---

## 2024-07-29

**Misha** - *10:02:03*

This is the exact problem I have. I just use a version of logarithmic scaling and treat all bets as independent. Unfortunately while this works in the long run, variance is high (as you know). I now have a dataset of 20 million points, prices, and probabilities that I can back-test in any way I like. No matter how many different strategies I have tried, nothing seems to reduce variance significantly without sacrificing POT%, which is important when your "edge" is only 4% on the market (before commission)

*Tags: General Technical*

---

**Misha** - *10:04:03*

I was asked why I stopped betting completely. The answer was that it was more important to build up the detailed dataset for back-testing than it was to keep betting. The data I have cannot be bought - it has to be collected yourself, which means a good up-to-date score and Betfair streaming prices

*Tags: Strategies*

---

**D C** - *10:05:50*

I find variance is only an issue when I find that I am getting more matched at higher prices. I don't back odds on because my models are shit in that price region (something still working on) so I just omit them. I accept the variance and I certainly have bad days and losing weeks but I have an exposure cap per market and a (so far) large enough bank to see that through.

*Tags: Strategies*

---

**Misha** - *10:10:21*

I have no restriction on prices. Model is no better or worse on any price range

*Tags: Strategies*

---

**D C** - *10:10:27*

Rarely? No wonder you've problems with variance then. Does that not indicate some inherent bias/inaccuracy in your model prices if you are only ever backing one player

*Tags: Strategies*

---

**D C** - *10:14:54*

I'd have to disagree with your assurance that it shows your model is correct. If (as you have stated), your model is only ever selecting ONE player's odds as being favourable, then your notion is that your model is more correct than the market over the entire duration of that match? There are too many very smart and sophisticated operators out there for me to ever believe that that would be the case.

*Tags: Strategies*

---

**Misha** - *10:20:21*

Back-test. When betting it matches within +/- 0.5%. Have months of data to prove that. Every single time I bet I match the model

*Tags: Strategies*

---

**liam** - *10:20:53*

This is all theoretical though based on backtesting / simulation, IMO not sure you can state facts on liquidity constraints until you have proved a strategy live, anyone can create a graph like that

*Tags: Deployment, Strategies*

---

**D C** - *10:22:33*

Serious question [@U016535QCJ2](@U016535QCJ2) - are you just fucking trolling us here with this stuff or do you truly believe in what you are typing?

*Tags: General Technical*

---

**Misha** - *10:22:59*

I'm not sure you can believe it works. That's not my problem

*Tags: General Technical*

---

**D C** - *10:26:18*

Show me an actual profit graph for your model in 5 years and I'll take it all back. Good luck with it. Bye.

*Tags: Strategies*

---

**Misha** - *10:35:08*

Therein lies your problem not mine. Multiple ways to make money. You can have 5 different successful ways, and none negate the others

*Tags: General Technical*

---

**Misha** - *10:35:57*

Nothing I do negates anyone else's strategy. I never even implied that. So nothing I do for myself is ever making an assessment on anyone else

*Tags: Strategies*

---

**D C** - *10:42:21*

I don't have a problem Misha. I bet on markets that you don't. One of the few things you've said that I DO agree with is that each of our respective profitability has no bearing on the other. You losing £240K also has no bearing on my life. You talking absolute bollocks while telling people who DO know what they are on about that they are wrong, does have a bearing on my life. Specifically my blood pressure.



 If I said black was black, you'd say it was white, and because of that I just can't engage with you anymore.

*Tags: General Technical*

---

## 2024-07-30

**liam** - *17:18:25*

[https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/](https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/)

*Tags: Performance*

---

**A** - *17:38:10*

Fairly new to Python.



After profiling and openeing in snakeviz, it looks as though most of the work is slowed down by things in `functools`  module - guess it’s the `@cachedproperty`  decorators I used.



Will have a play with that.



Thanks for the pointers :pray:

*Tags: Getting Started, Performance*

---

**liam** - *18:13:30*

Fastest way to speed up python is to not run it, do you need to execute every tick?

*Tags: Performance*

---

## 2024-08-03

**AndyL** - *18:56:29*

First misstake of "inplay", simulate a strategy based on zero-delay, looks fantastic, try backtesting it and it's crap as after 1-second delay it instantly matches at a crap price, doh!

*Tags: Strategies*

---

## 2024-08-04

**Aryan Kapoor** - *10:09:09*

Was wondering what the quickest way to get your "current position" one a particular runner within a market would be in flumine. Pretty new to flumine and currently running it in simulation mode but not having much luck. Have tried runner.matches which just returns an empty list continuously even though I am getting matches on a couple orders in the backtest.

*Tags: Getting Started*

---

**liam** - *10:14:01*

Lots of helper functions in the `market.blotter`



[https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/markets/blotter.py#L161](https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/markets/blotter.py#L161)

*Tags: General Technical*

---

**liam** - *10:14:57*

How did you remove the delay? Assuming you are using flumine it adds the market delay for you

*Tags: General Technical*

---

**AndyL** - *10:46:56*

I was using flumine purely as data streaming then theoretically signal triggering at t0, then virtual matching at t+1second price 

*Tags: General Technical*

---

**AndyL** - *10:48:36*

I’m doing this to remove the adverse selection using flumine matching so I can theoretically find a signal that works… then work on an execution that will achieve it for real 

*Tags: General Technical*

---

**liam** - *10:49:13*

you need to add network latency as well so it will be more than 1s

*Tags: Performance*

---

**AndyL** - *10:50:05*

Latency ah yeah 

*Tags: Performance*

---

**AndyL** - *10:59:32*

oh! well that may explain things...!

The thing is i'm trying to find a way of "analysing" inplay prices and EV, and you have the issue of "signal", then 1second delay +latency.....

hence I tried to virtually simulate the behaviour

the problem with purely backtesting is the biggest effect is execution method

*Tags: Performance*

---

**liam** - *11:01:31*

I just use flumine, store signals/market data into `order.notes` and analyse in jupyter after running, I then know I can trust execution/matching (to a point)

*Tags: General Technical*

---

**Rob** - *17:57:44*

>  Takes my machine around 3 seconds to process a single market

Same here, until I add my processing.



When using `flumine`, how do I know when my processing is too slow, e.g. if `process_market_book` is still running when the next update arrives? I tried randomly waiting for 2s in my simulation, but I don't see anything in the logs.



I'm currently approximating how much "time" I have per market based on the # markets in my strategy and the amount of time they're active for e.g. there are 6 race meetings per day, with 7 races, that I'm active in for an average of 6 hours before the off, then if each market takes 2 minutes to simulate, there should be plenty of time

*Tags: Performance, Strategies*

---

**liam** - *18:33:10*

When simulating the code runs in a single process so you wont get any latency logs as it doesn’t impact anything.



Just profile your code, cprofilev 

*Tags: Performance*

---

## 2024-08-05

**AndyL** - *22:42:16*

So what’s probably more a problem is punters can’t afford it what with cost of living etc….

*Tags: General Technical*

---

**AndyL** - *23:09:47*

Im sort of delving with this question between prerace and inplay, as to where my modelling efforts are best placed. Inplay horses im up against all you lot and just a few punters, whereas prerace theres far more punter money flow, but then there is the Syndicate big money controlling things… !??

*Tags: Strategies*

---

## 2024-08-06

**K H** - *00:39:17*

(Mostly directed at the comment on where to direct your modelling efforts)



I don't know if thinking about it as being 'up against' anybody is the right framing. Theoretically, anytime you're in the market you're up against something far more intelligent than a single individual - studies tend to find the 'wisdom of the crowds' far wiser. And any market of decent liquidity on betfair is a decent sized crowd.



But you don't need to be wiser than the crowd (whether it's a crowd of PHDs or a crowd of children). You just need to find points where the crowd are biased (and the crowd is human, so it will be biased), or where it wavers (volatility), or indicators that the crowd is about to start moving in a certain direction.

Which is not to say it's easy - but there are about a thousand different angles you can attack the market from. And the crowd's actions aren't instantaneous, so with enough data there's a high likelihood an angle (many angles) can be found regardless of the crowd.



In general (others probably have more in depth advice), focusing your efforts where there is a lot of liquidity probably makes sense - if you had to look for a single criteria and could only pick along one dimension.

*Tags: Strategies*

---

**liam** - *06:59:07*

Agree with above however racing has the advantage of a lot of opportunities, you can spend an afternoon testing / debugging on live markets. This is invaluable when starting out, sports like cricket really slow your progression down due to the smaller number of markets. 

*Tags: Errors Debugging, Performance, Deployment*

---

**AndyL** - *07:51:14*

Thanks chaps, it’s a tricky one horses are definitely competitive and liquidity reducing but I feel I’ve invested a lot of effort already which I feel would be wasted moving to football. As to prerace vs inplay thats a tricky one too, im currently feeling an attraction to the high volatility of inplay at the moment.

My modeling I think is a bit too simple and trying to find a single linear feature model doesn’t possibly fit the opportunities I am trying to find…maybe!

Anyway I think ive been talking too much and not getting down to it….

Cheers all

*Tags: Feature Engineering, Strategies*

---

**Michael** - *07:52:30*

_"trying to find a single linear feature model"_



Yeah - that aint gonna work.

*Tags: Feature Engineering, Strategies*

---

**AndyL** - *09:52:51*

[@UGV299K6H](@UGV299K6H) sorry I meant a simple linear feature model, so my example previous was a simple linear model using the 3 core features: volume, queued and matched 

*Tags: Feature Engineering, Strategies*

---

**D C** - *10:52:55*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) IF you stick to pre-race you've got the added bonus in that you can use BSP as a measure to assess your success. Inplay that is not so easy to do. Inplay, you've got delays  - bet placement and internet latency (the latter being variable to some extent) so this makes back testing and simulating stuff more error prone. Then you've got things like offering prices versus taking (the former again harder to simulate and back test - impossible to PRECISELY assess the impact of your offers on the market). Your time is your own to spend of course but how much time have you invested in pre-off horses? Are you looking to move because you think increased volatility inplay is more attractive or because you feel you've hit a brick wall on the pre-race side? Or have you just become downbeat and demotivated because you haven't "cracket it" yet?

You've got to consider that you might never get there - I'm not trying to demotivate you at all but its an event with a non-zero probability. And if you flit around too much from sport to sport or mixing market phases you might do more harm than good.

I don't know your background but have you got the maths/stats chops to considered using some fundamental data instead/in addition your market based variables?

*Tags: Errors Debugging, Performance*

---

**D C** - *11:06:57*

I'd play to your own personal strengths rather than how much others have made. You're probably in one of the best places to ask questions and get generic advice but only you know where your strengths lie. And how much time to invest before moving on.

*Tags: General Technical*

---

**birchy** - *13:15:57*

With horse racing the pre play liquidity has dropped off a lot but there's a significant amount matched inplay, sometimes more than was matched at the off time. I've been hacking around with modelling recently and have had some success with a basic LR model that has about 30 features that are all market data based, but there are 3 or 4 features/signals that are particularly stronger than the others. Engineering features is a trivial exercise but finding good ones is much harder. I took the approach of using lots of features and then dropping the shit ones until I had something production ready. I'm not really a fan of the 'black box' approach but it seems to understand the market nuances better than I do. The bottom line is that it's bringing in some £'s, so I'm slowly convincing myself that it's a much easier route to take.

*Tags: Feature Engineering, Performance, Deployment, Strategies*

---

**birchy** - *13:27:04*

Yeah, anything you can derive from the market data we receive. There's a million ways to calculate signals from combinations of available & matched prices and volumes. I'd also add that I spent far too long trying to find strategies using just simulation and there's no substitute for live betting, which, ironically, was my mindset several years ago before I discovered flumine. For example, most of my strategies are taking prices and the only way to prove that is by running live.

*Tags: Deployment, Strategies*

---

**liam** - *19:47:47*

I need to build a process that processes historical data and creates predictions on a daily basis (every am) Does anyone have a decent design / system for this? I am currently thinking:



-&gt; machine or lambda to process the historical data to then store in a db 



-&gt; another to create the predictions using the db



-&gt; store predictions in s3 



-&gt; flumine strategy that handles the execution

*Tags: Data Quality, Strategies*

---

**liam** - *20:09:21*

I could use a flat file to store the historical data and move the prediction creation / execution into one as it isn’t time sensitive.



Maybe I have been brainwashed by the microservice way of doing things, I just like the separation of tasks/code

*Tags: Data Quality*

---

**thambie1** - *20:16:02*

I have one medium size machine, where I do all production work. Strategy development lives elsewhere. If haven't/don't want to host your flumine instances with other production jobs on the same machine.. I can see why you'd go serverless

*Tags: Deployment, Strategies*

---

**liam** - *20:30:26*

:joy:  happy to be questioned, I am way to far down that path now, the advantage for me is scalability 

*Tags: General Technical*

---

**birchy** - *21:58:03*

While we're mentioning S3... I'm at a point where I've got millions of markets saved via flumine market recorder and am wondering how others are filtering the files? e.g. say I want all GB horse races that turn inplay, what's the nicest way to find them? I've currently got a somewhat convoluted boto3 setup that paginates the market catalogues and then returns a list of market IDs that match my filters. I then download the files to local storage. I've played around with creating a local index file but it gets very big very quickly. I'm also pondering streaming direct from S3 -&gt; flumine rather than downloading them all as, similarly to [@U4H19D1D2](@U4H19D1D2), I'm processing through flumine to format data for model training.

*Tags: Getting Started, Data Quality, Strategies*

---

**Michael** - *22:06:08*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) I'd also encourage you to do simple AvB tests with your existing strategy to optimise your stakes and prices. When you're trying to get the most out of something it's tempting to see how high you can push the stakes but that's often counter-productive, maybe experiment with reducing them and see what happens over 1000 markets. Try offering more generous and less generous prices. The nice thing about that sort of thing is that it takes minimal work, just a couple of minutes then forget about it for a month or two. So much easier than sweating over a jupyter notebook.

*Tags: Strategies*

---

**Jonjonjon** - *22:09:18*

I use an sqlite database with the market catalogues in it.

*Tags: General Technical*

---

**birchy** - *22:10:07*

I'm a complete luddite to databases and have always used flat files for simplicity. But there's a point when even flat files become cumbersome. The issue I have is that I run 99% of my code on a Lightsail instance (which is plenty fast enough for my needs and allows access from multiple devices) but the local storage is limited. Traditionally, I've just downloaded the markets I'm concentrating on and then delete &amp; download when I move onto the next set. How bad is flumine performance when reading direct from S3 using smart_open() and boto3?

*Tags: Performance*

---

**Jonjonjon** - *22:14:36*

[@U01DPK9PGMC](@U01DPK9PGMC)  what sort of strategy do you use for offering more/less generous prices, when a tick has different value depending on the price level?



My market maker just quotes at the best available price. Would you recommend spamming quotes up to 'n' levels of depth?

*Tags: Strategies*

---

**Michael** - *22:20:49*

[@UPMUFSGCR](@UPMUFSGCR) I post prices all over the place depending on the strategy, I'm not certain what you mean by "spamming" but personally I don't back longer than BATL or lay shorter the BATB because I always want to be front of the queue, but I know other people do and they make it work. Broadly I just take a stab at fair value and add X margin to it, but that might not be relevant to what you're doing.

*Tags: Strategies*

---

**Michael** - *22:24:13*

Oh wait sorry [@UPMUFSGCR](@UPMUFSGCR) I think I misunderstood your question - do you mean at the points where the tick intervals change? I don't worry bout ticks at all, I just derive my odds however I'm doing it, round to the nearest tick and post.

*Tags: General Technical*

---

**Jonjonjon** - *23:23:02*

My database is around 1gb for a few years of racing data. If you give it a go it will be larger/smaller depending on what indices you choose to create and how compact you make your data. I don't think mine is that optimal.

*Tags: General Technical*

---

## 2024-08-07

**Gabriel Mocan** - *00:47:54*

Have you guys heard about QuestDB? I’m using it for my greyhound strategies. It’s a very optimized database for time series and finances, with a lot of useful specialized time functions. Performance is a key feature of it.





*Tags: Feature Engineering, Performance*

---

**Gabriel Mocan** - *00:55:40*

[@U4H19D1D2](@U4H19D1D2) i’m a versed system architect with solid bg on docker, pipelines and continuous integration, happy to help in this design and colaborate on the project. 

*Tags: General Technical*

---

**Tom** - *07:15:20*

I think you've sort of inadvertently explained why it's sort of self-regulating in the question itself. A single entity overcommitting to risk compared to the market volumes (and complexity of volumes) in a closed (forced result) market with a single pre-determined event that determines an outcome will begin encountering issues getting paid.

*Tags: General Technical*

---

**John Foley** - *08:37:41*

(I’ve tackled this exact problem you’re describing) 

*Tags: General Technical*

---

**Derek C** - *09:58:29*

I have several pets, slowly creating more cattle ... but in the proposed workflow above, if the output is predictions on S3 I would just use Athena as my database like I do for most stuff. My scheduling is EventBridge invoking lambda/Step functions.

*Tags: Performance*

---

## 2024-08-08

**Mo** - *10:35:14*

I have two different setups, both relatively old school. For the first there is an EC2 instance running 24/7 that processes new data on an hourly basis using a cron job. That “new data” is a combination of data scraped from APIs into an RDS Postgres database and log files uploaded to S3 from trading servers



For the second it’s a one off daily pipeline, also running on an EC2 box as a cron job. AWS EventBridge used to bring the box up and down so it’s only running for the duration of the pipeline. Database this time is Aurora Serverless for, in principle, cheaper database costs. In practice, regularly get fucked in the arse by IOPS

*Tags: Getting Started, Deployment, Strategies*

---

## 2024-08-17

**Jonjonjon** - *13:15:25*

Any insights?



Today I'm doing 10-200 bets per horse race. I'm wondering that if I'm iceberging lots of small orders,  am I giving away info?



However, my model does not work if I start trading larger quantities.

*Tags: Strategies*

---

## 2024-08-18

**Peter** - *12:44:45*

[@U016TGY3676](@U016TGY3676) If you're using the same paginator object, then they should be the same. But if you're creating a new paginator object because the session is interrupted, then there's no guarantee.



However, there may be a better way of handling this anyway. I use the S3 inventory feature to get a daily list of objects (or subsets with a key prefix) dumped into a bucket in parquet format (csv format also available). It gives you the same data but without the the need for iterating over a bonkers number of pages.



There are some limitations:

• The list is available no more frequently than daily (weekly or month are also available), but at a time determined by AWS, you have no control over this.

• Even when it arrives the data is typically 12 or more hours out-of-date, I suspect because they run the report against cached data.

But unless you need bang up-to-date data (which would be difficult anyway with buckets containing 1mn plus objects and the most recent at the end of the pagination), these probably aren't big issues. They haven't been for me.

*Tags: Errors Debugging, Feature Engineering, Deployment*

---

## 2024-08-19

**birchy** - *11:11:31*

I was thinking more at strategy level in Flumine, but that's definitely an option. :+1:

*Tags: Strategies*

---

**D C** - *11:13:14*

Well I don't use flumine, but AFAIK there is nothing else in the API that gives you that detail other than the name. You can get it off TPD but only for those markets. I've never had much luck with the racecard API so no idea if it is available in that.

*Tags: General Technical*

---

## 2024-08-20

**Aryan Kapoor** - *00:18:42*

Super dumb q but is there any way to get the market_id in flumine functions/classes where Market/MarketBook isn't parsed

*Tags: General Technical*

---

**liam** - *08:19:29*

Sorry I don't understand, you can see all the strategy functions [https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/strategy/strategy.py#L91|here](https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/strategy/strategy.py#L91|here)

*Tags: Strategies*

---

**liam** - *08:56:11*

the start function is when the strategy starts at runtime, `process_new_market` is when a market gets added to the stream, is that what you want?

*Tags: Strategies*

---

**Johnny Boston** - *15:50:47*

Anyone know why I would get an error like `"error_code": "INVALID_MIN_FILL_SIZE"` with this order?



:thread:

*Tags: Errors Debugging*

---

**Johnny Boston** - *15:51:30*

``` "status": "FAILURE", "errorCode": "BET_ACTION_ERROR", "marketId": "1.231956835", "instructionReports": [{"status": "FAILURE", "errorCode": "INVALID_MIN_FILL_SIZE", "instruction": {"selectionId": 71661945, "handicap": 0.0, "limitOrder": {"size": 0.55, "price": 8.4, "minFillSize": 1.35, "timeInForce": "FILL_OR_KILL"}, "customerOrderRef": "43da61bd07abd-139434572457729620", "orderType": "LIMIT", "side": "LAY"}}]}, "order_package": {"id": "37bb203a-0748-4c5d-9931-442a5c2880fd", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f64af923b90&gt;", "market_id": "1.231956835", "orders": ["139434572457729620"], "order_count": 1, "package_type": "Place", "customer_strategy_ref": "[http://ip-10-2-8-71.eu|ip-10-2-8-71.eu](http://ip-10-2-8-71.eu|ip-10-2-8-71.eu)", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": true}}```

*Tags: Errors Debugging, Strategies*

---

**thambie1** - *15:55:58*

Per the documentation, INVALID_MIN_FILL_SIZE: "The minFillSize must be greater than zero and less than or equal to the order's size. The minFillSize cannot be less than the minimum bet size for your currency"

*Tags: General Technical*

---

## 2024-08-21

**Derek C** - *07:47:07*

process_new_market gets called once per market, check_market_book gets called once per update within a market. Lots of us here create CSVs from the data so it might be worth you exploring the Flumine examples rather than 'doing something a bit different'. With streaming data it might be more appropriate to write everything to the same csv with market_id as a field, and then later on split the file into one per market, if that is a key requirement for you.

*Tags: General Technical*

---

**Derek C** - *08:26:15*

for multi-processing with backtesting, I typically would have different markets going to each process and then a separate output file for each that is named after the process, e.g. from Python mp.current_process().name. Easy to then split/concatenate this data in Pandas as needed afterwards.

*Tags: Feature Engineering*

---

## 2024-08-22

**liam** - *14:14:04*

Yes but it will only be rejected if the best available moves between your placement and when it reaches the matching engine, ie. latency + betDelay

*Tags: Performance*

---

**A** - *15:30:35*

You could also achieve this using middleware:



```class SomeMiddleware(Middleware):

  

  def add_market(self, market: Market):

    # this is called when a new market is added

    pass



  def __call__(self, market: Market):

    # this is called on every market update

    pass



  def remove_market(self, market: Market):

    # this is called when a market is done

    pass```

then somewhere:



```middleware = SomeMiddleware()

flumine.add_market_middleware(middleware)```



*Tags: General Technical*

---

## 2024-08-23

**Paul** - *16:37:43*

Keep in mind published race distances aren't accurate. Every day the BHA publish rails changes for a [https://www.britishhorseracing.com/racing/fixtures/upcoming/racecard/#!/2024/1887|racecard](https://www.britishhorseracing.com/racing/fixtures/upcoming/racecard/#!/2024/1887|racecard) like [https://api09.horseracing.software/bha/v1/racecourses/distancechanges/csv/62/dc_62|this](https://api09.horseracing.software/bha/v1/racecourses/distancechanges/csv/62/dc_62|this). Often the changes are quite small but sometimes in the Winter you'll see distance changes over 110 yds (which is half a furlong), as they peg out to avoid false ground.

*Tags: Errors Debugging*

---

## 2024-08-28

**liam** - *06:54:00*

[https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/](https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/)

*Tags: Performance*

---

**liam** - *12:40:45*

I can help if you give me a single file which replicates your issue

*Tags: General Technical*

---

**Aryan Kapoor** - *12:41:25*

Appreciate the help on this super basic issue will get something cleaner for you

*Tags: General Technical*

---

**liam** - *12:47:39*

the problem will be in what you aren't sharing

*Tags: General Technical*

---

**Aryan Kapoor** - *13:21:30*

Ive got a folder together with the problem

*Tags: General Technical*

---

**Aryan Kapoor** - *13:28:55*

I understand this is a very elementary problem so do appreciate all the help

*Tags: General Technical*

---

**liam** - *13:36:34*

In your strategy you are writing the headers for the 'theos.csv' file in `start`  so this is being wiped every time a new process starts, is this expected or the bug?

*Tags: Errors Debugging, Strategies*

---

**liam** - *13:53:30*

this is set at the strategy level but I imagine it should be per market?

*Tags: Strategies*

---

**Aryan Kapoor** - *13:58:22*

also with your point about it being set at a strategy level

*Tags: Strategies*

---

**Aryan Kapoor** - *13:58:59*

cause i think that could be an issue i need to fix anyways

*Tags: Errors Debugging*

---

**liam** - *13:59:26*

and store strategy stuff in



```strategy.context["yada"] = 123```

*Tags: Strategies*

---

**Aryan Kapoor** - *14:01:05*

i think that may be the issue and regardless thats an important thing to fix

*Tags: Errors Debugging*

---

**Aryan Kapoor** - *14:01:14*

thanks so much for all this helped a lot

*Tags: General Technical*

---

**liam** - *14:04:20*

np mate, let me know if this fixes it as I am fairly sure it will

*Tags: Errors Debugging*

---

## 2024-08-29

**liam** - *11:35:29*

bflw? flumine? streaming?

*Tags: General Technical*

---

**Bassam Ismail** - *11:35:58*

bflw marketstream

*Tags: General Technical*

---

**liam** - *11:36:55*

easy answer is use flumine and it handles everything for you if you are doing it your own way then you want something similar to this example to handle a reconnection



[https://github.com/betcode-org/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/betcode-org/betfair/blob/master/examples/examplestreamingerrhandling.py)

*Tags: General Technical*

---

**Aryan Kapoor** - *13:02:50*

is there a quick way to cancel all pending orders in flumine? im currently looping through all strategy orders and then using market.cancel_order but was wondering if there was a simpler way

*Tags: Strategies*

---

**liam** - *14:07:23*

Agreed, however pending is a state in flumine so wanted to remove any confusion.



This is the quickest / most efficient way, using a [https://github.com/betcode-org/flumine/blob/d03def30a778e9460e336b53b0689edb953f7c55/flumine/execution/transaction.py#L12|transaction](https://github.com/betcode-org/flumine/blob/d03def30a778e9460e336b53b0689edb953f7c55/flumine/execution/transaction.py#L12|transaction) means it will be sent in a single http request



```orders = market.blotter.strategy_orders(

    self, order_status=OrderStatus.EXECUTABLE

)

with market.transaction() as t:

    for order in orders:

        t.cancel_order(order)```



*Tags: Strategies*

---

## 2024-09-03

**Aryan Kapoor** - *09:00:07*

Running some flumine simulations and stumbled upon a market where I am getting this exchange message: "Order adjusted due to non runner 64796820".  Uncertain exactly the mechanics of this (which I will dive into later) but what i do know is that all the order data is getting saved again and it is messing up a bit of preliminary jupyter analysis I am doing. I was wondering if there was anyway to flag something like this so I could filter it out of my orders dataset.

*Tags: General Technical*

---

**liam** - *09:13:59*

Fix your code so that it doesn't dupe?

*Tags: Errors Debugging*

---

**Aryan Kapoor** - *09:16:03*

It seems like when the exchange rematches the trades it gets recorded as new trades in flumine - will look into it but do you have any suggestions im definitely not as across the workings for flumine

*Tags: General Technical*

---

**liam** - *09:36:04*

I think we are on different pages, if you can share some code that replicates the problem I can fix but it will be your code causing issues

*Tags: Errors Debugging*

---

**Aryan Kapoor** - *09:37:41*

Okay sure I get what the above meant theoretically though so I’ll try solve it myself I think I need to wrap my head around flumine more haha. If still struggling I’ll give you a message :smiling_face_with_tear: 

*Tags: General Technical*

---

**Aryan Kapoor** - *10:42:06*

my logging code is essentially identical to the example code here [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: General Technical*

---

## 2024-09-04

**A** - *19:09:38*

I think I’ve been following a similar path to you [@U01PJ5YMFBJ](@U01PJ5YMFBJ) - exploring various signals using purely price/volume/queued data, and drawing blanks when it comes to finding any positive expected value. I even had a little foray into machine learning, but could never find anything that would spit out EV of anything slightly above 0.



Frustrating, as I feel I have everything else nicely set up (production envs, reporting, data, alerting, post analysis charts, etc) - just missing the value part (retrospectively, I know I shouldn’t have invested so much time into the ‘fun’ stuff before finding an edge…).



I don’t think I’m ready to give up just yet, but I’m starting to feel disheartened - feeling like I’m just not smart or creative enough to compete.



You mentioned something along the lines of going back and tuning your “already profitable algorithm” - I’m curious on how that’s any different from what you’ve been trying already? Congrats on finding profit anyway - more than I’ve got so far :rolling_on_the_floor_laughing:

*Tags: Deployment*

---

## 2024-09-05

**Ralegh** - *14:34:13*

I mainly use linear models so Pearson makes sense

*Tags: Strategies*

---

**D C** - *18:44:55*

Can't you just fit a saturated model and remove things? Think they used to call that backward selection but it might not be trendy anymore.

*Tags: Strategies*

---

## 2024-09-06

**Ralegh** - *18:38:57*

(Higher is likely to be an error/leaky data)

*Tags: Errors Debugging*

---

## 2024-09-12

**Justice** - *14:03:25*

Curious to know how folks are handling bet sizing in running. My instinct was to use fractional kelly but due to the in-play delay, I have no idea what price I'll be matched

*Tags: Strategies*

---

**D C** - *16:29:02*

But you can also be the one making the cake, and leaving it on the table? As with all these types of question, it really depends on the sport and strategy doesn't it? I often find that there is a bias towards the horses and taking prices due to the number of people on this slack that do well off it. It's a very different game trying to grab a few quid at nice prices across a few seconds of opportunity in a 5 furlong race to getting matched in cricket where there can be tens of thousands up for grabs and 5 days to manage your money over.

*Tags: Strategies*

---

**Justice** - *17:13:43*

I am happy with my execution (when to bet) but I am using a simple level stake and can't help but feeling I'm leaving profits on the table

*Tags: General Technical*

---

**liam** - *17:55:22*

Stake using liability as a first step, I thought you were a flumine user?

*Tags: General Technical*

---

**Justice** - *18:48:45*

[@U4H19D1D2](@U4H19D1D2) Yeh I just have level stake fixed liability at the moment. I used to use flumine. I have my own trading framework written in C# for production code. And I do backtesting using a custom Rust plugin for polars dataframes

*Tags: Errors Debugging, Feature Engineering, Deployment, Strategies*

---

**Michael** - *18:54:43*

Yeah so in that case you can forget all about Kelly. Keep your stakes flat and small. I know it's frustrating but small stakes really is the way to go.

*Tags: Strategies*

---

## 2024-09-15

**Gooat** - *01:04:49*

I always wonder about many small models/ratings v many features and dealing with it in OMS.

*Tags: Feature Engineering, Strategies*

---

## 2024-09-18

**liam** - *20:43:54*

Ignoring the fact it’s a google sheet this has *lots* of ideas / papers / tutorials in it that would certainly help get started in new sports 



[https://docs.google.com/spreadsheets/u/0/d/1LPe8xYduoep9qCrNzBGdJHaHZ8dnmdHNnu7UXZKzawU/htmlview#|https://docs.google.com/spreadsheets/u/0/d/1LPe8xYduoep9qCrNzBGdJHaHZ8dnmdHNnu7UXZKzawU/htmlview#](https://docs.google.com/spreadsheets/u/0/d/1LPe8xYduoep9qCrNzBGdJHaHZ8dnmdHNnu7UXZKzawU/htmlview#|https://docs.google.com/spreadsheets/u/0/d/1LPe8xYduoep9qCrNzBGdJHaHZ8dnmdHNnu7UXZKzawU/htmlview#)

*Tags: General Technical*

---

## 2024-10-03

**JL** - *21:19:57*

oh it's wrong? I did it similar to: [https://www.quantstart.com/articles/Sharpe-Ratio-for-Algorithmic-Trading-Performance-Measurement/](https://www.quantstart.com/articles/Sharpe-Ratio-for-Algorithmic-Trading-Performance-Measurement/)

*Tags: Performance, Strategies*

---

## 2024-10-05

**Jonjonjon** - *10:43:46*

[@U012XF5CNPN](@U012XF5CNPN) are you saying that 90% of Devs don't have test coverage? I used to work on a Python platform at an investment bank, and there were automated tools for tracking testi coverage of commits.



Having said that, I haven't seen it since. I use Pycharm CE and there wasn't test coverage in that the last time I checked.

*Tags: General Technical*

---

**Jonjonjon** - *10:47:00*

I scanned a load of files yesterday and noticed lots of "available to back" orders with a liability of 1p or less. Is that a viable strategy? I thought Betfair had cut down on it.

*Tags: Strategies*

---

## 2024-10-07

**Paul** - *11:23:17*

[@UPMUFSGCR](@UPMUFSGCR) - depends. Software capability maturity model is a real thing. Not everyone is - or is able to - hit best practices in every box, due to other conflicting requirements. Anyone over the age of 30 probably didn't have it enforced on them when getting into the industry, so never felt the benefits, so drag their feet. TDD only _really_ came of age in the 2010s, as tools across Ruby (choice of startups on both sides of the Atlantic 2006-2015), Java, C# (enterprise, both) got good, and then mimicked in other languages like Python, C++, Go, Rust and so on afterwards. If you worked in investment banks, you probably know full well there are tons of things inside investment banks that are either ancient (COBOL, S/360s, AS/400s, et al), or shonky (VBA in Excel spreadsheets for portfolio management), or both (I was told about a Lotus Notes thing absolutely key to one bank KTLO operation a couple of years ago). Greenfield projects that are tech heavy (which banks have been throwing money at since Jane Street and co started to eat their lunch), will nearly always be TDD these days, but even then 90% of the devs I work with drag their feet and you're doing well to see 60%+ test coverage on most code bases.

*Tags: Strategies*

---

## 2024-10-14

**liam** - *10:33:11*

Weird as it lets me search it but then the link doesn't work



&gt; 

&gt; 

&gt; *liam*  [3:24 PM]

&gt; Does anyone know how much Decimal charge for the Cricket feed?

&gt; 49 replies

&gt; 

&gt; 

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [3:55 PM]

&gt; I'm on the hunt for a decent feed too!

&gt; 

&gt; 

&gt; 

&gt; *D C*  [4:02 PM]

&gt; Is it just a score feed? Is Betfair's not up to scratch?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [4:05 PM]

&gt; No comment

&gt; 

&gt; 

&gt; 

&gt; *D C*  [4:06 PM]

&gt; Understood

&gt; 

&gt; 

&gt; 

&gt; *liam*  [9:10 AM]

&gt; I have sent an email, will share the price if they do however I fear it might be out of my budget

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [9:56 AM]

&gt; I can give you a personal introduction if you don’t get anywhere

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [11:22 AM]

&gt; I think the main issue, not just for cricket, is Betfair's inability to use the widely accepted team or players names.

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [3:15 PM]

&gt; And the inability to match names strikes again :expressionless:

&gt; 

&gt; 

&gt; 

&gt; *liam*  [3:36 PM]

&gt; It's a complete fucking joke tbh, almost 3 years this has been in 'production'

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [3:48 PM]

&gt; you would have thought, it being the same team causing the issue as yesterday, they might have pre-empted it .....

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [3:49 PM]

&gt; what issues are you seeing [@U4H19D1D2](@U4H19D1D2)?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [3:51 PM]

&gt; The usual ongoing mapping issues and there is some sort of bug with the Lancashire game which is pushing out about a million updates per second on certain connections, seems like they just don't care

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [6:11 PM]

&gt; Wow, looks like this is bringing down my entire scraping infrastructure

&gt; 

&gt; 

&gt; 

&gt; *liam*  [6:19 PM]

&gt; Try restarting, doesn’t seem to be on every connection

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [6:21 PM]

&gt; Better but the damage has already been done

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [6:22 PM]

&gt; nice to know the cricket feed could take down all my recording

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [6:23 PM]

&gt; Well your mileage may vary depending on how much disk space your box has and whether you have appropriate alerts set up

&gt; 

&gt; 

&gt; 

&gt; *D C*  [6:24 PM]

&gt; Is this a recent problem or  long term? Just wondering if it's related to my recent server reboot as I am recording the cricket score feed on said server.

&gt; 

&gt; 

&gt; 

&gt; *liam*  [7:53 PM]

&gt; Recent, started today at 11:45ish, apparently now fixed

&gt; 

&gt; 

&gt; 

&gt; *liam*  [8:21 AM]

&gt; Looks like either it was never fixed or it started happening again today at about 1am

&gt; 

&gt; [8:22 AM]

&gt; 

&gt; This file is hidden because it was uploaded more than 90 days ago. [https://betcode-org.slack.com/plans?feature=unlimited_messages|Upgrade to a paid subscription](https://betcode-org.slack.com/plans?feature=unlimited_messages|Upgrade to a paid subscription) to view.

&gt; 

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [12:10 PM]

&gt; My machine must have been smoking, I wonder what's going on... (edited)

&gt; 

&gt; 

&gt; 

&gt; *liam*  [12:29 PM]

&gt; eventide issues apparently

&gt; 

&gt; [http://docs.eventide-project.org/](http://docs.eventide-project.org/)

&gt; 

&gt; 

&gt; *[http://docs.eventide-project.org|docs.eventide-project.org](http://docs.eventide-project.org|docs.eventide-project.org)*

&gt; *[http://docs.eventide-project.org/|Eventide](http://docs.eventide-project.org/|Eventide)*

&gt; Pub/Sub, Event Sourcing, Evented Microservices

&gt; 

&gt; 

&gt; 

&gt; *Gooat*  [6:48 PM]

&gt; Any ball park price numbers come back?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [7:03 PM]

&gt; Nope

&gt; 

&gt; 

&gt; 

&gt; *liam*  [11:29 AM]

&gt; For those interested

&gt; 

&gt;&gt; You are more than welcome to come direct but we do have a minimum fee for direct customers that may be prohibitive to you of £2k per month and £150 per game.

&gt; 

&gt; 

&gt; 

&gt; *D C*  [11:46 AM]

&gt; 2K a month AND £150 a game. No idea if that's cheap or expensive to be fair

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [11:48 AM]

&gt; Expensive compared to other sports but there’s easily enough liquidity in a match to extract that much EV

&gt; 

&gt; 

&gt; 

&gt; *liam*  [12:18 PM]

&gt; I read that as £150 a game, minimum 2k a month.

&gt; 

&gt; Update on the betfair feed is that I have continued hounding Neil to the point they have made some changes to the 'team' lots of politics/stupid code freezes but things have improved

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [12:20 PM]

&gt; I noticed last month there was a big jump in quantity of matches with the feed. Up around 100 matches.

&gt; 

&gt; [12:20 PM]

&gt; Is that seasonal or from the improvements?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [12:21 PM]

&gt; Both but we are now down to missing about 15% of the matches due to mapping, there where times last year where it was around 50%

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [12:22 PM]

&gt; Big improvement

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [2:03 PM]

&gt; How do you work out which markets are missing the feed?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [2:09 PM]

&gt; nightly script to reconcile between market/feed data

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [2:12 PM]

&gt; are we expecting every cricket match to have the feed?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [2:13 PM]

&gt; pretty much, expect sub ~3% to not have a feed depending on what betfair put up

&gt; 

&gt; 

&gt; 

&gt; *Gooat*  [5:38 PM]

&gt; Has anyone levelled up to big stakes from the feed?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [5:58 PM]

&gt; I am up to a few k exposure per market

&gt; 

&gt; 

&gt; 

&gt; *Gooat*  [6:02 PM]

&gt; Nice. Model or mix of price and flow? I struggled to avoid averse selection when testing it.

&gt; 

&gt; 

&gt; 

&gt; *liam*  [6:08 PM]

&gt; model

&gt; 

&gt; 

&gt; 

&gt; *Gooat*  [9:28 PM]

&gt; How did that game go?

&gt; 

&gt; 

&gt; 

&gt; *Sen*  [2:32 PM]

&gt; On this note - [@U4H19D1D2](@U4H19D1D2) how do you know Decimal is the best data provider in the cricket space? Something you've been able to test? (edited)

&gt; 

&gt; 

&gt; 

&gt; *liam*  [2:36 PM]

&gt; I don't, its just the one betfair provide

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [2:37 PM]

&gt; I can say that the people behind it have been involved in cricket data for at least a decade

&gt; 

&gt; 

&gt; 

&gt; *Sen*  [3:15 PM]

&gt; Another option is sportmonks - they look pretty decent too

&gt; 

&gt; 

&gt; 

&gt; *liam*  [3:15 PM]

&gt; Yeah, considerably cheaper

&gt; 

&gt; 

&gt; 

&gt; *D C*  [4:31 PM]

&gt; Has the Betfair cricket feed not improved ?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [7:23 PM]

&gt; Yes it has but it’s been hard work (for myself and Neil)



*Tags: Errors Debugging, Feature Engineering, Deployment, Strategies*

---

## 2024-10-25

**liam** - *08:39:16*

From the AUS team



&gt; NEW CONTENT   

&gt; Just in time for the weekend, Parts 1 &amp; 2 of our new series "How To Build A Soccer Bot" are now live on the Automation Hub! 

&gt; Part 1: [https://betfair-datascientists.github.io/modelling/howToBuildASoccerBotPartI/](https://betfair-datascientists.github.io/modelling/howToBuildASoccerBotPartI/) 

&gt; Part 2: [https://betfair-datascientists.github.io/modelling/howToBuildASoccerBotPartII/](https://betfair-datascientists.github.io/modelling/howToBuildASoccerBotPartII/)

*Tags: Deployment, Strategies*

---

## 2024-10-30

**Terry** - *09:10:49*

Can anyone offer any advice on this one?



i am using this to check if any orders exist before placing and order:



runner_context = self.get_runner_context(market.market_id, SEL, 0)



```# Check if there are any active investments or trades for the runner

if runner_context.invested or runner_context.live_trades:

    [http://logger.info|logger.info](http://logger.info|logger.info)(f"{runner_name} has Live Trades!")

    continue  # Skip placing an order if there is already an active investment or trade



else:

    SIDE = "LAY"

    order = trade.create_order(

    side=SIDE, 

    order_type=LimitOrder(

        size=LSTK,

        price=LPC,

        time_in_force='FILL_OR_KILL'

    )



    market.place_order(order)```

however if the order is killed, then flumine still thinks and order is placed and doesnt fire off another. I am sure theres a better way to achieve this!

*Tags: Deployment*

---

**liam** - *09:11:58*

What are your strategy settings? (and logs will tell you)



```strategy = LowestLayer(

    market_filter={"markets": markets},

    context={"stake": 2},

    max_live_trade_count=2

)```

*Tags: Deployment, Strategies*

---

**Terry** - *09:25:03*

```GREYS_2 = MyModel_LAY(

    market_filter=marketfilter,

    max_order_exposure=50,

    max_trade_count=1,

    max_live_trade_count=1, 

    max_selection_exposure=50,

    conflate_ms=1500,

    name="greys_LAY",

    

)```

*Tags: Deployment, Strategies*

---

**liam** - *09:25:21*

`max_trade_count=1,` that is your problem

*Tags: General Technical*

---

**liam** - *09:28:05*

`greys_2 = MyModelLay(`

*Tags: Strategies*

---

**liam** - *09:30:13*

the logs would have given you a warning describing exactly the problem btw, did they not?

*Tags: General Technical*

---

**Unknown** - *09:37:05*

```if runner_context.invested or runner_context.live_trades:

   [http://logger.info|logger.info](http://logger.info|logger.info)(f"{runner_name} has Live Trades!")

   continue ```

this code avoids logging an error. 12:02:18,536:INFO:Gloze has Live Trades!

*Tags: Errors Debugging, Deployment*

---

**liam** - *09:37:34*

no the flumine logs will tell you everything

*Tags: General Technical*

---

**Terry** - *09:39:06*

ill set to DEBUG

*Tags: Errors Debugging*

---

**Terry** - *11:18:40*

```RunOrders = market.blotter.strategy_selection_orders(self, SEL, matched_only=True)

                            

if len(RunOrders) > 0:



    logger.info(f"{runner_name} has {len(RunOrders)} matched order")

    continue  # Skip placing an order if there is already an active investment or trade



else:

    ##Place Bet```

*Tags: Strategies*

---

## 2024-11-07

**ShaunW** - *15:43:45*

No consensus, depends if you're having one bet a week or one per second, your average price, strategy track record, personal attitude to risk, how deep your pockets are, and if you need a profit at the end of the day or end of the year.  Too many variables, strategy types and personality types to be prescriptive.

*Tags: Strategies*

---

**thambie1** - *15:49:58*

Kelly criterion is often used

*Tags: Strategies*

---

**Ankit Aggarwal** - *15:55:46*

If I am not wrong Kelly is for order sizing and not position management. Also for kelly how do you calculate probability of win from backtested results of each order?

*Tags: Strategies*

---

**thambie1** - *16:00:44*

There are many variations of kelly, you can modify it such that it accounts for existing positions when calculating the size of a new order.

*Tags: Strategies*

---

**Ankit Aggarwal** - *16:10:02*

Ok that's great could you help with what the variation is called?

*Tags: General Technical*

---

**thambie1** - *16:14:36*

There's no name. I believe [@UBS7QANF3](@UBS7QANF3)'s library does this: [https://github.com/mberk/kelly](https://github.com/mberk/kelly)

*Tags: Strategies*

---

## 2024-11-09

**Jonjonjon** - *21:23:39*

Does Flumine simulate that behaviour when backtesting?

*Tags: General Technical*

---

## 2024-11-10

**thambie1** - *02:20:00*

sounds like a question for a backtest

*Tags: General Technical*

---

## 2024-11-14

**liam** - *10:03:13*

ie the betfair betDelay or the latency bet delay? Also what is the context?

*Tags: Performance*

---

**tone** - *10:06:19*

I'm trying to determine the potential value in shaving milliseconds from latency but I figure it might be possible to get a rough idea by shortening the betDelay. Say my orders take about 1200 ms to hit the market. 1000 betDelay and 200 code + wire latency. I figure if I could alter betDelay to, say, 900ms that would give me a steer on what shaving 100ms of code latency would bring!

*Tags: Performance*

---

**liam** - *10:09:11*

So you have these available



[https://github.com/betcode-org/flumine/blob/a9cd71befc6062b52ce65ca695b50a56a2e81344/flumine/config.py#L26](https://github.com/betcode-org/flumine/blob/a9cd71befc6062b52ce65ca695b50a56a2e81344/flumine/config.py#L26)



You can be clever and set them to negative values up to the betDelay



```from flumine import config

config.place_latency = -0.5```

*Tags: Performance*

---

**liam** - *10:17:19*

btw `100ms of code latency` is absurd, i aim for sub 1ms

*Tags: Performance*

---

## 2024-11-15

**Jonjonjon** - *12:06:51*

Did you use Chatgpt to help?

*Tags: General Technical*

---

**Ralegh** - *15:33:59*

Working streamlit dashboard so I don’t have to paste lines into a python repl every time I check todays stats :)

*Tags: General Technical*

---

## 2024-11-23

**D C** - *17:29:30*

Yeah me too but that's just based on prior jobs. I did make some good old fashined pen and paper notes on some ideas I had about it. Might see if I can dig them up. Not that I'd ever be using it for a task like this but Is it easy in python to do things like bit twiddling?

*Tags: General Technical*

---

**Ralegh** - *19:42:31*

Thumbs up for parquet and the fixed number of levels with padding

*Tags: Errors Debugging*

---

**Jhonny** - *20:33:14*

I'm new to this, but I store live esa and historical data in xtdb. previously, it was just in an binary format, which meant I had to read the whole file to query the market. xtdb time slices are pretty much instant, and allows me to play with many markets at once. drawback is it takes quite the amount of disk space

*Tags: Getting Started, Data Quality, Deployment*

---

**Oliver** - *22:12:18*

not exactly what you asked, but I moved away from the stream JSON format in my engine. The input is of course still JSON as from the stream, but I parse it straight into a point-in-time representation (practically just an array) which is only allocated once, so avoids the creation of a whole bunch of hashes and lists that then get thrown away every update. It makes processing a whole load faster - more of a point1 thing. This applies to both back testing and real world when done like this. In theory a timeseries of the point-in-time version could be something to get fancy with in serialisation/deserialisation e.g. arrow for a more natural representation, or something smarter re. compression maybe along the lines of gorilla timeseries compression.



regarding lookups, I've been aiming to do pre-processing to extract metadata like the market definitions, to place alongside market recordings. It should works fairly nicely in the archive format you get from the historical data API i.e. tar containing files for individual markets + combined streams. At the moment the best I actually do re. lookups is just reading the first message of a market's recording to determine if it matches the market filter, but if not skip on.



Doing that with fairly speedy disk (NVMe), a memory mapped .tar of recordings, and reading in a separate thread, doesn't feel bad even with implementations that have a load to `TODO: simplify/speed up/profile` littered throughout them. It takes about ~2m to go through a day of UK WIN horse racing markets, picking them out from all UK horse racing markets for a day.

*Tags: Data Quality, Performance*

---

## 2024-11-24

**Mo** - *11:12:59*

Thanks for the replies guys, has definitely given me something to think about. While I can see the advantages to converting to a tabular format, what I really have in mind is something capable of producing the standard MarketBook dicts/objects that betfairlightweight, betfairutil, betfairviz, and flumine work with so it can slot into existing workflows with minimal effort

*Tags: General Technical*

---

**liam** - *13:10:38*

Do you think this would solve the problems you are trying to solve vs just improving the speed of processing the raw stream? ie. get the Rust wrapper up to a level it can be used

*Tags: Performance*

---

**Mo** - *14:22:50*

Some use cases I have:



1. Get the market books for all markets just before they turned in play

2. Get the market books corresponding to the times that my bets got filled

3. Extract the winner(s) of every market

4. Calculate the total volume traded for every market

5. Get the market books before and after X minutes before the market start time

6. Speed up flumine backtests

*Tags: Performance*

---

**Mo** - *14:31:55*

[@U03FS7KM2NL](@U03FS7KM2NL) sure, easy enough to do these when you know you want them. The problem is when you come up with a new requirement and need to process the market again

*Tags: General Technical*

---

**Oliver** - *15:05:13*

a potentially cheap experiment would be to add a caching layer that takes your back testing input (recording JSON) and then pickles it. e.g.

```USE_PICKLE_CACHE = bool(os.evnrion.get('USE_PICKLE_CACHE'))



def my_pickle_cache(inner_function):

    """Use a cache if enabled with a non-empty USE_PICKLE_CACHE value in the environment."""



    if not USE_PICKLE_CACHE:

        return inner_function

    @functools.wraps(inner_function)

    def cacher(filename):

        cache_path = f'{filename}.pkl'

        with open(cache_path, 'rb') as f:

            return pickle.load(f)

        result = inner_function(filename)

        with open(cache_path, 'wb') as f:

            pickle.dump(result, f, pickle.HIGHEST_PROTOCOL)

        return result

     return cacher



@my_pickle_cache

def read_normal_json_input(filename) -> list[dict]:

    ...```

then you cache once. If IO rate is the issue (probably a natural follow on to test), you could replace the `with open(...)` stuff with `with gzip.open(...)` and use something like level 4 compression, which I suspect is around the sweet spot for maximising throughput.



The pickle version is a variable that can have a bit of impact on its performance, so something to experiment with, just like compression level.

*Tags: Performance*

---

**Jhonny** - *15:11:00*

[@UBS7QANF3](@UBS7QANF3) I don't see why #6 isn't possible. I mentioned I don't do it simply because my stack is in clojure, not python. But, yh, I do have backtest speed and flexibility (I'm still working on this part tho). Maybe even have 2 storage formats optimized for different needs if needed.

*Tags: Performance*

---

**Alex A** - *23:32:58*

If you’re familiar with a systems language like Rust or C++, you can solve your requirements 1 through 5 by just having a pretty minimal program in a systems language that processes the streams files into an orderbook, and doesn’t have to integrate with flumine or anything.

*Tags: General Technical*

---

**Alex A** - *23:37:22*

My bot is in C++, and running on my laptop using 16 threads I can process six weeks of horse racing in 90 seconds and spit out the answers to your first five questions, so doing it for a year of data would only take about fifteen minutes.

*Tags: General Technical*

---

## 2024-11-25

**Paul** - *10:01:29*

Some immediate and random thoughts:



1. The problem with optimisation is that you end up going down the route of specialisation, in essence over-fitting a solution to what you need right now. It feels though, like once you step back a bit you're just saying "we should make backtests faster, and the ability to snapshot state and key metrics like total volume matched, much faster". Is that right?

2. There's a lot of thought that has gone into time series data storage from other industries, much of it open source, that might yield great results with minimal work. The downside of those stacks is that they introduce another dependency that might be horrid to deal with practically. Most of the uplift from that work is likely getting you away from dealing with files on disk, and building in-RAM data structures that yield well to the algorithms the creators were interested in. Stealing that idea while keeping your on-disk formats is worth considering (fewer dependencies, easier to iterate, more adaptable down the line as you're not re-writing files, just in-RAM data structures).

3. If you want Flumine backtest integration (your number 6), my instinct is improving the Rust wrapper and then extending it to serialise in-RAM into different optimal structures is probably easier in the _long_ run, although there is a lot of upfront work with little immediate upside before you're ready to do the interesting bit, so I don't blame anyone for ducking it. 

As Kent Beck once wrote:



&gt; For each desired change, make the change easy (warning: this may be hard), then make the easy change

*Tags: General Technical*

---

**liam** - *10:07:43*

&gt; The dirty secret of market impact? It's messier than anyone admits. This article challenges conventional models by looking at how information actually flows through markets. Here's the real problem: impact hits differently when you're getting in versus getting out of positions - a fundamental asymmetry that most models miss entirely. Add in the fact that every major player's algos are scrapping for the same liquidity, and traditional execution models fall apart.

[https://x.com/imotw2/status/1860802661832155549](https://x.com/imotw2/status/1860802661832155549)

*Tags: Strategies*

---

**Joe** - *11:15:00*

[@UBS7QANF3](@UBS7QANF3) To answer 1-5 I have an IO front end with a little dsl that can extract data from the compressed json stream files, fast enough like [2min per year of global racing, but this is largely because there is minimal impedance mismatch between IO and compute and memory in my implementation, it is using very fast local storage and lots of cores for parallel decompression, this is the same point that <@US2RWCWKY](2min per year of global racing, but this is largely because there is minimal impedance mismatch between IO and compute and memory in my implementation, it is using very fast local storage and lots of cores for parallel decompression, this is the same point that <@US2RWCWKY) was making.



To back test / train / ML that same IO front end is used but to fill memory structures whose contents are 'implied' by the strategy being trained and shape/order optimised for the hardware (e.g. gpu structures will be totally different shape to cpu). In other words it only keeps the answers to the questions it is likely to need during training and puts them in the memory location that will guarantee the lowest latency during training given the expected order those questions will be asked in. This is the data equivalent to dead code removal during compilation followed by compile time optimisation. The training process then only iterates over these memory structures at high frequency. To do this and ensure that the 'logic' being trained will behave identically when run in production on live streams requires everything be written against interfaces that are implemented multiple times. This is similar to what [@U012XF5CNPN](@U012XF5CNPN) alluded to in point 3. It could also be thought of as a very application specific cache.



This would be quite a departure from how Flumine works where there is no real change between live and back testing other than inserting the simulated matcher.

*Tags: Performance, Deployment, Strategies*

---

## 2024-11-26

**Unknown** - *10:36:10*

[@UBS7QANF3](@UBS7QANF3) I went this way to implement binary format for market data what gave me possibility to analyze data and backtest different strategies quicker than always go through processing raw json streaming data.

*Tags: General Technical*

---

**Unknown** - *10:40:35*

I do not know if that applies to your workflow, but I implemented scripting to my backtest/analyze app, allowing me on live data or on specifically selected market/s to make live coding and testing. You can see that on the screenshot above, that Console view where I can type code and execute it.

*Tags: Deployment*

---

## 2024-11-27

**Peter** - *16:20:52*

Hi, I have a question about simulated match rates using a flumine backtest.

I'm submitting back orders into a racing market pre-off, at the best available back price, with a stake smaller than currently available. The backtest suggests my matching rate is about  ~10% - why is this? I would expect a significant amount of these orders to be matched in full.

*Tags: General Technical*

---

**Peter** - *16:45:15*

No - I've solved it - I was making an embarrassing error but as always you have to ask the question before you see it

*Tags: Errors Debugging*

---

**birchy** - *16:48:55*

Yes, I often discover the answer while writing the question. I believe it's called the 'rubber duck effect' or something similar.

*Tags: General Technical*

---

## 2024-11-30

**Unknown** - *10:08:50*

When developing a new strategy, how do you decide when to bin it and move on to another idea versus keep grinding away at it?

*Tags: Strategies*

---

**Mo** - *10:20:51*

Related question: how many hours of work is it normally for you to go from idea to profitable live trading strategy?

*Tags: Deployment, Strategies*

---

**tone** - *10:30:02*

Well, that depends on how you define "fail". You have an expectation of performance prior to OOS, and, invariably, performance is poorer than expectation in OOS, but you might be prepared to optimise if performance is still tolerable.

*Tags: Performance*

---

**PeterLe** - *10:46:00*

Where there is no 100% right answer a gut feel always helps :smile: if you have that niggling feeling “this may just work” it’s worth pursuing 

*Tags: General Technical*

---

**Mo** - *10:48:07*

Second related question: what is your workflow for going from idea to live trading strategy? Mine is usually:



1. Get told about an idea ([@U012XF5CNPN](@U012XF5CNPN) leaking alpha) or reading about one (in a book that [@UPMUFSGCR](@UPMUFSGCR) has dismissed out of hand)

2. Validate idea by doing some basic analysis on summary data eg BSP data

3. Confirm idea looks good in a backtest, mainly looking at the equity curve (is it up and to the right, how do the drawdowns and other variance characteristics look)

4. Validate backtest by live trading small stakes and comparing live trading results to backtests on the same events

5. Scale up stakes in line with live trading performance



*Tags: Performance, Deployment, Strategies*

---

**Mo** - *10:52:05*

Original question inspired by an outstanding message [@UGV299K6H](@UGV299K6H) once wrote which sadly seems to have been lost to the mists of time

*Tags: General Technical*

---

**AndyL** - *13:02:14*

I do think a lot of my problem in finding new strategies is I am "blinded" by the methodology of my existing strategies

*Tags: General Technical*

---

**Jonjonjon** - *13:41:19*

Didn't someone post some analysis about benchmarking performance Vs prices at some point in the future? Have you tried that?

*Tags: Performance*

---

**liam** - *13:41:33*

It’s this part that has stuck with me



&gt; Even if you start with something simple and apparently useless like WOM you can just push and push until you either get to the point where it works or you learn enough to move your understanding on.

Which sort of answers Mo question in regards to what I do, keep pushing and then move on but building on what I have learnt works / doesn’t work 

*Tags: General Technical*

---

**Michael** - *14:43:27*

I'm glad that people appreciated the suggestions above, but I would add that the context was a conversation with a player who was yet to experience substantial success and seemed to just be flitting about pointlessly, a more experienced player might see things a little differently and I expect would have a pretty good intuitive idea of what's likely to work. If someone like [@UBS7QANF3](@UBS7QANF3) thinks an idea is a goer then I'll bet it is and it'll just be a question of finding the right strategy for developing it. In that regard I think [@UPMUFSGCR](@UPMUFSGCR)'s suggestion of just stepping away for a bit to shift one's perspective is a good one.



For my own part if I have a new idea I'll start by seeing if there's anything in it at all with some very simple test, like does it generate a series of back bets that would produce a profit if they all matched immediately at BATB - whatever's relevant. If I can't find any reason for hope within a couple of hours I'll semi-forget about it and just do a quick write up of what my thinking was and how I tested it so that I can read over it at some future point and maybe think differently about it.



If I do find that an idea has at least some potential then I'll work on realising if for as long as it maintains my interest, but I rarely spend all that long because I favour getting something up and betting and then digging into the results over a lot of back testing or anything like that. That's a matter of approach though, others will doubtless have their own ways of doing things.



Finally; If I have an idea that I think has legs I basically never give up on it,  if I can't get it working then I just write a break-even version of it, start it going with minimal stakes and forget about it for months on end, even years if I've really forgotten. Eventually I come back to it with fresh eyes and tonnes of data and try again.

*Tags: Strategies*

---

**Mo** - *14:51:26*

[@UPMUFSGCR](@UPMUFSGCR), [@UGV299K6H](@UGV299K6H) - writing up notes is something I don't do nearly enough of. It's so helpful for clarifying thoughts

*Tags: General Technical*

---

## 2024-12-02

**Paul** - *16:46:02*

This is a naive understanding based on some interviews their traders have done, it is likely wrong. But in essence, they have a similar problem to trading on the exchange they just _know for certain_ that the money is sharp. Exchange traders have to guess. I think it's safe to assume it probably is, because the alternative is to think you're smarter than every major syndicate on Earth.

*Tags: Strategies*

---

**Paul** - *16:51:21*

I'll come back to that in a post later this month, perhaps. It's a good question.

*Tags: General Technical*

---

## 2024-12-05

**Jonjonjon** - *14:19:50*

Is the Betconnect functionality in Flumine just for arbing?

*Tags: General Technical*

---

## 2024-12-16

**Unknown** - *08:25:11*

EV Middleware



Whilst I have it open a few flumine users might find this useful, it calculates an ev for each order using a future price (seconds) Allows you to visualise ev against signals and/or analysis on signals where data sample is small, for example not just relying on profit.



```import logging

from collections import defaultdict

from flumine.markets.middleware import Middleware



logger = logging.getLogger(__name__)



DEFAULT_SECONDS = 5





class EVMiddleware(Middleware):

    """

    Store all LPT and calculate ev based on `seconds`

    since order `execution_complete`

    """



    def __init__(self, seconds=DEFAULT_SECONDS, notes_name="ev", record=True):

        self.seconds = seconds

        self.notes_name = notes_name

        self.record = record



    def add_market(self, market) -&gt; None:

        market.context["ev"] = defaultdict()



    def __call__(self, market) -&gt; None:

        if self.record and market.blotter:

            ev = market.context["ev"]

            for runner in market.market_book.runners:

                if runner.selection_id not in ev:

                    ev[runner.selection_id] = []

                ev[runner.selection_id].append(

                    (market.market_book.publish_time, runner.last_price_traded)

                )



    def remove_market(self, market) -&gt; None:

        if "ev" in market.context:

            ev = market.context["ev"]

            for order in market.blotter:

                ev_data = ev[order.selection_id]

                price = order.average_price_matched

                pt = order.date_time_execution_complete

                if not price or not pt:

                    continue



                final_price = None

                for _pt, _price in ev_data:

                    pt_delta = (_pt - pt).total_seconds()

                    if pt_delta &gt;= self.seconds:

                        final_price = _price

                        break

                if final_price and order.side == "BACK":

                    order.notes[self.notes_name] = (1 / final_price) - (1 / price)

                elif final_price and order.side == "LAY":

                    order.notes[self.notes_name] = (1 / price) - (1 / final_price)```

*Tags: General Technical*

---

**Gooat** - *19:54:16*

I've got old markets where I have prices and my orders but can't recreate the strategy part.

*Tags: Strategies*

---

**liam** - *19:59:17*

Still don’t understand the question 

*Tags: General Technical*

---

**Gooat** - *20:30:32*

Broadly speaking could a middleware solution insert the bets into a market and create the trade objects? Or should I jump to Pandas?

*Tags: Feature Engineering*

---

**liam** - *20:31:49*

Right, you have the order placed date time right? I would have thought just processing the files through bflw and logging the price x seconds after would be the quickest / easiest way of doing this 

*Tags: General Technical*

---

**Gooat** - *20:36:29*

Cheers, I'll start with the clear question next time.

*Tags: General Technical*

---

## 2024-12-18

**birchy** - *19:20:16*

Brilliant! I've been using a much simpler affair and analysing in pandas afterwards:



`def process_orders(self, market, orders):

    if not orders: return

    ltps = {r.selection_id: r.last_price_traded for r in market.market_book.runners if r.last_price_traded}

    for o in orders:

        if not o.elapsed_seconds: continue

        # update ev min/max

        if (

            o.elapsed_seconds &gt; 0

            and o.elapsed_seconds &lt;= 5

        ):

            ltp = ltps.get(o.selection_id)

            if ltp:

                o.notes['ev_ltp_min'] = min(o.notes['ev_ltp_min'], ltp)

                o.notes['ev_ltp_max'] = max(o.notes['ev_ltp_max'], ltp)`

*Tags: Feature Engineering*

---

## 2024-12-19

**Ankit Aggarwal** - *17:23:34*

Noob question, whats the Y axis of the graph?

*Tags: General Technical*

---

## 2024-12-21

**Paul** - *18:13:47*

Another angle: let’s suppose you have a fundamental model that spits out probabilities you think are better than the market in certain situations. Calibration plots let you quickly eyeball if you’re right or not. If nothing else, the alpha is not letting your own BS doom you into a trap of a massive leak.

*Tags: Strategies*

---

**AndyL** - *18:15:31*

is anyone else finding GB racing (pre-race) too competitive now? What small edges I had in the past seem to have dissappeared, i've just stopped my last strategy

*Tags: Strategies*

---

**Jonjonjon** - *18:55:55*

Sorry to hear that [@U01PJ5YMFBJ](@U01PJ5YMFBJ). My GB model has had a bad week, but is still up overall for the year

*Tags: Strategies*

---

**liam** - *19:07:40*

What is the ev telling you? Bad run or edge has gone? Question for both looking at that graph…

*Tags: General Technical*

---

**AndyL** - *19:11:04*

i'm tempted to "rollback" to an older version of the strategy, i've done that in the past to good effect

*Tags: Strategies*

---

**AndyL** - *19:12:42*

that's a no-green strategy

*Tags: Strategies*

---

**Jonjonjon** - *19:18:14*

The thing is, if you don't hedge, your EV is just an estimate of the true EV. I've seen models that beat or underperform the EV to a significant degree (where BSP is used as the proxy for it).

*Tags: Strategies*

---

**Jonjonjon** - *19:31:33*

&gt; take the above EVvBSP chart, how would you go about weening out the -EV bets ?

If your strategy is anything like mine, you might be bleeding cash on bets above odds of 100.

*Tags: Strategies*

---

**AndyL** - *19:35:26*

Mine is betting sub-10.0 odds

It’s based on a pricing model built from traded volume flow 

*Tags: Strategies*

---

**Dave** - *22:19:48*

The EV Vs BSP chart isn't very useful, you know that generally bsp is close to 0 bsp so ofc you're gonna see loss if you don't beat bsp. Though it sounds like you're alluding to the fact that you're beating bsp less and less over the last few months. Sounds like either competition has stepped up/entered or your model become stale.

*Tags: Strategies*

---

**Jonjonjon** - *22:21:05*

It's a tough problem. Not sure if this is the answer, but doesn't the type of race change in September?

*Tags: General Technical*

---

## 2024-12-22

**PeterLe** - *11:55:38*

That's the crux of the problem isn't it; how to cut out the -ev by acting on a better signal. Sounds easy but not so! :grinning:

*Tags: General Technical*

---

**ShaunW** - *15:54:56*

BSP might be considered as a long term EV benchmark but unless you're betting them all then you're being selective, is that highly selective subset of horses also 0ev at bsp?



Closing/hedging at bsp might not always be optimal but at least it locks in a number rather than having the headache of working out if the EV of your bets is actually value or did they just beat the sp of the horses the strategy inadvertently selected.  Theoretically you could be backing horses at 2.5 that sp at 2.3, but those horses should have been 2.6 even though the set ALL horses at 2.3 are 2.3.  I think :thinking_face:

*Tags: Strategies*

---

## 2024-12-24

**Unknown** - *00:14:56*

Bizarrely, a totally different GB racing strategy, this on "in-running", exhibiting the same change June '24 :thinking_face:

*Tags: Strategies*

---

**foxwood** - *09:58:52*

Quite dramatic. Since you're offering I'd look for things that might have changed suddenly that could impact your signal(s). Latency, timing, server upgrade, software upgrade - faster/slower performance, cross matching are a few. Left field thought is that whoever you were feeding from pre-off went wholly inplay 6 months ago and affected you on both counts.

*Tags: Performance, Deployment*

---

**AndyL** - *10:00:46*

I think in reality it is just random as both strategies are TOTALLY different!

1: pre race , simple market making 

2: in running, complex pricing model

*Tags: Strategies*

---

**D C** - *11:59:58*

I thought you said this was a real strategy [@U01PJ5YMFBJ](@U01PJ5YMFBJ)?

*Tags: Strategies*

---

**ShaunW** - *14:43:46*

What's the EV chart look like for that in-running strategy, how much of that flatline is just the randomness of the won/lost outcome ?

*Tags: Strategies*

---

**AndyL** - *15:15:31*

My live returns for this strategy over the year match the backtest roughly 

*Tags: Deployment, Strategies*

---

**Rob** - *16:45:43*

If it helps, I've done pre-race for about 8-months since starting using flumine. I use ML models and have found that they last for about 2 months before degrading.



I will soon find out if this is seasonality, or just that the world keeps changing!

*Tags: Strategies*

---

**AndyL** - *17:11:23*

I need to grow a brain to work out how to find bigger edges, but im probably a bit old for that…!

*Tags: General Technical*

---

**Dave** - *18:46:39*

Edges can erode. It also depends on whether you've modelled your edge correctly - your approach may just be one that's not robust to time, and therefore needs some recalibration every so often (whereas if you understand it better, your model may be more time invariant)

*Tags: Strategies*

---

**liam** - *19:06:38*

But you can try and answer some of these questions, has your roi dropped? EV? Is it match rate? Less opportunities? Simulate with full match etc. 

*Tags: General Technical*

---

**Dave** - *19:11:09*

I am just trying to provide some evidence that, contrary to everyone telling [@U01PJ5YMFBJ](@U01PJ5YMFBJ) that he has no edge, that he may in fact have an edge but a) it doesn't stand the test of time because it just isn't modelled well enough and his features may just need to be more time-invariant, or b) simply he was relying on external behaviour which is no longer present. I noticed a step change in behaviour around June too, so I suspect a change in external factors :slightly_smiling_face:

*Tags: Feature Engineering, Strategies*

---

**AndyL** - *20:10:19*

[@U0128E7BEHW](@U0128E7BEHW) Your strategy exhibiting the same turn in June, is interesting. Looks like your's is more resilient (larger edge) than mine. Mine was very basic.

*Tags: Strategies*

---

## 2024-12-25

**Leo** - *12:56:37*

Interesting thread, my fundemental pre race jumps racing model is showing a remarkably similar pattern, although last month or so have been much better. I just put it down to time of year/ natural variance with that type of model. Maybe need to do some more digging myself. Anyway time to get off slack on Christmas Day  

*Tags: Strategies*

---

## 2024-12-26

**Unknown** - *21:55:50*

The Green vs NoGreen game, the same strategy, but one with the hedge bet removed, which one would you "ride" ?!

*Tags: Strategies*

---

## 2024-12-27

**liam** - *06:59:56*

When you create a strategy like this how focussed are you on the initial bet in terms of signal?

*Tags: Strategies*

---

**AndyL** - *09:14:31*

Finding a profitable "real" signal is hard, I spent most of this year trying to find one! hence why I pickup the pennies with the above strategy...

*Tags: Strategies*

---

**liam** - *10:28:40*

To answer your question, the straightness of that initial bet pnl is directly related to the ev. You can replicate this by only including bets if ev above x 

*Tags: General Technical*

---

**D C** - *14:06:22*

Does flumine not have facility to recreate virtual prices without requesting them? Sure I've seen people talk about doing this but perhaps this is within code that people like to keep closed source. If you can get access to such code you can see for yourself how it is calculated.

*Tags: General Technical*

---

**liam** - *14:53:54*

Was XM actually on for that market?



flumine doesn’t do any XM replication 

*Tags: General Technical*

---

**AndyL** - *15:27:04*

[@U4H19D1D2](@U4H19D1D2) If I play back a recorded XM market through Flumine, can it tell virtual bets, or does it just playback "raw"/real offers ?

*Tags: General Technical*

---

**liam** - *18:02:28*

flumine will playback whatever you give it 

*Tags: General Technical*

---

## 2025-01-02

**liam** - *21:16:10*

You are asking the wrong questions 

*Tags: General Technical*

---

**liam** - *21:28:19*

Simple is good, the problem is liquidity and adverse selection in the scenario you describe. You need a high enough edge / signal to be able to profit in inplay racing 

*Tags: General Technical*

---

**liam** - *21:41:33*

As in fixed £1 for all odds?



A 5f race will look very different to a NHF race in the closing stages 

*Tags: Errors Debugging*

---

**AndyL** - *21:41:58*

&gt;  As in fixed £1 for all odds?

i've tried both flat and liability

*Tags: Errors Debugging*

---

**AndyL** - *21:58:13*

no not quite, the 1s and 25% were arbitrary for posing the question above

the reality of my strategy, is yes i've got an aribtrary seconds, but not the 25% i've analysed various signals to indicate abnormal value position

but yeah, applied to all race types...!?!

*Tags: Strategies*

---

## 2025-01-04

**D C** - *14:50:48*

Interesting. I've just done a very basic DB profit analysis looking at averaged profit in bands of days 1-10,11-20,21-30 (ignored 31st for simplicity) and I see the opposite of what you are - best performance in the FINAL third of the month (on average) by quite some margin.

*Tags: Performance*

---

**AndyL** - *22:00:35*

Several things come up in strategy discussions that on thought seem to contradict a bit, 1) keeping it simple and 2) making it profitable. As from my experience in trying to successfully implement an idea I end up applying "filters" and many "if's" which inherently makes it complex and usually succumbs to being "fitted". I usually end up with the later because my "simple" idea doesn't quite work...! So I then invariably try and make it more complex (i.e. not simple!) :thinking_face:

*Tags: Strategies*

---

**thambie1** - *22:28:28*

Adding a ton of filters, usually indicates a lack of understanding. You keep adding filter after filter to try and get it profitable, but what's the narrative? Why do those filters make sense? If a filter you thought would help doesn't, why not? What belief did you have about the strategy/dataset that turned out to be untrue?

*Tags: Strategies*

---

**AndyL** - *22:38:05*

I see where you're coming from, and I definitely do that a lot..... I typically convince myself altering a filter make sense in some way!

I feel my main weakness is a lack of ingenuity in coming up with a good idea. The one i'm currently working is creating a price model based on the traded volume back/lay on a selection over recent timeframe, constructing a "book" based on that volume. Now using that model directly seemed useless, as sometimes the model vs actual price was wildly wrong, so I thought maybe I could use it's "gradient" as an indicator, eg. actual price goes up over 10seconds, model price stable/down, then "back" bet... and from there I created filters to make that work over my 3 month sample, eg.seems to work well for price &gt; 8.0 ! ..... and then go to a 1 year backtest and find it's crap!!

*Tags: Strategies*

---

**thambie1** - *22:49:12*

What's the underlying reason why you think a price model based on traded volume will work? How can you confirm or falsify that reason? I know[@UGV299K6H](@UGV299K6H) talked about this idea before, if that's all you're going off of and you haven't found any evidence confirming it as a interesting approach you likely won't get far.

*Tags: Strategies*

---

**AndyL** - *22:52:16*

so what else is there to go on for a market based strategy? as a "market" is basically people trading "volume"?? Be that volume happening in terms of it being "matched", "queued", or "cancelled" ?

*Tags: Strategies*

---

## 2025-01-05

**Mo** - *09:33:16*

&gt; sometimes the model vs actual price was wildly wrong

Working out why this is the case sounds like the key to your success

*Tags: Strategies*

---

**AndyL** - *09:43:37*

Thanks Mo, yeah see what you mean about the model, could be tricky that bit

*Tags: Strategies*

---

**AndyL** - *09:44:19*

I foresee adding extra params/filters to better model

*Tags: Strategies*

---

**AndyL** - *10:07:20*

My thoughts on why the model is not always accurate is thinking about it I wouldn’t expect just basing on a combination of traded / queued volume to model well on its own, which then makes me think is it useful at all…. ? Hence why I tried using gradients thinking maybe that was being modeled better 

*Tags: Strategies*

---

**Mo** - *11:42:29*

I don't think you're thinking about this in the right way. Your thought process seems to have been:



1. Model isn't accurate

2. Try something else

Rather than thinking about _why_ it isn't accurate. What are the specific circumstances where it most significantly diverges from the market?

*Tags: Strategies*

---

**Unknown** - *12:46:04*

As promised, here is the first update on my yearly goal for my new strategies to equal or exceed my take home profit from existing strategies



The first attached plot shows cumulative net profit. Some comments:



1. The existing strategies have very high costs associated with them. Therefore I am using 37.5% of gross profit as net profit

2. In contrast, the new strategies have very low costs associated with them, therefore I am using 59% of gross profit where, hopefully obviously, this is almost entirely the 40% premium char - ahem, expert fee

3. The new strategies have made approximately 0.4% of the net profit of the existing strategies to date. LFG!

4. Since 2nd January I'm ahead of my syndicate share so haven't _personally_ been trading the existing strategies hence the flat line

The second attached plot shows the cumulative net profit only for the new strategies. Some comments:



1. As you can see, they had a bit of a rough time yesterday

2. I was planning on increasing the stakes but this has given me some pause. Note that I'm just being a pussy, the correct thing is still to increase the stakes. It's arguably sensible to wait until Monday anyway to minimise the impact on live trading from restarts

The third attached plot shows the relative profit for the new strategies. Some comments:



1. `Strategy2` got deployed yesterday and only had two bets, one of which accounts for almost all of the loss for that strategy. The price was fine (according to BSP) but the staking was a bit too high for a newly deployed strategy. Lesson learned

2. `Strategy1` is a generic strategy that I'm keen to also deploy to football but it looks like this will require some elbow grease as I'll need to optimise some market (sport) dependent parameters

3. `Strategy0` trades all horse races regardless of country; `Strategy1` and `Strategy2` only do GB racing for now. I'd like to expand them to other countries but that's fairly low down the priority list

My focus for now will be `Strategy2`:



• `Strategy2` bets into markets that have multiple winners; think forecasts/exactas/trifectas/superfectas/quinellas etc.

• I'm doing some foundational work to both my shin ([https://github.com/mberk/shin](https://github.com/mberk/shin)) and kelly ([https://github.com/mberk/kelly](https://github.com/mberk/kelly)) packages to allow for markets with more than 1 winner. Everyone will be able to take advantage of these

• `Strategy2` currently only bets on the favourite. Being able to bet on all runners is the next step

Some general comments:



1. Knowing that I "had to" share this update was excellent motivation, as hoped. Without it, I might have dragged my heels over getting `Strategy2` deployed

2. Hopefully you will agree I've been remarkably open with this update but will also understand why certain details have been held back. You're welcome to ask my anything at all - whether I can give a straight forward answer is another matter. If you know some of the details from things I've previously told you in confidence then obviously please refrain from including these in your questions. If in doubt you can always DM me

3. My overall philosophy with these strategies is this: I'm hoovering up pennies. Hopefully there will be enough pennies that they will add up to something meaningful. There are inefficiencies every-fucking-where. Not necessarily massively profitable inefficiencies but it is free money being left on the table and the more of these I can hit as soon as possible the better my money printing machine will become. Having chunky profitable strategies to begin with helps as this encourages doing more volume

Fire away...

*Tags: Deployment, Strategies*

---

**Dave** - *13:31:51*

Complexity is not necessarily a bad thing (for instance if you use a tree or NN model then likely the decisions they make are far more complex than your handcrafted ifs). However typically the relationship between features and target is observable/make practical sense, so even if your logic is mostly a black box / super complicated, a simple regression of feature Vs target would suggest "yes it makes sense to include this feature/indicator". But if you don't practice good hygiene with train/test splitting / avoiding leakage etc, or simply don't use enough data, then you will have a useless model. And ofc this isn't specific to ML models, despite the terminology. Stuff like "it only works on odds of 8 and above" is _likely_ overfitting.

*Tags: Feature Engineering, Strategies*

---

**AndyL** - *15:48:19*

Nice one Mo, My other post is inrunning horses model, you’re giving me some added motivation now, although I’m not running live!!

*Tags: Deployment, Strategies*

---

**Dave** - *16:00:30*

AHH don't get me wrong - NNs won't solve your problem. Robust hypothesis testing + hygiene with out-of-sample testing is where you'll find patterns that are real. And your hypothesis testing can be done either via simple regression, or fitting something more complex like trees / NNs.

*Tags: General Technical*

---

**D C** - *17:26:17*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) just a suggestion, but have you thought of trying some fundamental modelling instead of purely market signal type approaches. Something pretty basic just to give you a better idea of model building and regression? Your talk of just throwing shitloads of features into a NN suggests that you are hoping to just stumble across something.

*Tags: Feature Engineering, Strategies*

---

## 2025-01-06

**Mo** - *11:58:23*

[@U012XF5CNPN](@U012XF5CNPN) - yes, adverse selection is the big problem. No one is forced to take your offers so when you do get matched it's much more likely because you're wrong. In addition to that, making prices on Betfair is a really shit deal:



1. No maker/taker fee structure unlike most (virtually every?) other financial exchanges

2. No Last Look mechanism unlike being a bookmaker/some trading venues

3. No idea who your counterparty is unlike being a bookmaker/some trading venues

I wonder if the big market makers do have one or more of these although I've never heard anything to that effect

*Tags: Strategies*

---

**Paul** - *16:51:35*

I can think of some cliff edges that are worth thinking about when sizing: to deal with minimum stake/payout/liability pieces (which I do plenty of as I'm market making, but might just be me leaking alpha here). I stumbled into a three figure mistake the other week by allowing a MARKET_ON_CLOSE to actually lapse because parts of the order chipped away dropping the liability &lt; £10... and even if I coded up to spot that and move it to PERSIST, it often never matched... and then left me exposed elsewhere... repeat that enough and it doesn't half add up :money_with_wings:



I mention it in this context because let's suppose you are sizing to a payout, but the minimum liability (not payout), to enter BSP auction is £10. If you're near minimums for either (say £10), you've effectively built a strategy around a price inflection point. £20 payout at 2.0 has a £10 liability so if you hedge to auction that will work :white_check_mark: . At 1.99 it doesn't, and will LAPSE :no_entry: So, there you go, an actual cliff edge based on a price.

*Tags: Strategies*

---

**lorenzo** - *18:41:34*

&gt; equity curve all day long...

Does this mean you backtest in flumine every day and compare it to make sure it's the same? And then if it is you are confident that it will behave like the rest of your backtest?

*Tags: General Technical*

---

**Mo** - *18:45:27*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) yeah but it’s not just about EROI, the other question is what is the capacity. Probably not very much for these strategies

*Tags: General Technical*

---

**Mo** - *18:46:42*

[@U05REBJ21EC](@U05REBJ21EC) misinterpreted your original question but yeah I would backtest the events that were live traded and compare bets placed between the two. Or just not give a shit and wait for the results to play out

*Tags: Deployment*

---

**lorenzo** - *18:50:47*

well I'll keep the questions coming (hopefully other people get something of them to): When you say 'not very much capacity for these' do you/others have any good literature / approaches on modelling this?

*Tags: Strategies*

---

**birchy** - *19:08:32*

Hopefully this will spark up some much needed discussion and [@U012XF5CNPN](@U012XF5CNPN) dripping some more alpha. [@UBS7QANF3](@UBS7QANF3)  Just as a general question, do you use models or more of a statistical approach/`if EV &gt; 0: get_stuck_in()`?

*Tags: Strategies*

---

**Mo** - *19:10:21*

Strategies 0 and 1 have no fair probability, they will just bet if certain conditions are met. Strategy2 has a fair probability/model

*Tags: Strategies*

---

**birchy** - *19:14:01*

Brilliant. I'm mostly operating strategy 0 &amp; 1 style based on market state but have had moderate success with xgboost/lgbm regressions. Like you, I have a couple of WIP's which are scooping pennies but I'm finding them troublesome to scale up.

*Tags: Strategies*

---

**Dave** - *19:24:04*

Suspect you'll find your strategies 0/1 to have higher capacity but lower ROI than your strategy 2

*Tags: Strategies*

---

**liam** - *21:32:35*

Some great articles on this blog (relatively new)



[https://www.sharpsresearch.com/blog/ai-is-easy/|AI is Easy, Spend Your Time on Your Dataset, not Your Model](https://www.sharpsresearch.com/blog/ai-is-easy/|AI is Easy, Spend Your Time on Your Dataset, not Your Model)



[https://www.sharpsresearch.com/blog/time/|Time, A Note on Market Inefficiency and the Future of Sports Betting](https://www.sharpsresearch.com/blog/time/|Time, A Note on Market Inefficiency and the Future of Sports Betting) 

*Tags: Strategies*

---

**Mo** - *22:28:25*

[@U0128E7BEHW](@U0128E7BEHW) - assume you mean (mean daily pnl / std daily pnl) * sqrt(365)?



• Strategy0: 1.63. However, it looks to me that the strategy only _started_ working in 2021. Sharpe ratio since 2021 is 2.47

• Strategy1 Pre-event: 2.97

• Strategy1 In-play: 3.89

• Strategy2: 3.05

My guess for undisclosed reasons is Strategy0 has the lowest capacity followed by Strategy2 followed by Strategy1

*Tags: Strategies*

---

**Jonjonjon** - *22:42:48*

When those talk about time series, do you think they are referring to some unobservable latent measure of team performance, or time series of prices on a runner?

*Tags: Performance*

---

## 2025-01-07

**AndyL** - *06:57:55*

It’s mind boggling to me that you have a strategy that works both pre and in-play :thinking_face: !

*Tags: Strategies*

---

**Jonjonjon** - *11:22:30*

Interesting.... Have you had much luck incorporating stuff like that into your price action models?



I tried adding stuff like hour of day, day of week, recent performance Vs other runners,  but didn't really get anything worthwhile from it.

*Tags: Performance, Strategies*

---

**Paul** - *18:11:02*

The problem is we think "horse racing" or "cricket" or "greyhounds" and smooth out. We might spot obvious differences like country or clear liquidity changes, but there absolutely 100% is a different set of people on the other side of the screen on different days and different hours of the day, and that manifests in how the market moves and develops.

*Tags: General Technical*

---

**D C** - *18:27:00*

Not sure I would know how to show BSP was more or less accurate over a time range or a day of the week. Either you have faith in it as a metric or not though surely? Same with those who consider Pinnacle closing lines (Buchdahl) as the "benchmark". It's either usable or not surely? If we get down the route of "BSP is inaccurate on Monday afternoons for dog racing" what kind of can of worms does that open for assessing EV?



I should probably say that I only really use profit as my measure of EV and don't really concern myself with estimating it (other than as deviation of current prices from my models) so I am probably not the best person to be discussing it.

*Tags: Strategies*

---

**Paul** - *18:33:14*

The question then if that's a useful data point: is it actionable? Or is it just something that happens, and if you try and take action you're over-fitting? Depends on the strat, obvs

*Tags: General Technical*

---

**D C** - *18:33:53*

OK maybe seeing a concrete analysis will help me get to grips with this. Just seems odd to me - we are talking about things are individually unknowable and unmeasurable. And all things have some degree of natural variation. I mean fair play to you if you can find some way of doing the numbers and using the results to your advantage.

*Tags: General Technical*

---

**Jonjonjon** - *19:17:50*

Is prize money in the streaming data? As a mere mortal part-timer my data is limited to what I've recorded.

*Tags: General Technical*

---

**liam** - *19:23:49*

I am going to go out on a limb here and disagree, you would be trying to find a signal in a very noisy dataset and even if you do find one it will be weak, very weak.



The sharper the crowd / the sharper the market, yes, however that doesn't mean your models should be any different. The only variable that does get impacted by this is execution as adverse selection becomes a real problem.

*Tags: Strategies*

---

**Mo** - *19:46:26*

[@UUE6E1LA1](@UUE6E1LA1) just sounds like a simple mixed effects model/analysis of variance situation where day of week is a factor

*Tags: Strategies*

---

## 2025-01-08

**D C** - *08:37:10*

Yeah I don't think anyone would suggest that SP is accurate for every runner all of the time, especially in horse racing which is not entirely "clean" anyway. But we can never know if any single SP is accurate or not - it's not like detecting a biased coin or a weighted die or a dodgy roulette wheel - we have no reference model to compare with. I suppose people with strong fundamental models for racing might be able to sniff out a dodgy runner based on strong market move and large deviation from "expected" implied probability.

But even if we could show that BSP is "wrong" in certain situations how would it benefit? If you have a 6 runner race, the favourite price could be spot on (say 2.0) and yet the BSP of all other runners in that race could be off. Similarly, 2 prices could be way off and yet all others could be spot on. At best all we could ever really show is that BSP (or SPs with margin removed - whatever estimator) is inaccurate in certain situations. But we kind of know that anyway don't we??

*Tags: Strategies*

---

**Paul** - *09:05:26*

I think we might be getting into arguments about arguments here.



1. Do you believe that the quality of events and the intentions of competitors in every event is equal? If yes, crack on, nobody here will convince you of anything, no matter how much data we provide

2. If you believe quality and intent varies, do you believe that in pre-event betting, these differences might manifest themselves in terms of betting counter parties, their intentions and actions? If you think bettors are all robots who do the same thing over and over again, fine, move on, nothing to see here.

3. If you do believe it might change behaviour, the question then become how do you qualify and quantify those changes. My hypothesis is we should see differences in behaviour (liquidity, volatility, BSP calibration, and so on), that align variation in quality. In horse racing I believe quality significantly varies from mid-week betting (outside of major festivals), to weekend betting. I believe AW and Turf racing are in essence different sports with different fundamentals, different trainers, jockeys, horses, breeding lines, _everything_, that I should expect to see differences in pre-event betting behaviour, and by looking at the data and filtering for day of week, day vs evening, turf vs AW, handicap vs stakes, at different grades, and so on, I will see variations that might be strategy impacting unless I account for them

*Tags: Strategies*

---

**D C** - *09:41:17*

At this point everything is belief and opinion. I do not believe that BSP is accurate all of the time. I do believe it is accurate on the larger scale and is "good enough" for certain things (such as _*estimating*_ the EV of a pre-off strategy). I know that Newtonian mechanics is not "correct" - but it's good enough for most things we do in life.



I also believe that there is enough smart money in the market to ensure that BSP (or closing prices in general) will likely never be "miles off" long term but it can/will be in certain conditions. If said conditions were easily identifiable, I believe that that same smart money would likely be on it in a flash and would be exploiting it until it no longer existed. The "pothole in the road would have been filled"  - we might be able to detect it's existence historlcally but for now it has "gone". And if we have detected that it once existed, have we truly identified something beneficial (especially if we start with the belief that BSP is not accurate all of the time anyway)?



Totally believe that AW and turf are different - as are flat and jumps and my own models reflect this. Totally believe that market behaviour and participants can be different at different times of day/days of week/class or racing/division/league/formats or any other characteristic of any sport. But I also believe in CLV in the broad sense and that if there were some simple exploitable loophole in BSP, then smart players would close it quickly by exploiting it.



This stuff makes for an interesting chat - and likely would make for an interesting Phd or Masters thesis. I look forward to seeing your numbers/analysis as it is an interesting topic and has provided some interesting discourse but (as you may have noted) I have taken great pains to apply the word "believe" to the above - which are nothing more than my opinions.

I am afraid that I don't have the time to come up with evidence to support these beliefs/opinions. I might have the inclination to do so if I could see that it would be of use in increasing profits. That's likely a shortcoming on my part. But I'll happily tip my hat to anyone who is able to (or already does) exploit this for financial gain!!!

*Tags: Strategies*

---

**Dave** - *19:16:01*

Unlikely it affects many here though. If your model sucks then time of day won't help you.

*Tags: Strategies*

---

**liam** - *19:17:33*

Agreed, you can focus on time of day / moon cycle etc once you have a decent model 

*Tags: Strategies*

---

## 2025-01-09

**Paul** - *08:57:14*

When handling pre-event, I am convinced that some models are decent in some market conditions, and not in others, and those market conditions correlate to quality of event, which correlates to day of week and time of day in horse racing.

*Tags: Strategies*

---

**Paul** - *08:59:21*

I'm not saying the moon affects racing, or some celestial calendar is at play where the gods decide that variance will differ because today's day starts with an S. I'm saying that racing on a Saturday is a different sport to racing on a Tuesday, in the same way that EPL is different to 2nd division French football, and that the markets reflect that change because the entire industry operates that market differently through necessity. If you're trading pre-event, not factoring that in could mean you have a decent model that works over _there_ but does not work _here._

*Tags: Strategies*

---

**Jonjonjon** - *09:20:32*

I totally agree with that. However, when I tested adding time/date features to my model they didn't improve it:disappointed: . So while we can easily observe that volumes vary by hour and day of week, I haven't been able to use it for anything useful.:disappointed:

*Tags: Feature Engineering, Strategies*

---

**D C** - *09:37:37*

To be fair, my main disagreement with things you said is isolated to this statement:



_"Do you think BSP is uniformaly fair across all days of the week and afternoon vs evening racing? Last time I looked (a long time ago), I was not convinced it was. Before I go into that rabbit hole, I think I need to break out Jupyter this weekend and show my working, perhaps."_



Not sure anyone is saying that markets don't vary from day to day - weekend volumes usually higher on Saturdays than weekdays for racing - but that would not be the case for a Royal Ascot Tuesday or a Chelthenham Thursday. But I can see no reason why a fundamental model based purely on historical team/player/participant data used to predict outcome probabilties would behave any differently on any given day of the week than another. I could see that perhaps external factors might make it appear that way on occasion - EPL game on a Monday night involving a "fresh" team and a team with heavy euro/cup schedule might throw up an "upset" - but I'd expect a decent fundamental model to incorporate player fatigue / overloaded schedule in some way.  Was the "upset" caused by the fatigue? Or because it was played on a Monday? Or just natural variation?

*Tags: Strategies*

---

**Paul** - *10:02:11*

OK, now we're getting into interesting hot takes. Yes, I think fundamentals vary based on external factors (several advent of alpha ideas circle around that, a little laboriously, but hey 25 ideas is hard...), and yes a fundamental model should take that into account. What I'm saying is that your _counterparties_ change. How sharp/soft the market is, changes. Most of the money is originally sourced from recreational punters who have habits and biases and tendencies about how they bet and that markets therefore reflect that.

*Tags: Strategies*

---

**Paul** - *10:08:07*

Put another way:

1. Why is Saturday volume or major festival volume different? Different set of punters, different betting styles, different habits, different takes, and so on. :white_check_mark: 

2. I think that and other factors around quality, directly influence volatility, liquidity, usage of BSP (and usage determines its accuracy), and other metrics. Let's call that, for lack of actual analysis in hand right now, an open question. :question: I want to try and do some of that analysis and share it.

3. I think for pre-event models, including those that are using fundamentals, that _might_ affect strategy outcomes in some cases. Again open question :question: - not sure we can answer this, but perhaps there can be a way to look at a strategy based on those factors once #2 is better understood to help understand it for each of our own models. Let's call that more an open brain fart we all need to figure out for ourselves :dash: 

*Tags: Strategies*

---

**D C** - *11:35:13*

Reminds me of a bloke on twitter I used to talk to. Now I don't actually believe this story but IIRC said he'd made about 750k over lifetime and was part of a group of other players who used to bribe hotel staff at hotels that football players stayed in to provide a list of players who turned up to stay for a scheduled match so that they could bet if a critical player was missing.

It doesn't sound right to me, as I thought teams just coached down to the opposition when they play away (but I could be completely wrong on that and it could all be completely true). The problem I have is that my brain can only really come up with this kind of "idea" when it comes to "thinking outside the box". I try to change the way I think but it's easier said than done!!

*Tags: General Technical*

---

## 2025-01-10

**Unknown** - *11:24:01*

I haven't really settled on a definitive schedule for publishing these but I figured I'd probably be too busy this weekend so I'm getting this week's update in early



It's been a hard and frustrating week grinding away on `Strategy2`



First up, as mentioned in my [https://betcode-org.slack.com/archives/CTPL3R3FU/p1736081164005719|last post](https://betcode-org.slack.com/archives/CTPL3R3FU/p1736081164005719|last post), I increased stakes on `Strategy0` and `Strategy1` on Monday (2025-01-06). My plan is for these to increase in line with the notional bankroll I have set aside for this collection of personal strategies, and for this bankroll to grow organically. If I find a strategy that I'm extremely bullish on and think it could benefit from an injection of capital then I will obviously do so but that's not (yet?) the case for `Strategy0` or `Strategy1`.



So, `Strategy2`. I've been alternating between thinking I've cracked it and thinking all I have is a horribly overfit hot mess. I started off very quickly making progress in overhauling the model and staking which:



1. Permitted trading on all runners, not just the favourite

2. Widened the number of markets that could potentially be traded

3. Takes current position into account, hopefully resulting in smoother returns

The progress was such that on Wednesday (2025-01-08), I switched off "V1" and deployed "V2". I observed the bets for V2 in a few races and noticed it wasn't doing a great job of beating BSP. Small sample size obviously but I decided to switch it off and dig into the backtests a bit more



First up I confirmed in the backtest that bets which beat BSP do great and bets which don't beat BSP are bad. So it's not like my model is so good that it can identify times BSP is wrong (at least in aggregate)



This strategy and, to a lesser extent, `Strategy1`, has a variety of filters applied to the trading signals. Things like requiring a minimum expected ROI (EROI) to bet, but also not exceeding a maximum EROI. Also some traded volume and other market "quality" indicators like overround/spread in an attempt to restrict to situations where confidence in the model predictions is likely to be highest



Initially I had optimised these filters using backtest realised returns but based on my increased understanding of the importance of beating BSP for this strategy, I went back and included expected value calculations in the optimisation process. This suggested that I'd overfit many of these filters and so I went back and re-optimised them using expected value with the result that several of the filters were discarded in their entirety. The backtest performance although arguably now more likely to reflect future returns looked significantly less impressive by the end of this process



For both backtesting and the live trading that's been done with the strategy, such that it is, I've been focusing on UK horse racing. However, I'm very keen to get it working in other jurisdictions and also on greyhounds. For obvious reasons:



1. The more markets it can trade, the quicker live feedback can be had

2. More markets traded = smoother returns

3. If it can widely scaled then the per bet performance doesn't need to be amazing

I do have some problems with lack of data, however:



1. I haven't been scraping greyhound markets for very long

2. I only realised this week I haven't been scraping horse racing `OTHER_PLACE` markets (now corrected)

3. No doubt there are other market types I'm still not scraping that I am ultimately interested in (e.g. Each Way, AvB, Without favourite, all exactas/quinellas/forecasts/reverse forecasts/perfectas/superfectas/trifectas/etc. and all aliases thereof)

This lack of market data to backtest against is one reason why I'm so keen to nail down the model building process and to rely less heavily on the market based filters. If I can be confident that I'm building a market beating model in cases where I do have the market data, then I am more inclined to cautiously deploy the model to other markets and rely on live results for feedback



I mentioned in my last update that as part of this strategy I wanted to look at extending both Shin's method and Kelly staking to handle markets with multiple winners. I've had some generic insights into these that I think people may find useful:



1. The R package [https://cran.r-project.org/web/packages/implied/index.html|implied](https://cran.r-project.org/web/packages/implied/index.html|implied) already implements Shin's method for multiple winners. However, it simply finds a value of `z` numerically using the existing formula. I don't believe this is correct as the starting assumptions under which Shin derived his model are not applicable for multiple winners. Nevertheless, this approach may be practically useful if not conceptually sound

2. A recently developed alternative to Shin is [https://github.com/gotoConversion/goto_conversion|GOTO (Gambling Odds To Outcome) Conversion](https://github.com/gotoConversion/goto_conversion|GOTO (Gambling Odds To Outcome) Conversion). This method apparently has serious chops, having been used in several gold medal winning solutions on Kaggle. It appears to be able to handle cases with multiple winners trivially. However, I observed some issues using the method in practice. Firstly, the code does not seem to be very well written, requiring me to fork the repo and make some changes. Secondly, the method produced negative probabilities for the BSP on a completely unremarkable horse racing WIN market. Also worth noting that most if not all of the motivating analysis supporting the method focuses on markets with few winners like football and therefore there's much weaker evidence that it would outperform Shin in horse racing. The repo is not configured for me to create a GitHub issue to discuss these concerns with the author so I have reached out to him on LinkedIn but have yet to receive a response

3. ChatGPT, o1 specifically, has been extremely useful for strategic discussions about developing `Strategy2`. In my opinion, a product like it is becoming increasingly essential if you work by yourself

4. I'm no longer planning on extending Kelly staking to handle market with multiple winners but while looking at it I noticed that the way I've always done fractional Kelly staking is wrong. This has implications for my existing strategies and it's gratifying that looking at new markets in new ways can lead to widely applicable fresh insights in this way

Update on performance with reference to the attached plots:



1. I got rotated back into the syndicate strategies for a day, made a load of money and got rotated out

2. New strategies took a few days to dig themselves out of the hole but they've done it

3. Looks like `Strategy1` in play is really carrying water

New strategies net profit up to 0.77% (from 0.4%) of net profit of existing strategies so everything is moving in the right direction!



Plan for the coming week is to continue to grind away on `Strategy2`, mainly focusing on the model, and hopefully getting it to the point where it can be redeployed to UK racing. Next step after that will be getting it deployed to Australian racing then US racing then perhaps rest of world if it seems worthwhile, followed by greyhounds. I haven't thought about what's next after `Strategy2` but last count I have ideas for 11 other strategies on my list. Other syndicate members will be back to work on Monday so I'll be back to working on existing strategies next week as well

*Tags: Performance, Deployment, Strategies*

---

**Mo** - *12:23:29*

Answer to my question appears to be: yes

*Tags: General Technical*

---

**Alex A** - *12:27:43*

So, does the syndicate work by basically each day whichever of you has the least running profit runs your strategy in their personal account and the others chill?

*Tags: Strategies*

---

**Leo** - *12:43:36*

Are these all price action based strategies/models ? 

*Tags: Strategies*

---

**birchy** - *16:59:57*

Tired, yes. Retired...not yet. :grinning: 

To clarify, 70% of the bets I got _matched_ beat BSP, but that was obviously only when the BSP was wrong. And of course BSP rarely sums to exactly 1.0, hence the question regarding normalisation. So yeah, I suspected that simply 'beating BSP' wasn't your strategy. :wink:

*Tags: Strategies*

---

## 2025-01-11

**Unknown** - *08:01:58*

Is the concept that beating BSP = Profit based on the assumption that the BSP in question is actually performing as expected. I`m struggling to see how a Back bet beating BSP 2.3 would work if the underlying trend is that it loses more than expected. Same if you were beating BSP 5.5 as it wins more than expected.

*Tags: General Technical*

---

**Mo** - *08:09:38*

1. How are you working out "expected"?

2. There are 350 prices on the Betfair price ladder. You should expect some of the outcomes to be below and some to be above expectation, especially given limited sample sizes

3. To reiterate, I am sure that strategies can be profitable _without_ beating BSP. All I am saying is that I have observed for this particular strategy that it's an important indicator of bet value

*Tags: Strategies*

---

**Jonjonjon** - *09:23:12*

[@UBS7QANF3](@UBS7QANF3) do you know how your syndicate's overall performance would be affected, if on a particular day two of you traded, instead of just one?

*Tags: Performance*

---

**Jonjonjon** - *11:20:56*

[@UBS7QANF3](@UBS7QANF3) the reason for my question is that if 2 run it and you make double, then you could scale more. I know it's unlikely, which leads me to ask how much people settled on my current staking?

For my staking I tend to use your A/B code.

*Tags: General Technical*

---

**Jonjonjon** - *11:44:21*

Yes we could. Is there a reason your stakes aren't double what they are? For me, it's because my performance declines.

*Tags: Performance*

---

## 2025-01-12

**Mo** - *13:29:11*

Very different I’d say. The syndicate strategies are highly scalable and less focused on the strength of the predictions and more on the engineering of the overall system. Currently the personal strategies are focused on smaller scale market inefficiencies where the modelling is much more important

*Tags: Strategies*

---

## 2025-01-14

**Unknown** - *15:32:15*

Apologies in advance to anyone I've already asked about this...

I've been working on a model for GB dogs. No price data or market signals involved at all - it is all fundamental modelling using data freely available to anyone. I'm usually of the opinion that models from such data are going to be worthless but I decided to give it a go anyway.

I've come up with a model that I believe has squeezed as much as it can from the data available. Model summary as follows:

1. Model built to generate win probabilities for dogs ONLY for A grade races

2. Model built using data from 2018 to 2023 (inclusive) using classical statistical modelling only

I've taken the model parameters and using 2024 data have generated some rough numbers to see how well calibrated it is. I've also run a quick and dirty backtest by approximating profit backing to BSP whenever BSP is greated than my model's 0EV price (I realise this step may well be pointless). For what it is worth, this test shows tiny profit after commission.



Looking at the numbers (attached) it looks like the model is reasonably well calibrated - I do have some concerns that it lacks the power to predict high probabilies for runners as the vast majority of fitted prbabilities are 0.4 or under. This might be OK though as these are graded races where each dog is supposed to be roughly the same current standard.



My questions are as follows:



1. Is the model shite and can/should it be abandoned now based on attached numbers

2. Assuming no to 1. what is a "good" next step to take (e.g. full backtest using highest price traded, full backtest using price X seconds from scheduled start etc.)

3. Again assuming no to 1. is it "good enough" to be worth pursuing with the view to augmenting the dataset with some more obscure/less easily available data

A lot of questions here but pre-off fundamental models are new to me. Inplay, backtesting or running strategies without backtest is easier to grasp - you know your model value has a finite lifetime and you need to pull any unmatched money before it gets too stale. Pre-off, your probability is fixed - but the market changes so how do you evaluate the execution (literature tells you value is not around for a long time with bookies, but betfair pre-off dog and horse markets suggest othewise - do you ride a trend in your favour until it reverses or fill your boots while you can) ??



I was going to keep asking people things via DM, but I figured that this is generic enough of a modelling problem to possibly be worth asking about publicly and resulting discourse might be helpful to others (even if only as an example of something that is ultimately useless and should be abandoned)

*Tags: Getting Started, Errors Debugging, Strategies*

---

**Justice** - *15:43:33*

For what it's worth, the first 2 things I look at are calibration (as done above but with a 95% CI) and then, I plot the distribution of probabilities from my model and BSP and compare. This quickly highlights if there's a problem discriminating short/long prices. I'd then isolate those cases and analyse what pieces of information my model is missing using domain knowledge (of which I have none for dogs). I'd then attempt to encode this information into the features for my model

*Tags: Feature Engineering, Strategies*

---

**John Foley** - *15:50:50*

your questions:

1. looks like enough to keep trying imo

2. i'd probably pick a time e.g. 30mins from off (but try several) and full backtest

3. hard to know off this alone but generally yes adding meaningful variables should help



things id be trying next:

- normalise predictions per race and repeat backtest

- try to find probablility range where it performs best (maybe short price favs its shit, bigger prices its good)

- repeat your calibration step splitting by equal bin sizes instead of arbitrary 10% intervals

*Tags: General Technical*

---

**John Foley** - *16:01:44*

haha very fair!

in a 3 runner race if your model has predicted win probabilities 0.6, 0.7, 0.4

normalized probabilities would be 0.35, 0.41, 0.24. each is divided by the sum 0.6 + 0.7 + 0.4

*Tags: Strategies*

---

**D C** - *16:02:44*

Ah right thanks - the normal meaning!! Actually my probabilities are already normalised in this model.

*Tags: Strategies*

---

**Mo** - *16:08:58*

Calculate EROI using BSP then bucket into ten equally sized bins and check realised ROIs. Does the pattern look sensible? ie when you think you have higher margin, do you? If your margin is too high, is this a problem? ie does it indicate you have missed something the market has not

*Tags: General Technical*

---

**D C** - *16:13:10*

[@UBS7QANF3](@UBS7QANF3) For this particular "backtest" where I have looked at profit taking BSP if it is greater than model price, I see a clear drop off of profit if BSP is more than double "my" price. Basically if looks too good to be true, it ends up being shite. But if I know this in advance and it remains constant it might not be a problem.

*Tags: Strategies*

---

**D C** - *16:14:51*

The conceptual problem I have I think is that if BSP for dogs is any good then I want my prices to be close to the BSP. Therefore is there any value in measuring profit based on deviation from it?

*Tags: General Technical*

---

## 2025-01-15

**Matt** - *15:44:55*

> When those talk about time series, do you think they are referring to some unobservable latent measure of team performance, or time series of prices on a runner?

it is more of conceptually around *metric  or feature volatility*, *not Market Volatility*. (and i look it from a more pure model building POV)

Time series, especially in NBA (or sports with a lot of matches in a season), around performance of players and teams seems to be a neglected statistic. Things like a RAPM are great for a player metric in NBA, but adding a feature of volatility, hurst, etc to that players epm also lets the model know how chaotic that players performance is or if its mean reverting (depending on what you are using for time analysis).



If you sort NBA players by their RAPM strength (lets say DEC 1) and then do it again FEB 1, people that are more consistant will be at the top both times, and people that were (or arrived) at the top have higher variance. this as an added feature will let a model understand this.



in the end, i think it can be a signal of latent states (maybe seeing some player, lineup or team that all stabilized, or went volatile. But the intention is to let the model you are building know that "this person is playing erratic for this metriuc, this person is playing consistant for this metric, this team is...)

[@UPMUFSGCR](@UPMUFSGCR)

*Tags: Feature Engineering, Performance, Strategies*

---

**Charlie (Fugazi)** - *22:35:45*

I had a model built that might help. Had every greyhound ever in its database until I gave up on it around a year ago



[https://drive.google.com/file/d/1HXVIQMgnD8CUwTzTCsRcgAPJPV7181ck/view?usp=sharing|https://drive.google.com/file/d/1HXVIQMgnD8CUwTzTCsRcgAPJPV7181ck/view?usp=sharing](https://drive.google.com/file/d/1HXVIQMgnD8CUwTzTCsRcgAPJPV7181ck/view?usp=sharing|https://drive.google.com/file/d/1HXVIQMgnD8CUwTzTCsRcgAPJPV7181ck/view?usp=sharing)



Bet angel thread with a bit more info



[https://forum.betangel.com/viewtopic.php?p=368464&amp;hilit=Greyhound+model#p368464|https://forum.betangel.com/viewtopic.php?p=368464&amp;hilit=Greyhound+model#p368464](https://forum.betangel.com/viewtopic.php?p=368464&amp;hilit=Greyhound+model#p368464|https://forum.betangel.com/viewtopic.php?p=368464&amp;hilit=Greyhound+model#p368464)

*Tags: Strategies*

---

## 2025-01-19

**Unknown** - *12:59:23*

It's been a frustrating week trying to get `Strategy2` to the point where I'd be comfortable redeploying it



On Monday I started by increasing the exposure limit on `Strategy1` for in play horses to bring it in line with what I've used for backtesting. I haven't spent much time using backtests to gauge how much I can scale everything up but that's perhaps something to start thinking about



I wasn't planning on doing this but I decided on Monday to also compare `Strategy0`'s live bets to the backtest and discovered some big problems with my execution which meant I'd missed out on - very roughly - about 15,000 bets over the past three months. This was largely down to handling the start of races and using the `marketStartTime`. I've completely overhauled the logic and it appears to have fixed things. I made a separate tweak that should also help reduce the number of missing bets but, checking that now, it's been somewhat less successful



Right, back to `Strategy2`. I've definitely made significant improvements to the model over the past week but none of the changes have translated into a meaningful shift in backtest performance. Intuitively, where the model is strongest it must just be agreeing with the market. However, I did have a small breakthrough last night with the filters I'm applying to try to avoid adverse selection and I think the strategy is just about at the stage where I'm happy to start live testing it again today. I don't think the backtest performance is quite good enough but the thing that convinced me it's worth live testing is that I can run it against many more markets than I have scraped prices for - i.e. all of the `OTHER_PLACE` markets. As in my last update, I can use BSP to quickly assess whether it's actually finding value on those markets. The breakthrough I'm referring to should apply to pretty much any taker strategy so I'm excited to apply it to other strategies



So the plan for the rest of the day is to manually run the strategy against today's UK racing and I also still owe my syndicates another couple of hours work for the week. Looking to next week, I should fit `Strategy2`'s model to Australian racing and backtest it again to see how that's looking. I do have one other big model improvement on my to-do list that I'll action and a couple of suggestions from o1 that I think are worth exploring but I haven't thought about what's next beyond that. I'll review my list of strategy ideas and have another chat with o1



A couple of general findings from this week:



1. It really helps to step away from the computer. All of my big breakthroughs in thinking came from taking a shower or cooking dinner or walking the dog

2. o1 continues to be indispensable. It's at least as good as if I had a junior quant working for me

Update on performance:



1. Got rotated back into syndicate strategies yesterday. We've been undertaking a large reconciliation exercise, the end result of which is that I'm quite a bit behind of my share so I imagine I'll be in rotation for a bit now

2. New strategies had a bad couple of days midweek and have been absolutely crushing it since then

New strategies net profit up to 2.3% of existing strategies net profit!

*Tags: Errors Debugging, Performance, Deployment, Strategies*

---

## 2025-01-20

**jp** - *09:51:06*

"It really helps to step away from the computer. All of my big breakthroughs in thinking came from taking a shower or cooking dinner or walking the dog"



I can really relate to this—I experience the exact same thing myself.

*Tags: General Technical*

---

## 2025-01-23

**ShaunW** - *18:17:00*

That's the $64,000 question.

*Tags: General Technical*

---

**thambie1** - *18:54:16*

The distribution of prices could also be problematic for calculating the p-value. Without your dataset it's hard to dig further. But I'd bet a pretty large amount that graph and that p-value don't go together.

*Tags: General Technical*

---

**Mo** - *19:01:07*

Can you be more specific about how the distribution of prices could be a problem?

*Tags: General Technical*

---

**Justice** - *20:24:06*

Does the calculation for the p-value assume your returns are normally distributed? I keep it simple and use a bootstrapping to analyse the statistical significance of my strategy

*Tags: Strategies*

---

## 2025-01-26

**Unknown** - *08:28:15*

Big week this week and it all came out of the meetup on Tuesday



I had an idea for a new strategy while chatting to [@U01PJ5YMFBJ](@U01PJ5YMFBJ) and while it took a few days to go from idea to implementation (both o1 and DeepSeek made a complete hash of it), the backtest looks great - see first attached plot. I've live tested it on a couple of races and am looking to deploy it more widely today. This is now `Strategy3`



Also at the meetup, [@U05SRUKGYCC](@U05SRUKGYCC) made some comments that gave me some perspective on `Strategy2`. It helped cement my feeling that it's not contrarian enough and I need to revert to the original vision of the strategy. I've started work on a third iteration of it with promising signs. I had really hoped I was going to be able to report today that I'd gone live with _both_ `Strategy2` and `Strategy3` this week but sadly not quite there yet



Existing strategies got absolutely crushed this week and that unfortunately coincided with it being my turn to run them. My already considerable shortfall is now even larger. Although they've been running for years, variance like this is still stressful when you're personally making the losses even if it all gets accounted for. However, these emotional factors are easier to handle being part of a group that can provide perspective and shared experience



New strategies flat on the week so far after a bad day yesterday (my naturally pessimistic disposition thinks there's still plenty of time for them to get further in the hole for the week) but the losses seem in line with variance and I'm not at all concerned (yet). I did increase stakes on `Strategy0` on Monday and it's probably past time I increased stakes on `Strategy1` as well



Plan for the coming week is to get `Strategy3` as widely deployed as possible - I'll need to check backtest performance on US/rest of world horses, and see what I can do about testing it on greyhounds as well. I have a feeling the strategy should work well inplay but backtests look awful. I'm actually specifically interested in getting it working for inplay football but that probably needs a fair bit more work than just tuning some parameters



Development-wise the main focus will be trying to finally get `Strategy2` over the line

*Tags: Performance, Deployment, Strategies*

---

**AndyL** - *09:09:59*

Glad to be of help :grinning:

*Tags: General Technical*

---


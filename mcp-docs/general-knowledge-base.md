# General Knowledge Base

*Generated from Slack chat history - 8111 technical conversations*

---

## 2017-03-11

**agberk** - *12:48:06*

np - thanks for the library, has been a lot of help :slightly_smiling_face:

*Tags: General Technical*

---

**agberk** - *12:56:32*

Do you want to just abandon [https://github.com/liampauling/betfairlightweight/pull/46](https://github.com/liampauling/betfairlightweight/pull/46) ?  I can do another one with just the socket cleanup code in `stop` and calling it before the `raise`

*Tags: General Technical*

---

**liam** - *12:57:41*

out of interest are you using the library in production/live or just testing?

*Tags: Deployment*

---

**agberk** - *13:00:11*

yeah we're using it prod; we actually ran into [https://github.com/liampauling/betfairlightweight/issues/32](https://github.com/liampauling/betfairlightweight/issues/32) and put in a workaround, but didn't see the issue until you had done something about it

*Tags: General Technical*

---

**agberk** - *13:12:15*

Although I guess maybe not a lot of people are using python for interfacing

*Tags: General Technical*

---

## 2017-04-06

**jfo** - *20:09:59*

just getting to grips with it, python isn’t my main language so having fun wrestling with that at the same time :smile:

*Tags: General Technical*

---

**jfo** - *20:16:09*

using the streaming api that is

*Tags: General Technical*

---

**liam** - *20:16:37*

What market? I didn’t think you could use streaming with the delayed key

*Tags: General Technical*

---

**liam** - *20:18:21*

Interesting, I guess they are now able to delay streaming as well as the live key would be updating every couple of 10ms with Chelmsford still on

*Tags: Deployment*

---

**jfo** - *20:20:28*

I’m not too hot on queues in python but i just have an infinite loop which called `get` on the queue, it blocks until it has data at that point. Is that good practice?

*Tags: General Technical*

---

**liam** - *20:24:59*

or you can set blocking to False (get(False)) and then add error handling for an empty queue if you want your code non blocking

*Tags: Errors Debugging*

---

## 2017-04-12

**jfo** - *20:07:43*

[@U4JSCQ05N](@U4JSCQ05N) this any good? [http://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github](http://stackoverflow.com/questions/19042389/conda-installing-upgrading-directly-from-github)

*Tags: Getting Started*

---

## 2017-04-14

**favetelinguis** - *20:07:57*

[@U4H19D1D2](@U4H19D1D2) thanks works now but seems like market book is not json serializable using mb.jason() im getting the error

*Tags: Errors Debugging*

---

**favetelinguis** - *20:07:59*

TypeError: ObjectId(‘58f11d3c63b1476ade4ac24c’) is not JSON serializable

*Tags: Errors Debugging*

---

**liam** - *22:01:14*

Hmm streaming? 

*Tags: General Technical*

---

## 2017-04-15

**favetelinguis** - *07:09:03*

yes streaming

*Tags: General Technical*

---

## 2017-04-17

**jfo** - *19:30:13*

[@U4H19D1D2](@U4H19D1D2) got any tips for install flumine with `pip` ?

*Tags: Getting Started*

---

**jfo** - *19:30:37*

just getting `Could not find a version that satisfies the requirement flumine (from -r requirements.txt (line 2)) (from versions: )`

*Tags: General Technical*

---

**liam** - *19:31:10*

Flumine isn't on pypi

*Tags: General Technical*

---

**jfo** - *19:34:44*

you do all your dev in python?

*Tags: General Technical*

---

**liam** - *19:37:03*

Yeh all python although learning golang at the mo 

*Tags: General Technical*

---

**liam** - *19:46:09*

[@U4JSCQ05N](@U4JSCQ05N) how are you finding rxpy? I was tempted to make streaming async but I couldn't work out an easy way to do it, using queue makes everything simple 

*Tags: General Technical*

---

## 2017-04-19

**favetelinguis** - *17:07:05*

this has been my inspiration and the next step is to acually apply it and build a trading bot

*Tags: Strategies*

---

**favetelinguis** - *17:31:56*

it is best for really complex unstructured data and deep models

*Tags: Strategies*

---

**favetelinguis** - *17:32:12*

try pandas and scikit-learn

*Tags: Feature Engineering*

---

**liam** - *17:33:30*

Used pandas a bit, nice and quick 

*Tags: Feature Engineering*

---

**favetelinguis** - *17:33:52*

yes pandas is nice as long as your data fits in memory

*Tags: Feature Engineering, Performance*

---

**liam** - *17:34:39*

Want to be able to take advantage of serverless and run a strategy through a couple thousand markets in a few seconds 

*Tags: Deployment, Strategies*

---

**favetelinguis** - *17:35:01*

serverless you mean AWS lambdas?

*Tags: Deployment*

---

**favetelinguis** - *17:36:10*

betfair api is the prefect playground

*Tags: General Technical*

---

**jfo** - *17:37:13*

Do you know how much data you are transferring per month [@U4H19D1D2](@U4H19D1D2)? Was looking at vps costs for streaming data, don't have access to live yet so can't do calculations :thinking_face:

*Tags: Deployment*

---

**favetelinguis** - *17:37:55*

AWS is free for imcomming data?

*Tags: Deployment*

---

**favetelinguis** - *17:38:43*

btw betfair is deployd in AWS ireland right?

*Tags: Deployment*

---

**liam** - *17:50:25*

Yeh, you only pay for out beyond the free tier, streaming has reduced this for me a lot. There is a data centre in London as well but I haven’t done any latency tests

*Tags: Performance*

---

## 2017-04-20

**agberk** - *00:29:14*

[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com) is served out of Ireland, Dublin (though not sure it's AWS) - but you get routed through a DDoS protection service in London

*Tags: Deployment*

---

## 2017-04-27

**liam** - *21:07:58*

Streaming?

*Tags: General Technical*

---

**liam** - *21:09:11*

Yeh streaming data is cached so as the day goes on memory use will increase but shouldn't be that much, you seeing big increases? 

*Tags: Performance*

---

**jfo** - *21:09:55*

Yea, jumped up to GB’s after 5 mins which makes me think I’m doing something wrong, but i’m just dumping data out to csv + database

*Tags: General Technical*

---

## 2017-05-01

**jfo** - *13:19:41*

getting timeout errors on stream :thinking_face:

*Tags: Errors Debugging*

---

**jfo** - *13:34:24*

```Exception in thread My Socket:

Traceback (most recent call last):

  File "/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py", line 801, in __bootstrap_inner

    self.run()

  File "/usr/local/Cellar/python/2.7.13/Frameworks/Python.framework/Versions/2.7/lib/python2.7/threading.py", line 754, in run

    self.__target(*self.__args, **self.__kwargs)

  File "/usr/local/lib/python2.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 160, in _read_loop

    raise SocketError('[Connect: %s]: Socket %s' % (self.unique_id, e))

SocketError: [Connect: 2]: Socket ('The read operation timed out',)```

*Tags: Errors Debugging*

---

**jfo** - *13:41:40*

```MARKET_FILTER = streaming_market_filter(

    event_type_ids=['7'],

    country_codes=['GB', 'IE'],

    market_types=['WIN', 'PLACE'],

)

MARKET_DATA_FILTER = streaming_market_data_filter(

    fields=['EX_ALL_OFFERS', 'EX_MARKET_DEF', 'EX_LTP', 'EX_TRADED', 'EX_TRADED_VOL'],

    ladder_levels=3

)```

*Tags: General Technical*

---

**jfo** - *13:53:06*

```INFO:betfairlightweight.streaming.listener:[Subscription: 2]: SUCCESS

ERROR:betfairlightweight.streaming.listener:[Subscription: 0] SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 202 markets whereas max allowed number was: 200

INFO:betfairlightweight.streaming.listener:[Subscription: 0]: FAILURE```

*Tags: Errors Debugging*

---

**jfo** - *13:55:26*

thanks for the help

*Tags: General Technical*

---

## 2017-05-08

**liam** - *16:30:33*

What sort of latency have you got at the moment? Easy way to access is to log the following place_order_response.elapsed_time 

*Tags: Performance*

---

**jfo** - *16:36:05*

still in the process of putting together my own order placing strategy so using betangel server with streaming to get up and running before coding it all. so not sure on latency, at what number does latency become bad? obviously the lower the better

*Tags: Performance, Deployment, Strategies*

---

**liam** - *16:47:14*

I average around 0.2s for placeOrders on AWS (Ireland region) BA is good but when using automation or the excel integration there is going to be a big delay between your trigger and the order hitting the market. When it comes to In play racing milliseconds can count. So when it comes to getting matched you need to process your trigger quicker or reduce the latency on your orders 

*Tags: Performance, Deployment*

---

## 2017-05-09

**jfo** - *14:59:54*

[@U4H19D1D2](@U4H19D1D2) more questions if you don't mind haha :smile:

*Tags: General Technical*

---

**jfo** - *15:00:34*

what kind of plan are you on with aws? I had a look at the autoscaling stuff but no sure if its overkill, might just stick with a lightsail box to begin with?

*Tags: Deployment*

---

**jfo** - *15:28:45*

This is for building my own bot with your lib and ditching betangel. I'm not sure if its worth investing the time wrestling aws to be able to scale things up when needed or just get the lightsail box, but then not if that would be enough transfer per month :thinking_face:

*Tags: Deployment*

---

**liam** - *15:30:20*

If your new take advantage of the free tier and boot up an ec2 micro 

*Tags: Deployment*

---

**liam** - *15:31:16*

I run all of my programs off the AWS ami which is based on ubuntu Linux I think 

*Tags: Deployment*

---

## 2017-05-11

**Unknown** - *18:54:13*

[@U4H19D1D2|liam](@U4H19D1D2|liam) uploaded a file: [https://betfairlightweight.slack.com/files/liam/F5C7ZU0Q2/screenshot.png|cpu.png](https://betfairlightweight.slack.com/files/liam/F5C7ZU0Q2/screenshot.png|cpu.png)

*Tags: General Technical*

---

**liam** - *18:54:42*

Seeing 50% reduction in CPU using v1.1 (streaming), strongly recommend an upgrade :slightly_smiling_face:

*Tags: General Technical*

---

**liam** - *18:56:27*

[https://github.com/liampauling/betfairlightweight/issues/65](https://github.com/liampauling/betfairlightweight/issues/65)

*Tags: General Technical*

---

**liam** - *18:57:05*

I think there is more to do to streamline the streaming cache

*Tags: General Technical*

---

## 2017-05-12

**jfo** - *08:39:16*

i'm new to python so interesting stuff :slightly_smiling_face:

*Tags: Getting Started*

---

**jfo** - *08:42:51*

haven't done anything on it for a while, but i'm interested to see the speed of it vs python

*Tags: Performance*

---

## 2017-05-17

**liam** - *14:08:33*

The client creates a new session on each request unless you supply a session object, as [http://requests.post|requests.post](http://requests.post|requests.post) will create a new session if not supplied. I use a session per thread as I was getting errors when trying to share

*Tags: Errors Debugging*

---

**agberk** - *14:13:05*

so by default its thread safe; and presumably you get better performance creating a session for each thread to use?

*Tags: Performance*

---

**agberk** - *14:16:43*

haha yeah whenever I need to see how I should do something idiomatically in python I check to see what's going on in requests

*Tags: General Technical*

---

**agberk** - *14:24:00*

do you write python for your day job?

*Tags: General Technical*

---

**agberk** - *14:31:24*

yeah all python - did have some legacy erlang apps which are pretty much retired

*Tags: General Technical*

---

**agberk** - *14:32:09*

Although I'm actually interested in using Erlang for the process management and having it spawn python processes since that's what it's good at

*Tags: General Technical*

---

**agberk** - *14:33:14*

I read an interesting use case - some gaming middleware company that does/did the game lobby / matchmaking for CoD switched from... I forget what, C++ or Java or something, to python and then put Erlang on top to manage the processes

*Tags: General Technical*

---

**liam** - *14:40:50*

sorry using lambda to start/stop containers via the python wrapper

*Tags: General Technical*

---

**agberk** - *14:40:51*

Plus you can easily drop into writing some python for custom plugins etc.

*Tags: General Technical*

---

**liam** - *14:41:58*

docker speeds things up for me, can go from no ec2 instances available to a program running and i just have to install docker

*Tags: Getting Started, Performance, Deployment*

---

**agberk** - *14:54:15*

Yeah that is an advantage - I've got the AWS app on my phone but you're right that it doesn't give me the fine grained control to restart my apps, components of them etc.

*Tags: Deployment*

---

## 2017-05-19

**richard** - *15:13:12*

The example part 1 errors "Certificate folder not found in /certs". I don't recall ever needing to make certificates to run my bots of old. Am I going to need to now to use this package?

*Tags: Errors Debugging*

---

**agberk** - *15:25:02*

Just taking a gander from the betfairlightweight code; if you were to do an interactive login and obtain a session token you could set this manually on the BaseClient session_token field

*Tags: General Technical*

---

## 2017-05-20

**liam** - *09:07:16*

Sorry, issue 32: [https://github.com/liampauling/betfairlightweight/issues/32](https://github.com/liampauling/betfairlightweight/issues/32)

*Tags: General Technical*

---

**jfo** - *11:55:42*

[@U4H19D1D2](@U4H19D1D2) do you run sql in aws aswell?

*Tags: Deployment*

---

**liam** - *12:22:54*

i think i have found the problem

*Tags: General Technical*

---

**liam** - *12:22:57*

[https://github.com/liampauling/betfairlightweight/blob/master/betfairlightweight/resources/streamingresources.py#L147](https://github.com/liampauling/betfairlightweight/blob/master/betfairlightweight/resources/streamingresources.py#L147)

*Tags: General Technical*

---

**liam** - *12:31:30*

i have a django api running through zappa/aws lambda so dont communicate directly to the db

*Tags: Deployment*

---

**jfo** - *12:32:38*

so your python posts to an api which saves to sql?

*Tags: General Technical*

---

**liam** - *12:33:38*

used to spend £10 on hosting it but using aws lambda it costs me under £1

*Tags: Deployment*

---

**liam** - *12:44:21*

so the django app (called pinhole) is setup to record order/strategy data and some market level data. When it comes to recording market book data i zip that up and send it to s3, then have lambda process which includes parsing and loading to MySQL depending on market type or simply adding a record of what it is into pinhole which I can then query to get the bucket location for back testing or processing later. But yeh if you want to record/log something else you have to create a model/migrate/build the view/add to the api so its limited in that respect. But that is what elasticsearch is good for which i use for logging as it can pretty much take anything you chuck at it

*Tags: Getting Started, Strategies*

---

## 2017-05-21

**richard** - *23:30:00*

So I created the certificates and attached to my Betfair account. Now I'm back to trying part1 with trading=betfairlightweight.APIClient(...) passing in my username, password, app_key and cert_files folder.

*Tags: Strategies*

---

**richard** - *23:30:52*

On trying the trading.login() line I now get:APIError: None

Params: None

Exception: [('system library', 'fopen', 'No such process'), ('BIO routines', 'FILE_CTRL', 'system lib'), ('SSL routines', 'SSL_CTX_use_certificate_file', 'system lib')]

*Tags: Errors Debugging, Strategies*

---

## 2017-05-22

**liam** - *06:55:30*

Actually what version of python/requests are you using? You able to update requests and try again?

*Tags: General Technical*

---

**richard** - *16:02:23*

At some point liam I'd love to understand a bit more about how you use AWS. I've just started on it now I've come back to Betfair and already the cost I'm incurring simply capturing price and volume data is looking like it could be optimised. I have the code on EC2 and have tried both MySQL and Aurora for the database. It's this RDS piece that is the highest. I'm thinking automatically dump to S3 on a daily/weekly basis(?), but haven't yet looked at comparative costs of storage.

*Tags: Deployment*

---

**liam** - *16:12:04*

I'll be honest my process changes almost weekly and is more focussed around budget, want to do a blog post on this topic soon. Are you new to AWS? Because you can run a ec2.micro for free for a year which is what I originally did. I now zip and store json in s3 as soon as a market is complete, much cheaper than db storage 

*Tags: Getting Started, Deployment*

---

**richard** - *16:14:31*

I think there was a big jump when I put an old 300GB database on there!

I've since taken it down and so I'll see how things level off just collecting new data.

*Tags: General Technical*

---

**richard** - *16:16:09*

Makes me weep that I used to have a dedicated server at nearly £200/month for my betting - this was when AWS was just in its infancy though.

*Tags: Deployment, Strategies*

---

**liam** - *16:17:20*

Yeh for databases AWS seems to be a rip off 

*Tags: Deployment*

---

**jfo** - *16:30:37*

i've just got my stuff running on aws lightsail, the smallest one, has mysql running on there and well as a couple of python scripts, not very scalable but it simple and cheap

*Tags: Deployment*

---

**richard** - *16:32:30*

Not seen lightsail yet. I went to an AWS-ome Day recently and even the instructors say there's too much for them to remember.

*Tags: Deployment*

---

**liam** - *16:35:44*

I went to awsloft last year, a week of interactive sessions / tutorials going through a lot of the key areas and it was all for free! They seemed to have stopped it this year 

*Tags: Deployment*

---

**richard** - *16:36:57*

I'm down in London this week for Strata Data. Tomorrow there is a tutorial session on "Building your first AWS Big Data App", so I'll be there ears open.

*Tags: Deployment*

---

**liam** - *19:10:54*

Looking through my error logs I can see that an order stream restarted twice because rac and rc (regulator auth code and regulator code) weren't provided, any ideas why an order wouldn't have this?

*Tags: Errors Debugging*

---

**liam** - *19:42:11*

[https://www.betfair.com/www/sports/exchange/readonly/v1/bymarket?currencyCode=GBP&amp;locale=en_GB&amp;marketIds=1.131810553&amp;rollupLimit=2&amp;rollupModel=STAKE&amp;types=MARKET_STATE,RUNNER_STATE,RUNNER_EXCHANGE_PRICES_BEST,RUNNER_DESCRIPTION](https://www.betfair.com/www/sports/exchange/readonly/v1/bymarket?currencyCode=GBP&amp;locale=en_GB&amp;marketIds=1.131810553&amp;rollupLimit=2&amp;rollupModel=STAKE&amp;types=MARKET_STATE,RUNNER_STATE,RUNNER_EXCHANGE_PRICES_BEST,RUNNER_DESCRIPTION)

*Tags: Strategies*

---

## 2017-05-23

**richard** - *12:03:05*

So in the Amazon workshop this morning we built an app to process weblogs. It strikes me that this could easily translate to capturing Betfair data.

For those that are interested here are the components of the AWS universe that we used.

*Tags: Deployment*

---

**Unknown** - *12:03:43*

[@U5GAP1M9C|richard](@U5GAP1M9C|richard) uploaded a file: [https://betfairlightweight.slack.com/files/richard/F5HH13EUF/amazon_big_data_application.jpg|Amazon Big Data Application.jpg](https://betfairlightweight.slack.com/files/richard/F5HH13EUF/amazon_big_data_application.jpg|Amazon Big Data Application.jpg)

*Tags: General Technical*

---

**richard** - *12:12:46*

We haven't discussed costs at all (apart from "free to upload data to AWS"), so you could be right.

*Tags: Deployment*

---

**richard** - *12:20:52*

The workshop we followed was on QwikLabs. One of the links he's just finished-up with for something similar is an Amazon project: [https://aws.amazon.com/getting-started/projects/build-log-analytics-solution/](https://aws.amazon.com/getting-started/projects/build-log-analytics-solution/)

*Tags: Deployment*

---

**liam** - *20:56:15*

[https://github.com/liampauling/betfairlightweight/blob/master/betfairlightweight/filters.py#L39](https://github.com/liampauling/betfairlightweight/blob/master/betfairlightweight/filters.py#L39)

*Tags: General Technical*

---

## 2017-05-24

**jfo** - *18:00:57*

```SportsAPING/v1.0/placeOrders

Params: {'instructions': [{'orderType': 'LIMIT', 'selectionId': 5687409, 'limitOrder': {'persistenceType': 'LAPSE', 'price': 1.02, 'size': 1}, 'side': 'LAY'}], 'marketId': '1.131837764'}

Exception: None

Error: {u'message': u'ANGX-0002', u'code': -32099, u'data': {u'exceptionname': u'APINGException', u'APINGException': {u'errorCode': u'INVALID_INPUT_DATA', u'requestUUID': u'prdang-2882bf-04180911-00ee30c51a', u'errorDetails': u'market id passed is invalid'}}}

Full Response: {u'jsonrpc': u'2.0', u'id': 1, u'error': {u'message': u'ANGX-0002', u'code': -32099, u'data': {u'exceptionname': u'APINGException', u'APINGException': {u'errorCode': u'INVALID_INPUT_DATA', u'requestUUID': u'prdang-2882bf-04180911-00ee30c51a', u'errorDetails': u'market id passed is invalid'}}}}```

*Tags: Errors Debugging*

---

**jfo** - *18:02:15*

basically copied the code from here: [https://github.com/liampauling/betfairlightweight/blob/master/examples/exampletwo.py](https://github.com/liampauling/betfairlightweight/blob/master/examples/exampletwo.py)

*Tags: General Technical*

---

**liam** - *18:17:30*

Odd, the error is the size, needs to be above 2 for gbp or £10 liability 

*Tags: Errors Debugging*

---

**liam** - *18:53:11*

At least the free sample contains LTP, might use that for doing some basic football modelling

*Tags: Strategies*

---

**Unknown** - *20:58:32*

[@U4H19D1D2|liam](@U4H19D1D2|liam) uploaded a file: [https://betfairlightweight.slack.com/files/liam/F5JBZVCHM/image_uploaded_from_ios.jpg|Image uploaded from iOS](https://betfairlightweight.slack.com/files/liam/F5JBZVCHM/image_uploaded_from_ios.jpg|Image uploaded from iOS)

*Tags: General Technical*

---

## 2017-05-25

**jfo** - *16:59:33*

anyone used an ORM which persists to sql in python? something akin to activerecord?

*Tags: General Technical*

---

**seaders** - *17:13:56*

If you use it, definitely do with alembic on top, that handles your table/model migrations like a charm as well

*Tags: Strategies*

---

## 2017-05-26

**agberk** - *09:28:26*

Interesting the pro version says tick data a frequency of 50ms - for some reason I thought the streaming API said it did calculation on a 150ms cycle

*Tags: General Technical*

---

**agberk** - *10:28:38*

Yeah a 200ms matching cycle is what I remember; although for some reason there's a figure of 50ms as well - I thought maybe it was mentioned in the streaming orders but nothing on that page

*Tags: General Technical*

---

**jfo** - *15:16:12*

got an exception i've not seen before: `Exception: Timeout value connect was (3.05, 16), but it must be an int or float.`

*Tags: Errors Debugging*

---

**jfo** - *15:16:17*

think its python related?

*Tags: General Technical*

---

**jfo** - *15:26:07*

```Metadata-Version: 1.1

Name: requests

Version: 1.2.3

Summary: Python HTTP for Humans.

Home-page: [http://python-requests.org](http://python-requests.org)

Author: Kenneth Reitz

Author-email: [mailto:me@kennethreitz.com|me@kennethreitz.com](mailto:me@kennethreitz.com|me@kennethreitz.com)

License: Copyright 2013 Kenneth Reitz

Location: /usr/lib/python2.7/dist-packages```

*Tags: General Technical*

---

## 2017-05-27

**liam** - *07:03:26*

[!channel](!channel) as a warning v1.2 (will release later this morning) will have breaking changes to streaming, the unique_id parameter in requests will be removed and turned into a incrementing id based on what is provided when creating the stream. Refactor also removes the ability to have market and order stream running through the same socket, this is to prevent race conditions and receiving old updates after a change request. The unique id used for a request is then returned, see branch listenerstream_refactor. Any objections?   

*Tags: General Technical*

---

**heedfull** - *07:59:25*

Hi folks auto inviter is up [https://betfairlightweight.herokuapp.com](https://betfairlightweight.herokuapp.com) (not tested yet)

*Tags: General Technical*

---

**liam** - *10:09:46*

[https://github.com/liampauling/betfairlightweight/releases/tag/1.2](https://github.com/liampauling/betfairlightweight/releases/tag/1.2)

*Tags: General Technical*

---

**liam** - *11:33:51*

Yeh I know, sorry, I thought about it but the break was to streaming which to be fair is still in development. Even betfair are still making changes to the interface 

*Tags: General Technical*

---

**heedfull** - *23:34:26*

Hi folks getting this error when running `python setup.py test` 

```

ImportError: Failed to import test module: test_accountresources

Traceback (most recent call last):

  File "/usr/local/Cellar/python3/3.6.1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/unittest/loader.py", line 153, in loadTestsFromName

    module = __import__(module_name)

  File "/betfairlightweight/tests/test_accountresources.py", line 8, in &lt;module&gt;

    from tests.tools import create_mock_json

  File "/betfairlightweight/tests/tools.py", line 1, in &lt;module&gt;

    from mock import Mock

ModuleNotFoundError: No module named 'mock'

```

*Tags: Getting Started, Errors Debugging*

---

**heedfull** - *23:35:50*

is there a way to get the tests to run against a python 3.6 environment?

*Tags: General Technical*

---

## 2017-05-28

**jfo** - *14:56:09*

```[ec2-user@ip-172-26-5-22 ~]$ date

Sun May 28 13:56:03 GMT 2017```

*Tags: Deployment*

---

## 2017-05-29

**liam** - *13:02:16*

i have never used a limit order with these options, probably needs to be fixed

*Tags: Errors Debugging*

---

**jfo** - *18:11:53*

Sorry. More daft questions but i’m seeing this error: `__init__() takes at least 3 arguments (5 given)` It says its coming from: 

```place_orders = self.client.betting.place_orders(

                            market_id=str(book.market_id),

                            instructions=[instruction]

                        )```

*Tags: Errors Debugging, Strategies*

---

**jfo** - *18:12:27*

the `instructions=[instruction]` line specifically, not sure if python has some kind of deferred execution here or something?

*Tags: General Technical*

---

**liam** - *18:15:17*

What is instruction? And what init is erroring?

*Tags: Errors Debugging*

---

## 2017-05-31

**liam** - *16:34:25*

Historical data has been released:

*Tags: Data Quality*

---

**liam** - *16:37:40*

Thinking about implementing a .historical endpoint to lightweight which would be coupled to streaming taking advantage of the listener and market cache. This would would enable backtesting, thoughts? 

*Tags: General Technical*

---

**agberk** - *16:53:24*

That would be pointing to a local file downloaded from the historic data service which itself is in the streaming API format?

*Tags: General Technical*

---

**liam** - *16:54:57*

Downloading and streaming straight off the historical site would be tricky but might be possible 

*Tags: General Technical*

---

**agberk** - *16:56:07*

It's not that you get access to an API which they are streaming some data separate from the live API or whatever

*Tags: Deployment*

---

**agberk** - *16:56:44*

As in trying to make it work by dl'ing/streaming from the historic site doesnt seem worth it

*Tags: General Technical*

---

**agberk** - *17:02:31*

the data spec makes it seem like its just whats coming off the streaming api

*Tags: General Technical*

---

## 2017-06-02

**agberk** - *14:44:05*

[@U4H19D1D2](@U4H19D1D2) am running into situations quite frequently today where the stream / socket stops receiving data but I'm not getting an exception or the read loop isn't exiting as I'm expecting; I know there was something recent discussion on subscribing multiple times on a socket, or perhaps creating multiple streams or something - I'm about to go look at the issues to refresh my memory - I'm still on v1.0.0 so it might be the problem is fixed just thought I'd check to see whether describing that behaviour made you think of anything obvious

*Tags: Errors Debugging, Performance*

---

**agberk** - *14:44:36*

The main read loop is in my own thread

```

def betfair_stream_run(config, order_queue):

    betfair_api = betfairlightweight.APIClient(

        config['betfair_username'],

        config['betfair_password'],

        config['betfair_application_key'],

        config['betfair_certificate_dir'])



    while True:

        try:

            betfair_api.login()

            betfair_stream_socket = betfair_api.streaming.create_stream(

                unique_id=1,

                description=config['description'],

                listener=StreamListener(output_queue=order_queue)

            )

            betfair_stream_socket.subscribe_to_orders(

                unique_id=12345,

            )

            betfair_stream_socket.start()

        except betfairlightweight.exceptions.SocketError:

            logger.warning('SocketError in betfair stream thread.')

            time.sleep(5.0)

        except betfairlightweight.exceptions.APIError as e:

            logger.error(e, exc_info=True)

            time.sleep(5.0)

```

*Tags: Errors Debugging*

---

**agberk** - *14:46:31*

So even if the _error_handler on the listener causes _data to call stop() it should loop again, create a fresh stream and subscribe on that new stream object

*Tags: Errors Debugging*

---

**agberk** - *14:47:24*

I've got debug level logging switched on for betfairlightweight now so I'll be able to see what output there is if any when it happens again

*Tags: Errors Debugging*

---

**liam** - *14:52:23*

Are you running that in a thread? Python has a habit of not showing an error if a thread dies due to a third party library like lightweight erroring 

*Tags: Errors Debugging*

---

**agberk** - *14:54:54*

Yeah it's in a thread but I've got an excepthook for logging uncaught exceptions and I'm aware of [https://bugs.python.org/issue1230540](https://bugs.python.org/issue1230540) and am using one of the workarounds in there

*Tags: Errors Debugging*

---

**seaders** - *15:01:40*

I've poked them a few times about auto-generating models from that thing definition they have

*Tags: Strategies*

---

**seaders** - *15:01:58*

your library is nearly at the stage that all the models follow nearly the exact same pattern,

*Tags: Strategies*

---

**agberk** - *15:27:23*

```

Traceback (most recent call last):

  File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner

    self.run()

  File "/usr/lib/python2.7/threading.py", line 754, in run

    self.__target(*self.__args, **self.__kwargs)

  File "/home/aaron/workspace/xxx/betfair_order_feed.py", line 94, in betfair_stream_run

    betfair_stream_socket.start()

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 49, in start

    self._read_loop()

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 177, in _read_loop

    self._data(received_data)

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 204, in _data

    if self.listener.on_data(received_data) is False:

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/streaming/listener.py", line 83, in on_data

    self._on_change_message(data, unique_id)

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/streaming/listener.py", line 108, in _on_change_message

    self.stream.on_subscribe(data)

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/streaming/stream.py", line 40, in on_subscribe

    self._process(book_data, publish_time)

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/streaming/stream.py", line 132, in _process

    self._caches[market_id] = OrderBookCache(publish_time=publish_time, **order_book)

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/resources/streamingresources.py", line 488, in __init__

    self.runners = [OrderBookRunner(**i) for i in kwargs.get('orc', [])]

TypeError: __init__() got an unexpected keyword argument 'hc'

```

*Tags: Errors Debugging*

---

**liam** - *18:43:19*

Also wasn't even aware of exception hooks 

*Tags: Errors Debugging*

---

## 2017-06-03

**liam** - *09:38:00*

version 1.3 released, bug fixes and initial attempt at a Historical end point [https://github.com/liampauling/betfairlightweight#historical-data](https://github.com/liampauling/betfairlightweight#historical-data)

*Tags: Errors Debugging*

---

**jfo** - *11:15:34*

`python setup.py test`

*Tags: Getting Started*

---

**jfo** - *11:15:44*

is that the standard way of running a test suite in python?

*Tags: General Technical*

---

**Unknown** - *11:51:43*

[@U4H19D1D2|liam](@U4H19D1D2|liam) uploaded a file: [https://betfairlightweight.slack.com/files/liam/F5N8L19T6/-.js|Untitled](https://betfairlightweight.slack.com/files/liam/F5N8L19T6/-.js|Untitled)

*Tags: General Technical*

---

**jfo** - *12:58:26*

I’ve been using vs code, just figured out how to do the tests in that

*Tags: General Technical*

---

**jfo** - *13:31:14*

off topic: [https://www.reddit.com/r/cscareerquestions/comments/6ez8ag/accidentally_destroyed_production_database_on/?st=J3H99RCH&amp;sh=3cf68044](https://www.reddit.com/r/cscareerquestions/comments/6ez8ag/accidentally_destroyed_production_database_on/?st=J3H99RCH&amp;sh=3cf68044)

*Tags: Deployment*

---

**liam** - *16:16:16*

Oh dear, not sure he can be blamed for that. At work the first thing we do on production databases is remove the ability to drop tables /delete without being an admin 

*Tags: Deployment*

---

## 2017-06-05

**agberk** - *13:54:26*

[@U4H19D1D2](@U4H19D1D2) - another error I've just run into with the latest version

```

Exception in thread Thread-1:

Traceback (most recent call last):

  File "/usr/lib/python2.7/threading.py", line 801, in __bootstrap_inner

    self.run()

  File "/usr/lib/python2.7/threading.py", line 754, in run

    self.__target(*self.__args, **self.__kwargs)

  File "/home/aaron/workspace/xxx/betfair_market_feed.py", line 42, in betfair_market_feed

    locale=config.get('markets', {}).get('locale')

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/endpoints/betting.py", line 158, in list_market_catalogue

    return self.process_response(response, resources.MarketCatalogue, elapsed_time, lightweight)

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/endpoints/baseendpoint.py", line 100, in process_response

    return [resource(elapsed_time=elapsed_time, **x) for x in result]

  File "/home/aaron/tmp/venv/betfairlightweight/local/lib/python2.7/site-packages/betfairlightweight/resources/bettingresources.py", line 229, in __init__

    self.runners = [RunnerCatalogue(**i) for i in kwargs.get('runners', [])]

TypeError: __init__() takes at least 5 arguments (4 given)

```

Now the docs do say there are 4 required attributes on a RunnerCatalogue: [http://docs.developer.betfair.com/docs/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-RunnerCatalog](http://docs.developer.betfair.com/docs/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-RunnerCatalog)

They've sent through a list of runners where one is missing the runnerName:

```

runners: [{u'handicap': 0.0, u'runnerName': u'Knowle/Zelenay', u'selectionId': 5717065, u'sortPriority': 1}, {u'handicap': 0.0, u'runnerName': u'Marrero/Paes.', u'selectionId': 8786145, u'sortPriority': 2}, {u'handicap': 0.0, u'sortPriority': 3, u'selectionId': 8781561}]

```

I know a lot has changed in terms of the resources - how come this sort of thing worked (or maybe silently failed is a better way of describing it? lol) before? Did the Beckett resource model just None anything that wasn't present?

*Tags: Errors Debugging, Strategies*

---

**liam** - *14:46:31*

Ok thanks will fix that, yeh Becket would set it as None if not present 

*Tags: Errors Debugging*

---

**agberk** - *14:48:13*

I raised [https://github.com/liampauling/betfairlightweight/issues/95](https://github.com/liampauling/betfairlightweight/issues/95)

*Tags: General Technical*

---

## 2017-06-06

**magiclevinho** - *15:11:50*

Hello All! How to get an inplay soccer match's actual result, and the time elapsed in match?

*Tags: General Technical*

---

**liam** - *16:33:41*

So the data is parsed into a Scores resource, use a debugger and investigate what's in there. Pycharm is good for this 

*Tags: Errors Debugging*

---

**magiclevinho** - *17:08:51*

Wow! Thanks, this pycharm is really great! I was using default python editor. :S Now I see what I was missing! Thank You!

*Tags: General Technical*

---

**agberk** - *21:10:24*

Does seem like it's going to be very infrequent - I think positional args -&gt; required parameters and kwargs -&gt; optional parameters is the right way to do things; still unsure about how to solve the issue

*Tags: General Technical*

---

## 2017-06-07

**magiclevinho** - *18:34:20*

Again I have a problem, if I get the actual in-play event in 'events' variable and then I would like to loop through the events with a for loop like "for event in events:" then it skips multiple events. Why is it doing like that?

*Tags: General Technical*

---

## 2017-06-08

**magiclevinho** - *07:55:24*

how to feed the parameter 'market_start_time=' ?

*Tags: General Technical*

---

**liam** - *08:10:42*

[https://github.com/liampauling/betfairlightweight/blob/master/betfairlightweight/filters.py#L56](https://github.com/liampauling/betfairlightweight/blob/master/betfairlightweight/filters.py#L56)

*Tags: General Technical*

---

**agberk** - *09:55:25*

Anyone have experience with python requirements.txt files? I've got three packages, A, B, C. A depends on B and B depends on C. They're all in private repos so I'm specifying the github URL in the requirements file. In A's requirements file I list B's URL, and in B's requirement's I list C's URL, however when trying to install A's requirements it goes to try and install B and complains it can't find C. Do I have to list my dependency's dependencies in the requirements file? Seems like it defeats the point...

*Tags: Getting Started*

---

## 2017-06-09

**magiclevinho** - *17:03:01*

Traceback (most recent call last):

  File "C:\Program Files\JetBrains\PyCharm Community Edition 2017.1.3\helpers\pydev\pydevd.py", line 1585, in &lt;module&gt;

    globals = debugger.run(setup['file'], None, None, is_module)

  File "C:\Program Files\JetBrains\PyCharm Community Edition 2017.1.3\helpers\pydev\pydevd.py", line 1015, in run

    pydev_imports.execfile(file, globals, locals)  # execute the script

  File "C:/Users/john/Documents/Pythonprojects/betfair_00/main.py", line 27, in &lt;module&gt;

    in_play_only=True,

  File "C:\Python27\lib\site-packages\betfairlightweight\endpoints\betting.py", line 84, in list_events

    (response, elapsed_time) = self.request(method, params, session)

  File "C:\Python27\lib\site-packages\betfairlightweight\endpoints\baseendpoint.py", line 43, in request

    raise APIError(None, method, params, 'ConnectionError')

betfairlightweight.exceptions.APIError: SportsAPING/v1.0/listEvents 

Params: {'filter': {'eventTypeIds': [u'1'], 'inPlayOnly': True}} 

Exception: ConnectionError

*Tags: Getting Started, Errors Debugging, Strategies*

---

**magiclevinho** - *17:03:25*

What is this problem? I keep getting it after a while.

*Tags: General Technical*

---

**liam** - *17:12:10*

Wrap the request in try / except with BetfairError and it will catch it 

*Tags: Errors Debugging*

---

## 2017-06-10

**Evaldas** - *19:16:20*

hey guys!

Im trying to fix my by bot here a bit, by putting all markets under one api stream, instead of creating new ones. And I cant figure out how do you "feed" stream to all threads? If I have running n market threads and one of them calls get() from stream queue.Queue() that object is gone, right? So how do I know which of the threads should call the queue to get latest MarketBook?

*Tags: Errors Debugging*

---

**liam** - *19:33:47*

I use an event driven framework, numerous threads that are streams and then an event handler in main which processes the market books. I then use the unique id to determine which market book should be sent to which strategy. This prevents race conditions 

*Tags: Strategies*

---

**liam** - *19:55:41*

So you can have one or more market streaming threads 'putting' data in and an event handler in main 'getting' data and processing (placing orders / recording data), why do you want a separate thread per market? 

*Tags: General Technical*

---

**Evaldas** - *20:00:48*

one more. does betfair start streaming new markets? i dont need to resubscribe?

*Tags: General Technical*

---

**Evaldas** - *20:27:16*

oh gee, I understand now. get data from stream and put it into market/strategy object, took me a while, kinda new to this :slightly_smiling_face: thanks

*Tags: Getting Started, Strategies*

---

## 2017-06-13

**agberk** - *14:14:22*

[@U4H19D1D2](@U4H19D1D2) I mentioned a while ago about having issues with streaming connections that were stopping unexpectedly but weren't throwing an exception - may have found the issue

*Tags: Errors Debugging*

---

**agberk** - *14:17:15*

And the stack trace suggested it was here: [https://hg.python.org/cpython/file/2.7/Lib/ssl.py#l660](https://hg.python.org/cpython/file/2.7/Lib/ssl.py#l660)

*Tags: General Technical*

---

**agberk** - *14:18:03*

This is running betfairlightweight v1.0.0 and was at this point: [https://github.com/liampauling/betfairlightweight/blob/1.0/betfairlightweight/streaming/betfairstream.py#L173](https://github.com/liampauling/betfairlightweight/blob/1.0/betfairlightweight/streaming/betfairstream.py#L173)

*Tags: General Technical*

---

**agberk** - *14:20:50*

`part` is `b''` in the linked betfairlightweight code now and so it just proceeds as normal, comes back round for the next loop drops into recv and an underlying read in ssl.py which blocks and will never get anything because the receiving of the EOF means the connection is closed I think?

*Tags: General Technical*

---

**agberk** - *14:21:34*

I've opened a ticket with BDP asking them to look at a specific connectionId where this has happened to see if they can tell me how it was closed and whether there was some error or whatever

*Tags: Errors Debugging*

---

**agberk** - *14:22:59*

I'm also going to try setting the field on the socket `suppress_ragged_eofs` to False and run my code - if that's the issue then I should expect to see these exceptions get raised in my logs following by an automatic restart which my code takes care of

*Tags: Errors Debugging*

---

**agberk** - *14:25:04*

If that is indeed the issue then the solution might be either to change that suppress_ragged_eofs flag, or maybe to take some different action like throwing an exception if part is empty - though I'm not too sure whether that happens a lot anyway? Or if that can only happen if a ragged EOF comes through

*Tags: Errors Debugging*

---

**agberk** - *14:28:03*

The next question is whether EOF on an SSL socket means it should be closed?

*Tags: General Technical*

---

**liam** - *14:30:54*

Hmm interesting, I don't think I have encounter this or maybe I have but my error catching is hiding it. Will be interesting to see what betfair say along with your test. Would an alternative of trying to catch all potential socket errors at .recv work?

*Tags: Errors Debugging*

---

**agberk** - *14:32:42*

If that `suppress_ragged_eofs` flag is set to True then there wouldn't be an exception to catch unfortunately

*Tags: Errors Debugging*

---

**liam** - *14:35:06*

If EOF means that betfair has closed the connection then hiding it will cause an issue no? Most likely a timeout error 

*Tags: Errors Debugging*

---

**agberk** - *14:39:07*

I don't quite understand the point of the flag - because if EOF means your connection is done and it's suppressing it... then is just the point of it to stop an exception being thrown? As in if it wasn't there, whenever the connection got closed while you were receiving you'd get an exception, but this way you'll just get a 0 length string and its down to the user to take some action if the return is a 0 length string because they should know that's what it means? And it stops them from having to catch an exception?

*Tags: Errors Debugging*

---

**liam** - *14:43:23*

Sometimes it's as though there are only a handful of people actually using the api. I'm still waiting on an answer about cancel requests not having an error code sometimes 

*Tags: Errors Debugging*

---

## 2017-06-20

**joelpobar** - *22:36:13*

[@U4H19D1D2](@U4H19D1D2) any thoughts on integrating Reactive Extensions for Python ([https://github.com/ReactiveX/RxPY](https://github.com/ReactiveX/RxPY)) for betfairlightweight?

*Tags: General Technical*

---

## 2017-06-23

**alex** - *09:13:54*

Hi! With async=True betfairlightweight starts in a separate thread. What is the correct way of handling network disconnects, etc. in this case?

*Tags: General Technical*

---

**heedfull** - *09:21:23*

Hi, newbie question here, get this error when I’m trying to get some markets back. It say’s I’m missing a required field, but I can’t for life of me find it

```

betfairlightweight.exceptions.APIError: SportsAPING/v1.0/listMarketCatalogue

Params: {'maxResults': 100, 'sort': 'FIRST_TO_START', 'filter': {'marketStartTime': {'to': None, 'from': '2017-06-23T09:08:57Z'}, 'eventTypeIds': '7', 'marketCountries': 'GB', 'marketTypeCodes': 'WIN'}, 'marketProjection': ['EVENT', 'MARKET_START_TIME']}

Exception: None

Error: {'code': -32602, 'message': 'DSC-0018'}

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32602, 'message': 'DSC-0018'}, 'id': 1}

```

*Tags: Errors Debugging*

---

**heedfull** - *09:21:31*

any help greatly appreciated

*Tags: General Technical*

---

**gerg** - *19:49:35*

Im new to this group..How can I use this to get historical data of item soccer of year 2016 ? Any help is greatly appreciated

*Tags: Getting Started, Data Quality*

---

**gerg** - *20:29:16*

[@U4H19D1D2](@U4H19D1D2) Thanks for answering my query.. Where can I get these historical data for free (I am not using it for commercial purpose)?

*Tags: Data Quality*

---

## 2017-06-28

**liam** - *10:15:31*

Do you get an error? / what's the response?

*Tags: Errors Debugging*

---

**Evaldas** - *10:27:13*

oh, I found my problem

*Tags: General Technical*

---

**liam** - *12:21:02*

What file are you getting that error in? Is it in the response?

*Tags: Errors Debugging*

---

**Evaldas** - *12:33:24*

that was exception from betting.place_orders

*Tags: Errors Debugging, Strategies*

---

**Evaldas** - *12:46:16*

i get error:



place_order()

  File "/home/daan/Python/GreyHoundBot_streaming/bf_api_test.py", line 28, in place_order

    place_orders = trading.betting.place_orders(market_id=market_id, instructions=[instruction])  # list

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/endpoints/betting.py", line 299, in place_orders

    return self.process_response(response, resources.PlaceOrders, elapsed_time, lightweight)

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/endpoints/baseendpoint.py", line 102, in process_response



    return resource(elapsed_time=elapsed_time, **result)

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/resources/bettingresources.py", line 679, in __init__



    PlaceOrderInstructionReports(**i) for i in kwargs.get('instructionReports')

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/resources/bettingresources.py", line 679, in &lt;listcomp&gt;

    PlaceOrderInstructionReports(**i) for i in kwargs.get('instructionReports')

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/resources/bettingresources.py", line 659, in __init__

    self.instruction = PlaceOrderInstruction(**instruction) if instruction else None

TypeError: __init__() got an unexpected keyword argument 'marketOnCloseOrder'

*Tags: Errors Debugging, Strategies*

---

**liam** - *12:48:59*

Yeh it can't handle the sp response betfair returns, need to fix this 

*Tags: Errors Debugging*

---

**liam** - *12:49:27*

Setting lightweight to False will hide the error 

*Tags: Errors Debugging*

---

**Evaldas** - *13:31:19*

Exception in thread Thread-2:

Traceback (most recent call last):

  File "/usr/lib/python3.4/threading.py", line 920, in _bootstrap_inner

    self.run()

  File "/usr/lib/python3.4/threading.py", line 868, in run

    self._target(*self._args, **self._kwargs)

  File "/home/daan/Python/GreyHoundBot_streaming/greyMain.py", line 104, in start_orders_socket

    self.betfair_orders_socket.start(async=False)

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/streaming/betfairstream.py", line 49, in start

    self._read_loop()

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/streaming/betfairstream.py", line 177, in _read_loop

    self._data(received_data)

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/streaming/betfairstream.py", line 204, in _data

    if self.listener.on_data(received_data) is False:

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/streaming/listener.py", line 84, in on_data

    self._on_change_message(data, unique_id)

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/streaming/listener.py", line 109, in _on_change_message

    self.stream.on_subscribe(data)

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/streaming/stream.py", line 40, in on_subscribe

    self._process(book_data, publish_time)

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/streaming/stream.py", line 134, in _process

    self._caches[market_id] = OrderBookCache(publish_time=publish_time, **order_book)

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/resources/streamingresources.py", line 494, in __init__

    self.runners = [OrderBookRunner(**i) for i in kwargs.get('orc', [])]

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/resources/streamingresources.py", line 494, in &lt;listcomp&gt;

    self.runners = [OrderBookRunner(**i) for i in kwargs.get('orc', [])]

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/resources/streamingresources.py", line 454, in __init__

    self.unmatched_orders = [UnmatchedOrder(**i) for i in uo] if uo else []

  File "/home/daan/Python/GreyHoundBot_streaming/bfmaster/betfairlightweight/resources/streamingresources.py", line 454, in &lt;listcomp&gt;

    self.unmatched_orders = [UnmatchedOrder(**i) for i in uo] if uo else []

TypeError: __init__() missing 1 required positional argument: 'pt'

*Tags: Errors Debugging*

---

**liam** - *13:41:39*

Pt is persistence type, yeh this needs to be fixed as well. Surprised no one else has encountered this but I guess sp isn't popular... 

*Tags: Errors Debugging*

---

**liam** - *18:37:32*

Will push a minor update tonight to fix the sp issue, any other patches to add?

*Tags: Errors Debugging*

---

**liam** - *22:08:10*

1.3.2 released [https://github.com/liampauling/betfairlightweight/releases/tag/1.3.2](https://github.com/liampauling/betfairlightweight/releases/tag/1.3.2)

*Tags: General Technical*

---

## 2017-06-30

**liam** - *11:55:39*

[@U4H3EEV45](@U4H3EEV45) did you ever get a response regarding the EOF error?

*Tags: Errors Debugging*

---

## 2017-07-02

**agberk** - *10:55:50*

I think I might have seen the error again but with more info; I'll check the logs tomorrow when i go into work hopefully they havent been rotated

*Tags: Errors Debugging*

---

**agberk** - *10:59:50*

Just going through the updates - have seen [https://github.com/liampauling/betfairlightweight/issues/103](https://github.com/liampauling/betfairlightweight/issues/103) before

*Tags: General Technical*

---

**agberk** - *11:00:28*

Will need to remind myself of specifics but I'm probably in favour of raising an error; will double check tomorrow and add comments to the issue

*Tags: Errors Debugging*

---

**Unknown** - *22:02:51*

[@U4H19D1D2|liam](@U4H19D1D2|liam) uploaded a file: [https://betfairlightweight.slack.com/files/liam/F63LZQX46/image_uploaded_from_ios.jpg|Image uploaded from iOS](https://betfairlightweight.slack.com/files/liam/F63LZQX46/image_uploaded_from_ios.jpg|Image uploaded from iOS)

*Tags: General Technical*

---

## 2017-07-03

**andrew** - *10:27:46*

Hi guys, I’m new here. Just wondering how you manage the 200 market limit in the streaming API? It seems (for horses) you need to subscribe to individual marketIds to stay under the limit, but then adding a single marketId means receiving fresh images for all other markets. Any way to avoid this?

*Tags: General Technical*

---

**liam** - *10:58:00*

I think you can request this to be increased. But I normally limit using the streaming market filter 

*Tags: General Technical*

---

**liam** - *19:03:56*

Anyone finding that the 'customer_strategy_refs' is not filtering the order stream?

*Tags: Strategies*

---

**liam** - *21:01:18*

This is for marketbook polling so the trigger is if a strategy is after a market e.g. Its inplay 

*Tags: Strategies*

---

**Unknown** - *21:05:43*

[@U4H19D1D2|liam](@U4H19D1D2|liam) uploaded a file: [https://betfairlightweight.slack.com/files/liam/F63BZ3WKW/image_uploaded_from_ios.jpg|Image uploaded from iOS](https://betfairlightweight.slack.com/files/liam/F63BZ3WKW/image_uploaded_from_ios.jpg|Image uploaded from iOS)

*Tags: General Technical*

---

## 2017-07-09

**liam** - *06:44:13*

The reasoning behind it is that lightweight is only really beneficial on marketbook requests as that is when you want to reduce latency, I don't use it but if I were to use it I would want to have a full resource for all the other requests. Happy to remove but wanted it to be dynamic as I am not sure how others are using it

*Tags: Performance*

---

## 2017-07-10

**liam** - *10:50:16*

Testing like this is one of the main reasons to build an API instead of communicating directly with a db, always causes problems in the end. Although if you are using a wrapper to communicate then you just need to mock the request. But then it sounds like you are after integration testing which gets tricky 

*Tags: General Technical*

---

**jfo** - *18:09:12*

yep, pretty much, i’m not confident in my python skills so looking to cover as much as possible with a few tests as possible

*Tags: General Technical*

---

## 2017-07-11

**limx0** - *09:36:41*

potentially, but could be overthinking the problem

*Tags: General Technical*

---

**limx0** - *10:43:25*

and re the inplay, inplay markets have the 5s delay, so if using requests your python thread will hang waiting 5s for a response on an order insert (if you arent using the async / return immediately flag in betfair)

*Tags: General Technical*

---

**liam** - *11:12:47*

To get around the delay I just start a new thread but I know this isn't very pythonic and starting a new thread is also slow 100ms ish I think 

*Tags: Performance*

---

**seaders** - *11:14:36*

But there's python patterns for sleeping threads

*Tags: General Technical*

---

**agberk** - *11:18:28*

If the strategy isn't speed dependent then spawning threads on demand is what I do

*Tags: Performance, Strategies*

---

**agberk** - *11:19:15*

Haven't really gotten into python3 yet but there is threadpool stuff it looks like: [https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor](https://docs.python.org/dev/library/concurrent.futures.html#threadpoolexecutor)

*Tags: General Technical*

---

## 2017-07-15

**magiclevinho** - *18:06:35*

Could You help me with the following?

I save my bet_ids to a file, after that I read the bet_ids from the file to check if the bet won or not. The problem is, that not all bet_ids return data back i.e. the order returns 0 length,  but some work. What could be the problem? Why my bet ids do not return back data?

*Tags: General Technical*

---

## 2017-07-20

**liam** - *12:33:14*

Thanks [@U5D4ZBEAG](@U5D4ZBEAG) is this breaking now or is it planned? I'm on holiday at the moment (typical) and haven't seen any major errors on my builds. However I can login on my phone and merge if required 

*Tags: Errors Debugging*

---

**jfo** - *20:19:43*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1499105036304652](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1499105036304652)

*Tags: General Technical*

---

**Unknown** - *20:39:44*

[@U4H19D1D2|liam](@U4H19D1D2|liam) uploaded a file: [https://betfairlightweight.slack.com/files/liam/F6CEGHP0E/image_uploaded_from_ios.jpg|Image uploaded from iOS](https://betfairlightweight.slack.com/files/liam/F6CEGHP0E/image_uploaded_from_ios.jpg|Image uploaded from iOS)

*Tags: General Technical*

---

**Unknown** - *20:39:52*

[@U4H19D1D2|liam](@U4H19D1D2|liam) uploaded a file: [https://betfairlightweight.slack.com/files/liam/F6BHP3CMQ/image_uploaded_from_ios.jpg|Image uploaded from iOS](https://betfairlightweight.slack.com/files/liam/F6BHP3CMQ/image_uploaded_from_ios.jpg|Image uploaded from iOS)

*Tags: General Technical*

---

**Unknown** - *20:39:56*

[@U4H19D1D2|liam](@U4H19D1D2|liam) uploaded a file: [https://betfairlightweight.slack.com/files/liam/F6BMKDX1B/image_uploaded_from_ios.jpg|Image uploaded from iOS](https://betfairlightweight.slack.com/files/liam/F6BMKDX1B/image_uploaded_from_ios.jpg|Image uploaded from iOS)

*Tags: General Technical*

---

## 2017-07-22

**jfo** - *16:30:49*

i’m getting a timeout error

*Tags: Errors Debugging*

---

## 2017-07-26

**magiclevinho** - *05:15:29*

Until now my program worked, now I'm getting JSON error during login:

Traceback (most recent call last):

  File "D:/01_PhD/Fogadas/BETFAIR0,5/main.py", line 39, in &lt;module&gt;

    trading.login()

  File "C:\Python27\lib\site-packages\betfairlightweight\endpoints\login.py", line 26, in __call__

    (response, elapsed_time) = self.request(self.url, session=session)

  File "C:\Python27\lib\site-packages\betfairlightweight\endpoints\login.py", line 41, in request

    response_data = response.json()

  File "C:\Python27\lib\site-packages\requests\models.py", line 894, in json

    return complexjson.loads(self.text, **kwargs)

  File "C:\Python27\lib\json\__init__.py", line 339, in loads

    return _default_decoder.decode(s)

  File "C:\Python27\lib\json\decoder.py", line 364, in decode

    obj, end = self.raw_decode(s, idx=_w(s, 0).end())

  File "C:\Python27\lib\json\decoder.py", line 382, in raw_decode

    raise ValueError("No JSON object could be decoded")

ValueError: No JSON object could be decoded

*Tags: Errors Debugging, Strategies*

---

**liam** - *10:31:36*

What are the logs saying at the debug level? 

*Tags: Errors Debugging*

---

**liam** - *16:12:49*

Interesting, this is one of the reasons why I want to move to using the rest endpoint. The status code error catcher would then hopefully catch the error. [@U5Q3P94NT](@U5Q3P94NT) did you create the issue on GitHub? 

*Tags: Errors Debugging*

---

**liam** - *16:56:25*

Random question but anyone getting socket timeout errors at around 06:03am UTC? Had it every day this week 

*Tags: Errors Debugging*

---

## 2017-08-09

**jfo** - *18:56:43*

whats the best way to use python logging?

*Tags: General Technical*

---

**liam** - *19:04:10*

[!channel](!channel) please note that if you are using streaming and not using version &gt;= 1.4.3 it will fail tomorrow when betfair add the extra objects to the market definition. [@U5D4ZBEAG](@U5D4ZBEAG) I noticed today that it is priceLadderDefinition in streaming and priceLadderDescription on the exchange api so I have updated and tested successfully on the integration streaming endpoint.

*Tags: General Technical*

---

## 2017-08-10

**agberk** - *14:47:45*

[@U4UUKF9LG](@U4UUKF9LG) - this is my favourite post regarding python logging best practice [https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/](https://fangpenlin.com/posts/2012/08/26/good-logging-practice-in-python/) - I use YAML for config files in general, and I have a `logging` key which contains the python logging config; you can use the `logging.config.dictConfig` function and pass it a dictionary of the config wherever you got it from

*Tags: General Technical*

---

## 2017-08-19

**jfo** - *14:50:49*

found it: [https://www.betfair.com/www/sports/exchange/readonly/v1/bymarket?currencyCode=GBP&amp;locale=en_GB&amp;marketIds=1.131810553&amp;rollupLimit=2&amp;rollupModel=STAKE&amp;types=MARKET_STATE,RUNNER_STATE,RUNNER_EXCHANGE_PRICES_BEST,RUNNER_DESCRIPTION](https://www.betfair.com/www/sports/exchange/readonly/v1/bymarket?currencyCode=GBP&amp;locale=en_GB&amp;marketIds=1.131810553&amp;rollupLimit=2&amp;rollupModel=STAKE&amp;types=MARKET_STATE,RUNNER_STATE,RUNNER_EXCHANGE_PRICES_BEST,RUNNER_DESCRIPTION)

*Tags: Strategies*

---

## 2017-08-25

**seaders** - *11:51:34*

I'm getting a crapton of 404 errors from it

*Tags: Errors Debugging*

---

**liam** - *15:25:11*

I took it off the website, I can only assume it's a betfair error, you able to check the website to see if that is also error'ing 

*Tags: Errors Debugging*

---

**seaders** - *15:28:48*

no hint in your original commit for RaceCard, [https://github.com/liampauling/betfairlightweight/commit/53b922e48857ac2e865d90e39acb4194be3c5e8f](https://github.com/liampauling/betfairlightweight/commit/53b922e48857ac2e865d90e39acb4194be3c5e8f)

*Tags: General Technical*

---

**seaders** - *15:54:00*

I was getting 100% 404 errors for the past I don't know how long

*Tags: Errors Debugging*

---

**seaders** - *16:05:15*

hilarious, I'm just checking here.  stopped chatting with my mate at 12.25, last racecard error reported at 12.44

*Tags: Errors Debugging*

---

## 2017-08-26

**liam** - *11:04:09*

welcome any comments on PR #115, minor changes to handicap (defaults to 0 rather than None), more examples added and a very small breaking change to the place order response (order-&gt;limit_order) [https://github.com/liampauling/betfairlightweight/pull/115](https://github.com/liampauling/betfairlightweight/pull/115)

*Tags: General Technical*

---

**liam** - *11:07:45*

yep no problem, will change this version to 1.4.4, and add a warning that it will change at 1.5.0

*Tags: General Technical*

---

**liam** - *11:09:13*

also thinking about changing the github name to just betfair so it comes higher in searches, will keep the pip name to betfairlightweight though. I think this will just involve changing the git remote?

*Tags: General Technical*

---

## 2017-08-27

**magiclevinho** - *16:36:31*

Can somebody help me? How to get available fund to bet? I know there is th function "trading.account.get_account_funds()" but how to give it params? Is the wallet "Main Wallet"? and what is the session?

*Tags: Strategies*

---

**liam** - *16:40:23*

Why do you want to give it parameters? Betfair have removed the aus wallet so wallet does nothing for now, although they are talking about adding the ability for more wallets. Session is for passing a requests session, reduces latency when making multiple requests however can be ignored for most requests 

*Tags: Performance*

---

**magiclevinho** - *16:44:20*

thank you for the help

*Tags: General Technical*

---

## 2017-08-28

**seaders** - *18:49:59*

(this might be a more general BF question rather than your lib)

*Tags: General Technical*

---

## 2017-08-30

**seaders** - *11:49:36*

I've put in a PR with the fix, [https://github.com/liampauling/betfair/pull/119](https://github.com/liampauling/betfair/pull/119)

*Tags: Errors Debugging*

---

**liam** - *12:09:52*

That would explain the errors 

*Tags: Errors Debugging*

---

## 2017-09-07

**magiclevinho** - *11:16:02*

I try to investigate betfairlightweight exception, but I don't know how to do it.

I have a try, except betfairlightweight.exceptions.APIError function implemented, and if I print "print(str(betfairlightweight.exceptions.APIError.message))" the error message, then I only get "&lt;attribute 'message' of 'exceptions.BaseException' objects&gt;" as response. How can I see what was the excetion cause?

*Tags: Errors Debugging*

---

**liam** - *12:23:03*

except BetfairError, e:

*Tags: Errors Debugging*

---

**liam** - *12:23:25*

That should print the error message 

*Tags: Errors Debugging*

---

**liam** - *12:25:23*

And for py3 you need to use: except BetfairError as e:

*Tags: Errors Debugging*

---

## 2017-09-08

**magiclevinho** - *14:05:04*

[@U4H19D1D2](@U4H19D1D2) Do You have any clue why do I get a lot of Connection Error while running my program? I can successfully ping [http://google.com|google.com](http://google.com|google.com) without loss. Is there some switch for retry on Connection Error? Now I use try except method in a while loop but if there are a lot of matches to query it barely can succeed.

*Tags: Errors Debugging*

---

**liam** - *15:43:03*

Is it a timeout error? 

*Tags: Errors Debugging*

---

**magiclevinho** - *16:50:26*

[@U4H19D1D2](@U4H19D1D2) No, it says Exception: ConnectionError. Idk if it is timeout or not?

*Tags: Errors Debugging*

---

**magiclevinho** - *17:22:47*

in time it takes 2 sec before the error appears.

*Tags: Errors Debugging*

---

## 2017-09-09

**gerg** - *04:42:37*

I need you help in getting Live Odds

*Tags: Deployment*

---

**gerg** - *04:44:28*

I downloaded the historical data from the website and I could clearly see "OODS" column in it, Following are the column fields in historical data 

"SPORTS_ID","EVENT_ID","SETTLED_DATE","FULL_DESCRIPTION","SCHEDULED_OFF","EVENT","DT ACTUAL_OFF","SELECTION_ID","SELECTION","ODDS","NUMBER_BETS","VOLUME_MATCHED","LATEST_TAKEN","FIRST_TAKEN","WIN_FLAG","IN_PLAY"

*Tags: Data Quality*

---

**gerg** - *04:46:39*

How can I get data in similar format (Atleast the OODS) for a certain date range using this python API? Any ideas will be greatly helpful, I couldn't find any help after googling

*Tags: General Technical*

---

**gerg** - *18:42:27*

Thanks for replying to my query, So do I need to download it manually ? I cant use this python wrapper to do it?

*Tags: General Technical*

---

## 2017-09-10

**liam** - *07:53:50*

[!channel](!channel) 

Welcome any thoughts on v1.5 PR, want to push to master later this morning as I need get_event_timelines in production. Note the breaking changes to place order response and removal of historical endpoint



[https://github.com/liampauling/betfair/releases](https://github.com/liampauling/betfair/releases)

*Tags: Deployment*

---

## 2017-09-11

**liam** - *16:36:05*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1497360179025752](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1497360179025752) [@U4H3EEV45](@U4H3EEV45) did you ever isolate this error? I think I am seeing the same thing at least once a week 

*Tags: Errors Debugging*

---

## 2017-09-13

**richard_h** - *01:02:23*

[@U4H19D1D2](@U4H19D1D2) does betfairlightweight use a specific design pattern?

*Tags: General Technical*

---

**seaders** - *01:06:13*

I've noticed a few more similar APIClients pop up on github, similar to betfairlightweight,

*Tags: General Technical*

---

**liam** - *13:06:09*

You can increase the timeout but because there should be a heartbeat, the library assumes that the connection is broken so stop() is called to tidy it up. You can then subscribe again or resubscribe using the clk / initial_clk. Aaron posted an example of how he deals with it which is similar to mine but async = true can hide these errors 

*Tags: Errors Debugging*

---

**liam** - *13:20:50*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1496411076282793](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1496411076282793)

*Tags: General Technical*

---

**liam** - *14:25:59*

Here is another example which uses resubscribe [https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L79](https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L79)

*Tags: General Technical*

---

**seaders** - *19:25:58*

```





def betfair_stream_run(bf_market_stream):

    """

     :param betfairlightweight.streaming.BetfairStream bf_market_stream:

    """

    market_id = None

    market_ids = ['1.134151108', '1.134182080']



    while bfstream.continue_streaming:

        market_id = next(m_id for m_id in market_ids if m_id != market_id)

        print 'starting up, m_id:', market_id, \

            'inital_clk:', bf_market_stream.listener.initial_clk, \

            'clk:', bf_market_stream.listener.clk



        bf_market_stream.subscribe_to_markets(

            market_filter=dict(marketIds=[market_id]),

            market_data_filter={

                'fields': ['EX_BEST_OFFERS_DISP', 'EX_BEST_OFFERS',

                           'EX_ALL_OFFERS',

                           'EX_TRADED', 'EX_TRADED_VOL', 'EX_LTP',

                           'EX_MARKET_DEF',

                           'SP_TRADED', 'SP_PROJECTED'],

                'ladderLevels': 3},

            initial_clk=bf_market_stream.listener.initial_clk,

            clk=bf_market_stream.listener.clk)

        

        bf_market_stream.start()```

*Tags: General Technical*

---

**seaders** - *19:26:14*

I'm getting an error when it's going to that second market

*Tags: Errors Debugging*

---

**seaders** - *19:38:36*

ok, replacing the above with 

```

def betfair_stream_run(bf_market_stream):

    """

     :param betfairlightweight.streaming.BetfairStream bf_market_stream:

    """

    market_id = None

    market_ids = ['1.134151108', '1.134182080']

    last_market_ids = None



    while bfstream.continue_streaming:

        market_id = next(m_id for m_id in market_ids if m_id != market_id)



        new_market_ids = {market_id}

        subscribe_kwargs = {}

        if new_market_ids == last_market_ids:

            subscribe_kwargs.update(

                initial_clk=bf_market_stream.listener.initial_clk,

                clk=bf_market_stream.listener.clk)

        last_market_ids = new_market_ids



        print 'starting up, m_id:', market_id, \

            'subscribe_kwargs:', subscribe_kwargs



        bf_market_stream.subscribe_to_markets(

            market_filter=dict(marketIds=new_market_ids),

            market_data_filter={

                'fields': ['EX_BEST_OFFERS_DISP', 'EX_BEST_OFFERS',

                           'EX_ALL_OFFERS',

                           'EX_TRADED', 'EX_TRADED_VOL', 'EX_LTP',

                           'EX_MARKET_DEF',

                           'SP_TRADED', 'SP_PROJECTED'],

                'ladderLevels': 3},

            **subscribe_kwargs

        )



        bf_market_stream.start()```

*Tags: General Technical*

---

**liam** - *19:51:01*

If its a 'new' market and you provide clk i assume it will just provide a delta and therefore error when the marketbook attempts to be created as there is no def 

*Tags: Errors Debugging*

---

**liam** - *20:31:24*

Should probably try and catch this and raise an informative error...

*Tags: Errors Debugging*

---

**seaders** - *20:47:28*

so the actual error happens in [https://github.com/liampauling/betfair/blob/b4e85fdc595a74610f85599eb15792411e3e108c/betfairlightweight/resources/streamingresources.py#L329](https://github.com/liampauling/betfair/blob/b4e85fdc595a74610f85599eb15792411e3e108c/betfairlightweight/resources/streamingresources.py#L329)

*Tags: Errors Debugging*

---

**liam** - *20:51:22*

I assume the cache is None and the update isn't an 'img', so should be able to catch it here: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/stream.py#L124](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/stream.py#L124)

*Tags: General Technical*

---

**seaders** - *20:55:54*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1505327158000262](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1505327158000262)

*Tags: General Technical*

---

**liam** - *21:00:26*

I see, yeh because subscriptions aren't addictive I assume you just get an update. It's annoying that you dont get an error from betfair 

*Tags: Errors Debugging*

---

## 2017-09-14

**liam** - *08:14:02*

 0.15.1 released which 'should' fix the 'connection closed by server' / empty recv error sometimes seen when running a socket for a while. [@U5D4ZBEAG](@U5D4ZBEAG) I also noticed that betfair have added more data to racecard responses (currently failing) I am thinking for these non public endpoints (racecard  and inplayservice) we either add kwargs or dynamically create the class to prevent having to update every time they make a change but welcome any thoughts / PR's

*Tags: Errors Debugging, Deployment*

---

## 2017-09-15

**seaders** - *10:29:03*

```

   File "/home/ubuntu/Envs/bfstream/lib/python2.7/site-packages/betfairlightweight/endpoints/betting.py", line 158, in list_market_catalogue

    return self.process_response(response, resources.MarketCatalogue, elapsed_time, lightweight)

  File "/home/ubuntu/Envs/bfstream/lib/python2.7/site-packages/betfairlightweight/endpoints/baseendpoint.py", line 100, in process_response

    return [resource(elapsed_time=elapsed_time, **x) for x in result]

  File "/home/ubuntu/Envs/bfstream/lib/python2.7/site-packages/betfairlightweight/resources/bettingresources.py", line 248, in __init__

    self.runners = [RunnerCatalogue(**i) for i in kwargs.get('runners', [])]

TypeError: __init__() takes at least 5 arguments (4 given)```

*Tags: Errors Debugging, Strategies*

---

**seaders** - *10:31:15*

to wrap process_response in try/catch, and catch and raise any error there, but with a `invalid_response` field (or something like that) that preserves the unhandled response from BF?

*Tags: Errors Debugging*

---

**seaders** - *10:32:13*

so that when it bubbles up, we could check a `BetFairLightweightError.invalid_response` to know exactly what's breaking the response processor

*Tags: Errors Debugging*

---

**seaders** - *10:34:29*

like, per BF's documentation, [http://docs.developer.betfair.com/docs/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-RunnerCatalog](http://docs.developer.betfair.com/docs/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-RunnerCatalog) what you have should work, but they were obviously not sending one of selectionId, runnerName, handicap or sortPriority, but ew don't see that because the response is swallowed &amp; lost with the exception

*Tags: Errors Debugging, Strategies*

---

## 2017-09-19

**liam** - *09:25:58*

[@U5D4ZBEAG](@U5D4ZBEAG) so you want to raise an error but catch the response from betfair? Seems reasonable for the main endpoints. For racecard and inplayservice I think dynamically creating the class is the only way as they are constantly changing, either using the old becket type class or one of the python serialisation / marshalling libraries. 

*Tags: Errors Debugging*

---

**seaders** - *11:54:41*

you already have "_error_handler" in there for the request

*Tags: Errors Debugging*

---

## 2017-09-26

**seaders** - *17:54:49*

[@U4H19D1D2](@U4H19D1D2) is 

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L479](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L479)

correct?

*Tags: General Technical*

---

**seaders** - *17:57:33*

but in the `streaming_update`

*Tags: General Technical*

---

## 2017-10-04

**jfo** - *18:18:59*

anyone done any async callbacks in python? i’m looking at using the accepted answer here: [https://stackoverflow.com/questions/1239035/asynchronous-method-call-in-python](https://stackoverflow.com/questions/1239035/asynchronous-method-call-in-python)

*Tags: General Technical*

---

**jfo** - *18:19:09*

wondering if there are better ways in python?

*Tags: General Technical*

---

**liam** - *18:59:42*

what version of python are you using?

*Tags: General Technical*

---

## 2017-10-16

**liam** - *20:11:53*

Anyone got experience with serverless / pickling data via http? Currently have a backtesting library as a serverless function on AWS which I currently send market ids which are then downloaded from s3 and processed. However I want to be able to send ‘strategies’ which would be a list of classes. When I attempt to unpicked I get all sorts of errors about not being able to find the module. 

*Tags: Errors Debugging, Deployment*

---

## 2017-10-18

**will** - *09:24:09*

you cant pickle functions, either sending a config which can be parsed by your backtesting library into a strategy, OR, send a pointer to the strategy which already exists in your backtesting library works. You cannot pickle partial functions or lambdas.

*Tags: Strategies*

---

**liam** - *09:55:47*

Hmm I guess deploying the strategy then executing is the only option 

*Tags: Deployment, Strategies*

---

## 2017-10-28

**Unknown** - *21:28:28*

I followed the instructions for creating the cert for non interactive login. I think it did it correctly. When I try to upload as instructed, I get the above error. Has anyone seen that before? Cheers.

*Tags: Errors Debugging*

---

**seaders** - *22:22:10*

Must be a problem on BF's side, probably need to contact the help support

*Tags: General Technical*

---

**GC** - *22:27:31*

Cheers for the help [@U5D4ZBEAG](@U5D4ZBEAG)

*Tags: General Technical*

---

## 2017-10-30

**GC** - *00:55:06*

Any good documentation on EX_BEST_OFFERS vs EX_ALL_OFFERS? I can't wrap my head around it. For most of my responses in the Horse Racing market, the responses are the exact same if I switch the value.

*Tags: General Technical*

---

**Ian** - *10:04:14*

The library looks great - I’m a Delphi .net/c# dev but enjoying learning python

*Tags: General Technical*

---

**liam** - *11:14:12*

[!channel](!channel) market streaming is broken, fixing now [http://forum.bdp.betfair.com/showthread.php?t=1831&amp;page=11#106](http://forum.bdp.betfair.com/showthread.php?t=1831&amp;page=11#106)

*Tags: Errors Debugging*

---

**liam** - *11:37:41*

1.5.5 fixes the issue, [https://pypi.python.org/pypi/betfairlightweight/1.5.5](https://pypi.python.org/pypi/betfairlightweight/1.5.5)

*Tags: Errors Debugging*

---

**Unknown** - *13:09:32*

[@U5D4ZBEAG](@U5D4ZBEAG) uploaded a file: [https://betfairlightweight.slack.com/files/U5D4ZBEAG/F7R9M7GM6/pasted_image_at_2017_10_30_01_09_pm.png|Pasted image at 2017-10-30, 1:09 PM](https://betfairlightweight.slack.com/files/U5D4ZBEAG/F7R9M7GM6/pasted_image_at_2017_10_30_01_09_pm.png|Pasted image at 2017-10-30, 1:09 PM)

*Tags: General Technical*

---

**seaders** - *13:10:08*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1509368970000432](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1509368970000432)

*Tags: General Technical*

---

**seaders** - *13:10:22*

it's irritating that it doesn't error out there

*Tags: Errors Debugging*

---

**seaders** - *13:10:41*

some calls it will error out if you give it things it doesn't expect, some calls it won't

*Tags: Errors Debugging*

---

## 2017-10-31

**OT** - *08:03:10*

[@U4H19D1D2](@U4H19D1D2), would it be too much to ask of you to make a quick example of how to feed a historical file into the gofair package? I am trying to replicate the betfairlightweight historical example so I can back to back speed test the two.

*Tags: Performance*

---

## 2017-11-02

**liam** - *17:39:12*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L108](https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L108)

*Tags: General Technical*

---

**liam** - *17:45:04*

I normally add some error catching and attempt a login() if it fails

*Tags: Errors Debugging*

---

**seaders** - *17:55:23*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py#L26](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py#L26)

*Tags: Deployment*

---

**seaders** - *18:08:47*

tests fixed

*Tags: Errors Debugging*

---

**liam** - *18:42:41*

i started but got really bored so just focussed on parsing the historical data, much much quicker

*Tags: Data Quality*

---

**liam** - *18:43:01*

dont think i can see much advantage on using golang for live data

*Tags: Deployment*

---

**seaders** - *18:47:05*

I don't use the historical data too much as we record basically everything we use in our strats

*Tags: Data Quality*

---

## 2017-11-12

**magiclevinho** - *17:22:47*

Hello Mates!

I m trying to get the live scores for a particular match, but I get connection error. What I use is:

trading.in_play_service.get_scores(event_ids=[event.event.id])

I can successfully get the events, but not the scores!

*Tags: Errors Debugging, Deployment, Strategies*

---

**liam** - *17:25:46*

[@U4UUKF9LG](@U4UUKF9LG) nice you get streaming working? 

*Tags: General Technical*

---

## 2017-11-19

**OT** - *15:37:31*

is it just me or is golang project structure quite frustrating? compared to python, anyway

*Tags: General Technical*

---

## 2017-11-20

**liam** - *11:39:54*

a nasty hack could be to raise a specific error at that level and then catch it at listener.on_data then break out of the read loop

*Tags: Errors Debugging*

---

## 2017-12-02

**POR** - *12:10:37*

[@U4H19D1D2](@U4H19D1D2) Hi Liam, just found your stuff for extracting the Betfair Historical data to .csv format. I used the examplestreaminghistorical.py you have it github. I'm just wondering what I need to edit to extract all the data fields to a file, not just the specific ones like "Time,MarketId,Status,Inplay,SelectionId,LastPriceTraded". I'm not great at getting my head around monogramming.

*Tags: Data Quality*

---

**OT** - *13:51:05*

doesn't seem to cause any damage, just makes my app shutdown a bit messy with some socket errors

*Tags: Errors Debugging*

---

**seaders** - *14:51:30*

I havenae said anything because as you said, it doesn't do much damage, and it's a bit of a PITA to 100% fix

*Tags: Errors Debugging*

---

## 2017-12-03

**OT** - *11:26:06*

how about only raising the socket error when it's running, ie: ```if len(part) == 0 and self._running is True: ```

*Tags: Errors Debugging*

---

**liam** - *13:30:53*

need to add a break or continue somewhere to prevent line 222 executing which would error otherwise

*Tags: Errors Debugging*

---

**Lennart** - *17:27:22*

Hey everyone, just wanted to leave a quick message introducing myself. I come from an econ/finance background and my coding experience is limited to some data analysis in python with both financial and sports data. I'm excited to get going on betfair! Very happy to have found this repo and slack group so but thanks for that to everyone involved!

*Tags: General Technical*

---

## 2017-12-11

**Tom** - *13:44:57*

Hi Liam, Thanks for posting your betfair code on github.  I'm currently scraping data that I'll use for machine learning. I'm currently after highest price and lowest price matched data to establish ranges that I can use for machine learning on tennis data from 2016. I've downloaded a tar file which I planned to use with a python script and OS walk path. When I open the file there's thousands of subfolders and zipped files and I'm having issues trying to normalize the data to CSV.  I’ve seen your 

[https://github.com/liampauling/betfair/blob/master/examples/examplehistoricdata.py](https://github.com/liampauling/betfair/blob/master/examples/examplehistoricdata.py) file and I’m wondering since you have experience with betfair historical data if you could point me in the right direction? All I want to do really is build a list or dictionary in either json or csv to show Player names tournament name and highest lowest price matched inplay. Would you know if this data is available on the basic plan through betfair api?

Whats the best way to get betfair data into a pandas dataframe?

Your advice would be greatly appreciated! Many Thanks, Tom

*Tags: Data Quality, Feature Engineering*

---

## 2017-12-13

**aag** - *09:10:08*

[@U4H19D1D2](@U4H19D1D2) also can you help with this that it is not tolerant to network disconnect or if betfair goes down, then it doesnt reconnect

*Tags: General Technical*

---

**liam** - *09:13:26*

The data isn't perfect and yes I have seen the score update incorrectly. Betfair also offer score data (only tennis) but you need to get your appkey authorised. Not sure what you mean by disconnect, it's an http api (not streaming) so you have to call it 

*Tags: General Technical*

---

**aag** - *09:14:56*

[@U4H19D1D2](@U4H19D1D2) ok I understand, network error in case of streams, in http yes it doesnt matter

*Tags: Errors Debugging*

---

## 2017-12-14

**Tom** - *01:37:31*

Hi Liam, This is where I downloaded the data from [https://historicdata.betfair.com](https://historicdata.betfair.com)  I'm trying to oswalk the data to pull out what I need but it seems the json isn't following json standard. Have you had issues parsing betfair json before?  "json standard only allow one top level value', Is the error, have you come across that error before?

*Tags: Errors Debugging*

---

**seaders** - *01:41:34*

That error kinda seems like the file has content like "{...} {...}"

*Tags: Errors Debugging*

---

**Tom** - *01:44:30*

That's the first few lines. I formatted the json to make it easier to read. That part I posted seems fine. My IDE isn't crying. The below bit is giving me a json standard error

*Tags: Errors Debugging*

---

**Unknown** - *02:10:36*

[@U5D4ZBEAG](@U5D4ZBEAG) uploaded a file: [https://betfairlightweight.slack.com/files/U5D4ZBEAG/F8EEJVB42/image.png|image.png](https://betfairlightweight.slack.com/files/U5D4ZBEAG/F8EEJVB42/image.png|image.png)

*Tags: General Technical*

---

**seaders** - *02:17:40*

```

stream = apiclient.streaming.create_historical_stream(

    directory='/Users/seaders/Downloads/data/xds/historic/BASIC/27641921/27641921',

    listener=listener)

```

*Tags: General Technical*

---

**seaders** - *02:17:54*

and she's worked without a problem

*Tags: General Technical*

---

**seaders** - *02:20:06*

```from betfairlightweight import APIClient

from betfairlightweight.streaming import StreamListener, MarketStream





class HistoricalStream(MarketStream):

    def __init__(self, listener):

        super(HistoricalStream, self).__init__(listener)

        print('Time,MarketId,Status,Inplay,SelectionId,LastPriceTraded\n')



    def on_process(self, market_books):

        for market_book in market_books:

            for runner in market_book.runners:

                print(

                    market_book.publish_time, market_book.market_id,

                    market_book.status, market_book.inplay,

                    runner.selection_id, runner.last_price_traded or '')





class HistoricalListener(StreamListener):

    def _add_stream(self, unique_id, stream_type):

        if stream_type == 'marketSubscription':

            return HistoricalStream(self)





apiclient = APIClient('aa', 'bb', 'cc')



stream = apiclient.streaming.create_historical_stream(

    directory='/Users/seaders/Downloads/data/xds/historic/BASIC/27641921/27641921',

    listener=HistoricalListener(max_latency=1e100))

stream.start(async=False)

```

*Tags: Performance*

---

**Tom** - *02:24:28*

That's the format I get when I open it in the text editor. The problem I'm having trying to return a dict of only the data I want

*Tags: General Technical*

---

**seaders** - *02:28:54*

you're receiving batches of json, 1 per line, in the historical data files you get from BF

*Tags: Data Quality*

---

**Tom** - *02:30:29*

I added the .json manually once I got my first error

*Tags: Errors Debugging*

---

**seaders** - *02:34:46*

re-reading your initial question,

*Tags: General Technical*

---

**seaders** - *02:35:54*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1513215451000008](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1513215451000008)

*Tags: General Technical*

---

**seaders** - *02:38:57*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/betfairstream.py#L286](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/betfairstream.py#L286)

*Tags: General Technical*

---

**Tom** - *02:45:43*

I must check my packages I'm even getting an error running that tiny piece of code. "Expecting property name enclosed in double quotes:" I'll have come back to it in the evening. thanks for you help seaders:+1:

*Tags: Errors Debugging*

---

**seaders** - *02:47:24*

just remember that the whole file *isn't* json, so renaming the file ".json" ain't really helpful

*Tags: General Technical*

---

## 2017-12-15

**Unknown** - *00:38:33*

I'm going to save the headache and use the lightweight api to get the data. I am however having trouble generating an app key. I'm getting a http 404 error from the following link

404 error means server is unable to preform request? Is this service down at the moment?

*Tags: Errors Debugging, Deployment*

---

**seaders** - *00:40:29*

```

apiclient = APIClient('aa', 'bb', 'cc')



stream = apiclient.streaming.create_historical_stream(

    directory='/Users/seaders/Downloads/data/xds/historic/BASIC/27641921/27641921')

stream.start(async=False)```

*Tags: General Technical*

---

## 2017-12-16

**swt** - *20:10:41*

Hi, I would like to modify the examplehistorical.py to upload historical data to SQL. Can You help me how to print Event, Market and Selection data. I found Market name is "name", Event name is "eventName", Selection name is the "name" of runners but I could not figure out how to print them. Many thanks

*Tags: Data Quality*

---

## 2017-12-18

**liam** - *11:44:58*

[@U6PGKT04U](@U6PGKT04U) you able to be more specific? / does this help: [https://betfairlightweight.slack.com/files/U4H19D1D2/F89QVUUF9/-.py](https://betfairlightweight.slack.com/files/U4H19D1D2/F89QVUUF9/-.py)

*Tags: General Technical*

---

## 2017-12-19

**liam** - *15:46:38*

I use the strategy filter with place orders and the order stream 

*Tags: Strategies*

---

**Tom** - *23:19:26*

For example name  



   "status": "ACTIVE",

            "sortPriority": 1,

            "id": 8597476,

            "name": "Stan Wawrinka"

          },

          {

For example if I try "market_book.name" method

I get an error

 AttributeError: 'list' object has no attribute 'name'





Is there a list of definitions I can use in order to expand on the data I'm extracting?

*Tags: Errors Debugging*

---

## 2017-12-20

**liam** - *08:14:14*

If you are using pycharm use debug and have a look inside the MarketBook, there’s loads of data in there 

*Tags: Errors Debugging*

---

## 2017-12-21

**Tom** - *21:28:50*

I've noticed that eventName = none in streaming resources. I'm not sure why certain definitions in Market definitions are selected to =none. Is there any reason why certain market definitions are set to null?

*Tags: General Technical*

---

**liam** - *21:34:41*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L141](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L141)

*Tags: General Technical*

---

**liam** - *21:49:48*

Yeh the streaming side is a bit complicated, have a look at the baseclient/baseendpoint.py/baseresource.py to get an idea of how it all fits together. Its been about 2yrs, most of that has been updates from people on here to deal with betfair changing things

*Tags: General Technical*

---

## 2017-12-22

**Alexander** - *08:53:01*

Hello, I'm new here. Alexander from Sweden :)

This question isn't really about the betfairlightweight library, but I guess you know the answer. 

I have tried to understand if delayed appkey can place bets on Betfair but with delayed time, or is it only live appkey that can do that?when I read om Betfair I think that delayed appkey can place bets and then I read again and I think it can't.



And Happy Christmas to you all :smile:

*Tags: Deployment*

---

**Alexander** - *09:06:56*

Thank you for the quick reply.

Will read more about the API and this library and do lots of testing before I try to place bets. 

Was a question that was nagging in my head so I had to ask :blush:

*Tags: General Technical*

---

## 2017-12-23

**Lennart** - *18:24:48*

Hey Guys - only tangentially related to the API but I think you might be able to help - Are there any good options for getting live (or as close as possible to live) updated in-game data? I'm thinking game score, team on the pitch/court, etc. .

*Tags: Deployment*

---

## 2017-12-24

**OT** - *15:03:39*

thanks, so in historical data it's just correlated to the favourites

*Tags: Data Quality*

---

## 2017-12-31

**OT** - *09:48:53*

Does anybody else have problems downloading historical data from the API? sometimes the content of the market file is just "Error" .. Is this betfair rate-limiting me or something?

*Tags: Data Quality, Errors Debugging*

---

**liam** - *10:09:28*

Got an example? Im fixing bugs at the moment

*Tags: Errors Debugging*

---

**liam** - *10:43:07*

[@U7QLCB7HA](@U7QLCB7HA) / [@U5D4ZBEAG](@U5D4ZBEAG) made some big fixes, either of you able to check I haven't done anything stupid? Should close 3 open issues [https://github.com/liampauling/betfair/pull/147](https://github.com/liampauling/betfair/pull/147)

*Tags: Errors Debugging*

---

**OT** - *11:09:53*

Ok no errors on histrical download in the last hour. Definitely don't hit their api with more than one request at a time

*Tags: Errors Debugging*

---

**Tom** - *19:17:33*

Hi, I'm trying to modify the baseresource.py file in resources folder. and I've noticed that all import statements that start with 

from .baseresource import BaseResource is throwing this error



ModuleNotFoundError: No module named '__main__.baseresource'; '__main__' is not a package



Is there a particular reason for this?

*Tags: Errors Debugging*

---

## 2018-01-01

**Unknown** - *02:03:42*

[@U847PLYN6](@U847PLYN6) uploaded a file: [https://betfairlightweight.slack.com/files/U847PLYN6/F8LQAGH0U/-.txt|Untitled](https://betfairlightweight.slack.com/files/U847PLYN6/F8LQAGH0U/-.txt|Untitled)

*Tags: General Technical*

---

**liam** - *17:26:55*

Tom what are your trying to do? Looks like you have the working directory setup incorrectly. Regarding the other question, the variable name directory is misleading as it only does one file at a time, I think it did loop through a folder but I then changed it to make the code simpler 

*Tags: Getting Started*

---

**Tom** - *23:22:15*

[@U4H19D1D2](@U4H19D1D2) I'm trying to loop through all the text files(json objects) I've downloaded from betfair historical website and extract, time, player name event name and last traded price. I'm struggling to modify the code to get eventName and I'm still trying to figure out what I need to over-ride in order to get a loop to work to run through a file directory.  I would like to know how to extract last traded price, event name runners for all events between a date range? Is there much modifications needed to lightweight to do that?

*Tags: General Technical*

---

**Unknown** - *23:56:42*

[@U5D4ZBEAG](@U5D4ZBEAG) uploaded a file: [https://betfairlightweight.slack.com/files/U5D4ZBEAG/F8LQYLCE7/-.py|Untitled](https://betfairlightweight.slack.com/files/U5D4ZBEAG/F8LQYLCE7/-.py|Untitled)

*Tags: General Technical*

---

## 2018-01-09

**Ian** - *15:03:06*

Afternoon gents - apologies for the basic request but is there any docs referring to extracting historical data and is [http://www.betfairpromo.com/betfairsp/prices/](http://www.betfairpromo.com/betfairsp/prices/) the best place to obtain data? I presume volume etc must be recorded by oneself and a historical record built that way? I’ve looked at places such as [http://www.juststarthere.com/historical-horse-racing-price-movement-data.html](http://www.juststarthere.com/historical-horse-racing-price-movement-data.html) but not sure how ligitimate that data is as I thought BF held the copyright on it?

*Tags: Data Quality*

---

**Tom** - *22:05:22*

[@U5D4ZBEAG](@U5D4ZBEAG) Thanks so much for your help. This has helped me a lot.

*Tags: General Technical*

---

## 2018-01-10

**mbk** - *16:21:36*

I joined this channel because I am planning to use the streaming functionality and have a problem

*Tags: General Technical*

---

**mbk** - *16:24:58*

My plan is to empty the queue in a deque of fixed length 1 so that I only keep the latest from the output_queue

*Tags: Errors Debugging*

---

**mbk** - *16:27:29*

btw - can you recommend any sources that helped you design the client as it is?

*Tags: General Technical*

---

## 2018-01-11

**Ian** - *17:39:18*

OverflowError: Maximum recursion level reached  - i presume this is an exception raised in the library -

*Tags: Errors Debugging*

---

**Ian** - *17:39:37*

am i doing soemthing wrong - should I be error checking at this level?

*Tags: Errors Debugging*

---

**seaders** - *17:42:08*

I've just tried that and it ran without a problem?

*Tags: General Technical*

---

**Unknown** - *18:02:25*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RL4N9B7/-.txt|Untitled](https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RL4N9B7/-.txt|Untitled)

*Tags: General Technical*

---

**Ian** - *18:03:58*

when I run this outwith the debugger, I receive the following

*Tags: Errors Debugging*

---

**Unknown** - *18:04:20*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RGRAHT4/-.txt|Untitled](https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RGRAHT4/-.txt|Untitled)

*Tags: General Technical*

---

**Unknown** - *18:05:31*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RDT4T51/-.txt|Untitled](https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RDT4T51/-.txt|Untitled)

*Tags: General Technical*

---

**liam** - *18:07:22*

got to be ujson causing the error

*Tags: Errors Debugging*

---

**Unknown** - *18:23:22*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RE41NF5/-.py|Untitled](https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RE41NF5/-.py|Untitled)

*Tags: General Technical*

---

**Unknown** - *18:24:59*

[@U4H19D1D2](@U4H19D1D2) uploaded a file: [https://betfairlightweight.slack.com/files/U4H19D1D2/F8RE50199/-.py|Untitled](https://betfairlightweight.slack.com/files/U4H19D1D2/F8RE50199/-.py|Untitled)

*Tags: General Technical*

---

**liam** - *18:26:45*

Looks like its some sort of recursive error in the market filter function, probably due to locals but I have no idea why

*Tags: Errors Debugging*

---

**seaders** - *18:38:19*

Python version? Operating system you're on?

*Tags: General Technical*

---

**Unknown** - *18:48:31*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F8REKQ0UT/-.txt|Untitled](https://betfairlightweight.slack.com/files/U7R5CEDAL/F8REKQ0UT/-.txt|Untitled)

*Tags: General Technical*

---

**Ian** - *19:00:07*

[@U5D4ZBEAG](@U5D4ZBEAG) thanks - I’m new to python and BF - c# dev

*Tags: Getting Started*

---

**seaders** - *23:41:42*

[@U7R5CEDAL](@U7R5CEDAL) can you throw me a screenshot of your folder structure (all names of python file names &amp; folders), and I'll set up the exact same, to see if I can repro my side

*Tags: General Technical*

---

## 2018-01-12

**seaders** - *00:25:49*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1515694558000321](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1515694558000321)

do you mean 1.5.3?

*Tags: General Technical*

---

**Unknown** - *06:39:50*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F8SM6N746/screen_shot_2018-01-12_at_06.39.30.png|Screen Shot 2018-01-12 at 06.39.30.png](https://betfairlightweight.slack.com/files/U7R5CEDAL/F8SM6N746/screen_shot_2018-01-12_at_06.39.30.png|Screen Shot 2018-01-12 at 06.39.30.png)

*Tags: General Technical*

---

**Ian** - *06:51:35*

I’ve looked into this a bit more - taken a clean install of ubuntu and installed anaconda3, then bflw lib. same as MacOs - rec error when debugging. However,also same as MacOS, if I run outwith the VSCode IDE, it seems to run, I get what looks to be the correct output?

*Tags: Getting Started, Errors Debugging*

---

**Unknown** - *06:52:17*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RNM58UB/output.txt|output.txt](https://betfairlightweight.slack.com/files/U7R5CEDAL/F8RNM58UB/output.txt|output.txt)

*Tags: General Technical*

---

**Unknown** - *06:58:36*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F8SMASRE2/recorder_py.py|recorder.py](https://betfairlightweight.slack.com/files/U7R5CEDAL/F8SMASRE2/recorder_py.py|recorder.py)

*Tags: General Technical*

---

**Ian** - *06:59:46*

I’ve added in try blocks to catch any exception. When I run this as python recorder.py, it runs seemingly without error. I will keep digging - apologies for clogging up the chat

*Tags: Errors Debugging*

---

**Ian** - *07:08:29*

So, when I use PyCharm, it debugs without error so that must mean the issue is within VSCode?

*Tags: Errors Debugging*

---

**seaders** - *09:35:28*

in PyCharm / regularly: `{'lightweight': None, 'session': None, 'locale': None, 'filter': {'textQuery': 'Horse Racing'}, 'self': &lt;betfairlightweight.endpoints.betting.Betting object at 0x10dc10048&gt;}`

*Tags: Strategies*

---

**seaders** - *09:39:04*

in VSCode: `{'lightweight': None, 'session': None, 'locale': None, 'filter': {'textQuery': 'Horse Racing', 'args': {'with_orders': None, 'market_start_time': None, 'market_type_codes': None, 'market_countries': None, 'market_betting_types': None, 'in_play_only': None, 'turn_in_play_enabled': None, 'bsp_only': None, 'venues': None, 'market_ids': None, 'competition_ids': None, 'event_ids': None, 'event_type_ids': None, 'text_query': 'Horse Racing', 'args': {...}}}, 'self': &lt;betfairlightweight.endpoints.betting.Betting object at 0x10ac0f278&gt;}`

*Tags: Strategies*

---

**seaders** - *11:15:47*

So the *literal* python code, even the compiled code should be the same, as well as any underline system code, should be the same

*Tags: General Technical*

---

**seaders** - *11:17:35*

[@U7R5CEDAL](@U7R5CEDAL) it's only an issue when you're debugging

*Tags: Errors Debugging*

---

**seaders** - *11:17:55*

`Debug -&gt; Start without debugging`, and it runs right

*Tags: Errors Debugging*

---

**seaders** - *11:18:20*

so what VSCode has done is, for their 'debug' windows,

*Tags: Errors Debugging*

---

**seaders** - *11:18:41*

they've 'debug' info available, and they obviously have their own "locals" variable

*Tags: Errors Debugging*

---

**Ian** - *11:18:46*

Given I’m new to python mo code/debug workflow will be quite frequent lol

*Tags: Getting Started, Errors Debugging*

---

**seaders** - *11:19:03*

which overrides python's "locals" system one

*Tags: General Technical*

---

**seaders** - *11:21:00*

in pycharm / eclipse, all their debugging stuff is done via pydevd

*Tags: Errors Debugging*

---

**seaders** - *11:21:47*

yeah, I'm not 100% sure what level the python things are on VS

*Tags: General Technical*

---

**seaders** - *11:22:47*

anywho, you know what to do -&gt; [https://github.com/Microsoft/vscode-python/issues?utf8=%E2%9C%93&amp;q=locals](https://github.com/Microsoft/vscode-python/issues?utf8=%E2%9C%93&amp;q=locals)

*Tags: General Technical*

---

**Ian** - *11:23:11*

Yup :slightly_smiling_face: - thanks again for your help

*Tags: General Technical*

---

## 2018-01-18

**mikey155** - *12:10:47*

OT, I've been downloading historical data today successfully. I have an issue which may be related - I don't know. The only way I can get at the json data is if I open the bzip files in notepad via 7Zip - exactly the way described in the Betfair guidance on their help page. The problem is I can only do this one fille at a time. I need a way that I can get to, and open in json format, large batches of files.

*Tags: Data Quality*

---

## 2018-01-23

**Ian** - *11:22:12*

Morning all - minor update re the recursion issue I was having with vscode last week, confirmed as an issue : [https://github.com/Microsoft/vscode-python/issues/581](https://github.com/Microsoft/vscode-python/issues/581)

*Tags: General Technical*

---

## 2018-02-01

**liam** - *06:11:16*

Historical data doesn’t contain Xm prices (bdatb/bdatl) 

*Tags: Data Quality*

---

**Jonatan (skyw)** - *16:06:22*

Thanks! This helps alot! Then it will be super easy to work with!

*Tags: General Technical*

---

**Jonatan (skyw)** - *16:10:55*

although, Is there any reason why not competition_id is not implementet in market_streaming_filter as it is in market_filter. I find it hard to filter out events in english premier leuague for example

*Tags: General Technical*

---

**Jonatan (skyw)** - *17:06:20*

yeah read the document now, I wonder if the just forgot it in the documentation or lazy. Well thanks for the answares.

*Tags: General Technical*

---

**liam** - *17:35:00*

They use Kafka for streaming I think they made a decision on the structure of the streams which makes filtering by competition difficult, that’s the response I got anyway

*Tags: General Technical*

---

## 2018-02-03

**Jack** - *15:57:27*

so does your strategy look at the horses themselves or is it more so spotting patterns in how the market is moving ?

*Tags: Strategies*

---

## 2018-02-04

**Ben** - *13:33:57*

Quick question, I was trying to streaming api. It is telling me "I don't have authorisation" . Does it require some kind of approval from Betfair? 

*Tags: General Technical*

---

**liam** - *13:56:12*

Welcome yeh raise a ticket with support (clarification: your app key has to be authorised to allow streaming) 

*Tags: General Technical*

---

**Jonatan (skyw)** - *16:44:44*

there are documentation on betfairs developer page how to do it

*Tags: General Technical*

---

## 2018-02-05

**Unknown** - *15:24:24*

[@U92CASP1B](@U92CASP1B) uploaded a file: [https://betfairlightweight.slack.com/files/U92CASP1B/F94RP7SAJ/-.py|Untitled](https://betfairlightweight.slack.com/files/U92CASP1B/F94RP7SAJ/-.py|Untitled)

*Tags: General Technical*

---

**liam** - *15:26:05*

streaming?

*Tags: General Technical*

---

**NicolasW** - *16:50:56*

Hi, I can see a thread on "maximum recursion level reached" which also happens to me when I test my app in debug mode. Is there a solution to still run my app in debug mode?

*Tags: Errors Debugging*

---

**NicolasW** - *16:51:57*

INFO:betfairlightweight.streaming.stream:[Stream: None]: "MarketStream" created

Traceback (most recent call last):

  File "C:\Users\Nicolas\source\repos\BF stream test\BF stream test\BF_stream_test.py", line 47, in &lt;module&gt;

    sys.exit(int(main() or 0))

  File "C:\Users\Nicolas\source\repos\BF stream test\BF stream test\BF_stream_test.py", line 32, in main

    streaming_unique_id = stream.subscribe_to_markets(market_filter=market_filter,market_data_filter=market_data_filter,conflate_ms=10000)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 131, in subscribe_to_markets

    self._send(message)

  File "C:\Program Files (x86)\Microsoft Visual Studio\Shared\Python36_64\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 245, in _send

    message_dumped = json.dumps(message) + self.__CRLF

OverflowError: Maximum recursion level reached

Press any key to continue . . .

*Tags: Errors Debugging*

---

**seaders** - *16:54:10*

So, bflightweight uses a python thing called "locals", which is a way to get all current local variables

*Tags: General Technical*

---

**seaders** - *16:54:50*

Unfortunately VS Code overrides this, in debug mode, and puts different stuff in locals

*Tags: Errors Debugging*

---

**seaders** - *16:56:49*

The error happens in the library, but that's a horrible bug on Code's side, you should *never* be doing things like that

*Tags: Errors Debugging*

---

**jfo** - *18:01:28*

[@U4H19D1D2](@U4H19D1D2) you still runner docker in ec2?

*Tags: Deployment*

---

**jfo** - *18:08:16*

are you manually setting up the ec2 instances with docker? or are you using the container service stuff?

*Tags: Deployment*

---

**liam** - *18:10:29*

I use terraform for infra and the a mix of portainer (for monitoring, its great) and then my own deployment library using the docker python wrapper (also great) I have found that Sports trading programs don't really suit ECS / container services

*Tags: Deployment, Strategies*

---

**Unknown** - *19:20:42*

Hi, Does anybody know if Betfair historical data purchases have sportsbook data from downloads off the site? The data I have has identical selection id's but different market i.d. The event name is the same. Any idea what two markets these are? I'm guessing one is match odds would the other one be match odds sportsbook?

*Tags: Data Quality*

---

## 2018-02-08

**swt** - *20:17:22*

hi, can you help me what is the syntax to filter today markets at market_start_time?            market_catalogues = trading.betting.list_market_catalogue(

                filter=filters.market_filter(

                    event_type_ids=[recorded_id], 

                    market_countries=[mc_id], 

                    market_type_codes=[mtc_id], 

                    turn_in_play_enabled=True,

                    market_start_time=date.today().strftime('%Y-%m-%d')

                ),

                market_projection=['MARKET_START_TIME', 'RUNNER_DESCRIPTION'], 

                max_results=1000

            )

*Tags: Strategies*

---

## 2018-02-17

**Ian** - *16:44:14*

afternoon all. I'm looking at the streaming endpoint and following the exmaple at [https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**Ian** - *16:45:03*

I receive the following error when calling print(market_books)

*Tags: Errors Debugging*

---

**Unknown** - *16:45:21*

[@U7R5CEDAL](@U7R5CEDAL) uploaded a file: [https://betfairlightweight.slack.com/files/U7R5CEDAL/F9AP5LS2E/-.py|Untitled](https://betfairlightweight.slack.com/files/U7R5CEDAL/F9AP5LS2E/-.py|Untitled)

*Tags: General Technical*

---

**Ian** - *16:48:40*

I'm beginning to dig into the problem but posting it up here incase anyone has seen this following the examples

*Tags: General Technical*

---

**Ian** - *16:50:05*

stream.start(async=True) seems to be the first problem (naturally)

*Tags: General Technical*

---

**seaders** - *17:06:02*

Can't be 100% about it, but I get that problem intermittently and just assume it's something going a bit off on Betfair's side

*Tags: General Technical*

---

**seaders** - *17:07:36*

If it happens again, it sends me an error email, then waits 5 minutes and connect again

*Tags: Errors Debugging*

---

**Ian** - *17:08:41*

I can logon to BF - my key was enabled for streaming

*Tags: General Technical*

---

**seaders** - *17:12:49*

can you do a "normal" connect to BetFair API?

*Tags: General Technical*

---

**seaders** - *17:19:13*

that'd be your problem

*Tags: General Technical*

---

**Ian** - *17:22:37*

i'll submit a ticket - i just thought that streaming would be "better" than polling but maybe I should just start there..

*Tags: General Technical*

---

**seaders** - *17:23:19*

it's better to get all the info as well (logging in with both keys, and get the error messages as well)

*Tags: Errors Debugging*

---

**Ian** - *17:29:07*

in your experience performance is typically better on streaming, or not?

*Tags: Performance*

---

**seaders** - *17:29:52*

"performance"?

*Tags: Performance*

---

**seaders** - *17:30:20*

all my strategies are completely on the streaming service

*Tags: General Technical*

---

**Ian** - *17:33:26*

ok - i understand it is not an easy question to answer :slightly_smiling_face:

*Tags: General Technical*

---

**Ian** - *17:34:34*

Thanks for your help again [@U5D4ZBEAG](@U5D4ZBEAG)

*Tags: General Technical*

---

## 2018-02-21

**richard_h** - *21:20:18*

Getting SocketErrors about once a week or so, you guys have any solutions to restart things when this happens?

*Tags: Errors Debugging*

---

**liam** - *21:31:16*

[https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77](https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77)

*Tags: General Technical*

---

## 2018-02-22

**Henry** - *12:19:31*

`Exception: Certificate folder not found in /certs/`

*Tags: Errors Debugging*

---

**Henry** - *12:29:01*

Then come with `requests.exceptions.ConnectionError: ('Connection aborted.', IsADirectoryError(21, 'Is a directory'))`

*Tags: Errors Debugging*

---

**Henry** - *23:22:00*

Just wondering is data streaming api for free?

*Tags: General Technical*

---

**Unknown** - *23:38:01*

[@U9CDT999N](@U9CDT999N) uploaded a file: [https://betfairlightweight.slack.com/files/U9CDT999N/F9DKW7KAR/image.png|image.png](https://betfairlightweight.slack.com/files/U9CDT999N/F9DKW7KAR/image.png|image.png)

*Tags: General Technical*

---

**Henry** - *23:38:33*

Isn't it only paid for live data? Or Steam API is included as well

*Tags: Deployment*

---

## 2018-02-23

**liam** - *07:30:06*

host var, defaults to production 

*Tags: Deployment*

---

**agberk** - *10:33:31*

Streaming API isn't turned on by default so you need to raise a ticket with Betfair developer support asking them to activate it on your account

*Tags: General Technical*

---

## 2018-02-26

**liam** - *07:03:53*

[@U9CDT999N](@U9CDT999N) you can use lightweight to process [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**liam** - *07:11:25*

its been mentioned before I think, here is an email from August detailing the results from a questionnaire they did:

*Tags: General Technical*

---

**liam** - *07:11:27*

Hi,

 

Following the recent developer survey we’d like to provide some feedback on some of the points raised by customers both generally and in terms of additional feature requests.

 

We’ve grouped these into distinct categories:

 

Market Data &amp; Feeds

 

We are constantly exploring ways of incorporating new data feeds into the API.    Unfortunately, there are restrictions relating to the onward supply of specific data via the API (specifically football related data) but we are investigating the incorporation of other data feeds at the moment, including improved feeds for horse racing.

 

The inclusion of raceType is on the current API roadmap and we are looking to integrate this data as soon as possible but don’t have a specific ETA.

 

We are working to make improvements to the consistency of market names &amp; abbreviations across the platform. 

 

We don’t have any plans to add specific results data into the API.  Customer who require results data following market settlement can use the historical data service (via [https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home)).  This data includes the result for each runner by name within the free BASIC data files.

 

We don’t have any plans to remove the time delay associated with in-play betting.  This is in place to protect customers when betting in-play and watching transmission described as “live” that may be actually delayed.

 

Stream API

 

We have an existing backlog item to add additional filters to the Stream API (including competitionId) but no plans to make any changes to the existing filter name.

 

We don’t have any plans to change the way we conflate data via the Stream API but historical data is available via [https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home) in the same format as provided via the Stream API if required for testing &amp; analysis purposes.

 

Pricing &amp; Charges

 

We don’t have any current plans to increase/reduce the £299 fee for Live Application Keys.

 

We received some requests to include an indicator within the API relation to transaction charging.   For transaction charging, we recommend that customers count unique the number of betId’s created in a single hour (0000-0059, 0100-0159, 0200-0259).  Any transaction fees are offset by the following (Commission + Implied Commission) ÷ 2

where Implied Commission = market losses x 3% which is calculated on a daily basis. Full details can be found via [http://www.betfair.com/aboutUs/Betfair.Charges/](http://www.betfair.com/aboutUs/Betfair.Charges/) &gt; Transaction Charges.

 

If there any specific questions/queries you’d like us to follow up with you directly please get in touch via Developer Support ([https://developer.betfair.com/support/](https://developer.betfair.com/support/))

 

Kind Regards

 

Neil

Betfair Developer Program

*Tags: Data Quality, Feature Engineering, Deployment, Strategies*

---

## 2018-03-02

**liam** - *07:44:45*

[@U6XQL6FJR](@U6XQL6FJR) what error are you getting? Only listRaceDetails works without your appKey being authorised, even then its limited to Tennis. Use the inplayservice endpoint which is what the website uses.

*Tags: Errors Debugging*

---

## 2018-03-04

**Unknown** - *02:30:44*

[@U9CDT999N](@U9CDT999N) uploaded a file: [https://betfairlightweight.slack.com/files/U9CDT999N/F9J68V9L2/image.png|image.png](https://betfairlightweight.slack.com/files/U9CDT999N/F9J68V9L2/image.png|image.png)

*Tags: General Technical*

---

**Henry** - *02:34:58*

How can i actually get information of all past matches as only event ids are included in historical data and i am looking for their corresponding matches

*Tags: Data Quality*

---

## 2018-03-07

**liam** - *12:52:01*

list_market_book is listMarketBook, have you had a look at the examples? Or use a debugger (recommend pycharm) and have a look at what is in that market book response 

*Tags: Errors Debugging*

---

**Tom** - *18:19:35*

Hi, 

Do you know what credentials needs to be in the .bash_profile for logging into betfair with the api?

I have a dev key generated and I'm trying to follow the non interactive bot login documentation.



From the setup wiki this is the example for .bash_profile

export JohnSmith = "a4586nfgXY"

export usernamepassword = "password"



Should mine be?



export Tom = "zwefu45338"  # made up key

export usernamepassword = "123login" # made up password



I also have the bash profile set up in the bin folder of my virtualenv do you know is that ok for the login system to work?



Thanks,

Tom

*Tags: Getting Started*

---

**Unknown** - *18:23:48*

[@U847PLYN6](@U847PLYN6) uploaded a file: [https://betfairlightweight.slack.com/files/U847PLYN6/F9L698K0S/traceback.py|traceback](https://betfairlightweight.slack.com/files/U847PLYN6/F9L698K0S/traceback.py|traceback)

*Tags: Errors Debugging*

---

## 2018-03-08

**Tom** - *19:28:26*

[@U4H19D1D2](@U4H19D1D2) I have the vars set up in a .bash_profile like so



export appkey =“123xyz123xyz”

export password = “somepassword123”

export username = “someusername123”



I've created the .bash_profile file in betfairlightweight/certs/home

would this be correct?

*Tags: General Technical*

---

## 2018-03-10

**liam** - *08:06:19*

It’s a lightweight calculated value, how long the request took, can be ignored unless you want to monitor latency 

*Tags: Performance*

---

**liam** - *08:06:24*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/baseendpoint.py#L49](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/baseendpoint.py#L49)

*Tags: General Technical*

---

**Unknown** - *13:15:38*

[@U847PLYN6](@U847PLYN6) uploaded a file: [https://betfairlightweight.slack.com/files/U847PLYN6/F9MPG8TV2/-.py|Untitled](https://betfairlightweight.slack.com/files/U847PLYN6/F9MPG8TV2/-.py|Untitled)

*Tags: General Technical*

---

**Unknown** - *13:23:01*

[@U847PLYN6](@U847PLYN6) uploaded a file: [https://betfairlightweight.slack.com/files/U847PLYN6/F9NRTHAR5/screen_shot_2018-03-10_at_13.22.23.png|Screen Shot 2018-03-10 at 13.22.23.png](https://betfairlightweight.slack.com/files/U847PLYN6/F9NRTHAR5/screen_shot_2018-03-10_at_13.22.23.png|Screen Shot 2018-03-10 at 13.22.23.png)

*Tags: General Technical*

---

**Tom** - *13:25:48*

not sure what that belongs too. I deleted that folder and ran the code and still the same error. can someone check if test_login.py is working for them?

*Tags: Errors Debugging*

---

**Unknown** - *20:59:54*

[@U4H19D1D2](@U4H19D1D2) I fixed the unittest imports but I'm still getting an an error     raise self._error(response)

betfairlightweight.exceptions.LoginError: API login: UNKNOWN



6.9e-05



do all the certs have to be in a list or tuple named cert_files? attaching screenshot of how I've set up the certs.

*Tags: Errors Debugging*

---

**Unknown** - *21:13:04*

[@U847PLYN6](@U847PLYN6) uploaded a file: [https://betfairlightweight.slack.com/files/U847PLYN6/F9MQ06SF5/-.py|Untitled](https://betfairlightweight.slack.com/files/U847PLYN6/F9MQ06SF5/-.py|Untitled)

*Tags: General Technical*

---

**liam** - *22:02:48*

[https://underround.wordpress.com/2017/04/14/betfairlightweight-v1-0/](https://underround.wordpress.com/2017/04/14/betfairlightweight-v1-0/)

*Tags: General Technical*

---

**Tom** - *22:10:24*

[@U4H19D1D2](@U4H19D1D2) thanks for your help so far. When I run that code it says that the "Exception: Certificate folder not found in /certs/ should there be another folder one deep?

*Tags: Errors Debugging*

---

**Tom** - *22:11:34*

my certs folder is PycharmProjects/betfair/betfairlightweight/certs

*Tags: General Technical*

---

## 2018-03-12

**Tom** - *20:57:48*

[@U4H19D1D2](@U4H19D1D2) I've got the login script you sent me working when I pass in my username and password directly to APIClient( 'myuser', 'mypass', app_key=None) If I add my username and password to .bash_profile  I get this error 

betfairlightweight.exceptions.LoginError: API login: ACCOUNT_PENDING_PASSWORD_CHANGE



What's really strange is I deleted the .bash_profile at the top level directory I set up in the project and at the home directory of my mac too and there's no reference to api_key, I've api_key set to none and I'm getting a session token.  if I try to add or filter markets I get an error then that my api key is invalid. I'd like to get the bash_profile working to keep the login system as is.

*Tags: Errors Debugging*

---

**Unknown** - *20:59:05*

[@U847PLYN6](@U847PLYN6) uploaded a file: [https://betfairlightweight.slack.com/files/U847PLYN6/F9N47MJD7/screen_shot_2018-03-12_at_20.58.24.png|Screen Shot 2018-03-12 at 20.58.24.png](https://betfairlightweight.slack.com/files/U847PLYN6/F9N47MJD7/screen_shot_2018-03-12_at_20.58.24.png|Screen Shot 2018-03-12 at 20.58.24.png)

*Tags: General Technical*

---

**Jonatan (skyw)** - *21:10:56*

Have you tried google your error message?

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *21:12:11*

It is superclear if you try to read instead of just blindly asking questions.

*Tags: General Technical*

---

**Tom** - *21:15:35*

[@U92CASP1B](@U92CASP1B)  yes I have. The error message is saying change my password. My password is fine. I'm logged in so I've no idea why the api is triggering that error

*Tags: Errors Debugging*

---

**Tom** - *21:22:24*

On liams Github? Yes. I can login by passing my credentials to trading = betfairlightweight.APIClient(' ', ' ', ' ')  But I can't login using the .bash_profile method.

*Tags: Strategies*

---

## 2018-03-14

**klozovin** - *11:43:52*

How often does Betfair Streaming send data down the pipe? I can't find anything in the documentation... I'm guessing it has to aggregate data on some timescale? Milisecond? Couple of milis?

*Tags: General Technical*

---

## 2018-03-18

**erlend** - *10:53:19*

Noob question but if I want to aggregate historical data, should I use [https://github.com/liampauling/betfairdata](https://github.com/liampauling/betfairdata) or [https://github.com/liampauling/betfair](https://github.com/liampauling/betfair)?

*Tags: Data Quality*

---

**liam** - *15:24:31*

Betfairdata doesn’t work since they removed it from the site, you have to use the historical streaming data they now provide 

*Tags: General Technical*

---

## 2018-03-19

**mbk** - *16:14:07*

hey guys, I have been using the stream and am generally happy with it. I start the stream with async=True and then snap prices whenever necessary. However, sometimes there is a connection error and the underlying stream stops. From that moment on, the prices are all stale. What would you say is the best way to deal with situations like that?

*Tags: Errors Debugging*

---

## 2018-03-20

**liam** - *07:21:01*

[@U8RT48D8X](@U8RT48D8X) when using the stream you can’t just thread it off and forgot about it, you need to handle any errors and resubscribe or subscribe again if required 

*Tags: Errors Debugging*

---

**liam** - *07:23:02*

Here is an example of one pattern [https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1505305250000059](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1505305250000059)

*Tags: General Technical*

---

**liam** - *07:24:18*

Or to resubscribe instead: [https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77](https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77)

*Tags: General Technical*

---

## 2018-03-23

**istb** - *14:23:38*

First of all, thanks for making such an easy to use package :slightly_smiling_face:. Does anyone mind if I fix [https://github.com/liampauling/betfair/issues/165](https://github.com/liampauling/betfair/issues/165)?

*Tags: Errors Debugging*

---

**liam** - *14:26:59*

Hi mate, I was going to have a look at this tonight, you mention stopping threads (you can’t do this in python) We has this issue before and I though it was fixed but should be able to mimic the following but then I am surprised closing the socket is returning empty rather than raising an error



 [https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/betfairstream.py#L214](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/betfairstream.py#L214)

*Tags: Errors Debugging*

---

**liam** - *14:28:04*

[https://stackoverflow.com/questions/323972/is-there-any-way-to-kill-a-thread-in-python](https://stackoverflow.com/questions/323972/is-there-any-way-to-kill-a-thread-in-python)

*Tags: General Technical*

---

**liam** - *14:28:17*

Hence why we decided to use an if running raise error

*Tags: Errors Debugging*

---

**istb** - *14:31:10*

If you kept track of all the threads in the class, and set ```self._running = False``` then call  t.join() for each t  where t is a thread, then close the socket, I think this would fix it?

*Tags: Errors Debugging*

---

**istb** - *14:46:12*

yeah that fixes it

*Tags: Errors Debugging*

---

**Unknown** - *14:48:22*

[@U9UB05M3J](@U9UB05M3J) uploaded a file: [https://betfairlightweight.slack.com/files/U9UB05M3J/F9W4H2QR5/proposed_fix_for_stream_stop__.py|proposed fix for stream.stop()](https://betfairlightweight.slack.com/files/U9UB05M3J/F9W4H2QR5/proposed_fix_for_stream_stop__.py|proposed fix for stream.stop())

*Tags: Errors Debugging*

---

**istb** - *15:02:55*

ty for your help [@U4H19D1D2](@U4H19D1D2)

*Tags: General Technical*

---

**Unknown** - *15:16:03*

[@U9UB05M3J](@U9UB05M3J) uploaded a file: [https://betfairlightweight.slack.com/files/U9UB05M3J/F9W55PUNB/pasted_image_at_2018_03_23_03_15_pm.png|Pasted image at 2018-03-23, 3:15 PM](https://betfairlightweight.slack.com/files/U9UB05M3J/F9W55PUNB/pasted_image_at_2018_03_23_03_15_pm.png|Pasted image at 2018-03-23, 3:15 PM)

*Tags: General Technical*

---

**Unknown** - *15:16:22*

[@U9UB05M3J](@U9UB05M3J) uploaded a file: [https://betfairlightweight.slack.com/files/U9UB05M3J/F9UHLC20H/pasted_image_at_2018_03_23_03_16_pm.png|Pasted image at 2018-03-23, 3:16 PM](https://betfairlightweight.slack.com/files/U9UB05M3J/F9UHLC20H/pasted_image_at_2018_03_23_03_16_pm.png|Pasted image at 2018-03-23, 3:16 PM)

*Tags: General Technical*

---

**istb** - *15:23:39*

Ah I see, I am not sure how to do that

*Tags: General Technical*

---

## 2018-03-26

**marco** - *17:28:05*

Hi, can someone please point me to the documentation regarding the certs mentioned in the github repository which are required to login?

*Tags: General Technical*

---

**marco** - *19:10:33*

I was trying to run the code regarding streaming in the github README. I get "betfairlightweight.exceptions.SocketError: Connection closed by server" on the last line. I wonder if this is supposed to work with a test app key which is what I have...

*Tags: Errors Debugging, Deployment*

---

**liam** - *19:11:27*

Any other errors on the DEBUG logs? But I am fairly sure it won't work with the delayed key unless authorised

*Tags: Errors Debugging*

---

**liam** - *19:13:29*

*authorised for streaming

*Tags: General Technical*

---

**liam** - *19:14:36*

although you should get a 'app key not authorised' error if used correctly

*Tags: Errors Debugging*

---

**marco** - *19:17:30*

it is verbatim what is in the "streaming" section of the README

*Tags: General Technical*

---

**liam** - *19:18:38*

I recommend running the example as you are probably not starting the stream quick enough (10s timeout between you running subscribe and start or it will error)

*Tags: Errors Debugging*

---

**liam** - *19:18:39*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**marco** - *19:22:37*

liam, I got the correct error now:  NOT_AUTHORIZED: AppKey is not configured for service

*Tags: Errors Debugging*

---

## 2018-03-27

**liam** - *12:42:38*

I agree with making it clean but then I don’t want to hide errors from the user 

*Tags: Errors Debugging*

---

**istb** - *12:48:22*

I used pip install to install initially (which is when I got the issue):

```In [1]: import betfairlightweight



In [2]: betfairlightweight.__version__

Out[2]: '1.6.0'

```

But when making the changes I cloned from github master. Which is also 1.6.0.

Perhaps they are slightly different?

*Tags: Getting Started*

---

**liam** - *14:14:13*

It is when streaming, don’t think it’s present with snap though 

*Tags: General Technical*

---

## 2018-04-01

**seaders** - *21:35:15*

```  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/streaming/betfairstream.py", line 60, in start

    self._read_loop()

  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/streaming/betfairstream.py", line 198, in _read_loop

    self._data(received_data)

  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/streaming/betfairstream.py", line 231, in _data

    if self.listener.on_data(received_data) is False:

  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/streaming/listener.py", line 115, in on_data

    self._on_change_message(data, unique_id)

  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/streaming/listener.py", line 146, in _on_change_message

    self.stream.on_update(data)

  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/streaming/stream.py", line 55, in on_update

    self._process(data[self._lookup], publish_time)

  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/streaming/stream.py", line 154, in _process

    order_book_cache.update_cache(order_book, publish_time)

  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/resources/streamingresources.py", line 506, in update_cache

    runner.update_unmatched(order_changes['uo'])

  File "/home/ubuntu/virtualenvs/lbbsports/lib/python3.6/site-packages/betfairlightweight/resources/streamingresources.py", line 476, in update_unmatched

    self.unmatched_orders.append(UnmatchedOrder(**unmatched_order))

TypeError: __init__() got an unexpected keyword argument 'lsrc'```

*Tags: Errors Debugging*

---

## 2018-04-02

**liam** - *13:00:32*

If you set logging to DEBUG what does the full message look like?

*Tags: Errors Debugging*

---

## 2018-04-03

**liam** - *19:08:25*

v1.6.1 which fixes this but it doesn't output the value into the stream yet

*Tags: Errors Debugging*

---

**liam** - *19:56:02*

its unbelievable isn't it, testing something but it ended up in production and they didn't even realise!

*Tags: Deployment*

---

## 2018-04-05

**OT** - *12:27:41*

does the historical data api have some kind of rate limiting on it? because often I just get bz2 files with no market content

*Tags: Data Quality*

---

## 2018-04-11

**OT** - *10:39:24*

betfair have fixed up the historical data api.. working great, now.

*Tags: Data Quality, Errors Debugging*

---

**OT** - *17:33:54*

of course not! just the ol' "we found an error and fixed it"

*Tags: Errors Debugging*

---

## 2018-04-26

**mbk** - *20:46:59*

hey guys, a while ago I asked about how best to use the stream in async mode, and Liam suggested I follow a pattern like [https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1505305250000059](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1505305250000059). Alternatively he pointed to [https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77](https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77). Broadly my implementation is to follow the example from agberk, where I run the `betfair_stream_run` in it's own thread. However, with that approach I can not call `stop` on the stream because that's inside of another thread. So how could I then disconnect from the betfair socket?

*Tags: General Technical*

---

**mbk** - *20:47:07*

thanks in advance for your help :slightly_smiling_face:

*Tags: General Technical*

---

## 2018-05-01

**erlend** - *21:50:21*

Hi, has anyone found a good way to use market catalogue data with streaming?

*Tags: General Technical*

---

## 2018-05-05

**favetelinguis** - *21:04:43*

I just tried to use betfairlightweight with historic BASIC data and found it very odd that the first message does not contain the complete market book state, it is just a bunch of delta messages. How are you using the historic data if you dont know the entire start state in the first message, you sort of need to consume enogh messages for all runners to have gotten a new delta message?

*Tags: Data Quality*

---

## 2018-05-06

**favetelinguis** - *07:10:45*

not an issue with betfairlightweight just a complaint about the data betfair sells and a question how others are dealing with it :slightly_smiling_face:

*Tags: General Technical*

---

**favetelinguis** - *07:14:07*

hehe no problem I might be off with my question, i will explain my thoghts better 1sec

*Tags: General Technical*

---

**favetelinguis** - *07:15:03*

Lets say you have a race with 10runners, when you use the betfair API the fist message you get contains information about all the runners right?

*Tags: General Technical*

---

**liam** - *07:19:08*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/stream.py#L124](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/stream.py#L124)

*Tags: General Technical*

---

**liam** - *07:24:23*

Which markets? I use flumine to store data straight to s3 and it costs me pennies 

*Tags: General Technical*

---

**favetelinguis** - *07:27:28*

wow that would be very nice, format is not an issue i can fix that as long as the data is there

*Tags: Errors Debugging*

---

**liam** - *07:29:32*

I can give you an AWS session key with access to just that bucket and you can download, probably tomorrow though (when his headache has gone)

*Tags: Deployment*

---

**liam** - *07:33:36*

No problem, if you could provide an index in return, ie what markets are actually in there and if there are any empties/corrupt as I know there will be a few days missing that would be great 

*Tags: General Technical*

---

**liam** - *07:36:13*

Yep just have flumine running full time, uses hardly any ram or cpu. From market open to finish all data

*Tags: General Technical*

---

**favetelinguis** - *07:37:07*

on the test API i guess betfair would not allow that on the production endpoints?

*Tags: Deployment*

---

**liam** - *07:37:47*

Production but using an account I also trade on 

*Tags: Deployment*

---

## 2018-05-12

**richard** - *15:27:00*

Hi all, I'm trying to add stop losses to my bot. I'm wondering if it's possible to add a back bet at less than the market rate, so if the market ever falls to that level it will automatically get matched? - is this or any other stop loss type method possible on the betfair API?

*Tags: General Technical*

---

## 2018-05-15

**Chris** - *10:14:08*

Hi all, just joined this group and getting my head round the Betfair historical data

*Tags: Data Quality*

---

## 2018-05-16

**erlend** - *14:49:20*

Any idea how to get the market catalogues for historical data? When I run list_market_catalogue() with my list of downloaded market_id's it just comes back empty.

*Tags: Data Quality*

---

## 2018-05-25

**OT** - *10:56:57*

[@U9RMY3JHK](@U9RMY3JHK) I didn;t know 3rd parties sold that stuff. does it come with betfair IDs mapped? I'd love some runner metadata to go with the historical data :smile:

*Tags: Data Quality*

---

## 2018-05-27

**Disco** - *10:42:19*

Hello. Does anybody here (_except Liam_) use *flūmine* on regular basis? Need some help with setting it up (particularly, getting selected event types. I'm new in API but think that maybe somebody has the code so there won't be a need to reinvent the wheel).

*Tags: General Technical*

---

## 2018-05-28

**OT** - *12:08:50*

[@U4H19D1D2](@U4H19D1D2) ok thanks. I tried to fix it before but i was loathe to load it up with if statements or add more classes. i'll see what you think before I proceed

*Tags: Errors Debugging*

---

**OT** - *13:38:57*

pushed to fix that stupid error on issue 168. good lesson that get() can be dangerous because it fails silently

*Tags: Errors Debugging*

---

**liam** - *14:14:59*

1.6.2 pushed, minor fixes, thanks [@U7QLCB7HA](@U7QLCB7HA)

*Tags: Errors Debugging*

---

## 2018-05-30

**Jonatan (skyw)** - *23:10:08*

I might be missing something but how do we catch exceptions from the stream thread?

*Tags: Errors Debugging*

---

## 2018-05-31

**liam** - *05:49:44*

[https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77](https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77)

*Tags: General Technical*

---

**Mihail** - *13:04:46*

Good afternoon, can someone tell me how to receive real-time game events?

*Tags: General Technical*

---

## 2018-06-01

**Rory** - *14:44:25*

firstly, nice work on the Python wrapper. It's a real time saver ...

*Tags: General Technical*

---

**Rory** - *14:45:00*

quick question ... is it possible to get historical score/timeline data?

*Tags: General Technical*

---

**Jonatan (skyw)** - *23:52:14*

Thanks liam, General question which probably have been up before, how do everyone handle subscription to new markets, continously updating new markets as they come.

*Tags: General Technical*

---

## 2018-06-02

**liam** - *07:02:00*

Streaming does that for you unless I have misunderstood?

*Tags: General Technical*

---

**Jonatan (skyw)** - *13:36:00*

I guess you are running in async mode then reguluary checking changes if change, stop stream and subscribe to new updated markets? If that is the case how do you catch exceptions from betfair thread?

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *13:37:11*

Im going tro setup something with message queues later and spawn a thread that starts the stream in sync mode to catch those exceptions.

*Tags: Getting Started, Errors Debugging*

---

## 2018-06-06

**Mihai** - *20:53:50*

Hello guys, I'm new to Python &amp; after trying the steps mentioned here: [https://underround.wordpress.com/2017/07/05/historical-data/](https://underround.wordpress.com/2017/07/05/historical-data/) 

-&gt; I got the following error: AttributeError: 'APIClient' object has no attribute 'historical'. I've adapted both codes using the code from examplestreaminghistorical.py &amp; 



-&gt; After running the first code (updated based on xamplestreaminghistorical.py) I only get: 

INFO:betfairlightweight.streaming.stream:[Stream: None]: "MarketStream" created

INFO:betfairlightweight.streaming.stream:[MarketStream: HISTORICAL] 1.136292168 added



-&gt; After running the second code I get the below error:

Traceback (most recent call last):

  File "historic_data_2.py", line 44, in &lt;module&gt;

    listener=listener

  File "C:\Users\jdoe\AppData\Local\laragon\bin\python\python-2.7.13\lib\site-packages\betfairlightweight\endpoints\streaming.py", line 58, in create_historical_stream

    listener.register_stream('HISTORICAL', 'marketSubscription')

  File "C:\Users\jdoe\AppData\Local\laragon\bin\python\python-2.7.13\lib\site-packages\betfairlightweight\streaming\listener.py", line 25, in register_stream

    self.stream = self._add_stream(unique_id, operation)

  File "historic_data_2.py", line 33, in _add_stream

    unique_id, self.output_queue, self.max_latency, self.lightweight

  File "historic_data_2.py", line 16, in __init__

    super(HistoricalStream, self).__init__(unique_id, output_queue, max_latency, lightweight)

TypeError: __init__() takes exactly 2 arguments (5 given)



Could you please help? I'm trying to get the files in a readable format.

*Tags: Getting Started, Errors Debugging, Performance*

---

**Ben** - *21:23:12*

I think there is a problem in your tutorial is on the custom HistoricalStream class

*Tags: General Technical*

---

**Ben** - *21:23:30*

super(HistoricalStream, self).__init__(unique_id, output_queue, max_latency, lightweight) try to replace by:

*Tags: Performance*

---

**Ben** - *21:24:05*

and def __init__(self, unique_id, output_queue, max_latency, lightweight): just above by

*Tags: Performance*

---

**Mihai** - *21:25:06*

Ok.. so I managed to get an output using only the file from examplestreaminghistorical.py.

*Tags: General Technical*

---

**Ben** - *21:25:30*

and also return HistoricalStream(

                unique_id, self.output_queue, self.max_latency, self.lightweight

            ) by HistoricalStream(self)

*Tags: Performance*

---

**Ben** - *21:26:13*

yes this example is simple in examplestreaminghistorical

*Tags: General Technical*

---

**Unknown** - *21:29:56*

[@UB28PPRBJ](@UB28PPRBJ) uploaded a file: [https://betfairlightweight.slack.com/files/UB28PPRBJ/FB29M8V1N/image.png|image.png](https://betfairlightweight.slack.com/files/UB28PPRBJ/FB29M8V1N/image.png|image.png)

*Tags: General Technical*

---

## 2018-06-07

**liam** - *05:30:37*

market_def.name / event_name  [https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L128](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L128)

*Tags: General Technical*

---

**Mihai** - *18:55:16*

I've tried integrating the above on examplestreaminghistorical.py with no success. Any ideas, please?

*Tags: General Technical*

---

**liam** - *20:00:09*

Any errors? / code??

*Tags: Errors Debugging*

---

## 2018-06-10

**Unknown** - *10:42:32*

[@U5Q3P94NT](@U5Q3P94NT) uploaded a file: [https://betfairlightweight.slack.com/files/U5Q3P94NT/FB4A3EC1X/image.png|image.png](https://betfairlightweight.slack.com/files/U5Q3P94NT/FB4A3EC1X/image.png|image.png)

*Tags: General Technical*

---

**liam** - *10:44:07*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/filters.py#L100](https://github.com/liampauling/betfair/blob/master/betfairlightweight/filters.py#L100)

*Tags: General Technical*

---

**Unknown** - *10:57:28*

[@U5Q3P94NT](@U5Q3P94NT) uploaded a file: [https://betfairlightweight.slack.com/files/U5Q3P94NT/FB45PKHQQ/image.png|image.png](https://betfairlightweight.slack.com/files/U5Q3P94NT/FB45PKHQQ/image.png|image.png)

*Tags: General Technical*

---

## 2018-06-13

**klozovin** - *23:46:44*

Docs for the `Runner` type say that the `adjustmentFactor` is required, but I'm getting responses without that field. 



Is it just a error in the docs, or is there more to it?

*Tags: Errors Debugging*

---

## 2018-06-18

**mbk** - *13:16:36*

According to the documentation, this uses the default roll-up model STAKE, with roll-up of minimum stake:

[http://docs.developer.betfair.com/docs/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-ExBestOffersOverrides](http://docs.developer.betfair.com/docs/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-ExBestOffersOverrides)

*Tags: Strategies*

---

**mbk** - *13:16:52*

my question is: does anybody know what minimum stake is? I can't find where it's defined

*Tags: General Technical*

---

**liam** - *13:35:02*

sorry you said streaming

*Tags: General Technical*

---

**liam** - *13:43:27*

and here: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/metadata.py#L66](https://github.com/liampauling/betfair/blob/master/betfairlightweight/metadata.py#L66)

*Tags: General Technical*

---

## 2018-07-04

**Unknown** - *11:16:45*

[@U5D4ZBEAG](@U5D4ZBEAG) uploaded a file: [https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBJMHNAEP/api-referenceguide-offlineversion-22ndjuly2015.pdf|API-ReferenceGuide-OfflineVersion-22ndJuly2015.pdf](https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBJMHNAEP/api-referenceguide-offlineversion-22ndjuly2015.pdf|API-ReferenceGuide-OfflineVersion-22ndJuly2015.pdf)

*Tags: General Technical*

---

**Jonatan (skyw)** - *20:53:56*

Solved it.

*Tags: General Technical*

---

## 2018-07-05

**klozovin** - *17:06:39*

It logs me in successfully even if supply gibberish keys, but the header is required... otherwise I get `INPUT_VALIDATION_ERROR`

*Tags: Errors Debugging*

---

## 2018-07-09

**klozovin** - *16:26:54*

[@U93H3483E](@U93H3483E) yeah, AWS and Linode are okay for that, that's what I'm planning. I was just wondering about Sporting Servers because they promise low latency, maybe lower than one can achieve with just a VPS in the UK

*Tags: Performance, Deployment*

---

**liam** - *16:41:23*

Im getting a lot streaming errors this afternoon, anyone else?

*Tags: Errors Debugging*

---

**favetelinguis** - *19:22:40*

the betfair developer docs has been down also, lots of problems

*Tags: General Technical*

---

## 2018-07-10

**Jonatan (skyw)** - *00:41:33*

[https://github.com/betfair/stream-api-sample-code/blob/master/stream-api-specification.pdf](https://github.com/betfair/stream-api-sample-code/blob/master/stream-api-specification.pdf)



Here is the pdf for streaming api from 2017 atleast

*Tags: General Technical*

---

**Ben** - *08:57:21*

hey guys i had a question, do you guys know a good database of historical data on horse racing? free/cheap?

*Tags: Data Quality*

---

## 2018-07-18

**liam** - *10:56:18*

The documentation site ([http://docs.developer.betfair.com|docs.developer.betfair.com](http://docs.developer.betfair.com|docs.developer.betfair.com)) is expected to be up and running within 1-2 weeks.

 

The main [http://developer.betfair.com|developer.betfair.com](http://developer.betfair.com|developer.betfair.com) site is likely to be unavailable for a further 4-6 weeks

*Tags: General Technical*

---

**Unknown** - *14:03:00*

[@U5D4ZBEAG](@U5D4ZBEAG) uploaded a file: [https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBSQ9SXSR/untitled.mov|Untitled.mov](https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBSQ9SXSR/untitled.mov|Untitled.mov)

*Tags: General Technical*

---

**klozovin** - *14:12:33*

and arguably, being on the edge of legality is what helped them to offer the best product :slightly_smiling_face:

*Tags: General Technical*

---

**seaders** - *14:20:24*

yep yep - Backend is all Python (web-framework - Flask, database: sql-alchemy on MySQL on Amazon RDS), then the frontend is mainly jquery-easyui and a little select2

*Tags: General Technical*

---

**klozovin** - *14:21:53*

Cool! Do you find Python is fast enough for your usecase?

*Tags: General Technical*

---

**klozovin** - *14:23:34*

So are you using the streaming API or request/response?

*Tags: General Technical*

---

**seaders** - *14:23:55*

also, from the stuff I'ma doin', it's all Python 3 with queues up the wazoo and all DB stuff lazy'd/delayed

*Tags: General Technical*

---

**seaders** - *14:23:58*

streaming API

*Tags: General Technical*

---

**liam** - *14:37:40*

Surely they hedge daily? Or does it not include markets that haven’t completed?



[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1531919211000348](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1531919211000348)

*Tags: General Technical*

---

**seaders** - *14:40:15*

I was talking with a guy who helps creates the horse odds for PaddyPower, and he was talking about like 3 GIANT bets they made

*Tags: General Technical*

---

**seaders** - *15:03:20*

end up getting that, and working there for 3 years, then got bored and applied to a few places as a Python / Ruby dev

*Tags: General Technical*

---

**seaders** - *15:31:56*

I do have a few buddies I've met along the way that help out for a lil while every now and again, but yeah, I'm the only dev, or dev-like person

*Tags: General Technical*

---

**Rory** - *15:54:26*

there's obviously a fuzzy matching module for Python that I haven't got around to trying, that may help with the aliases

*Tags: General Technical*

---

**Unknown** - *15:56:42*

[@U5D4ZBEAG](@U5D4ZBEAG) uploaded a file: [https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBTPYKTRV/image.png|image.png](https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBTPYKTRV/image.png|image.png)

*Tags: General Technical*

---

**Unknown** - *16:01:01*

[@U5D4ZBEAG](@U5D4ZBEAG) uploaded a file: [https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBTQ2LMKR/image.png|image.png](https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBTQ2LMKR/image.png|image.png)

*Tags: General Technical*

---

**seaders** - *16:52:59*

bits, yeah.  you may have seen my whinge in [#C4H05KKMY|random](#C4H05KKMY|random) that BF has the "vanilla" /first round of fixtures up

*Tags: Errors Debugging*

---

**seaders** - *16:54:34*

I've a few viewers for those things that both serve as an aggregate viewer for those "debug" things, but also for overround opportunities across platforms

*Tags: Errors Debugging*

---

**klozovin** - *16:55:52*

Yeah, as I've started it - name matching will be first class service, so that I can get logging and debugging, etc... it's just too important to be tucked away with no introspection

*Tags: Errors Debugging*

---

## 2018-07-23

**seaders** - *16:27:40*

I put in a week, or 2s work of taking over the async my stuff about a year ago, and doing some funky custom stuff with the queues, and basically haven't had a problem since

*Tags: General Technical*

---

**liam** - *16:30:59*

yeah but my backtesting framework is now single threaded so repeatable and runs of a generator spitting out update one by one (historical data), I know there is a word for this but can't remember

*Tags: Data Quality*

---

## 2018-07-24

**Unknown** - *10:17:22*

[@UBS7QANF3](@UBS7QANF3) shared a file: [https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBJMHNAEP/api-referenceguide-offlineversion-22ndjuly2015.pdf|API-ReferenceGuide-OfflineVersion-22ndJuly2015.pdf](https://betfairlightweight.slack.com/files/U5D4ZBEAG/FBJMHNAEP/api-referenceguide-offlineversion-22ndjuly2015.pdf|API-ReferenceGuide-OfflineVersion-22ndJuly2015.pdf)

*Tags: General Technical*

---

**Mo** - *10:24:32*

[https://betfairdevelopersupport.zendesk.com/hc/en-us/articles/360006589232-How-do-I-access-the-Betfair-Exchange-API-Stream-API-Documentation-](https://betfairdevelopersupport.zendesk.com/hc/en-us/articles/360006589232-How-do-I-access-the-Betfair-Exchange-API-Stream-API-Documentation-)

*Tags: General Technical*

---

**liam** - *20:54:37*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L171](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L171)

*Tags: Strategies*

---

**Jonatan (skyw)** - *20:55:55*

Ah sry I mean in the streaming api

*Tags: General Technical*

---

## 2018-07-26

**liam** - *11:06:18*

so to fix I added that if statement

*Tags: Errors Debugging*

---

**erlend** - *19:52:06*

Hi, does anyone know how to filter out harness racing from the horse racing markets?

*Tags: General Technical*

---

## 2018-07-30

**Ian** - *14:11:35*

Hey all,  fresh box, trying to get dev env up and running. I receive the following error when I try and import bflw

*Tags: Errors Debugging*

---

**Ian** - *14:11:56*

(casbi) ian@KYOFBU C:\code\casbi\casbi

$ python casbi.py

Traceback (most recent call last):

  File "casbi.py", line 5, in &lt;module&gt;

    import betfairlightweight

  File "C:\Users\ian\Envs\casbi\lib\site-packages\betfairlightweight\__init__.py", line 3, in &lt;module&gt;

    from .apiclient import APIClient

  File "C:\Users\ian\Envs\casbi\lib\site-packages\betfairlightweight\apiclient.py", line 2, in &lt;module&gt;

    from . import endpoints

  File "C:\Users\ian\Envs\casbi\lib\site-packages\betfairlightweight\endpoints\__init__.py", line 4, in &lt;module&gt;

    from .betting import Betting

  File "C:\Users\ian\Envs\casbi\lib\site-packages\betfairlightweight\endpoints\betting.py", line 277

    customer_strategy_ref=None, async=None, session=None, lightweight=None):

                                    ^

SyntaxError: invalid syntax



(casbi) ian@KYOFBU C:\code\casbi\casbi

$

*Tags: Errors Debugging, Strategies*

---

**liam** - *14:24:44*

version? python version? system info?

*Tags: General Technical*

---

**Ian** - *14:30:18*

(casbi) ian@KYOFBU C:\code\casbi\casbi       

$ ver                                        

                                             

Microsoft Windows [Version 10.0.17134.191]   

                                             

(casbi) ian@KYOFBU C:\code\casbi\casbi       

$ python --version                           

Python 3.7.0                                 

                                             

(casbi) ian@KYOFBU C:\code\casbi\casbi       

$ pip freeze                                 

betfairlightweight==1.7.0                    

certifi==2018.4.16                           

chardet==3.0.4                               

ciso8601==2.0.1                              

idna==2.7                                    

requests==2.19.1                             

ujson==1.35                                  

urllib3==1.23                                

                                             

(casbi) ian@KYOFBU C:\code\casbi\casbi       

$

*Tags: General Technical*

---

## 2018-07-31

**t1234** - *09:56:15*

Where did the API documentation go? [http://docs.developer.betfair.com](http://docs.developer.betfair.com) not responding

*Tags: General Technical*

---

## 2018-08-03

**Matt P** - *13:29:38*

With regards to my employer, only a couple of years ago it actually used to be gross misconduct to bet at work. Fortunately now though they're much more relaxed and betting from the office isn't a problem

*Tags: Strategies*

---

**seaders** - *13:42:28*

One of the big things BF have a problem with is you re-publishing their odds.  If you're not doing that, nor hinting at doing that, they *shouldn't* have a problem with it. If ever you want to re-publish their odds, you need to agree very specific licensing terms with them

*Tags: General Technical*

---

## 2018-08-12

**Rob** - *10:51:53*

I had a quick question - I haven't used the streaming API before, but for the approach I'm taking I'm quite happy to get data regularly, say, the state of each horse race every 1 second

*Tags: General Technical*

---

**Rob** - *10:53:03*

is there any reason to use the streaming API in this case? or is it easier to stick with the request based API?

*Tags: General Technical*

---

**liam** - *12:48:26*

Just set conflation to desire update time, streaming is much nicer on CPU, cleaner and less code. Just make sure you catch errors 

*Tags: Errors Debugging*

---

**Rob** - *18:41:19*

thanks [@U4H19D1D2](@U4H19D1D2) - I've got some code streaming the data now. Are there any obvious errors that I should definitely look out for?

*Tags: Errors Debugging*

---

**liam** - *19:04:43*

There has been a few discussions regarding this (have a search) but this is what I do (in a separate thread) [https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77](https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L77)

*Tags: General Technical*

---

**Rob** - *22:52:28*

thanks - that looks useful (in fact, flumine is close to what I'm trying to do myself, so will take a look)

*Tags: General Technical*

---

**Rob** - *22:54:11*

once you've made a call to `self._socket.start(async=False)` or equivalent, are there occasions where problems occur that can also be caught? Or is it a case of checking for that some other way, then restarting the stream if needed?

*Tags: General Technical*

---

## 2018-08-13

**liam** - *06:41:51*

Not anymore, streaming has been tested / used a lot, I must process about a million updates + a day and have done since May 2017 

*Tags: General Technical*

---

## 2018-08-16

**Tom** - *20:37:23*

Hi, I'm trying to modify this code to use lightweight in order to gather tennis data from betfair api on a cronjob and save to a model. I'm having an isssue with getting competition names from list.market.catalogue, I'm getting the following error AttributeError: 'tuple' object has no attribute 'competition' Can anyone help? Code and stacktrace posted below. Thanks

*Tags: Errors Debugging, Strategies*

---

**liam** - *21:00:24*

Recommend getting pycharm and debugging if you haven’t already 

*Tags: Errors Debugging*

---

**Tom** - *21:07:28*

Hi Liam, I have pycharm community but I'm calling this script from a django base command and django isn't supported in the community edition so debugging isn't doing me much help:(

*Tags: Errors Debugging*

---

**Tom** - *21:15:17*

The original code was

                m = next((x for x in markets if x.market_id == b.market_id) , None)

But I was getting the following error

AttributeError: 'dict' object has no attribute 'market_id'

*Tags: Errors Debugging*

---

**Disco** - *21:35:44*

Does historical data contains some spain/italy exchanges data aswell?

*Tags: Data Quality*

---

**seaders** - *21:39:11*

for the "regular" api client, the passive GET/POST calls, it's not *quite* as relevant (for regular usage), but for the streaming area, there's a huge difference between the regular "object" return, and the "raw" mode, what betfair gives over the streaming service

*Tags: General Technical*

---

**Rory** - *21:41:36*

&gt;there's a huge difference between the regular "object" return, and the "raw" mode, what betfair gives over the streaming service

*Tags: General Technical*

---

**Rory** - *21:41:45*

do you mean in performance?

*Tags: Performance*

---

**Rory** - *21:42:28*

just about to start using live streaming ... so will find out soon :grin:

*Tags: Deployment*

---

**Rory** - *21:43:32*

yes - I've seen similar from the historical data

*Tags: Data Quality*

---

## 2018-08-21

**Tom** - *20:43:44*

betfairlightweight.exceptions.StatusCodeError: Status code error: 503

*Tags: Errors Debugging*

---

## 2018-08-22

**Matt P** - *15:23:54*

Hi guys, question re streaming... I'm looking to stream UK racing prices say, 10 minutes before the off. I've managed to gather all the market _ids for the day and begin pulling data, but how would be the best way to go about updating (in real time) the list of markets I'm subscribed to?

*Tags: General Technical*

---

**liam** - *15:24:52*

filter on racing / uk and let streaming do it for you

*Tags: General Technical*

---

**Matt P** - *15:48:29*

I'm getting constant warning messages printed, that's what made me suspect I was doing something wrong: "WARNING:betfairlightweight.streaming.listener:[Listener: 13]: stream already registered, replacing data"

*Tags: General Technical*

---

**Matt P** - *15:50:53*

pretty new to this streaming business and its quite the learning curve!

*Tags: Getting Started*

---

**liam** - *15:56:15*

you should try working at the socket level! bflw abstracts a lot of the complication away

*Tags: General Technical*

---

**liam** - *15:58:17*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/betfairstream.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/betfairstream.py)

*Tags: General Technical*

---

**Matt P** - *15:58:20*

I was going to ask if there was a part of python i could read up on that might make this whole endeaver a bit easier!

*Tags: General Technical*

---

## 2018-08-28

**favetelinguis** - *12:23:02*

I guess the problem then is if the process that generates the tissue price changes this would not be a valid way to align markets either :slightly_smiling_face:

*Tags: General Technical*

---

**favetelinguis** - *13:01:54*

Since its not the same horses in both markets but it is likely the same market partitioners you want to model the behavior of the partitioners

*Tags: Strategies*

---

## 2018-08-30

**favetelinguis** - *08:28:19*

[@U4H19D1D2](@U4H19D1D2) would you mind giving some more details about how to interpret overround as an indicator?

*Tags: General Technical*

---

**jfo** - *09:29:04*

python man.. :joy:

*Tags: General Technical*

---

**seaders** - *11:04:36*

Again, you should be deciding things like this for your own strategy

*Tags: Strategies*

---

**seaders** - *11:06:45*

And kinda the same with your strategy, figuring out exactly what you want from the market, and how you'd want it to look for your "action 1", or "action 2" will serve you better

*Tags: Strategies*

---

**Ben** - *13:17:57*

Sorry out of topic question, does someone have a documentation on the new betdaq push api? 

*Tags: General Technical*

---

## 2018-09-07

**George** - *13:42:42*

Hi everyone. Just getting starting with this code and I have a simple question. How would I log-in without using "cert" (is that possible?) Or alternatively how would I log some data without using "cert"? Thanks in advance.

*Tags: General Technical*

---

**George** - *13:45:54*

thank you, that looks helpful!

*Tags: General Technical*

---

## 2018-09-11

**Ian** - *18:19:30*

evening all - just a question about storage_engine - is this something I need to setup on amazon prior to running flumine?

*Tags: Getting Started*

---

## 2018-09-12

**Ian** - *10:10:37*

another flumine q - looking at the code it would seem that it creates an APIclient - this will then look for  username in environment or take it from betfairlightweight settings - could someone advise where I set these?

*Tags: General Technical*

---

## 2018-09-13

**George** - *09:49:52*

Flumine doesnt seem to be doing anything... here is the stdout, any help?

```

2018-09-13 08:49:06,647 | INFO | ['flumine/main.py'] | main.py | main

2018-09-13 08:49:06,647 | WARNING | Market Filter not provided, defaulting to GB and IE racing | main.py | main

2018-09-13 08:49:06,647 | WARNING | Market Data Filter not provided, defaulting to None | main.py | main

2018-09-13 08:49:06,648 | INFO | Recorder created 6d4ba5e2 | recorder.py | recorder

2018-09-13 08:49:06,649 | INFO | Starting stream: 1000.0 | flumine.py | flumine

```

*Tags: General Technical*

---

**liam** - *10:03:44*

storage_engine = storageengine.Local('/fluminetests')

*Tags: General Technical*

---

**liam** - *10:04:21*

can you set logging to DEBUG on line 15

*Tags: Errors Debugging*

---

**liam** - *10:09:37*

the retry line is catching that error

*Tags: Errors Debugging*

---

**liam** - *10:15:54*

do a pull and you should get v0.5.3b and all should be fixed

*Tags: Errors Debugging*

---

**George** - *14:53:19*

Hi, me again - what is the right way to stop the stream?   flumine.start() never seems to exit. Thanks

*Tags: General Technical*

---

**liam** - *14:57:14*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

## 2018-09-19

**George** - *10:34:33*

do you have an example of how to parse historical data with a market-data-filter? (e.g. if I only want to parse 3 levels of the book). currently taking hours to parse the raw BF data. Thanks

*Tags: Data Quality*

---

**George** - *15:18:18*

I don't think it's viable to reduce the number of times I call that line, so I have to try to figure out how to make each call a bit faster

*Tags: General Technical*

---

**liam** - *15:18:56*

Your first issue is python is slow

*Tags: Performance*

---

**Ian** - *18:13:37*

Evening all - I'm using the following, in a clone of the latest flumine

*Tags: General Technical*

---

**Ian** - *18:13:41*

(flumine) ian@KYLBU C:\code\flumine

$ pip freeze

betfairlightweight==0.0.0b3

boto3==1.9.4

botocore==1.12.4

certifi==2018.8.24

chardet==3.0.4

ciso8601==2.0.1

docutils==0.14

idna==2.7

jmespath==0.9.3

python-dateutil==2.7.3

requests==2.19.1

retrying==1.3.3

s3transfer==0.1.13

six==1.11.0

ujson==1.35

urllib3==1.23

*Tags: General Technical*

---

**Ian** - *18:15:29*

I'm receiving 'AppKey is not configured for service' when I try and start flumine

*Tags: General Technical*

---

## 2018-09-20

**George** - *08:31:18*

Anyone know how to fix this issue?

*Tags: Errors Debugging*

---

**George** - *10:45:19*

Liam, in case you are interested, I think I may have fixed it myself. I've submitted a pull request so you can see what I did.

*Tags: Errors Debugging*

---

**George** - *11:52:21*

if you agree we can safely ignore for historical data, then i guess it could be solved by checking a "historical" flag

*Tags: Data Quality*

---

## 2018-09-21

**liam** - *10:04:58*

[@UCQB6S222](@UCQB6S222) Yeah i find the data is shit, I am very wary of adding something as extreme as that because it impacts those who record data manually (like myself) where these issues don’t seem to be a problem

*Tags: General Technical*

---

**George** - *10:25:19*

how do you think we can solve the problem without ignore images?

*Tags: General Technical*

---

**George** - *14:19:46*

Here's a snippet of SP data - anyone know how to interpret this?

```{u'actualSP': 6.4,

 u'backStakeTaken': [{u'price': 6.4, u'size': 2414.66}],

 u'farPrice': 5.87,

 u'layLiabilityTaken': [{u'price': 6.4, u'size': 563.85}],

 u'nearPrice': 6.2}```

You would hope that 1 + 2414.66 / 563.85 == 6.4, but that's definitely not true...

*Tags: General Technical*

---

## 2018-09-23

**cieria** - *15:03:43*

Hello! I am trying to parse some historical and output into a file, using the code in the example here [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py). I have downloaded the historical data from betfair, an archive "data.tar". If unzipped, it yields a folder structure like C:/data/xds/historic/BASIC/ $many_folders$ /archives.bz2. In the example code, when initiating the historical stream I have tried giving the "directory" parameter as the the path to the BASIC dir, as well as the path to the actual bz2 files, but both cases fail with error "IsADirectoryError" or with "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xdf in position 12: invalid continuation byte" if I give the path to a bz2 archive

*Tags: Data Quality, Errors Debugging*

---

## 2018-09-24

**agberk** - *17:49:54*

[@U5D4ZBEAG](@U5D4ZBEAG) you've seen a lot of incorrect names right? Do they ever get resolved/changed during the event?

*Tags: General Technical*

---

**agberk** - *17:54:41*

Not a loaded question, but you trust those matches after you've made them for the remainder of the event?

*Tags: General Technical*

---

## 2018-10-08

**liam** - *09:02:28*

v1.8.0 released, minor bug fixes and 'LoginInteractive' endpoint added so a user can login without certs, note that this will fail if you have certs setup on your account  [https://github.com/liampauling/betfair/blob/master/HISTORY.rst](https://github.com/liampauling/betfair/blob/master/HISTORY.rst)

*Tags: Getting Started, Errors Debugging*

---

## 2018-10-09

**bb** - *11:44:57*

I am getting the following error when trying to place a limit on close order, could anyone shed some light on what I am missing as have been staring at this for a while without much luck

*Tags: Errors Debugging*

---

**bb** - *12:02:57*

I saw that but am not sure what mandatory parameter is missing, cant see anything in the documentation that I dont have

*Tags: General Technical*

---

**bb** - *12:06:07*

thanks a lot for your help

*Tags: General Technical*

---

## 2018-10-18

**LK** - *10:17:15*

OK. that's clear then. I can manage that. Thank you again for the quick reply. I'm pretty sure I will have more questions, so I will be back for more haha.

*Tags: General Technical*

---

**LK** - *15:56:57*

My question is the 2 trades at 6.8 and 7 for sizes of 0.17, they were not visible in the available to lay side of the book?

*Tags: General Technical*

---

**LK** - *16:10:30*

I'll take a look and see if I can reconcile that with my historical data. Thanks for pointing me in the right direction again.

*Tags: Data Quality*

---

**LK** - *16:47:19*

Do I understand correctly that in live-streaming data you can opt to see virtual (I would like them implied) prices? But when you buy historical data you only get the actuals back and lay priceladders. (Theoretically you could calculate the virtual prices yourself)

*Tags: Data Quality, Deployment*

---

**LK** - *16:51:32*

ok. Allthough that is present in the 'advanced' data I see. But I was assuming that just meant best back/lay (like a BBO in stock-trading). That could still mean that virtual / implied prices are not in the advanced historical data.

*Tags: Data Quality, Strategies*

---

**LK** - *16:55:13*

ok. got it again. sorry for all the newbie questions guys

*Tags: General Technical*

---

**seaders** - *16:55:38*

np, these aren't noob questions you're asking!

*Tags: General Technical*

---

**seaders** - *16:56:11*

(normally with the historical data, the question is how to unzip them, what are they, where did they go, and what does x, y and z mean!)

*Tags: Data Quality*

---

## 2018-10-19

**ReddyK** - *13:37:48*

Keen to understand how betfairlightweight works, right now playing around with it

*Tags: General Technical*

---

**ReddyK** - *13:40:33*

Btw, Could contribute to the repo, not a python person but could understand and write the code

*Tags: General Technical*

---

**ReddyK** - *14:26:44*

Hi, I just went through the documentation, it seems like in order to place bet we will need run a pythn command from terminal?

*Tags: General Technical*

---

**rafaelmarch3** - *17:15:12*

Thanks agberk, I'll take a look. Another question, is there an interface to retrieve historical data without downloading the JSON files and creating a parser for them ?

*Tags: Data Quality*

---

**Rory** - *18:08:31*

quick question ....

*Tags: General Technical*

---

**liam** - *18:51:38*

If you set requests to debug I think you can see the requests

*Tags: Errors Debugging*

---

**liam** - *18:53:28*

Well your not using streaming so that counts.. :)

*Tags: General Technical*

---

**Rory** - *18:54:07*

why am I not using streaming ... pretty sure I meant to look at that ... DAMN

*Tags: General Technical*

---

**Rory** - *18:54:46*

I actually have streaming hooked up for in-play bets ... but not fully tested

*Tags: General Technical*

---

**Rory** - *18:56:54*

will have a look at streaming first .. thanks for the pointer

*Tags: General Technical*

---

**liam** - *18:59:01*

Integrating streaming should be plug and play as I have tried to keep it simple and it’s much cleaner than constantly polling. Only issue which you are likely to have is that it doesn’t include orders that have already been matched 

*Tags: General Technical*

---

**liam** - *19:28:39*

Betdaq has released streaming now though

*Tags: General Technical*

---

## 2018-11-02

**ak** - *11:52:48*

Hello all. There's been a lot of helpful discussions here, thank you!



I just acquired some sample data for the PRO Betfair exchange data. May I check with you if my understanding of the data is correct?



In particular, I have the following sequence of updates for some `(market_id, selection_id)`:



```

           pt            atb                        atl            trd

1517684220247            NaN             [(8.8, 10.53)]            NaN

1517684221450            NaN              [(8.8, 7.46)]            NaN

1517684223688            NaN  [(7.6, 3.59), (7, 17.98)]            NaN

1517684228928            NaN                        NaN  [(6.2, 1.06)]

1517684228928  [(6.2, 0.94)]                        NaN            NaN

1517684229024            NaN                         []            NaN

1517684229361            NaN                         []            NaN

1517684234009  [(5.8, 3.83)]                        NaN            NaN

1517684234188            NaN                         []            NaN

1517684234286            NaN                [(7, 3.86)]            NaN

```



For rows where there are no updates for `atb` and `atl`, is it correct to assume that there's no changes on the ladders? So that we should forward fill just like the following?



```

           pt             atb                        atl            trd

1517684220247  [[5.7, 55.06]]             [[8.8, 10.53]]            NaN

1517684221450  [[5.7, 55.06]]              [[8.8, 7.46]]            NaN

1517684223688  [[5.7, 55.06]]  [[7.6, 3.59], [7, 17.98]]            NaN

1517684228928  [[5.7, 55.06]]  [[7.6, 3.59], [7, 17.98]]  [[6.2, 1.06]]

1517684228928   [[6.2, 0.94]]  [[7.6, 3.59], [7, 17.98]]            NaN

1517684229024   [[6.2, 0.94]]                         []            NaN

1517684229361   [[6.2, 0.94]]                         []            NaN

1517684234009   [[5.8, 3.83]]                         []            NaN

1517684234188   [[5.8, 3.83]]                         []            NaN

1517684234286   [[5.8, 3.83]]                [[7, 3.86]]            NaN

```



Also, I'm wondering why did `atb` and `atl` move without any trades? Is this due to virtual bets as mentioned by [@UDCDFJ0NS](@UDCDFJ0NS) a couple of weeks ago?

*Tags: General Technical*

---

**liam** - *11:55:55*

I assume you are not using bflw if you are looking at this level or is this for your own understanding?

*Tags: General Technical*

---

**ak** - *12:34:52*

Thanks for the quick reply. Yes, this is for my own understanding. I had a quick look at bflw for parsing, but did not see which module that parsed raw JSON files. If I'm mistaken, could you please point me to the right direction?

*Tags: General Technical*

---

**liam** - *12:38:56*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

## 2018-11-03

**Matt P** - *13:19:38*

thanks, been getting BET_ACTION_ERROR and suspected this was the reason

*Tags: Errors Debugging*

---

**seaders** - *13:23:34*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/utils.py#L3](https://github.com/liampauling/betfair/blob/master/betfairlightweight/utils.py#L3)

*Tags: General Technical*

---

## 2018-11-07

**George** - *09:01:31*

Hi guys - quick question about the Betfair matching algorithm. I sent a LAY order for more than £25 with a limit price of 250. Here is the state of the available_to_lay book at the time the order was sent:

£6.62 @ 200

£7.01 @ 220

£7.01 @ 270

So you would think I'd get filled for £13.63 at an average price of 210.3, right?

*Tags: General Technical*

---

**George** - *09:04:05*

My question is - did I *really* get part of my order filled at 270 even though my limit price was 250? Do Betfair seriously allow this as long as the avgPriceMatched is below the limit price?

*Tags: General Technical*

---

**seaders** - *12:50:13*

hmmm, BF do *a lot* of dumb things, but I'm not 100% sure what your problem is with this strategy is [@UCQB6S222](@UCQB6S222)?

*Tags: Strategies*

---

**George** - *13:12:49*

The problem with using a non-Fill or Kill order is that I would want any unmatched portion to be cancelled and not rest on the book. I think it's a valid and not uncommon strategy to say I'd rather not be the only guy offering liquidity at the top level of the book.

*Tags: Strategies*

---

## 2018-11-11

**stephencornelius** - *18:47:12*

Hi All, does anyone know whether theres an issue using the streaming api to just gather data without placing any bets?



I recall betfair said that on the previous api they would cut you off after a period of time if you werent placing bets and just wondered if this was true of the streaming api as well?



My reason being that I want to build up some price data first which i will use to test the betting functionality. Its a pretty normal use case but can understand betfair being against it since they sell it themselves now

*Tags: Strategies*

---

## 2018-11-12

**George** - *11:16:19*

does anyone know about latency within Betfair's systems - as in, if my order reaches their servers at time X, roughly how long does it take before the order gets matched?

*Tags: Performance, Deployment*

---

**seaders** - *11:25:33*

in my strat, my code places any strategy in a "ORDER_PLACED" status, when the strategy decides to place an order

*Tags: Strategies*

---

**seaders** - *11:26:01*

in another part of the code, that order is placed, and that strategy is placed on hold until it receives an order update

*Tags: Strategies*

---

**George** - *11:27:17*

interesting. i am not 100% following all the details but this is good info. are you saying that async mode has better latency when sending multiple orders in a single message?

*Tags: Performance*

---

**liam** - *11:29:05*

I don't think it decreases latency because sometimes you will get a response from placeOrders before you get an orderStream update

*Tags: Performance*

---

**George** - *11:33:28*

still interested in what people think the internal latency (receiving order at their server through to order being placed) is, roughly

*Tags: Performance, Deployment*

---

**liam** - *11:53:49*

aws eu-west

*Tags: Deployment*

---

## 2018-11-13

**Daydreamer** - *14:18:13*

Hi guys ,

Quick question. Do I pay commission for a single back bet?

*Tags: General Technical*

---

**seaders** - *14:31:05*

[https://en-betfair.custhelp.com/app/answers/detail/a_id/413/~/exchange%3A-what-is-commission-and-how-is-it-calculated%3F](https://en-betfair.custhelp.com/app/answers/detail/a_id/413/~/exchange%3A-what-is-commission-and-how-is-it-calculated%3F)

*Tags: General Technical*

---

## 2018-11-15

**Unknown** - *22:31:39*

any ideas how to restart the streaming easily? I've been getting SocketErrors more frequently lately for some reason.

*Tags: Errors Debugging*

---

## 2018-11-16

**liam** - *08:23:49*

Been discussed a few times, here’s how I do it [https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L84](https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L84)

*Tags: General Technical*

---

**LK** - *20:09:57*

what do you mean by "hacked out of the betfair website"? Do you think it's problematic to hammer their servers with this request? Would you recommend a different service?

*Tags: Deployment*

---

## 2018-11-19

**seaders** - *15:58:00*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1542642751105000](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1542642751105000)

[@UE72WCRR8](@UE72WCRR8) old = `[https://identitysso.betfair.com/api/](https://identitysso.betfair.com/api/)`, new = `[https://identitysso-cert.betfair.com/api/certlogin](https://identitysso-cert.betfair.com/api/certlogin)`

*Tags: General Technical*

---

**liam** - *20:09:54*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1542657033127900](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1542657033127900) Please note that this library is not maintained by Betfair themselves, the docs they provide should be referred to for how to use. I have put up a few examples and the wiki (very dated) but I strongly recommend reviewing the documentation they provide

*Tags: General Technical*

---

## 2018-11-20

**jhaa** - *15:55:13*

I am aware that betfair does not provide this wrapper and  I am very grateful that you decide to open source your work. I always try to figure out stuff myself before I ask however the information is somewhat spread out(wiki, examples, betfair docs, github issues, google) and sometimes I miss things. I would volunteer to update some of the documentation at some point when I feel qualified.

*Tags: General Technical*

---

**liam** - *17:00:06*

No problem, yes please do, the examples sometimes have bugs / could certainly be bulked out further 

*Tags: Errors Debugging*

---

## 2018-12-02

**Donkey** - *10:55:18*

I’m looking for some help please with the .bash_profile content.

Currently set with (fake username/api/pass this is just an example, I use the real ones on my server):

```export username="johnsmith"

export appKey="1234567890"

export password="abcdef"```



In the exampleone.py I have:

```# create trading instance

trading = betfairlightweight.APIClient('username', 'password', app_key='appKey')```



On execution I get however the following error:

```# python exampleone.py

Traceback (most recent call last):

  File "exampleone.py", line 10, in &lt;module&gt;

    trading.login()

  File "/usr/lib/python3.6/site-packages/betfairlightweight/endpoints/login.py", line 30, in __call__

    (response, elapsed_time) = self.request(self.url, session=session)

  File "/usr/lib/python3.6/site-packages/betfairlightweight/endpoints/login.py", line 52, in request

    self._error_handler(response_data)

  File "/usr/lib/python3.6/site-packages/betfairlightweight/endpoints/login.py", line 57, in _error_handler

    raise self._error(response)

betfairlightweight.exceptions.LoginError: API login: ACCOUNT_PENDING_PASSWORD_CHANGE```



But my actual password logs me in betfair account without any issues.



Thanks for the help.

*Tags: Errors Debugging, Deployment, Strategies*

---

**liam** - *11:07:39*

Can’t help in regards to that, you need to contact betfair, have you tried changing your password?

*Tags: General Technical*

---

**Donkey** - *11:14:57*

if I change in the code:

`trading = betfairlightweight.APIClient('username', 'password', app_key='appKey')`

for my actual username etc…   then the code works fine

*Tags: Strategies*

---

**JMaster** - *11:16:00*

[@U4H19D1D2](@U4H19D1D2) do you know how to get the Betfair certificates please? IT seems Betfairs API certificate site is down... [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA)

*Tags: General Technical*

---

**Donkey** - *11:16:44*

[@UEH82Q5HT](@UEH82Q5HT) I used the following website to generate the certs [http://www.bespokebots.com/betfair-ssl-certs.php](http://www.bespokebots.com/betfair-ssl-certs.php)  not sure if that can help you.

*Tags: General Technical*

---

**Donkey** - *11:22:20*

If I follow [https://github.com/liampauling/betfair/wiki/Tutorial](https://github.com/liampauling/betfair/wiki/Tutorial)  using just username/password then I get:

`betfairlightweight.exceptions.AppKeyError: AppKey not found in .bashprofile for ABCD, add or pass to APIClient`

_ABCD is not a real username_

*Tags: Errors Debugging*

---

**Donkey** - *11:48:14*

I guess I need to modify the line then, but for what please?

```# create trading instance

trading = betfairlightweight.APIClient('username', 'password', app_key='appKey')```

*Tags: Strategies*

---

**Donkey** - *11:50:24*

ok let me try, thanks for the help, appreciated

*Tags: General Technical*

---

**Donkey** - *11:59:04*

you’ve been great at helping me out, I’m on the right path now

*Tags: General Technical*

---

**Donkey** - *12:05:02*

final code for me and it works:

```# grep betfairlightweight.APIClient exampleone.py

trading = betfairlightweight.APIClient(username=os.environ.get('_USERNAME'), password=os.environ.get('_PASSWORD'), app_key=os.environ.get('_APIKEY'))```



and in ~/.bash_profile   (fake stuff below…) but use your real ones.

export _APIKEY=“12345”

export _USERNAME=“123ABCD”

export _PASSWORD=“67890”

*Tags: Strategies*

---

**Donkey** - *12:05:40*

evidence that it works:



```# python exampleone.py

[&lt;EventTypeResult&gt;]

7 Horse Racing 703

1 market catalogues returned

1.151985717 2m4f Nov Hrd 2018-12-02 12:25:00

18172586 Garrettstown 0.0

13148638 Paper Promise 0.0

21865555 The Con Man 0.0

18959506 Huntsmans Jog 0.0

14022736 Aint My Fault 0.0

21621734 The Sting 0.0

17209417 Blaydon 0.0

21865554 Salven 0.0

17627171 Terminal One 0.0

12201701 Cushy Butterfield 0.0

16910094 Hitman Fred 0.0

1.151985717 False OPEN 94163.98

18172586 ACTIVE 0.0```

*Tags: General Technical*

---

**Donkey** - *12:24:29*

alright, onto the next issue…

`"statusCode":"FAILURE","errorCode":"NOT_AUTHORIZED","errorMessage":"AppKey is not configured for service"`

Any idea how to enable this one please?

*Tags: Errors Debugging*

---

## 2018-12-03

**JMaster** - *02:11:03*

however when I run trading.login()   --&gt;  betfairlightweight.exceptions.LoginError: API login: CERT_AUTH_REQUIRED

*Tags: Errors Debugging, Strategies*

---

**liam** - *07:10:10*

bflw isn't picking up your certs but looks of things

*Tags: General Technical*

---

**JMaster** - *19:08:14*

[@U4H19D1D2](@U4H19D1D2) I am getting the below - sorry but have no idea how to resolve? Thanks in advance.

trading.login()

Traceback (most recent call last):

  File "&lt;input&gt;", line 1, in &lt;module&gt;

  File "C:\Users\mrmax\Desktop\MRHP\venv\lib\site-packages\betfairlightweight\endpoints\login.py", line 30, in __call__

    (response, elapsed_time) = self.request(self.url, session=session)

  File "C:\Users\mrmax\Desktop\MRHP\venv\lib\site-packages\betfairlightweight\endpoints\login.py", line 52, in request

    self._error_handler(response_data)

  File "C:\Users\mrmax\Desktop\MRHP\venv\lib\site-packages\betfairlightweight\endpoints\login.py", line 57, in _error_handler

    raise self._error(response)

betfairlightweight.exceptions.LoginError: API login: CERT_AUTH_REQUIRED

*Tags: Errors Debugging, Strategies*

---

**JMaster** - *19:11:04*

C:\Users\mrmax\Desktop\MRHP\betfair-master\betfairlightweight\certs\

*Tags: General Technical*

---

**liam** - *19:16:34*

Sorry no idea then, you normally get that error if you aren’t sending certs, does it work with curl?

*Tags: Errors Debugging*

---

**JMaster** - *19:23:33*

adding certs='...' went through, but still got same error when running trading.login()

*Tags: Errors Debugging, Strategies*

---

**JMaster** - *19:40:55*

['C:/Users/mrmax/Desktop/MRHP/betfair-master/betfairlightweight/certs/myusername.crt', 'C:/Users/mrmax/Desktop/MRHP/betfair-master/betfairlightweight/certs/myusername.key']

*Tags: General Technical*

---

**JMaster** - *19:47:56*

trading = betfairlightweight.APIClient('myusername', password='mypassword', app_key=('myappkey'), certs='C:/Users/mrmax/Desktop/MRHP/betfair-master/betfairlightweight/certs/')

*Tags: Strategies*

---

**JMaster** - *19:48:19*

I get no errors when I parse this in the console

*Tags: Errors Debugging*

---

**JMaster** - *19:52:50*

same error, without the brackets

*Tags: Errors Debugging*

---

**JMaster** - *19:57:12*

could it be something to do with the directories, in the error it shows venv/lib/.... where as when I did print(trading.cert) the path does not show the same (venv/lib/...)

*Tags: Errors Debugging, Strategies*

---

**liam** - *19:58:33*

Do you get the same error with trading.logininteractive()

*Tags: Errors Debugging, Strategies*

---

**JMaster** - *20:01:10*

trading.logininteractive()

Traceback (most recent call last):

  File "&lt;input&gt;", line 1, in &lt;module&gt;

AttributeError: 'APIClient' object has no attribute 'logininteractive'

*Tags: Errors Debugging, Strategies*

---

## 2018-12-12

**ak** - *07:53:48*

Hi all! I'm looking into some historical data. May I check why in `RunnerBook` sometimes `book.last_price_traded` is different to `book.ex.traded_volume[-1].price`?

*Tags: Data Quality*

---

**JMaster** - *13:34:27*

Resolved my issue. If any Windows users need a hand feel free to direct message me.

*Tags: General Technical*

---

**JMaster** - *13:38:13*

[@U4H19D1D2](@U4H19D1D2) [@U5D4ZBEAG](@U5D4ZBEAG) trying to list markets:





# make event request to find listed events

special_bet_events = trading.betting.list_events(

filter=filters.market_filter(

text_query='Special Bets'

)

)



# returns result

print(special_bet_events)



however, I get returned 12 &lt;EventResult&gt; instead of the 12 different event names. Please can you help.

*Tags: Strategies*

---

**seaders** - *13:40:16*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L94](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L94)

*Tags: Strategies*

---

**JMaster** - *16:01:33*

[@U5D4ZBEAG](@U5D4ZBEAG) my visualiser gets the same 12 events. with respect to the bettingresources.py, excuse the simple question, but do I run this within my script?

*Tags: Strategies*

---

**seaders** - *16:02:32*

1st step (for yourself, and for me to be able to give you any help) is use simple english to say *exactly* what you're after, and no more

*Tags: General Technical*

---

**seaders** - *16:04:20*

you would, however be *much* better off having a working debugger, and putting a breakpoint at that print statement line, and poking around what "special_bet_events" _is_

*Tags: Errors Debugging*

---

**seaders** - *16:05:03*

all the while having the source code and/or docs open (both the betfairlightweight docs, and the official betfair docs helps loads when you're starting off)

*Tags: General Technical*

---

**seaders** - *16:09:36*

(and in general having a good working debugger, with working breakpoints is just essential, *especially* when starting out, I truly can't stress that enough)

*Tags: Errors Debugging*

---

**JMaster** - *16:10:30*

any recommendation of working debugger?

*Tags: Errors Debugging*

---

**JMaster** - *16:14:38*

agreed, ATOM I found more effort than helpful for me. Ok, cheers [@U5D4ZBEAG](@U5D4ZBEAG) ill proceed with PyCharm

*Tags: General Technical*

---

## 2018-12-17

**seaders** - *16:32:27*

the old ones are deprecated from Thursday, and if you try to login with them on or after that date, you'll get an `CERT_AUTH_REQUIRED` error from BetFair

*Tags: Errors Debugging*

---

## 2018-12-28

**Rory** - *17:59:03*

quick question on soccer asian handicap historical data ...

*Tags: Data Quality*

---

**seaders** - *18:03:15*

They include draws, is the thing

*Tags: Deployment*

---

**seaders** - *18:05:17*

For my purposes, we never want that, so I exclude them. I do a python check to ensure the float handicap is not an int, and also that the handicap * 2 is an int

*Tags: General Technical*

---

**Rory** - *18:06:00*

I understand +1.0, +2.0, etc or +1.5, +2.5 (no draws)

*Tags: Deployment*

---

## 2019-01-02

**Rory** - *16:29:01*

another question ... do Betfair ids for soccer teams remain consistent ... e.g. selection_ids in the MarketDefinition.runners ... so the Arsenal ID is always 1096?

*Tags: General Technical*

---

**Rory** - *16:39:50*

brilliant, cheers [@U5D4ZBEAG](@U5D4ZBEAG)... I'm going back through the historical data and can see Utd as runner 48351 as far back as 2015 .... was thinking of using that as a lookup but not 100% sure I can rely on it :grin:

*Tags: Data Quality*

---

## 2019-01-05

**Rob** - *16:19:37*

I realise this may be obvious but streaming is new to me - does `output_queue.get()` get all updates when called, or is it 1 after another?

*Tags: Getting Started*

---

**liam** - *16:20:08*

It just a python queue so just the one 

*Tags: General Technical*

---

**Rob** - *16:20:11*

I have a timer running every 1 second, calling output_queue.get() then writing the data to a database

*Tags: General Technical*

---

**liam** - *16:21:26*

[https://docs.python.org/3/library/queue.html](https://docs.python.org/3/library/queue.html)

*Tags: General Technical*

---

**Rob** - *16:22:00*

I was trying to get data every second to store in a database so I'd have an easier time later on

*Tags: General Technical*

---

**liam** - *16:27:37*

Just remove your sleep and problem solved 

*Tags: General Technical*

---

## 2019-01-11

**seaders** - *16:55:06*

I keep getting an error from my bot saying it can't match teams from, [https://www.betfair.com/exchange/plus/football/competition/5627174](https://www.betfair.com/exchange/plus/football/competition/5627174)

*Tags: Errors Debugging*

---

## 2019-01-12

**Matt P** - *17:27:59*

Hi chaps, is anyone else having trouble logging into the historical data site? I'm having issues and not sure if it's at my end or theirs...

*Tags: Data Quality*

---

**Rory** - *18:16:34*

historical data is a bit of a mess ... times out regularly when trying to download a few months worth of data in my experience

*Tags: Data Quality*

---

## 2019-01-13

**Rob** - *20:14:43*

Hello all, can someone help me here (Python Newbie)? when i run "runner.ex.traded_volume" i get this &lt;betfairlightweight.resources.bettingresources.PriceSize object at 0x000002797A1AD828&gt;

*Tags: Strategies*

---

**Rob** - *20:16:16*

also when printing streaming updates, any way of converting this update to normal Json instead of Json CRLF?

*Tags: General Technical*

---

## 2019-01-21

**Rob** - *00:37:42*

[@U4H19D1D2](@U4H19D1D2) Hello Liam, how to get price and other data per runner using streaming? Im using marketbook_streaming update, how to isolate data from there? Without streaming i could do a loop for runner in marketbook and that would do it...thanks a lot

*Tags: General Technical*

---

**liam** - *07:51:42*

[@UDVM2FWGM](@UDVM2FWGM) have you seen the example? [https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**Rob** - *19:20:05*

[@U4H19D1D2](@U4H19D1D2)  thanks for the quick reply...yes, that's exactly what i am using apart from listener.snap( i dont know what's the diference) ....



Im trying to subscribe to just one market 



for market_book in market_books:

            Print(market_book.streaming_update)



And i get a crlf delimited json with everything from the active filters on it, all together, no idea how to extract for example the ltp or best offers separately, and for example if i try to validate the entire streaming update json line or to view it using an online jsonviewer it says json invalid...

*Tags: General Technical*

---

**liam** - *19:22:53*

Why are you interested in the update? In normal use you ignore it and use the MarketBook as per, bflw has done the heavy lifting and given you an up to date book 

*Tags: General Technical*

---

**Rob** - *21:46:01*

import os

import logging

import queue

import betfairlightweight

from betfairlightweight.filters import (

    streaming_market_filter,

    streaming_market_data_filter

)



#logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))

# OR TO GET MORE INFORMATIOJN logging.basicConfig(level=logging.DEBUG)





trading = betfairlightweight.APIClient('my [mailto:email@mail.com|email@mail.com](mailto:email@mail.com|email@mail.com)', 'my pass', app_key='my app key')

trading.login()

trading.session_token

print(trading.session_token)



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(

    output_queue=output_queue,

)





 #create filters (GB WIN racing)

market_filter = streaming_market_filter(

    market_ids=["1.153765710"]

)



market_data_filter = streaming_market_data_filter(

    fields=["EX_MARKET_DEF","EX_BEST_OFFERS","EX_LTP"],

    ladder_levels=3,

)



streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=0,  # send update every 1000ms

)



# start stream

stream.start(_async=True)





# check for updates in output queue

while True:

    market_books = output_queue.get()

    #print(market_books)



    for market_book in market_books:

        print(market_book.streaming_update

            #market_book.streaming_unique_id,  # unique id of stream (returned from subscribe request)

            #market_book.streaming_update,  # json update received

            #market_book.market_definition,  # streaming definition, similar to catalogue request

            #market_book.publish_time  # betfair publish time of update

        )

*Tags: Errors Debugging, Strategies*

---

**Rob** - *22:03:34*

getting the data like ltp, etc, without have to deserialize the output from "market_book.streaming_update

*Tags: General Technical*

---

**liam** - *22:04:58*

Streaming_update is there so you can see what caused the update but can be ignored 

*Tags: General Technical*

---

**Rob** - *22:10:58*

for market_book in market_books:

        print(market_book.streaming_update,str("end")

            #market_book.streaming_unique_id,  # unique id of stream (returned from subscribe request)

            #market_book.streaming_update,  # json update received

            #market_book.market_definition,  # streaming definition, similar to catalogue request

            #market_book.publish_time  # betfair publish time of update

        )

        for runner in market_book.runners:

            print(runner.last_price_traded) &lt;-- this does not work

*Tags: General Technical*

---

**Rob** - *22:56:36*

my auto complete doesn´t help at all, i must be missing some filters or something similar right?

*Tags: General Technical*

---

**Rob** - *23:35:33*

print(type(market_books[0])) gives me &lt;class 'betfairlightweight.resources.bettingresources.MarketBook'&gt;

*Tags: Strategies*

---

**Rob** - *23:36:14*

print(type(market_books[0].runners[0])) gives me &lt;class 'betfairlightweight.resources.bettingresources.RunnerBook'&gt;

*Tags: Strategies*

---

**seaders** - *23:40:35*

`market_books: List[betfairlightweight.resources.MarketBook] = market_books`

*Tags: General Technical*

---

**seaders** - *23:41:01*

(if you're using proper python 3 'n all that)

*Tags: General Technical*

---

**Rob** - *23:43:12*

while True:

    market_books = output_queue.get()



    print(type(market_books))

    print(type(market_books[0].runners[0]))

    market_books: List[betfairlightweight.resources.MarketBook] = market_books

*Tags: General Technical*

---

**Rob** - *23:43:55*

thanks for helping

*Tags: General Technical*

---

**seaders** - *23:46:51*

`market_books: List[betfairlightweight.resources.MarketBook] = output_queue.get()` will do the cast

*Tags: General Technical*

---

**Rob** - *23:52:36*

while True:

    books: List[betfairlightweight.resources.MarketBook] = output_queue.get()

*Tags: General Technical*

---

**Rob** - *23:52:53*

books: List[betfairlightweight.resources.MarketBook] = output_queue.get()

NameError: name 'List' is not defined

*Tags: Errors Debugging*

---

## 2019-01-22

**Rob** - *00:15:11*

while True:

    list[betfairlightweight.resources.MarketBook] = output_queue.get()

*Tags: General Technical*

---

**Rob** - *00:15:18*

it gives the error

*Tags: Errors Debugging*

---

**Rob** - *00:15:30*

TypeError: 'type' object does not support item assignment

*Tags: Errors Debugging*

---

**seaders** - *00:39:57*

at this point though, I do think you should maybe try up your python knowledge, cos what we're discussing really is that

*Tags: General Technical*

---

## 2019-01-28

**wsdlwizard** - *20:05:11*

Python 3.6

*Tags: General Technical*

---

## 2019-02-02

**jhaa** - *05:43:36*

why was list_runner_book not implemented? I can't see how to filter list_market_boook for selection ids so pulling more than necessary for now

*Tags: General Technical*

---

**wsdlwizard** - *09:56:27*

I am trying to simulate betting and need to instantiate an object of type CurrentOrder. My import is "from betfairlightweight.resources.bettingresources import PriceSize,CurrentOrder"  and my code is "test = CurrentOrder(5.5,'1234567',4.5,'1','2',1.0,'654987',datetime.datetime(2019,1,1,12,0,0),'PERSIST',datetime.datetime(2019,1,1,12,0,0),{'price':2.0,'size':20.0},'3',123456,'BACK',0.0,0.0,0.0,0.0,0.0,'SUCCESS')

" I get the error "TypeError: type object argument after ** must be a mapping, not float " any ideas? Many thanks.

*Tags: Errors Debugging, Strategies*

---

**wsdlwizard** - *10:39:38*

Thanks for the reply, yes it looks messy I should have said the problem is with the PriceSize bit. I have tried all sorts of combinations. I should be able to instantiate my own CurrentOrder object sholdnt I? The problem I am having is how to write the price,size bit I have tried lists [2.0,3.0] but still get the same type error

*Tags: Errors Debugging*

---

**seaders** - *17:53:42*

and I'm getting order info no problem via the wrapper

*Tags: General Technical*

---

**jhaa** - *17:58:03*

thanks for your help

*Tags: General Technical*

---

**wsdlwizard** - *21:08:40*

Thank you this solved it, I had used the wrong order of parameters. I used the order from the comments between the """ and """ I changed the order to this and it works fine thank you! test = CurrentOrder('1234567',5.5,4.5,1.0,'12321546','PERSIST','PERSIST',datetime.datetime(2019,1,1,12,0,0),'','654321','BACK',0.0,0.0,20.0,0.0,0.0,'GOOD',{'price':2.0,'size':20.0},datetime.datetime(2019,1,1,12,0,0))

*Tags: General Technical*

---

## 2019-02-03

**jhaa** - *18:17:49*

that creates a different error

*Tags: Errors Debugging*

---

## 2019-02-04

**liam** - *18:25:49*

FYI Betfair are restricting TLS to 1.2 



[https://forum.developer.betfair.com/forum/developer-program/announcements/27206-betfair-api-login-tls-1-2-from-7th-february-2019](https://forum.developer.betfair.com/forum/developer-program/announcements/27206-betfair-api-login-tls-1-2-from-7th-february-2019) 



[https://www.calazan.com/how-to-check-if-your-python-app-supports-tls-12/](https://www.calazan.com/how-to-check-if-your-python-app-supports-tls-12/)

*Tags: General Technical*

---

## 2019-02-08

**rafaelmarch3** - *17:04:19*

Haven't tested my code since last year, now I'm getting a certificate error that I was not having before. Do you thing this is the cause ?

*Tags: Errors Debugging*

---

**rafaelmarch3** - *17:12:47*

Hi folks, my digita certificate used to work fine with my python-developed code. Last time I connected was December last year. Now I'm getting a certificate issue:





 betfairlightweight.exceptions.LoginError: API login: CERT_AUTH_REQUIRED



Do you have any idea what may be happening ?

*Tags: Errors Debugging*

---

**agberk** - *17:16:19*

most likely need to update your version of betfairlightweight

*Tags: General Technical*

---

## 2019-02-10

**jgnz** - *11:58:00*

anyone able to help with my error that ive posted on the betfair forum [https://forum.developer.betfair.com/forum/sports-exchange-api/exchange-api/27338-always-getting-invalid_runner-errors-when-trying-to-place_orders](https://forum.developer.betfair.com/forum/sports-exchange-api/exchange-api/27338-always-getting-invalid_runner-errors-when-trying-to-place_orders)

*Tags: Errors Debugging*

---

**jgnz** - *14:08:07*

and i have to be careful with the errors i get back, they can be misleading

*Tags: Errors Debugging*

---

**seaders** - *14:09:04*

Main thing, for yourself, is to read specific error for as specific a "hint" as possible

*Tags: Errors Debugging*

---

**seaders** - *14:09:49*

so in your case, `BET_ACTION_ERROR`and `INVALID_RUNNER`

*Tags: Errors Debugging*

---

**seaders** - *14:10:40*

`BET_ACTION_ERROR` means placeOrder docs, and `INVALID_RUNNER`, the instruction therein

*Tags: Errors Debugging*

---

**jgnz** - *14:14:50*

ive been very infrequently trying to scrape bet365 for years, but often use all of this as a way to find things to learn.  sqlalchemy, pandas, modules etc  ive only recently come back to bet365 and betfair with a little more knowledge

*Tags: Feature Engineering*

---

**jgnz** - *14:31:54*

ah ok, so its fine to send a handicap of 0? ie its never null on a runner and we dont create an error if you send 0 when placing an order?

*Tags: Errors Debugging*

---

## 2019-02-12

**seaders** - *11:48:48*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1549972023018400](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1549972023018400)

*Tags: General Technical*

---

## 2019-02-16

**liam** - *17:36:45*

[@U92CASP1B](@U92CASP1B) is that using the latest version of bflw? I updated them but never tested 

*Tags: General Technical*

---

## 2019-02-17

**Marcel** - *13:48:17*

Hello, nice there is a Phyton Betfair API community! I am new to Python and the Betfair API. Currently I use a vendor application in combination with Excel VBA.



To improve speed and flexibility I would like to build a bot in Python for betting on Match Odds soccer. Does someone has a script which generates a coupon of matches for the day with betting functionality which I can use as a starting point to add my criteria to place bets on those markets? That would be great!

*Tags: Getting Started, Performance, Strategies*

---

**Marcel** - *15:36:36*

[@U4H19D1D2](@U4H19D1D2) I have read the examples, although it seems to me that these are related to horse racing? I have also read the API reference guide from above. It looks pretty tough to me to set up. Since I am also new to Python I have a lot to learn. For that reason I was wondering if someone had already made a script for the soccer coupon.

*Tags: Getting Started*

---

## 2019-02-19

**klozovin** - *13:58:52*

It seems that I can't use the delayed API key with streaming services, only the live one... is this how it's supposed to be?

*Tags: Deployment*

---

**klozovin** - *14:10:39*

My live key is activated (it seems at least), I can authenticate to the streaming API just fine... do you think I should ask them to activate the delayed key as well? I was planning on using the delayed key for development and testing...

*Tags: Deployment*

---

**seaders** - *14:13:50*

are ya not supposed to use the delayed for streaming?

*Tags: General Technical*

---

**klozovin** - *14:14:36*

[@U5D4ZBEAG](@U5D4ZBEAG) that's what I (implicitly) thought... never checked with BF, though. I've been using my delayed key for all of my testing of the non-streaming API

*Tags: General Technical*

---

**klozovin** - *14:17:41*

Why do you think you're wrong?

Btw, it says in the table bellow that delayed key can be used for Streaming, but one has to apply

*Tags: General Technical*

---

**klozovin** - *18:05:07*

Can't find it in the docs for Streaming API - what happens when the market is suspended? Does that get streamed down?

*Tags: General Technical*

---

**klozovin** - *18:35:34*

man, although there's lots of docs for the Streaming API I still find it hard to get my head around everything...

*Tags: General Technical*

---

**liam** - *18:40:04*

You not using bflw?

*Tags: General Technical*

---

**klozovin** - *18:47:12*

thanks! that helps...

*Tags: General Technical*

---

**klozovin** - *18:47:49*

I'm not using bflw, I'm now trying to implement something of my own... so much work goes into this :smile:

*Tags: General Technical*

---

## 2019-02-20

**ross o reilly** - *16:33:09*

Hi there! I am interested in scraping golf market data. For a given golf market, how to i call listMarketCatalogue in python to return a dataframe where the first column is the selection id and the second column is the players name? I'm really struggling to find the correct way to to this. thanks

*Tags: Feature Engineering*

---

**klozovin** - *20:15:40*

When streaming data do people usually stream prices with _virtual bets_ or without them? What would be the point of not seeing virtual bets?

*Tags: General Technical*

---

**klozovin** - *21:06:14*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1550696647009000](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1550696647009000)



sure, no problem :slightly_smiling_face: was it hard to get it right? days work? weeks work? :smile:

*Tags: General Technical*

---

## 2019-02-24

**liam** - *18:34:50*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L774](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L774)

*Tags: Strategies*

---

## 2019-02-26

**ShahGar** - *13:05:07*

Why is the bet stakes/liquidities retrieved from the streaming api "much lower" from those retrieved from the ordinary betfair api?

*Tags: General Technical*

---

**seaders** - *13:06:52*

specifically if you can show what it is via the streaming, the normal call, and via the website

*Tags: General Technical*

---

**seaders** - *13:07:17*

(oft times when you do that, you might realise what the solution is yourself, but it certainly helps us to give you a hand, too)

*Tags: General Technical*

---

## 2019-03-04

**seaders** - *15:51:09*

So, with some of you guys struggling to get yourselves up and running, and maybe want a bit of a "jump-start", I all but guarantee I'll be able to help, to a greater extent than just on here (but the help I give and advice I give here will still definitely continue)

*Tags: General Technical*

---

## 2019-03-15

**Newbie99** - *22:01:03*

I think, this a 2 second question for someone (not streaming related).



```

current_order_list = trading.betting.list_current_orders(

    order_by="BY_MATCH_TIME",

    sort_dir="LATEST_TO_EARLIEST",

    record_count="20",

    lightweight=False)





print(current_order_list._data)

```



That works fine, except I can't seem to work out the correct way to iterate through Bet_id, I assumed I could just do something like:



```

for a in current_order_list:

    for b in a.currentOrders:

        print(b)

```



But that returns:



```

TypeError: 'CurrentOrders' object is not iterable

```



testing a[0] doesn't work either, so clearly I'm missing the correct way to iterate through this, as I've tried various alternatives and I can't figure it out....although I'm sure its a blindingly obvious answer! Would anyone care to put me out of my misery on this one?

*Tags: Errors Debugging, Strategies*

---

**seaders** - *22:10:03*

`.list_current_orders` returns a _single_ CurrentOrders object - [https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L556](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L556)

*Tags: Strategies*

---

**seaders** - *22:13:29*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L215](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L215)

*Tags: Strategies*

---

**seaders** - *23:45:28*

```market_filter = streaming_market_filter(

    market_ids=['1.155209408'])

market_data_filter = streaming_market_data_filter(

    fields=['EX_TRADED', 'EX_MARKET_DEF'],

    ladder_levels=3)```

*Tags: General Technical*

---

## 2019-03-16

**richard** - *00:02:19*

I’m still getting the same empty list but it’s now tomorrow so I’ll come back to it in the morning. Thanks for the help so far.

*Tags: General Technical*

---

**richard** - *00:16:10*

Thanks for the help in getting me where i wanted to for one night. Time for bed.

*Tags: General Technical*

---

**Unknown** - *10:44:01*

Trying to understand `time_range` arguments. If I specify the below then I get all day correctly. However, if I pass ```filters.time_range("2019-03-16 11:00:00", "2019-03-16 14:00:00")``` in the hope of getting a smaller window I get a DSC-0018 error. Any ideas please?

*Tags: Errors Debugging*

---

**richard** - *10:50:29*

Solved ```filters.time_range("2019-03-16T11:00:00Z", "2019-03-16T14:00:00Z")```

*Tags: General Technical*

---

## 2019-03-24

**Rob (NZ)** - *21:38:28*

Hi all.. just introducing myself... Rob from New Zealand... I've been using betfair for awhile and 3rd party software (betangel) .. I'm now at the stage I want to start using the api and get some automation running without depending on others software.  I'm self learning python to do this and have a strong focus on data visualisation.  I've installed lightweight and really looking forward to using it.



Cheers Rob

*Tags: Getting Started*

---

**Paw** - *21:57:39*

Hey guys! Like Rob I've just discovered betfairlightweight and looking forward to developing some stuff on my own (as well as contributing to the library). I have extensive Java experience with strong focus on cloud computing (especially Google Cloud) and looking to catch up on my python

*Tags: General Technical*

---

## 2019-03-25

**Rob (NZ)** - *23:07:29*

Not sure if this is the ok to ask so please forgive me if not....  just trying to connect using the example python code on betfair and getting this error...   



 certs_path = r'/"certs"



is getting me 



File "&lt;ipython-input-23-e458ea84454c&gt;", line 11 certs_path = r'/"certs" ^ SyntaxError: EOL while scanning string literal 





Any ideas or pointers... 

*Tags: Errors Debugging*

---

## 2019-03-27

**Unknown** - *06:48:22*

Still getting this error...  will attach the code I'm using also

*Tags: Errors Debugging*

---

**liam** - *06:51:18*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L150](https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L150)

*Tags: General Technical*

---

## 2019-03-28

**Rob (NZ)** - *04:04:10*

Random question... anyone using betfairlightweight within lambdas on AWS..

*Tags: Deployment*

---

**Ajay kumar** - *10:39:51*

i just have a dumb question.... can i use this betfairlightweight to create my own betting site ?

*Tags: Strategies*

---

**Ajay kumar** - *10:40:51*

if any one build that can help me?

*Tags: General Technical*

---

## 2019-03-31

**Rob (NZ)** - *02:57:24*

Any documentation around sessions when using the non interactive login

*Tags: General Technical*

---

## 2019-04-02

**Paw** - *09:39:26*

[@UHD6B7RAQ](@UHD6B7RAQ) I went to the last betfair meetup they run and they mentioned one of the biggest problem they're facing is people stealing they live data

*Tags: Deployment*

---

## 2019-04-03

**LK** - *08:34:19*

why would one use AWS instead of VPS like this one? [https://www.tradingservers.co.uk/?language=english](https://www.tradingservers.co.uk/?language=english)

*Tags: Deployment, Strategies*

---

**liam** - *08:57:26*

Looks to be about double the price of AWS assuming you have it on 24/7 which you don’t need to do 

*Tags: Deployment*

---

**LK** - *09:39:51*

ping times from AWS lightsail instance of 3.50 p/mo ;

*Tags: Deployment*

---

**liam** - *14:00:38*

You get what you pay for with lightsail, I have always stuck with ec2 t2/t3

*Tags: Deployment*

---

**LK** - *16:02:28*

amazon ec2 dublin t3.medium

*Tags: Deployment*

---

**Paw** - *17:05:06*

so I'm an GCP expert but have no time right now to check latency from their servers

*Tags: Performance, Deployment*

---

**LK** - *17:17:29*

Often I feel speed as advertised seems to be confused with bandwidth. Bandwidth is not the issue, but latency and variance in latency are key.

*Tags: Performance*

---

**LK** - *17:18:20*

8.54 timedelta | latency min 9.17 ms | avg 10.10 | median 10.04 | 90th 11.42 | 95th 11.69 | max 11.84 ms

6.24 timedelta | latency min 9.17 ms | avg 10.29 | median 10.05 | 90th 11.24 | 95th 14.10 | max 14.29 ms

4.13 timedelta | latency min 9.26 ms | avg 10.32 | median 10.11 | 90th 10.99 | 95th 11.18 | max 17.97 ms

*Tags: Performance*

---

**LK** - *17:19:20*

these are stats for every 100 messages received from betfair, comparing the publishTime with the time on the server. Latency is 1-way latency, since it is market datastream.

*Tags: Performance, Deployment*

---

## 2019-04-04

**Rob (NZ)** - *00:04:09*

Hey all.. thanks for all the help and I'm now able to access and submit to the api ...  I can put on back and  lay bets for a fixed amount... where would the best place to find putting details around placing by liability  and potential that being below the minimum

*Tags: Errors Debugging*

---

**Rob (NZ)** - *00:09:19*

What's the best documentation I can find for betfairlightweight...

*Tags: General Technical*

---

**LK** - *07:55:33*

to measure marketstream latency I simply compare the publishTime on incoming messages with the current time (time.time()).

*Tags: Performance*

---

**LK** - *09:37:29*

I believe hping will not keep the connection open. You are implying I guess that betfairlightweight uses requests-session. For the market- and orderstream I agree that the connection is persistent, but not sure how this works for https requests?

*Tags: General Technical*

---

**LK** - *10:16:29*

[http://docs.python-requests.org/en/latest/user/advanced/#keep-alive](http://docs.python-requests.org/en/latest/user/advanced/#keep-alive)

*Tags: Deployment*

---

**liam** - *10:17:03*

if you set logging to debug and use a session, make requests every x seconds and increase x by 2 every time you will see that eventually another handshake will be made

*Tags: Errors Debugging*

---

## 2019-04-08

**Rob (NZ)** - *00:51:24*

Anyone have a example of placing a fixed liability bet  when laying ...  so just at say current market price...

*Tags: Errors Debugging*

---

**Rob (NZ)** - *01:12:53*

Cheers for the help with the adjusting and cancelling bet advice .. was able to code that up and implement that over the weekend...

*Tags: General Technical*

---

## 2019-04-09

**Rob (NZ)** - *09:01:25*

Hoping this might be a dumb question ... but is there a easy way of getting betID and assigning it as variable after a order has been successfully submitted... I can see the output and betID inside of the response but im just wondering if there is functionality to easily grab that variable so It can be reused further in the code

*Tags: General Technical*

---

**Rory** - *09:16:33*

place_instruction_reports is a list but this is already covered in the BetfairLightweight docs

*Tags: General Technical*

---

**Rob (NZ)** - *09:31:47*

awesome, cheers Rory,.. ive started off with the example code on betfair and just trying to learn python.... this was really helpful...  I had to switch placed_bet to order

*Tags: General Technical*

---

**Dado** - *12:31:59*

I'm having problems to get player name from selectorid into a TO_SCORE event...

*Tags: General Technical*

---

**Dado** - *12:35:17*

market_catalogue_filter = betfairlightweight.filters.market_filter(

		event_ids=['14843951'],

		market_type_codes=["MATCH_ODDS","CORRECT_SCORE"]

	)

	market_catalogues = trading.betting.list_market_book(

		filter=market_catalogue_filter,

		max_results='1000',

		market_projection=["RUNNER_DESCRIPTION"]

	)

*Tags: Strategies*

---

**Dado** - *12:39:38*

market_catalogue_filter = betfairlightweight.filters.market_filter(

		event_ids=[14843951],

		market_type_codes=["MATCH_ODDS","CORRECT_SCORE"]

	)

	market_catalogues = trading.betting.list_market_catalogue(

		filter=market_catalogue_filter,

		max_results='1000',

		market_projection=["RUNNER_DESCRIPTION","MARKET_START_TIME", "COMPETITION"]

	)

	print(len(market_catalogues))

*Tags: Strategies*

---

**Dado** - *12:43:23*

I was thinking that it's a problem of filters/parameters

*Tags: General Technical*

---

**Dado** - *12:44:38*

market_catalogue_filter = betfairlightweight.filters.market_filter(

		event_ids=[14843951],

		market_type_codes=["MATCH_ODDS","CORRECT_SCORE"]

	)

	market_catalogues = trading.betting.list_market_catalogue(

		filter=market_catalogue_filter,

		max_results=1000,

		market_projection=["RUNNER_DESCRIPTION","MARKET_START_TIME", "COMPETITION"]

	)

	print(len(market_catalogues))

*Tags: Strategies*

---

**Dado** - *12:45:02*

I was thinking that should be a problem of filters/parameters...

*Tags: General Technical*

---

**liam** - *12:47:21*

Can’t help more tbh as I am not on a safe computer 

*Tags: General Technical*

---

**Dado** - *12:48:49*

why do you need a safe computer to help me? :smile:

*Tags: General Technical*

---

**Dado** - *12:56:43*

market_catalogue_filter = betfairlightweight.filters.market_filter(

		market_ids=[14843951],

		market_type_codes=["MATCH_ODDS","CORRECT_SCORE"]

	)

	market_catalogues = trading.betting.list_market_catalogue(

		filter=market_catalogue_filter,

		max_results=1000,

		market_projection=["RUNNER_DESCRIPTION","MARKET_START_TIME", "COMPETITION"]

	)

	print(len(market_catalogues))

*Tags: Strategies*

---

**Dado** - *12:57:44*

market_catalogue_filter = betfairlightweight.filters.market_filter(

		market_ids=[1.156230909],

		market_type_codes=["MATCH_ODDS","CORRECT_SCORE"]

	)

	market_catalogues = trading.betting.list_market_catalogue(

		filter=market_catalogue_filter,

		max_results=1000,

		market_projection=["RUNNER_DESCRIPTION","MARKET_START_TIME", "COMPETITION"]

	)

	print(len(market_catalogues))

*Tags: Strategies*

---

**Dado** - *13:01:28*

market_catalogue_filter = betfairlightweight.filters.market_filter(

		market_ids=['1.156230909']

	)

	market_catalogues = trading.betting.list_market_catalogue(

		filter=market_catalogue_filter,

		max_results=1000,

		market_projection=["RUNNER_DESCRIPTION","MARKET_START_TIME", "COMPETITION"]

	)

	print(len(market_catalogues))

*Tags: Strategies*

---

**liam** - *13:02:48*

Open up your debugger and look at the response 

*Tags: Errors Debugging*

---

**Dado** - *13:08:39*

Solved! Thanks a lot for your help

*Tags: General Technical*

---

**liam** - *13:16:09*

Streaming simplifies this as the steaming object contains the market definition which is a slimmed down catalogue 

*Tags: General Technical*

---

## 2019-04-11

**Rob (NZ)** - *09:36:35*

awesome sounds good... whats the best documentation I should read up on it?

*Tags: General Technical*

---

**liam** - *10:18:00*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L193](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L193)

*Tags: Strategies*

---

**Unknown** - *10:41:01*

I've taken the other code and changed it over to list_runner_book but getting a error ... totally think is me just making a meal of it though

*Tags: Errors Debugging*

---

**Rob (NZ)** - *11:19:09*

But still getting the same error...

*Tags: Errors Debugging*

---

**liam** - *11:20:00*

Print betfairlightweight.__version__

*Tags: General Technical*

---

**Rob (NZ)** - *11:56:14*

Weird I'm now getting a maximum recursion level reached error

*Tags: Errors Debugging*

---

**Rob (NZ)** - *11:56:22*

Overflowerror

*Tags: Errors Debugging*

---

## 2019-04-12

**phil** - *09:39:51*

Really interested by this python app - I am about four months into learning Python, and I'm perhaps going to take this on as a side project. My first app was a roulette game which then tested various strategies against a virtual roulette table to see if it could win. It wasn't very sophisticated really, but it was great to try. What I hope to achieve with this app is to download historical data from betfair and test strategies against that

*Tags: Data Quality*

---

**phil** - *09:42:26*

UK horses. There's a guy I know who gave me a strategy I want to test. I expect to find it doesn't work but I want to know for sure. Do people (like you guys,...) make a living with this stuff&gt;

*Tags: Strategies*

---

**Dado** - *09:44:42*

anybody knows how to link MATCH ODDS to UPCOMING EVENTS data?

*Tags: General Technical*

---

**Dado** - *09:59:05*

def getUpcomingEvents(sport_id):

	thoroughbreds_event_filter = betfairlightweight.filters.market_filter(

		event_type_ids=[sport_id],

		in_play_only='false',

		market_start_time={

			'to': (datetime.datetime.utcnow() + datetime.timedelta(days=2)).strftime("%Y-%m-%dT%TZ")

		}

	)



	aus_thoroughbred_events = trading.betting.list_events(

		filter=thoroughbreds_event_filter

	)

*Tags: Strategies*

---

**Dado** - *09:59:48*

python...

*Tags: General Technical*

---

**Dado** - *10:00:51*

language is not a problem

*Tags: General Technical*

---

**Dado** - *10:01:09*

the problem is that the match odds are not linked with upcoming events data

*Tags: General Technical*

---

**liam** - *10:04:47*

sorry i don't really understand the problem without seeing the code

*Tags: General Technical*

---

**Dado** - *10:05:46*

thoroughbreds_event_filter = betfairlightweight.filters.market_filter(

		event_type_ids=[sport_id],

		in_play_only='false',

		market_start_time={

			'to': (datetime.datetime.utcnow() + datetime.timedelta(days=2)).strftime("%Y-%m-%dT%TZ")

		}

	)



	aus_thoroughbred_events = trading.betting.list_events(

		filter=thoroughbreds_event_filter

	)

*Tags: Strategies*

---

**Dado** - *10:06:28*

market_catalogue_filter = betfairlightweight.filters.market_filter(event_ids=upcoming_events_id,market_type_codes=['MATCH_ODDS','TO_SCORE','FIRST_GOAL_SCORER'])

	market_catalogues = trading.betting.list_market_catalogue(

		filter=market_catalogue_filter,

		max_results='1000'

	)

*Tags: Strategies*

---

**Dado** - *10:06:48*

price_filter = betfairlightweight.filters.price_projection(

		price_data=['EX_BEST_OFFERS']

	)



	market_books = trading.betting.list_market_book(

		market_ids=array_market_type_ids,

		price_projection=price_filter

	)

*Tags: Strategies*

---

**Dado** - *10:21:19*

price_filter = betfairlightweight.filters.price_projection(

		price_data=['EX_BEST_OFFERS']

	)



	market_books = trading.betting.list_market_book(

		market_ids=array_market_type_ids,

		price_projection=price_filter

	)

*Tags: Strategies*

---

**liam** - *10:32:19*

debugger is your friend

*Tags: Errors Debugging*

---

**Dado** - *16:04:05*

ok solved but...

*Tags: General Technical*

---

## 2019-04-17

**liam** - *11:47:06*

[@UH9MMLXAS](@UH9MMLXAS) use your debugger! It will tell you straight away what’s in those objects 

*Tags: Errors Debugging*

---

**Rob (NZ)** - *12:25:57*

Cheers Liam... I'll have to learn about the debugger... I'm just using Jupyter notebook

*Tags: Errors Debugging*

---

**Chris** - *15:10:35*

I can share the file in question here if that helps (i'm parsing the files using a combination of bash/python to just grab the last price so don't think it's particularly easy to share)

*Tags: General Technical*

---

**Chris** - *15:11:51*

The line in question is `{"op":"mcm","clk":"7248564363","pt":1534629521130,"mc":[{"id":"1.146002987","rc":[{"ltp":7.8,"id":16346804}]}]}`

*Tags: General Technical*

---

**Tom** - *16:15:10*

[@UFPEU7URG](@UFPEU7URG) That match in question kicked off at 21:15GMT and the timestamp you shared is 103 mins in(depending in timezone). Including half time break the odds seem right. That price of 7.8 would prob be close or even injury time.

*Tags: General Technical*

---

## 2019-04-18

**Chris** - *08:16:47*

Yes, everything has a time stamp, I've sorted the problem now by removing everything that is "inPlay: true" and then grabbing the last remaining value for each player in the file(s)

*Tags: General Technical*

---

## 2019-04-22

**Filippo Bovo** - *11:57:19*

Hi, my name is Filippo, and I have just joined this workspace.



Yesterday, I tested out the `betfairlightweight` Python package. It is really well written.



As I am quite new to the Betfair APIs, I wonder how I can get the price ladder data for a certain selection in a market.



I tried listing the runner and selection details straight from `betfairlightweight.APIClient`, but they don't seem to have details on the price ladder. My guess is that I should use a `betfairlightweight.StreamListener` to get such data, but I cannot test this as I am using a delayed app key, which does not support the Betfair Stream API.



Could you please let me know if using a stream listener is the only way to get the price ladder data?



Thanks.

*Tags: Getting Started*

---

## 2019-04-24

**liam** - *11:40:56*

Betfair AUS have created a easy to follow tutorial for those looking to get started using python [https://betfair-datascientists.github.io/api/apiPythontutorial/](https://betfair-datascientists.github.io/api/apiPythontutorial/)

*Tags: General Technical*

---

**OT** - *11:52:58*

I often wonder if these people that write the APIs have ever been a user and programmed a strategy for one. although betfair's is definitely the best I've used.

*Tags: Strategies*

---

**Filippo Bovo** - *21:33:20*

Yeah, I had already checked them out. They were helpful.

*Tags: General Technical*

---

## 2019-04-25

**Filippo Bovo** - *11:31:15*

I am a data scientist and have been using Pandas for about three years. Pandas was built with speed in mind using C (and Numpy), and it is good to manipulating static data. However, I have never tried dealing with live data with Pandas DataFrames. The main bottleneck I see in this case is appending and dropping data to an existing DataFrame. If I will test this in the future, I will let you know.

*Tags: Feature Engineering, Performance, Deployment*

---

## 2019-04-27

**jgnz** - *11:08:05*

hi all, is there anyway to get historical data for non sport events? when i browse through [https://historicdata.betfair.com/#/mydata](https://historicdata.betfair.com/#/mydata) it would appear not

*Tags: Data Quality*

---

## 2019-04-29

**Leon** - *16:08:47*

Hi all,  does anyone have experience of using the datetime functions of betfairlightweight? I'm currently using market_book.publish_time to get the time of each market update in the historic betfair data which gives me the date/time in standard human readable format (YYYY-MM-DD HH:MM:SS) which I appreciate is what most people would want, but I would like to keep the time as UNIX/Epoch time as it will be easier to process for the application I'm working on. Is this possible with betfairlightweight? I would like to use betfairlightweight for consistency as I'm using it to parse other elements of the data, although I probably could either convert the date/time back to Epoch (but these seems an unnecessary processing overhead) or come up with a way to parse the datetime field without using betfairlightweight - but neither of these are ideal. Many thanks in advance

*Tags: General Technical*

---

## 2019-04-30

**Jonatan (skyw)** - *16:03:28*

Hade wrong competitionID, always good to ask seem to solve most problems ; )

*Tags: General Technical*

---

## 2019-05-01

**liam** - *17:08:49*

its in the marketDefinition in streaming

*Tags: General Technical*

---

**liam** - *17:13:49*

your not using streaming?

*Tags: General Technical*

---

## 2019-05-03

**Rob (NZ)** - *11:54:57*

Cheers for the advice .. it really helps save time of over cooking for newbies like myself...   I'm basically wanting to wait till 30secs prior to the race time as one of my dependencies so just starting to look at the best way to do that now for automation

*Tags: General Technical*

---

## 2019-05-26

**liam** - *09:02:38*

v1.10.0 released which fixes a few bugs (vscode/stop) and a new type of historical data stream (generator) [https://github.com/liampauling/betfair/blob/master/HISTORY.rst](https://github.com/liampauling/betfair/blob/master/HISTORY.rst)

*Tags: Data Quality, Errors Debugging*

---

**Rob (NZ)** - *10:16:34*

cheers Liam,   I have been using the example code from the betfair website ...   would i just remove the event_id as I imagine thats filtering to just that single event and then put the code you provided down under the list_market_catalogue section?



market_catalogue_filter = `betfairlightweight`.filters.market_filter(event_ids=['28971066'])



market_catalogues = trading.betting.list_market_catalogue(

    filter=market_catalogue_filter,

    max_results='100',

    sort='FIRST_TO_START'

)



# Create a DataFrame for each market catalogue

market_types_mooney_valley = pd.DataFrame({

    'Market Name': [market_cat_object.market_name for market_cat_object in market_catalogues],

    'Market ID': [market_cat_object.market_id for market_cat_object in market_catalogues],

    'Total Matched': [market_cat_object.total_matched for market_cat_object in market_catalogues],

})



market_types_mooney_valley

*Tags: Feature Engineering, Strategies*

---

## 2019-05-27

**Newbie99** - *16:05:10*

Not sure if this helps:



```

    mkt_runners = []

    for market_catalogue in market_catalogues:

        for r in market_catalogue.runners:

                mkt_runners.append((r.selection_id, r.runner_name, market_catalogue.market_id, market_catalogue.event.name,

                                market_catalogue.event_type.name, market_catalogue.market_name))



    return mkt_runners

```



Similar to that should work

*Tags: General Technical*

---

## 2019-06-02

**Chris** - *21:14:03*

Trying to display data from a market using Pandas as per the example on the Betfair site (I think I remember seeing these weren't created by anyone here and weren't great, but nonetheless seems the best place to ask), basically have the same problem as this stackoverflow post (but this isn't me), any advice greatly appreciated... [https://stackoverflow.com/questions/55420596/index-error-when-using-betfairlightweight](https://stackoverflow.com/questions/55420596/index-error-when-using-betfairlightweight)

*Tags: Errors Debugging, Feature Engineering*

---

**Chris** - *21:22:23*

That's frustrating given it is from what seems to be someone at least associated with Betfair :thumbsdown: [https://betfair-datascientists.github.io/api/apiPythontutorial/](https://betfair-datascientists.github.io/api/apiPythontutorial/)

*Tags: General Technical*

---

**Chris** - *21:35:25*

I'm a relative Python noob, but from what I can gather there isn't a default set when there is no price or volume on a specific selection hence the list being empty

*Tags: General Technical*

---

**Chris** - *21:37:03*

I tried to set that with a try and except wrapper around the section, but due to my lack of knowledge with Python (and more specifically how the structure of data is working) that isn't right because it then sets all of the prices to the defaults I have set, rather than picking out the real prices and only hitting the except rule when no price is available, I guess this is because it isn't looping over the data but is in a list?

*Tags: General Technical*

---

## 2019-06-05

**Leon** - *14:21:52*

Hi [@U4H19D1D2](@U4H19D1D2) it was me who posted on stackoverflow - I've been given a solution now so thanks to all who contributed. You mentioned that 'the code hurts to read' - any chance you could give us a couple of pointers on how to improve it? (I'm not arguing that the code is well written, I'm just fairly new to python so looking for some pointers). Some parts of the code were taken from the betfair website as has been mentioned above (I had assumed that would be of reasonable quality, but maybe not :face_with_raised_eyebrow:)

*Tags: Getting Started*

---

## 2019-06-10

**William** - *07:09:06*

Anyone know how to get the open time off an individual horse race?

*Tags: General Technical*

---

**Conr** - *09:42:18*

Getting through the docs but have a question around the best flag for the  time football or tennis match starts (or ends)

*Tags: General Technical*

---

**liam** - *09:46:37*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L252](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L252)

*Tags: Strategies*

---

**liam** - *09:50:57*

streaming has an openDate but I think that is the same as startTime

*Tags: General Technical*

---

**liam** - *09:50:58*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L104](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L104)

*Tags: General Technical*

---

**Conr** - *09:51:51*

(Bit off streaming yet, but I have bookmarked it!)

*Tags: General Technical*

---

**liam** - *09:53:58*

let bflw do all the work

*Tags: General Technical*

---

**Rob (NZ)** - *11:06:20*

Can anyone tell me if my logic is ok, Im able to output say 5 races based on my criteria, based on getting that output should it be able to put the output into pandas for the horses  with say expect start time  (similar to what I think Conr is asking up above

*Tags: Feature Engineering*

---

**Mo** - *11:59:04*

(Having done `import pandas as pd`)

*Tags: Feature Engineering*

---

## 2019-06-11

**Rob (NZ)** - *09:20:22*

[@UBS7QANF3](@UBS7QANF3) Cheers mate, thats awesome and working a treat.....thanks for taking the time to help out, really appreciate it

*Tags: General Technical*

---

**Mo** - *12:07:17*

Happy to help :+1:

*Tags: General Technical*

---

**Mo** - *15:31:37*

Sorry wasn't it you that posted [https://stackoverflow.com/questions/55420596/index-error-when-using-betfairlightweight](https://stackoverflow.com/questions/55420596/index-error-when-using-betfairlightweight)?

*Tags: Errors Debugging*

---

**Unknown** - *17:48:35*

Hi guys, is there a way to calculate the % figures using BFLW? I was thinking SP_traded /SP_available ?

Or is there some boiler plate script lying around?

*Tags: General Technical*

---

**Chris** - *21:13:36*

Has anyone been able to get back and lay odds from the historical data?

*Tags: Data Quality*

---

## 2019-06-12

**Ian** - *06:31:19*

hi all - small question about running your code in production in the cloud - do you typically use compute in a VM connected to storage or or do you use the next level of abstraction / webapps etc.?

*Tags: Deployment*

---

**liam** - *07:22:50*

Containers on ec2 instances

*Tags: Deployment*

---

**Rob (NZ)** - *14:45:09*

It worked a treat for me...   I'd really like to put up some of the python code I've got to help others ... what platform would you recommend ? Github etc?

*Tags: General Technical*

---

**Mo** - *16:35:55*

Yes, GitHub is great. Depending on exactly what you want to do, Gists ([https://help.github.com/en/articles/about-gists](https://help.github.com/en/articles/about-gists)) might be better than a full blown repository

*Tags: General Technical*

---

## 2019-06-13

**Conr** - *09:15:38*

Hi guys, random q, can I download historical market data for free from Betfair (low frequency) and if so, any good repo's on how to process the json for a bit of back testing based on markets ?

*Tags: General Technical*

---

**jci** - *13:55:01*

hey all,



I'm trying to get information on the total amount matched on each runner in an event using streaming. When I execute the code below it returns 'None' for every runner.. any ideas what I'm doing wrong?



market_books = output_queue.get()

for market_book in market_books:

    for runner in market_book.runners:

        print runner.total_matched

*Tags: General Technical*

---

**liam** - *13:55:35*

is this live data?

*Tags: Deployment*

---

**Dan** - *22:30:15*

Hey, sorry numpty question but trying to get up and running I have an app key and SSL cert added to my account, how do I install betfairlightweight?  I've downloaded it from github but don't understand the $ pip install betfairlightweight instruction...where do I use this if not in python?

*Tags: Getting Started*

---

## 2019-06-14

**Mo** - *08:38:40*

`pip install betfairlightweight` would be run in a Command Prompt

*Tags: Getting Started*

---

**Mo** - *08:38:59*

`pip` is a separate application for installing Python packages

*Tags: Getting Started*

---

**Mo** - *09:11:17*

`pip` will already be installed as part of Python

*Tags: Getting Started*

---

**Mo** - *09:13:55*

I don't use Windows myself but I use PyCharm*: [https://www.jetbrains.com/pycharm/](https://www.jetbrains.com/pycharm/). I would definitely recommend using that to develop in and it should also be able to help you with things like package installation: [https://www.jetbrains.com/help/pycharm/installing-uninstalling-and-upgrading-packages.html](https://www.jetbrains.com/help/pycharm/installing-uninstalling-and-upgrading-packages.html)

*Tags: Getting Started*

---

**Mo** - *09:14:54*

(*I actually use IntelliJ with the Python plugin)

*Tags: General Technical*

---

## 2019-06-15

**Jonatan (skyw)** - *17:59:28*

Anyone know if historical data will be available for us again with bad domains  .SE

*Tags: Data Quality*

---

**liam** - *18:23:44*

Currently rebuilding the API / migrating the data. The background was that historical data was to be accessed through the web app, when released I reverse engineered the API and added to bflw as well as highlighting a few issues to betfair. They responded by fixing and adding the documentation, they then underestimated the load from people automating the downloading thus the refactor. 

*Tags: Data Quality, Errors Debugging*

---

## 2019-06-16

**Mo** - *09:29:15*

And you want to bet on the first runner? I can't remember pandas syntax off the top of my head but you probably want `first_runner_selection_id = runners_df.loc[0, 'Selection ID']`

*Tags: Feature Engineering*

---

**Nghia Nguyen** - *09:30:41*

runners_df is a pandas with these columns: 'Selection ID': selection_ids,

        'Best Back Price': best_back_prices,

        'Best Back Size': best_back_sizes,

        'Best Lay Price': best_lay_prices,

        'Best Lay Size': best_lay_sizes,

        'Last Price Traded': last_prices_traded,

        'Total Matched': total_matched,

        'Status': statuses,

        'Removal Date': scratching_datetimes,

        'Adjustment Factor': adjustment_factors

*Tags: Feature Engineering*

---

**Nghia Nguyen** - *09:56:37*

btw, do you know how to get the odds from this api?

*Tags: General Technical*

---

**Mo** - *10:05:57*

But you may prefer to use the streaming API: [https://github.com/liampauling/betfair#streaming](https://github.com/liampauling/betfair#streaming)

*Tags: General Technical*

---

**Mo** - *10:13:34*

No problem, let us know if you have any more questions

*Tags: General Technical*

---

## 2019-06-19

**Unknown** - *13:39:32*

Any idea why I'm getting such big difference between the Betfair page and the data from betfairlightweight, some are correct and become more correct the closer to the start, it's probably some parameter I'm not setting but can't seem to figure out which one

*Tags: General Technical*

---

**Jonatan (skyw)** - *15:22:15*

```

 fields = ['EX_LTP', "EX_BEST_OFFERS_DISP", 'EX_BEST_OFFERS', 'EX_MARKET_DEF', 'EX_TRADED_VOL']

 market_data_filter = streaming_market_data_filter(

      fields=fields,

      ladder_levels=3,

 )

```

*Tags: General Technical*

---

**Jonatan (skyw)** - *16:18:23*

this is the last row from the database, it converted to swedish krona

*Tags: General Technical*

---

## 2019-06-20

**Rob (NZ)** - *10:05:24*

Does anyone know of a good resource to describe the difference between the marketbook and streaming data and when you may want to use on or the other...

*Tags: General Technical*

---

**Mo** - *10:20:59*

betfairlightweight presents the streaming data to you identically as if you had called listMarketBook (with the addition of the marketDefinition). If you have streaming enabled on your account, it's hard to imagine any scenario where making listMarketBook calls would be preferred

*Tags: General Technical*

---

**Mo** - *11:57:02*

```

days_ago = (datetime.datetime.utcnow() - datetime.timedelta(days=14)).strftime("%Y-%m-%dT%TZ")

acct_statement_date_filter = betfairlightweight.filters.time_range(from_=days_ago)



more_available = True

from_record = 0

data = []

while more_available:

    account_statement_result = trading.account.get_account_statement(item_date_range=acct_statement_date_filter, from_record=from_record)

    more_available = account_statement_result.more_available

    from_record += len(account_statement_result.account_statement)

    data.extend(account_statement_result._data['accountStatement'])



recent_transactions = pd.DataFrame(data)

recent_transactions

```

*Tags: Feature Engineering, Strategies*

---

**Mo** - *12:00:53*

(FYI I'm just writing this off the top of my head so there could be some obvious typos or other errors)

*Tags: Errors Debugging*

---

**Rob (NZ)** - *12:15:34*

Thats really helped me , I want to be able to output my results and get it feeding into Tableau or another visualization tool

*Tags: General Technical*

---

**Unknown** - *12:23:59*

I had this code which was going off and passing out the unknownstatementitem , is it possible to repoint that to the output of the code you helped me with?

*Tags: General Technical*

---

**Mo** - *14:35:31*

Sorry I was trying to look into it but it looks like my account statement has an item that breaks betfairlightweight :thinking_face:

*Tags: General Technical*

---

**Dan** - *21:42:20*

Back with another basic question before I've even getting to login :confused: I get the following error when attempting to login

*Tags: Errors Debugging*

---

**Dan** - *21:43:24*

Traceback (most recent call last):

  File "&lt;input&gt;", line 1, in &lt;module&gt;

  File "C:\Users\danmi\PycharmProjects\bflight\venv\lib\site-packages\betfairlightweight\endpoints\login.py", line 30, in __call__

    (response, elapsed_time) = self.request(self.url, session=session)

  File "C:\Users\danmi\PycharmProjects\bflight\venv\lib\site-packages\betfairlightweight\endpoints\login.py", line 42, in request

    raise APIError(None, exception=e)

betfairlightweight.exceptions.APIError: None

Params: None

Exception: Certificate folder not found in C:/users/danmi/PycharmProjects/bflight/venv/Lib/site-packages/betfairlightweight/certs

*Tags: Errors Debugging*

---

**Dan** - *22:05:23*

trading = betfairlightweight.APIClient('deleted','deleted',app_key='deleted',certs='C:/users/danmi/PycharmProjects/bflight/venv/Lib/site-packages/betfairlightweight/certs')

*Tags: Strategies*

---

## 2019-06-21

**liam** - *05:05:56*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L150](https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L150)

*Tags: General Technical*

---

## 2019-06-24

**Erdem** - *23:19:28*

Hey i m new here. Can I ask question from here?

*Tags: General Technical*

---

## 2019-06-25

**Erdem** - *00:31:04*

Can you help?

*Tags: General Technical*

---

**Jonatan (skyw)** - *13:06:58*

Yeah, I checked that, Even tried the most basic example from the examples in betfairlightweight with same issue will investigate

*Tags: General Technical*

---

## 2019-06-27

**Erdem** - *12:17:22*

for python

*Tags: General Technical*

---

**Mo** - *12:21:05*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**Erdem** - *12:42:32*

thank you how can i activate my normal api key, just delayed one activated and i am getting "NOT_AUTHORIZED","errorMessage":"AppKey is not configured for service" error

*Tags: Errors Debugging*

---

## 2019-06-28

**Brenton Collins** - *14:18:42*

Hello, new to this group. Has anyone had issues with the OverflowError: Maximum recursion level reached python. it is happening when placing a bet in a loop, I can manage to place a bet by punching the data for each bet but as soon as I put it in a list and set the loop going I get an overflow error!!

*Tags: Getting Started, Errors Debugging*

---

**Mo** - *14:25:41*

I was going to say, it doesn't sound like anything specific to betfairlightweight

*Tags: General Technical*

---

**Brenton Collins** - *14:26:34*

I think it has something to do with using pickle to store data, I need to move to database...

*Tags: General Technical*

---

**OT** - *15:28:57*

Bit of a general python question, but does anyone ever just pick up the betfair api class instance from the globals, instead of passing it through into numerous classes? I know globals are a bit frowned upon in Python, but passing the class instance around like a hot potato really annoys me :smile:

*Tags: General Technical*

---

**liam** - *15:39:25*

[@U7QLCB7HA](@U7QLCB7HA) why multiple classes? For me I just use the api class for streaming order / market data and my execution class which handles place/cancel/replace, I then have a few threads for keep alive and catalogue requests when required. All handed the trading class at initialisation and then forgotten about 

*Tags: Deployment, Strategies*

---

**OT** - *15:42:46*

[@U4H19D1D2](@U4H19D1D2), in this case I am creating different threads for each sport/market so that betting strategy only runs within that thread, and then just passing the betfair_api object around. I'm doing it, but just looking to tidy it up a bit.

*Tags: Strategies*

---

**Mo** - *16:06:10*

I'm struggling to see the problem with that design :joy:

*Tags: General Technical*

---

**liam** - *17:33:21*

I have at most 1 or 2 market streams, using some logic to cast the net as wide as possible. A strategy then subscribed to a market and my handler then makes sure the strategy gets any updates for it. I can have 50ish strategies all on the same markets so would be pointless and expensive to be duplicating the markets in multiple streams 

*Tags: Strategies*

---

## 2019-06-29

**Unknown** - *14:41:39*

I created a simple web app to view the market ladder for a certain selection of a market, using `betfairlightweight` to get the data from Betfair — [https://github.com/FilippoBovo/dashfair](https://github.com/FilippoBovo/dashfair).



When I compared the market ladder that I get from the web app and the market ladder from the Betfair website, I noticed that they are different — see the screenshot below where on the left there is my web app and on the right the market ladder view in the Betfair website.



At first I thought that the information on the website was delayed, as I was refreshing it every second. However, I looked into it better, and the delay does not seem to be the cause of the mismatch between the two ladders.



To give more context, as you may see from line 143 of this file, I am using a `streaming_market_data_filter` with `fields=['EX_MARKET_DEF', 'EX_ALL_OFFERS']`.



Has anyone had this is issue before or has a clue on what is going on?



Thank you.

*Tags: General Technical*

---

**Filippo Bovo** - *16:15:23*

I am currently getting the (back) prices using `markets_books[0].runners[0].ex.available_to_back`. However, I noticed that there is also a `markets_books[0].streaming_update` dictionary. Should I use that dictionary instead?

*Tags: General Technical*

---

**Filippo Bovo** - *17:30:31*

I’ll post it tomorrow, as I’ve left home. Thank you for your help!

*Tags: General Technical*

---

## 2019-06-30

**Unknown** - *10:05:46*

[@UBS7QANF3](@UBS7QANF3), here is a 20 second screen recording of my web app (on the left) against the market ladder view on the Betfair website (on the right) refreshing once per second. In function `streaming_market_data_filter`, I am using only these two filters: `EX_MARKET_DEF` and `EX_BEST_OFFERS_DISP`. Still the market ladder in my web app and in the Betfair websites are different. Thank you for your help.

*Tags: General Technical*

---

## 2019-07-01

**Jonatan (skyw)** - *17:47:51*

anyone know how to get total liability for markets, I get 0 for all my orders.

*Tags: General Technical*

---

**Filippo Bovo** - *22:49:14*

[@UBS7QANF3](@UBS7QANF3), [@U4H19D1D2](@U4H19D1D2), you were right, it was a delay issue. I did not know that the ladder view in Betfair is delayed and that without logging in to Betfair there is also a delay. I have just compared my dashboard against a couple of markets by going to the main page and logging in, and they match with the dashboard. Thanks a lot for your help!

*Tags: General Technical*

---

## 2019-07-02

**liam** - *17:54:01*

[@UBS7QANF3](@UBS7QANF3) out of interest how do you record data? Judging by the size I assume that is not streaming data?

*Tags: General Technical*

---

**Mo** - *17:56:14*

Correct it’s not streaming but to clarify I mean Great Britain not Gigabytes :joy:

*Tags: General Technical*

---

## 2019-07-03

**liam** - *07:16:30*

Publish time is streaming only 

*Tags: General Technical*

---

## 2019-07-04

**Robert** - *11:53:49*

Hi I'm using the api to process historical betfair data.  I am using the streaming api.  I want to have the total volume traded for a horse runner but when using the runner book total_matched I see this is not increasing with respect to publish_time and it jumps around a lot.  Any ideas what is happening here? 

*Tags: General Technical*

---

**Robert** - *12:17:59*

thanks for the quick help -- ill look into it -- to get the total_matched I am just looking at the runner_book within the market_book and doing runner_book.total_matched -- is that correct?

*Tags: General Technical*

---

## 2019-07-05

**Robert** - *09:15:21*

well I can detect this issue by just asserting that the volumes are not increasing -- even without plotting -- the script I sent does an assert that will catch it  -- I am using python 2 and betfairlightweight 1.10.0 and a slightly older numpy version -- if you are seeing its all OK when using my script then maybe its my versions so Ill check with latest versions of libraries on python 3 and then get back to you

*Tags: General Technical*

---

**Robert** - *09:31:21*

have tried using python3 and numpy and betfairlightweight latest and it still has same result

*Tags: General Technical*

---

**Robert** - *09:45:50*

I have tried:

python version is 3.5.3 and betfaitlightweight==1.10.0

python version 2.7.12 and betfairlightweight==1.10.0

and get the same result

*Tags: General Technical*

---

**Robert** - *09:47:09*

I do start a new python session and see this each time

*Tags: General Technical*

---

**Robert** - *10:05:39*

I still see the same values with python 3.6.8

*Tags: General Technical*

---

**liam** - *11:04:31*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**Mo** - *11:14:01*

Python 3.6.3

betfairlightweight 1.10.0

*Tags: General Technical*

---

**Robert** - *11:34:00*

tbh I try many dates and markets and they all have the same problem

*Tags: General Technical*

---

**Robert** - *11:34:13*

thanks for the help with this issue

*Tags: General Technical*

---

## 2019-07-06

**Mo** - *09:45:36*

Your options are to scrape the data yourself or pay for Betfair historic data. Betfair is currently rewriting their historic data service so nothing recent is available and there are questions over the quality of the data that is available, so I would recommend scraping yourself. If you do that then you should absolutely use the streaming API rather the REST API.

*Tags: General Technical*

---

**Chris** - *20:23:06*

I seem to be unable to start streams, I get this error ‘ERROR:betfairlightweight.streaming.listener:[Subscription: 2] NOT_AUTHORIZED: AppKey is not configured for service’ :thinking_face:

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *20:24:13*

You have to Buy a live key or get a delayed key approved for the streaming markets, I had no success getting delayed key from them

*Tags: Deployment*

---

**liam** - *20:32:32*

Send an email to bfp asking for your key to be authorised for streaming, this is in addition to getting a live key (no cost though)

*Tags: Deployment*

---

**liam** - *20:32:48*

Legacy from when streaming was invite only 3 ish years ago 

*Tags: General Technical*

---

## 2019-07-07

**Filippo Bovo** - *19:23:09*

Has anyone used a Google Cloud instance to log in to the Betfair APIs? I am using a Google Cloud instance based in London (I also double checked that the IP address is based in London and it is). I get the following error when I try to log in `BETTING_RESTRICTED_LOCATION`. Has anyone had a similar experience with Google Cloud? Thanks.

*Tags: Errors Debugging, Strategies*

---

**Mo** - *19:38:30*

[https://support.developer.betfair.com/hc/en-us/articles/115003887111-Why-am-I-receiving-the-error-BETTING-RESTRICTED-LOCATION-](https://support.developer.betfair.com/hc/en-us/articles/115003887111-Why-am-I-receiving-the-error-BETTING-RESTRICTED-LOCATION-)

*Tags: Errors Debugging, Strategies*

---

**Filippo Bovo** - *19:46:17*

Thanks, Mo. I saw that post before but the link that they give is broken. I googled it and found the correct link, and geolocating the IP address gives Mountain View California. So, that's the problem. Thanks.

*Tags: General Technical*

---

**Newbie99** - *21:45:14*

In the Betfair front end, I can see this bet has lapsed, however for some reason it shows as follows for me, via the API:



```

{'bet_id': '170304700688', 'average_price_matched': 0.0, 'bsp_liability': None, 'handicap': 0, 'matched_date': None, 'order_type': 'LIMIT', 'persistence_type': 'LAPSE', 'placed_date': '07/06/2019, 14:51:06', 'regulator_code': 'REG_GGC', 'side': 'LAY', 'price': 330, 'size': 4, 'size_cancelled': 0, 'size_lapsed': 0, 'size_matched': 0, 'size_remaining': 4, 'size_voided': 0, 'status': 'EXECUTABLE', 'customer_strategy_ref': '', 'customer_order_ref': ''}

```



Should I not expect size_lapsed = 4 in this instance, or is there some other way I should be checking for this?

*Tags: Strategies*

---

## 2019-07-09

**Mo** - *11:34:56*

I think there are a few different python packages for pinging

*Tags: General Technical*

---

## 2019-07-12

**Newbie99** - *21:02:39*

possibly a very basic question....I keep getting orders randomly disappearing from my calcs and I can't see where in my code this is happening, so I think (perhaps incorrectly), its something really basic to do with how I'm capturing them from the queue.



Ignoring the functions, is there anything glaringly stupid here, that I have done, that could possibly result in missing items from queues (I've only noticed the order queue, but its not impossible I'm missing things from the market queue too and simply haven't spotted that):



```

while True:

    try:

        order_books = order_queue.get(block=True, timeout=0.5)

        market_books = market_queue.get(block=True, timeout=0.5)

    except Empty:

        try:

            market_books = market_queue.get(block=True, timeout=0.5)

        except Empty:

            update_all(order_books, market_books, mkt_runners, profitloss, trading, eo, mb, market_catalogues, start_time)

        finally:

            update_all(order_books, market_books, mkt_runners, profitloss, trading, eo, mb, market_catalogues, start_time)

    finally:

        update_all(order_books, market_books, mkt_runners, profitloss, trading, eo, mb, market_catalogues, start_time)

```

*Tags: Strategies*

---

**Newbie99** - *21:21:29*

I didn't realise it was possible to use a single queue, I thought they had to be split...is there a recommended approach (i.e. is different threads likely to be better or does it not make much difference performance wise)?

*Tags: Performance*

---

**liam** - *21:27:08*

This is a good read [https://github.com/python/cpython/blob/3.7/Lib/queue.py](https://github.com/python/cpython/blob/3.7/Lib/queue.py)

*Tags: General Technical*

---

**Newbie99** - *23:51:04*

apologies...one final dumb question before bedtime...



I was just trying to put a quick fix in, before I delve into a whole new threading solution, but what I thought would be a fairly simple logical process...isn't:



```

while True:

    x = market_queue.get()



    for y in x:

        if type(y) == type(MarketBook):

            print('MarketBook: ', y)

        else:

            if type(y) == type(CurrentOrders):

                print('CurrentOrders: ', y)

            else:

                print('No idea: ', y)

```



I assumed I could neatly split out the single queue like that (for now, I can come up with a better solution later), however it seems to think nothing belongs to either custom class, i.e:



```

No idea:  CurrentOrders

No idea:  CurrentOrders

No idea:  MarketBook

No idea:  MarketBook

```



(I don't get an error, so I believe the custom classes are imported correctly)...so what have I missed here...I know it's going to be something blindingly obvious!

*Tags: Errors Debugging*

---

## 2019-07-14

**Filippo Bovo** - *22:52:11*

Hello, everyone. When I log out of Betfair through the APIs after these steps, I get a `ConnectionError`:

1. The day before a match, I set up a scheduler that waits until 30 minutes before the start of a match to begin streaming a market.

2. 30 minutes before the start of the match, the scrip logs in again to Betfair if `trading.session_expired` is true and launches the market stream.

3. As soon as the match turns in-play the stream stops and the scrips logs out of Betfair as `trading.logout()` — at that point I've got the error.



Note that I run the above script in an AWS compute instance and that normally, I don't get errors when I log out.



Does anyone have a hint at why I get the error? Thanks.

*Tags: Errors Debugging, Deployment, Strategies*

---

**Filippo Bovo** - *22:52:36*

Here is the error that I get:

*Tags: Errors Debugging*

---

## 2019-07-15

**liam** - *09:19:11*

[@UHQV8CW1Y](@UHQV8CW1Y) are you saying it works on AWS but not locally? On the buggy instance what version of openssl are you running?

*Tags: Errors Debugging, Deployment*

---

**Filippo Bovo** - *23:05:02*

[@U4H19D1D2](@U4H19D1D2), sorry, maybe I did not explain myself correctly. It works well on my local machine. Moreover, it works well on the AWS instance if I log out a few minutes after logging in. However, when I try to schedule the start of the market stream the night before on AWS, the day after I get the error when logging out.

*Tags: Errors Debugging, Deployment*

---

## 2019-07-16

**William** - *09:45:19*

Excluding any reasons on my part (most likely). Any reason bets arent being received on my end when streaming?

*Tags: General Technical*

---

**William** - *12:11:19*

Sorry for all the questions this has been bugging me

*Tags: Errors Debugging*

---

**magiclevinho** - *16:08:55*

Dear All! My problem is that maybe i m not making valid request for traded volumes in match odds e.g.

I can see properly all things in market_book_CS[0] list only the HOME/AWAY/DRAW traded volumes.

My code is the following, which works for total traded volume only:

                    market_book_CS = trading.betting.list_market_book(

                        market_ids=[active_market_list[i+3]],

                        price_projection=filters.price_projection(

                            price_data=filters.price_data(

                                ex_all_offers=True

                            )

                        )

                    )

market_book_CS[0].total_matched

*Tags: Strategies*

---

**magiclevinho** - *16:22:26*

I m using python 3.7 if that matters?!

*Tags: General Technical*

---

## 2019-07-17

**liam** - *17:44:46*

thats your problem

*Tags: General Technical*

---

## 2019-07-18

**jhaa** - *16:48:41*

Thanks liam. I tried that before but received a 503 error - assumed it was because the event ended

*Tags: Errors Debugging*

---

**jhaa** - *17:18:50*

Do you know how much time I have after the fixture ends to grab the final result?

*Tags: Errors Debugging*

---

## 2019-07-19

**William** - *09:35:31*

If there are delays in the while loop while streaming does each iteration of the loop contain all up to date information or just the next in the queueu

*Tags: General Technical*

---

**Oliver Varney** - *10:20:14*

Hi all, I have a design choice question which I have been trying to wrap my head around for a few days. I want to capture market prices/volumes both via a normal request and via streaming. Looking into both, they return different data structures but there is alot of crossover. Do people design a further class as an interface to bridge the differences in structure and data returned by the different forms of request or do they keep things entirely separate? Apologies if this is a bit of a noob question or doesnt make sense :slightly_smiling_face:

*Tags: General Technical*

---

**Mo** - *10:24:50*

When you say you looked into both, did you look at the betfairlightweight code or the Betfair API documentation? Because one of the advantages of betfairlightweight is that it presents streaming data to you as MarketBooks as you would get from listMarketBook

*Tags: General Technical*

---

**Mo** - *10:25:40*

I'd also be interested in exactly what you're trying to do because it's hard to comprehend a use case where you'd want to use both streaming and the REST API

*Tags: General Technical*

---

**Oliver Varney** - *10:33:05*

I mean via the python betfairlightweight library. My understanding is this, that you can use two approaches to get data. One is a request which will retrieve data at a point in time (the time of the request) or via the streaming which will constantly check for updates and put these into a queue. Using the request approach it seems that you first have to get the events, then get the market catalogues then the market books, all of which are returned in separate classes. Using the streaming option the Market Book objects are returned with a completely different structure to that of which is returned if you make the request via, list_events, list_market_catalogue, list_market_book

*Tags: General Technical*

---

**Oliver Varney** - *10:33:56*

I probably only will need the request approach currently but im thinking about a future project that would be more suited to streaming data

*Tags: General Technical*

---

**Mo** - *10:34:48*

Can you give an example of how the streaming MarketBooks differ from what is returned from list_market_book?

*Tags: General Technical*

---

**Oliver Varney** - *10:35:13*

an example is that there is no Market Catalogue in the streaming option but there is a description

*Tags: General Technical*

---

**Oliver Varney** - *10:38:22*

I guess im thinking from a database perspective, what is the relationship between an event, market catalogues , market books, runner catalogue, runner book and How can these be represented in one unified structure across the different types of request

*Tags: General Technical*

---

**Oliver Varney** - *10:41:21*

okay cool, do you see where im coming from though, the data is very similar with some additions in the streaming, but structurally in different , e.g. no Market catalogue via streaming

*Tags: General Technical*

---

**Mo** - *10:41:50*

Right but you have the marketDefinition from streaming

*Tags: General Technical*

---

**Oliver Varney** - *10:42:08*

yes so this is where the unified front question comes from

*Tags: General Technical*

---

**Oliver Varney** - *10:42:26*

as its called marketDef in streaming

*Tags: General Technical*

---

**Mo** - *10:46:06*

I think it can be useful to have a separate scraping process for market catalogues (and other things like cleared orders) but anything price-related should be done through streaming and not list_market_book

*Tags: General Technical*

---

**Oliver Varney** - *10:48:04*

okay cool thanks for the help, was having a mind block. Just another quick question, but how do people handle the streaming when the API goes down

*Tags: General Technical*

---

**Oliver Varney** - *10:48:23*

do you just have exception handling that tries to reconnect?

*Tags: Errors Debugging*

---

**liam** - *10:55:55*

Normally streaming will give a 503 and you just have to wait but you do need to add error handling to start as you get the odd socket closure and connection error which is raised through a bflw exception 

*Tags: Errors Debugging*

---

**liam** - *11:13:09*

[https://github.com/liampauling/flumine/blob/6d209a8a98e834192837df924508c5899e381cb3/flumine/flumine.py#L84](https://github.com/liampauling/flumine/blob/6d209a8a98e834192837df924508c5899e381cb3/flumine/flumine.py#L84)

*Tags: General Technical*

---

**Oliver Varney** - *13:46:51*

cool thanks Liam and great work on the library's, really appreciate it. Just on flumine, is its purpose / use case to produce the historical streaming files that betfair produce and make you pay for (£200 for the horse racing ones!), or is there other use cases?

*Tags: General Technical*

---

## 2019-07-20

**Dawid** - *20:32:32*

hi, I started using betfairlightweight and I am very happy with it, thank you for sharing your work

*Tags: General Technical*

---

**Dawid** - *20:35:16*

I have one problem - I want lot of historical data to make my own analysis, there is a lot of data I can access (I am using race cards via betfairlightweight and csv files from [https://promo.betfair.com/betfairsp/prices](https://promo.betfair.com/betfairsp/prices)), however I can't find race results with final horse place

*Tags: Data Quality*

---

**Dawid** - *20:44:30*

is it from betfairlightweight or some different code ??

*Tags: General Technical*

---

**Dawid** - *21:05:51*

yes, lack of Betfair ids it's a big problem, so much extra work...

*Tags: General Technical*

---

## 2019-07-21

**Anshuman** - *06:23:20*

Hi, I have a delayed api key how to use that in the github betfair app to build my first working app. Little help of yours will help me a lot

*Tags: General Technical*

---

**Anshuman** - *14:30:28*

maybe this question is silly as I am very new with Betfair API

*Tags: General Technical*

---

**Anshuman** - *14:39:18*

can you please tell more about it how to do?

*Tags: General Technical*

---

## 2019-07-22

**Oliver Varney** - *12:10:07*

Morning all, hope everyone had a nice weekend! Liam just looking through the flumine code now and was wondering if you separate out your market recording via flumine from your trading programs or can the two be easily used in conjugation without hacking code or performance issues?

*Tags: Performance, Strategies*

---

**liam** - *12:11:03*

i run them separate, flumine uses a custom listener which uses minimal cpu/ram

*Tags: General Technical*

---

**Oliver Varney** - *14:03:04*

so in this case liam, would the flumine code reconnect as soon as it come back up ?

*Tags: General Technical*

---

**Mo** - *14:47:18*

Is that some kind of AWS log viewer?

*Tags: Deployment*

---

**Oliver Varney** - *15:22:25*

by default visual studios wasnt spitting these out to the console for me. I am however seeing a high latency message now. What is the typical latency people see via the streaming market option ?

*Tags: Performance*

---

## 2019-07-23

**Oliver Varney** - *08:24:33*

Just a general question again, apologies if its a bit of a noob question :slightly_smiling_face:. How do people go about adding in custom functionality. The marketbooks are return via streaming which I believe (correct me if im wrong) cache the marketbooks, apply an update and then return this. Do people alter the MarketBookCache class to add custom code directly into this, or just take the returned marketbooks and then create their own custom classes which are largely similar to that which is returned?

*Tags: General Technical*

---

**Mo** - *08:26:18*

I wouldn't change the betfairlightweight code but I can see use cases where you want to use a custom listener rather than have it do the caching for you

*Tags: General Technical*

---

**Oliver Varney** - *08:27:49*

Im just thinking about the best way to handle the returned data structure and how to add custom functionality into it

*Tags: General Technical*

---

**liam** - *08:37:07*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1563844455039600](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1563844455039600)



Interesting idea, I feel like it would need to be a presentation though as looking at code is confusing 

*Tags: General Technical*

---

**William** - *10:35:17*

Is the flumine sample code up to date?

*Tags: General Technical*

---

**liam** - *10:38:36*

Probably not, getting an error?

*Tags: Errors Debugging*

---

**Oliver Varney** - *11:39:05*

quick question on the historical stream, should this operate like the normal stream, i.e. have the same structure and return a market book cache via the output_queue.get()?

*Tags: General Technical*

---

**William** - *12:06:12*

Where is the no certification login performed and details provided in flumine?

*Tags: General Technical*

---

**William** - *12:10:13*

How do you do trading.login_interactive() in flumine without certs?

*Tags: Strategies*

---

**William** - *13:12:14*

Would one flumine instance for say UK horse racing suffice or better to separate into multiple for a single market type?

*Tags: General Technical*

---

**Mo** - *14:35:15*

[@U4H19D1D2](@U4H19D1D2) what's your plan for a permanent fix for the IPS URL?

*Tags: Errors Debugging*

---

## 2019-07-24

**William** - *09:16:39*

Would a filter in flumine only go for the current day so you dont exceed the maximum subscription limit?

*Tags: General Technical*

---

**liam** - *09:20:41*

Have a read of the streaming docs on the betfair site 

*Tags: General Technical*

---

**Unknown** - *09:34:24*

Apologies for the basic questions, but im getting an empty list returned. Could it be an issue with my file directory? It does expect a file / json file right, not a zip or btz ?

*Tags: General Technical*

---

**liam** - *09:35:50*

Can you set logging to DEBUG and let it run through?

*Tags: Errors Debugging*

---

**Oliver Varney** - *09:46:35*

yes it looks perfect for what I was looking to achieve. Does anyone looks to store this data in a database or a snapshot at particular interest points?

*Tags: General Technical*

---

**liam** - *09:51:38*

I just use flumine and store all data in the raw json 

*Tags: General Technical*

---

**Oliver Varney** - *09:54:19*

my use case is to feed it into a AI algorithm interest points / periods, so I think its easier for me to capture these from the json file and store these in a database and feed them directly.

*Tags: General Technical*

---

**Mo** - *10:00:25*

In my experience, you'll want to do some kind of processing on them first before putting them in a database. Storing the raw updates doesn't scale well

*Tags: General Technical*

---

## 2019-07-25

**Ian** - *18:31:21*

hi all - question on flumine - I have this running and the data is collected and stored just fine and I can see the JSON for each market - however the markets are never archived or removed from the cache - anything obvious I might be missing - it is a vanilla clone/run

*Tags: General Technical*

---

**liam** - *18:31:57*

yeah bflw doesn't remove them

*Tags: General Technical*

---

**Ian** - *19:29:36*

i was just slightly confused as i thought the pattern would be to look for the zip files, import them into db. but if I understand correctly i should just be taking data direct from the the FLUMINE_DATA  folder when market is closed?

*Tags: General Technical*

---

**liam** - *19:31:52*

 [https://github.com/liampauling/flumine/blob/master/flumine/storage/storageengine.py](https://github.com/liampauling/flumine/blob/master/flumine/storage/storageengine.py)

*Tags: General Technical*

---

**liam** - *19:32:14*

[https://github.com/liampauling/flumine/blob/6d209a8a98e834192837df924508c5899e381cb3/flumine/storage/storageengine.py#L136](https://github.com/liampauling/flumine/blob/6d209a8a98e834192837df924508c5899e381cb3/flumine/storage/storageengine.py#L136)

*Tags: General Technical*

---

## 2019-07-27

**William** - *09:36:11*

When back testing with streaming historical would you be modifying the on_process function to do most of the work?

*Tags: General Technical*

---

**Oliver Varney** - *11:23:07*

morning all, question on the traded price size (trd). I have just noticed that some of the numbers returned dont match the increments in the ladder. Is this correct / expected or have I made a mistake?

*Tags: General Technical*

---

**ash** - *22:29:57*

while on the subject of traded (trd), can anyone point me in the right direction when trying to determine if the traded price listed on the stream was on the Back or Lay side. In some cases I think I can see based on the changes to atb/atl. However, I am seeing occasions where this isn't adding up. For example in the picture below. i have a traded volume of 39.99 @ 12. but I have changes on both atl and atb for 12. As I understand it atb could have gone from 46.85 to 6.89 or atl could have gone from 40 to 0, or is the [12,0] always somebody canceling a bet, as opposed to being matched and all the money staked at that price been taken up?



I say, i am currently using the delayed streaming API. so this could potentially be an artefact of the data arriving in blocks? - this might explain my current level of understanding too.

*Tags: General Technical*

---

## 2019-07-28

**liam** - *06:22:33*

[@UKD8R5P9N](@UKD8R5P9N) you can modify process but I always use the generator to get the data and save to csv, I then load into a df / database for further processing. I think it might remove the on_process example as it over complicates things 

*Tags: General Technical*

---

**William** - *06:36:52*

How do you have the equivalent of the while loop when streaming?

*Tags: General Technical*

---

**William** - *06:37:26*

using historical data

*Tags: Data Quality*

---

**liam** - *06:42:05*

Exactly the same as you would with live data but use historical_stream

*Tags: Deployment*

---

**liam** - *07:00:04*

Python is slow 

*Tags: Performance*

---

**liam** - *17:17:56*

Not sure what you mean, you only get an update if there is an update, there is a variable which holds the update called streaming_update which you can use [https://github.com/liampauling/betfair/blob/285d694b096daefe80b53eded7b5712d3cbaa4c7/betfairlightweight/resources/bettingresources.py#L477](https://github.com/liampauling/betfair/blob/285d694b096daefe80b53eded7b5712d3cbaa4c7/betfairlightweight/resources/bettingresources.py#L477)

*Tags: Strategies*

---

## 2019-07-29

**Oliver Varney** - *12:04:42*

another question guys, apologies for flooding the chat. Do people try to calculate stats such as amount of money matched in the last x seconds on a runner. I was thinking about storing a approximation of market orders derived from the traded price volume that are not my own

*Tags: General Technical*

---

**Oliver Varney** - *19:00:15*

Yes its a bit of a pain, possibly but ugly I think. I have so many questions on non runners lol. Does anyone have any betfair documents on the topic, specifically which fields are effected and any relevant calculations. One thing im going round in circles currently is the traded price size (trd) and the total volume (tv). It seems as though the traded prices are updated (setting old values to zero and new ones to the adjusted amount), but would we also expect the size to change and the total volume (tv). In my mind size = price * stake , so with an updated price the size and total volume on the runner should also change,  or am I confusing size with stake.

*Tags: General Technical*

---

## 2019-07-30

**William** - *02:14:23*

Can the historical streaming mode run multiple races at once?

*Tags: General Technical*

---

**Unknown** - *07:33:43*

Size doesn’t change as size = stake * 2 (as that is what has been matched) If you place and match a £10 order that will count as £20 matched [https://en-betfair.custhelp.com/app/answers/detail/a_id/408/~/exchange%3A-in-a-horse-race%2C-how-will-non-runners-be-treated%3F](https://en-betfair.custhelp.com/app/answers/detail/a_id/408/~/exchange%3A-in-a-horse-race%2C-how-will-non-runners-be-treated%3F)

*Tags: General Technical*

---

## 2019-08-04

**Ian** - *15:26:21*

Hi all, on flumine, I'm seeing high latency issues. Searching back this is can be clock or CPU (or some other bottleneck). Time looks good, In the logs, CPU is never &gt;1.5% utilisation. Network solid:



PING [http://ie1-ang.betfair.com|ie1-ang.betfair.com](http://ie1-ang.betfair.com|ie1-ang.betfair.com) (84.20.208.147) 56(84) bytes of data.

64 bytes from 84.20.208.147 (84.20.208.147): icmp_seq=1 ttl=243 time=11.8 ms

64 bytes from 84.20.208.147 (84.20.208.147): icmp_seq=2 ttl=243 time=11.10 ms

64 bytes from 84.20.208.147 (84.20.208.147): icmp_seq=3 ttl=243 time=11.7 ms

64 bytes from 84.20.208.147 (84.20.208.147): icmp_seq=4 ttl=243 time=11.6 ms

64 bytes from 84.20.208.147 (84.20.208.147): icmp_seq=5 ttl=243 time=11.8 ms

64 bytes from 84.20.208.147 (84.20.208.147): icmp_seq=6 ttl=243 time=11.7 ms

64 bytes from 84.20.208.147 (84.20.208.147): icmp_seq=7 ttl=243 time=11.6 ms



Anything else I should check?

*Tags: Performance*

---

**liam** - *15:37:07*

You can hide by setting the max_latency on the listener 

*Tags: Performance*

---

## 2019-08-05

**Chris** - *08:55:14*

market_data_filter = streaming_market_data_filter(

    fields=[‘EX_BEST_OFFERS’, ‘EX_MARKET_DEF’],

    ladder_levels=3,

)

*Tags: General Technical*

---

**liam** - *08:55:58*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L24](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L24)

*Tags: General Technical*

---

**liam** - *08:56:06*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L129](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L129)

*Tags: General Technical*

---

**liam** - *15:05:07*

what warnings are you getting, on every update? average latency?

*Tags: Performance*

---

**Ian** - *15:06:49*

i would say on average one error every 24h - max 3 some never

*Tags: Errors Debugging*

---

**liam** - *15:08:26*

oh, i would ignore then, i get about the same, all that warning is telling you this [https://github.com/liampauling/betfair/blob/26da8986d292f13af50262cddf85449b2f731bac/betfairlightweight/streaming/stream.py#L50](https://github.com/liampauling/betfair/blob/26da8986d292f13af50262cddf85449b2f731bac/betfairlightweight/streaming/stream.py#L50)

*Tags: General Technical*

---

**Chris** - *19:18:33*

So is there no way to know the name of an event in streaming?

*Tags: General Technical*

---

**Conr** - *20:35:41*

Hi, anyone got any thoughts on how to connect without certs? I am trying to deploy an App to ping the api every 30 mins , but certs are proving troublesome when connecting

*Tags: Deployment*

---

**Conr** - *21:14:51*

Can I ask a stupid question: what is the difference between api and interactive .....is api higher frequency or something?

*Tags: General Technical*

---

## 2019-08-06

**richard_h** - *08:28:48*

Strange started seeing this recently and now I'm logged out for 20 minutes at a time  betfairlightweight.exceptions.LoginError: API login: TEMPORARY_BAN_TOO_MANY_REQUESTS

*Tags: Errors Debugging*

---

**richard_h** - *08:29:25*

probably unrelated to betfairlightweight.

*Tags: General Technical*

---

**Ian** - *08:56:57*

hi all - data recording - I now have flumine rolling away collecting data and am working on importing that to a db. what's the best approach to matching up the stream data to the meta data - e.g. have a parallel process collecting these using NG

*Tags: General Technical*

---

**liam** - *09:15:51*

I personally store the static data (event / catalogue) in a database and then streaming data via flumine. I then process the streaming data into database tables if and when required joining on market / event ids 

*Tags: General Technical*

---

## 2019-08-07

**Chris** - *20:01:00*

Just a note for anyone using flumine, if your credentials are not inserted correctly it wont run when you execute start() but wont say why (at least it did with me)

*Tags: General Technical*

---

**Chris** - *20:21:08*

has anyone had an issue where flumine creates the folders but dosent insert any data?

*Tags: General Technical*

---

**liam** - *20:21:42*

Set logging to debug to see what is going on 

*Tags: Errors Debugging*

---

**Chris** - *20:45:32*

DEBUG:betfairlightweight.streaming.listener:[Subscription: 1001]: HEARTBEAT: {‘op’: ‘mcm’, ‘id’: 1001, ‘clk’: ‘ALshANI2AOYg’, ‘pt’: 1565207117314, ‘ct’: ‘HEARTBEAT’}

*Tags: Errors Debugging*

---

**Chris** - *20:47:56*

DEBUG:botocore.args:The s3 config key is not a dictionary type, ignoring its value of: None

*Tags: Errors Debugging*

---

**liam** - *20:48:52*

Can’t help much without seeing your code

*Tags: General Technical*

---

**Chris** - *20:51:14*

I had to change FLUMINE_DATA to my own directory because I had access issues

*Tags: General Technical*

---

**Chris** - *20:58:52*

flumine.py

*Tags: General Technical*

---

**Chris** - *21:02:06*

still getting the error now though

*Tags: Errors Debugging*

---

**Chris** - *21:06:08*

I think this is moved into something I can potentially solve myself though now, thanks for the help :slightly_smiling_face:

*Tags: General Technical*

---

## 2019-08-08

**Chris** - *18:57:52*

flumine.flumine.FLUMINE_DATA = data_dir

flumine.resources.recorder.FLUMINE_DATA = data_dir

flumine.storage.storageengine.FLUMINE_DATA = data_dir

*Tags: General Technical*

---

**Chris** - *19:58:29*

does anyone know how to increase the max number of subscriptions in flumine?

*Tags: General Technical*

---

## 2019-08-09

**Robert** - *15:00:12*

Hi, we are doing streaming of historical berfair data

like in the example [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

and we are only interested in data &lt;1200s before the race starts and not the rest and its very slow to process all of the data

how can we filter out all the requests so I dont have to stream all the requests

*Tags: Performance*

---

## 2019-08-15

**Rob (NZ)** - *12:07:42*

Any advice for taking the leap from running code locally to putting it in AWS or GCP etc . I currently have a database with selections and some code that checks that every 30secs to look for bets to place .. if a bet meets the criteria then it places the bet ..  i use Jupyter notebooks so have an initial step that does the connection to the api so that seems to be persistent when I run other cells like the betting code etc.   Just wondering what type of structure others are using and if anyone knows of any good blogs on it etc. Happy to change my structure to something better but just not sure what that is..   wondering if a virtual machine is the way to go or possibly just use lambdas on AWS etc

*Tags: Deployment, Strategies*

---

**liam** - *12:09:09*

you can spin up an ec2 instance for a few dollars a month

*Tags: Deployment*

---

**Rob (NZ)** - *12:16:58*

Awesome so good logic would be to setup a ec2 instance and use docker to deploy on it with the python code and betfairlightweight and other python libraries...  any thoughts on then how I should have my database ... so that the selections can be picked up etc (in my current setup it's just a local mysql setup so just not sure what would be the best in the cloud )

*Tags: Getting Started, Deployment*

---

**liam** - *12:17:40*

(aws mysql)

*Tags: Deployment*

---

**liam** - *12:18:01*

that way you can connect from local and from ec2

*Tags: Deployment*

---

**liam** - *12:18:23*

although best practice is to have an API over the top as you should never connect to a database directly

*Tags: General Technical*

---

**Jonatan (skyw)** - *12:49:18*

If you never dealt with containers before, I would first try to dockerize your app and have a way to build new images easily as a first step,



I use pycharm which can make use of docker containers as python interpreter which makes it easy to develop with.



When you got your Dockerized app just get it into the cloud, without worrying about deployment cycle :slightly_smiling_face:



Then terraform I would suggest :slightly_smiling_face:

*Tags: Deployment*

---

**liam** - *12:50:06*

Infrastructure as code, manages the state of what you have deployed. As it’s code you can put it under version control, once you get used to it you never want to go back to using the AWS console 

*Tags: Deployment*

---

**Rob (NZ)** - *12:57:54*

Cheers Mo ... is the AWS Rds a RDBMS? My main experience is with a local server with mysql and then using phpmyadmin over the top of it so just trying to understand how these parts interact

*Tags: Deployment*

---

**Mo** - *13:05:55*

Redis is an in memory database but it has lots of functionality built upon that (you can use it for pubsub messaging for example). MySQL, Postgres etc would be RDBMSs. From your brief description, I suspect Redis might be a better fit for what you’re using MySQL for. Very generally speaking, I think Redis is better suited for live trading and a RDBMS for all of your “offline” data needs like backtesting, bet reconciliation etc

*Tags: Performance, Deployment, Strategies*

---

**Oliver Varney** - *20:35:23*

noon question here, not sure its a specific python thing but looking through the cache.py file in the Available class, in update the indentation on the else looks off.

*Tags: General Technical*

---

**Oliver Varney** - *20:35:52*

Not sure what affect this would have, or even its if something that you can do in python but not in other languages

*Tags: General Technical*

---

**liam** - *20:59:06*

[https://stackoverflow.com/questions/9979970/why-does-python-use-else-after-for-and-while-loops](https://stackoverflow.com/questions/9979970/why-does-python-use-else-after-for-and-while-loops)

*Tags: General Technical*

---

## 2019-08-16

**Ian** - *09:27:52*

I've found dockerising python development to be very liberating - i'm an oldschool desktop dev learning python/docker &amp; bflw/api at the same time. Using azure not aws. don't want to turn this into a docker conversation but I found this guide useful - onlything not working for me is the watchmedo

[https://hackernoon.com/efficient-development-with-docker-and-docker-compose-e354b4d24831](https://hackernoon.com/efficient-development-with-docker-and-docker-compose-e354b4d24831)

*Tags: Deployment*

---

**Rory** - *11:59:00*

[@U7R5CEDAL](@U7R5CEDAL) how do you debug Python code running inside a container?

*Tags: Errors Debugging*

---

**Rory** - *12:02:14*

&gt;I've found dockerising python development to be very liberating

*Tags: General Technical*

---

**Rory** - *12:03:51*

I take that to mean actual dev and not just deployments though ... unit tests obviously help but I'm still pretty keen on using a debugger ... and that's what holds me back from doing proper dev on containers (for Python at least. Eclipse has the notion of remote debuggers for Java).

*Tags: Errors Debugging, Deployment*

---

**Rory** - *12:04:23*

I believe the paid version of PyCharm has remote debugging for Python but I don't feel like shelling out for that :grin:

*Tags: Errors Debugging*

---

**liam** - *12:05:41*

unless you are using a slim/alpine image or something funky I see no benefit in debugging your code in a container. Normally flow is code/debug in an ide -&gt; create container -&gt; run tests using unittest or docker-compose-test -&gt; deploy

*Tags: Errors Debugging, Deployment*

---

**Ian** - *13:00:10*

I think the debug in container is for large teams - making environment setup for new devs less hassle 

*Tags: Getting Started, Errors Debugging*

---

**Rory** - *17:08:46*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1565953541263800](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1565953541263800)

*Tags: General Technical*

---

**Rory** - *17:16:15*

it would be nice to have a single environment where you did everything, instead of maintaining the same packages/versions/etc between the local machine and container ... there's really a lot of overlap between venv and containers ... both create a separate environment for you to work in ... ideally you'd just use one ... i.e point your IDE at a container that has everything set up ... want to try a new version of Python, then point to a different container .. but of course. debugging with containers is difficult which is why I asked the original question

*Tags: Errors Debugging*

---

**Rory** - *17:29:23*

those were my original thoughts when I sat down to do our project using BFLW but I ended up in local mode + containers after dev was done :grin:

*Tags: General Technical*

---

**Rory** - *17:33:30*

a little bit like Cloud9 ... where you can write a serverless function, debug it, test it and then deploy that same function ... all from within the IDE ... no messing around with different tools

*Tags: Errors Debugging, Deployment*

---

**William** - *22:01:02*

Still having difficulty with speed and historical streaming. Seems to be taking 20s to go through a single race but it is only using around 4% of CPU? Is it because it is limited by other factors? I am doing a fair bit of csv writing so that could be an issue.

*Tags: Performance*

---

## 2019-08-17

**William** - *00:05:16*

Or need to look through streaming_update?

*Tags: General Technical*

---

## 2019-08-18

**Dawid** - *23:12:59*

hey, quick question: CSV data from [https://promo.betfair.com/betfairsp/prices/](https://promo.betfair.com/betfairsp/prices/), I thought should be: BSP &gt;= PPMAX and BSP &lt;= PPMIN, but that's not the case

*Tags: General Technical*

---

## 2019-08-20

**liam** - *08:39:52*

Fairly sure that is not possible using the SP data, it’s very basic and full of errors 

*Tags: Errors Debugging*

---

## 2019-08-23

**liam** - *09:40:03*

Thanks to [@UDCDFJ0NS](@UDCDFJ0NS) I created a PR which fixes a pretty major bug with the streaming order cache when betting on handicap markets. I am surprised this hasn't been found before, is there anyone out there using order streaming with handicap markets? [@U4H3EEV45](@U4H3EEV45) [@UBS7QANF3](@UBS7QANF3) [@U5D4ZBEAG](@U5D4ZBEAG)?

*Tags: Errors Debugging, Strategies*

---

## 2019-08-28

**liam** - *19:50:51*

Smaller always smaller, making it a function of the odds helps as well, if you put £10 in at 100 you will find you only get 100% matched on losers 

*Tags: General Technical*

---

**liam** - *20:12:19*

Is your strategy still profitable if you do that?

*Tags: Strategies*

---

**liam** - *20:21:05*

That’s the problem, it will never be 100%, you need to iterate and record as much data as you can, doesn’t take many races to get an idea of if you have improved 

*Tags: General Technical*

---

## 2019-09-05

**JonJonJon** - *21:49:26*

Random question.... When the market book is downloaded with betfairlightweight it looks like the full market depth. Is that cpu intensive if I want to do it once a second for 10 markets, running on an Amazon micro instance?

*Tags: General Technical*

---

**liam** - *21:51:12*

Use streaming / are you sure you need the full depth?

*Tags: General Technical*

---

**JonJonJon** - *22:01:31*

I think I need the full depth. I read somewhere that using streaming can cause cpu issues if the market is busy, such as in the final minutes before a horse race. Have you experieced that before?

*Tags: General Technical*

---

**JonJonJon** - *23:25:57*

I tested the streaming api on my local machine. It seems to Use barely any cpu. Thanks for making such a great tool!

*Tags: General Technical*

---

## 2019-09-06

**liam** - *06:18:32*

Yeah it’s the opposite, streaming allows you to subscribe to 100s of markets without even touching CPU. I use micros and smalls for all of my programs 

*Tags: General Technical*

---

**stefan** - *12:40:28*

Hi all. Does anybody know if there is a way to filter by `eventTypeId` for historical data download, when your selected sport is `Other Sports`? Thanks :thumbsup:

*Tags: Data Quality*

---

**stefan** - *13:12:25*

Overall it looks like the api for historical data download has changed, and the way to obtain the file via `DownloadFile?filePath` is obsolete?

*Tags: Data Quality*

---

**stefan** - *13:13:17*

Getting an `ObjectMoved` from the API when trying via CURL as well.



```&lt;html&gt;&lt;head&gt;&lt;title&gt;Object moved&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h2&gt;Object moved to &lt;a href="/#/error.html"&gt;here&lt;/a&gt;.&lt;/h2&gt; &lt;/body&gt;&lt;/html&gt;```

*Tags: Errors Debugging*

---

**ash** - *13:26:04*

Question regarding possible delays in trd stream-data, compared with atb , atl , batb, batl?

Hi, I am trying to allocate if the trd amounts are either on the back or the lay side using atb,atl updates and comparing it with changes in the batb,batl. I am successful in most cases however am struggling to allocate in certain circumstances.



For example:



25134289 -- batbPre        #batb from previous stream update for SelectionID

        [[0, 12, 9.07], [1, 11.5, 1.4], [2, 10, 10.51], [3, 9.6, 26], [4, 9.2, 7.26], [5, 8.6, 7.13], [6, 8, 17], [7, 7.8, 22.05], [8, 5.9, 62.85], [9, 2.3, 15.58]]

25134289 -- batb        #current batb

        [[9, 2.3, 15.58], [8, 5.9, 62.85], [7, 7.8, 22.05], [6, 8, 17], [5, 8.6, 7.13], [4, 9.2, 7.26], [3, 9.6, 26], [2, 10, 10.51], [1, 11.5, 1.4], [0, 12, 9.07]]

batbSame        #change in batb from previous

        True

25134289 -- atbL

        [[12.5, 0], [12, 9.07]]

25134289 -- batlPre

        [[0, 14.5, 3.02], [1, 15, 4], [2, 15.5, 15], [3, 16, 2], [4, 18, 14.29], [5, 19, 3], [6, 30, 1.1], [7, 32, 2], [8, 34, 32.04], [9, 55, 2.45]]

25134289 -- batl

        [[9, 55, 2.45], [8, 34, 32.04], [7, 32, 2], [6, 30, 1.1], [5, 19, 3], [4, 18, 14.29], [3, 16, 2], [2, 15.5, 15], [1, 15, 4], [0, 14.5, 3.02]]

batbSame

        True

25134289 -- atlL

        [[14.5, 3.02]]

25134289 -- trd

         [[13, 11.63], [12.5, 27.64], [12, 28.85]]

25134289 -- assn trd  #interp of trd prices.

        UNKNOWN  --  [13, 11.63]  inplay - False

        Back1 [12.5, 27.64]

        Back1 [12, 28.85]



As you can see I have been able to determine the 12,5 and 12 prices as the back side, but struggling with 13. Has anyone found a successful solution for this? I am missing _something_

(I hope you can excuse the verbose question)

*Tags: General Technical*

---

## 2019-09-09

**Newbie99** - *21:57:30*

Actually one other way, that seems to be better is using list_market_books, using orders instead of matches and filtering on EXECUTION_COMPLETE, that approach gets most of the key info on each bet, although I can't see a way to filter on strategy...

*Tags: Strategies*

---

## 2019-09-10

**liam** - *08:45:12*

Yeah it looks like it, ‘being fixed’ but we have heard that before...

*Tags: Errors Debugging*

---

## 2019-09-11

**Chris** - *18:28:46*

Hi all, I was wondering if someone can help me with this.. I’ve been using flumine to store an odds stream. I’m looking at the correct score market that has 19 selections always (as far as I can tell), but when I print the lengths of the runner catalogue, the first is 19 but most of the others are between 1 and 4

*Tags: General Technical*

---

**Mo** - *18:30:20*

Aren’t these streaming updates where each message is going to contain updates for some small number of runners?

*Tags: General Technical*

---

**Mo** - *18:32:05*

I know nothing about flumine or this process for scraping odds data that seems popular but that would be my assumption 

*Tags: General Technical*

---

**liam** - *18:38:37*

It’s storing the raw data from the stream, in order to get a market book out you should read it using the betfairlightweight historical stream  

*Tags: General Technical*

---

**liam** - *18:47:02*

Yeah flumine is just recording the raw data just like the overpriced stuff you can buy from betfair but without the inaccuracies :smile:

*Tags: General Technical*

---

## 2019-09-12

**OT** - *10:36:39*

The streaming cache does not work without EX_MARKET_DEF , but is there away to disable use of the cache?

*Tags: General Technical*

---

**liam** - *10:37:46*

but yes, this is how flumine does it [https://github.com/liampauling/flumine/blob/master/flumine/listener.py](https://github.com/liampauling/flumine/blob/master/flumine/listener.py)

*Tags: General Technical*

---

**OT** - *10:38:30*

I'm just occasionally hitting this cache error, but I am definitely setting EX_MARKET_DEF. I don't actually want to disable the cache, I just want to debug around it. I think it's something to do with a market update coming in while I'm resubscribing to markets.

*Tags: Errors Debugging*

---

**OT** - *10:39:25*

I have been battling this error for a long time

*Tags: Errors Debugging*

---

**liam** - *10:41:03*

what error?

*Tags: Errors Debugging*

---

**OT** - *10:41:49*

betfairlightweight.exceptions.CacheError: "EX_MARKET_DEF" must be requested to use cache

*Tags: Errors Debugging*

---

**OT** - *10:58:06*

if the stream is expecting a fresh subscription, but it gets a delta (because it's already subscribed to that market), might this error occur?

*Tags: Errors Debugging*

---

**liam** - *10:58:49*

its due to the requirement of handling the shitty historical data

*Tags: Data Quality*

---

**OT** - *11:05:17*

just a thread crashing when it hits this error.

*Tags: Errors Debugging*

---

**OT** - *11:27:38*

there's gotta be some kind of meme for when time.sleep(1)  "fixes" your code

*Tags: Errors Debugging*

---

**OT** - *11:32:07*

would probably hit TOO_MUCH_DATA error, too

*Tags: Errors Debugging*

---

**Mo** - *11:32:17*

Doesn’t seem like a problem if it simplifies your code

*Tags: General Technical*

---

**OT** - *11:35:13*

Will probably rewrite this thing from scratch soon, then I can fix everything.

*Tags: Errors Debugging*

---

## 2019-09-13

**OT** - *10:48:33*

Do you guys and girls feel it's safe to coerce betfair's id strings into integers, for stuff like eventtype  .. with the exception of market_id of course which is clearly a bastard string

*Tags: Errors Debugging*

---

**Mo** - *13:55:06*

Has anyone else observed streaming price updates coming in some time after a market has been closed?



e.g

```

{"op": "mcm", "id": 1, "clk": "AK31jAUA9uWeBQCxp9IE", "pt": 1566406561261, "mc": {"id": "1.161522056", "marketDefinition": {"bspMarket": false, "turnInPlayEnabled": true, "persistenceEnabled": true, "marketBaseRate": 5, "eventId": "29425833", "eventTypeId": "1", "numberOfWinners": 1, "bettingType": "ODDS", "marketType": "OVER_UNDER_35", "marketTime": "2019-08-21T15:01:11.000Z", "suspendTime": "2019-08-21T15:01:11.000Z", "bspReconciled": false, "complete": true, "inPlay": true, "crossMatching": false, "runnersVoidable": false, "numberOfActiveRunners": 0, "betDelay": 5, "status": "CLOSED", "settledTime": "2019-08-21T16:55:57.000Z", "runners": [{"status": "WINNER", "sortPriority": 1, "id": 1222344}, {"status": "LOSER", "sortPriority": 2, "id": 1222345}], "regulators": ["MR_INT"], "countryCode": "PL", "discountAllowed": true, "timezone": "GMT", "openDate": "2019-08-21T15:01:11.000Z", "version": 2904477007, "priceLadderDefinition": {"type": "CLASSIC"}}}}

{"op": "mcm", "id": 1, "clk": "AMSpkgUA5rCkBQDMvdcE", "pt": 1566407145831, "mc": {"id": "1.161522056", "rc": [{"trd": [[1.05, 16.47], [1.02, 16.47], [1.11, 54.93]], "id": 1222344}]}}

```

*Tags: Strategies*

---

**Mo** - *15:46:16*

I’m curious how you handle this in flumine? I’ve not used it but I’m probably doing something similar to scrape data to one file per market and compressing on market closure

*Tags: General Technical*

---

**liam** - *16:10:15*

yeah event driven single threaded, dont care about latency

*Tags: Performance*

---

## 2019-09-14

**Mo** - *08:33:24*

I've been looking at the flumine code in more detail - what happens if you get a price update for a market that has been closed for more than `market_expiration`? It gets ignored? Do you know if that ever happens?

*Tags: General Technical*

---

**Newbie99** - *20:56:53*

I am getting updates, that I'm struggling to understand tonight...for example:



`{'id': '1.132099836', 'rc': [{'batb': [[0, 1.5, 313.6]], 'id': 10317011}]}`



I placed a £5 order @ 1.53, but it looks like potentially its rolled up somehow, which confused me, as the update before was:



`{'id': '1.132099836', 'rc': [{'batl': [[2, 1.59, 1.99], [1, 1.58, 500]], 'id': 10317011}]}`



Which suggest a £5 bet should not be rolled up.



The only thing I could think of, is possibly because I only have 3 ladder levels here:



`market_data_filter = streaming_market_data_filter(

    fields=['EX_BEST_OFFERS', 'EX_MARKET_DEF', 'EX_TRADED', 'EX_LTP'],

    ladder_levels=3)`



Given in the above example, the 1st / best price was the one that changed I don't get why it wouldn't show, but am I misunderstanding something here, or is this behaviour weird?

*Tags: General Technical*

---

**Newbie99** - *21:06:30*

It really looks like updates just aren't being sent, as I can see the order book has grown, with no updates coming through and that is just a print of:



`    market_books = market_queue.get()

    for market_book in market_books:

        print(market_book.streaming_update)`



I'm not manipulating anything at all, thats simply the raw update.

*Tags: General Technical*

---

**Mo** - *21:41:16*

The main question is whether the volume available to back at 1.5 was previously 308.6?

*Tags: General Technical*

---

**Newbie99** - *21:45:24*

A good question...not actually sure. I will play around with the ladder_level and see if somehow I can get any differing results.



Currently orders update instantly, but the market stream updates seem sporadic and just don't appear to make any sense (I even checked if I was using the delayed key just in case)!

*Tags: General Technical*

---

**Newbie99** - *21:52:39*

so that seems to have answered the question!

*Tags: General Technical*

---

**Newbie99** - *22:01:06*

out of curiosity, for this reason, does everyone just leave ladder levels as the max and just disregard whatever you don't need? I'm guessing that is probably best practice, as there's no real downside to doing so, and the other way (i.e. setting the ladder to what you were planning to use) creates problems as I've found!!

*Tags: General Technical*

---

**Newbie99** - *23:22:49*

I think I'm still missing something here...



`{'id': '1.132099836', 'orc': [{'id': 13231442, 'uo': [{'id': '178557559012', 'p': 48, 's': 2, 'side': 'B', 'status': 'E', 'pt': 'L', 'ot': 'L', 'pd': 1568499248000, 'sm': 0, 'sr': 2, 'sl': 0, 'sc': 0, 'sv': 0, 'rac': '', 'rc': 'REG_GGC', 'rfo': '', 'rfs': ''}]}]}

{'id': '1.132099836', 'orc': [{'id': 13231442, 'uo': [{'id': '178557559012', 'p': 48, 's': 2, 'side': 'B', 'status': 'EC', 'pt': 'L', 'ot': 'L', 'pd': 1568499248000, 'sm': 0, 'sr': 0, 'sl': 0, 'sc': 2, 'sv': 0, 'rac': '', 'rc': 'REG_GGC', 'rfo': '', 'rfs': ''}]}]}

{'id': '1.132099836', 'rc': [{'batl': [[9, 110, 3], [8, 100, 1.99], [7, 80, 4], [6, 75, 1.99], [5, 70, 2.01], [4, 65, 2.01], [3, 60, 4.01], [2, 55, 1.98], [1, 50, 7.4], [0, 48, 2]], 'id': 13231442}]}`



So  I placed a lay order for £2 @ 48, the first update comes through for the order and then I cancel it, hence the second update comes through, again for the order.



I then receive 1 market book update, showing the updated market with the order still live, having already received both of the order messages.



I can live with the weird ordering as that presumably can't be helped...but the issue is I never received another market book update.



I've now removed any reference to ladder levels completely and (initially) the updates appear to be working. Just to make sure my logic is (hopefully finally) sound, if I have no reference to ladder levels and therefore am intending to receive the entire non-virtual order book as you suggest, is there anything else I could have inadvertently (or just plain stupidly :slightly_smiling_face: set, that might be limiting the market book messages somehow?

*Tags: Deployment*

---

**Newbie99** - *23:31:06*

I'm definitely not getting all updates still, I managed to replicate the above situation again (different runner and market just to check).



my market data filters are just as follows:



`market_data_filter = streaming_market_data_filter(

    fields=['EX_BEST_OFFERS', 'EX_MARKET_DEF', 'EX_TRADED', 'EX_LTP'])`



No reference to ladders at all, so clearly I'm doing something somewhere else...but I can't work out where!

*Tags: General Technical*

---

## 2019-09-15

**Mo** - *09:36:04*

Although based on the most recent example I'm not convinced that is the problem

*Tags: General Technical*

---

**Newbie99** - *10:07:02*

I managed to replicate the issue again, this time, with EX_ALL_OFFERS, just to see.



My code, is stripped back, completely to basics here, so it should be obvious if something is wrong, but it looks okay to me, I can't see what would be withholding messages (and is there any possibility they aren't being sent somehow?)



```filters = betfairlightweight.filters



logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



trading = betfairlightweight.APIClient(accname, accpass, acckey, certs=path)

trading.login()



market_ids=['1.148222764','1.132099836','1.130856098']



market_queue = queue.Queue()

market_listener = betfairlightweight.StreamListener(output_queue=market_queue)

market_stream = trading.streaming.create_stream(listener=market_listener)

market_filter = streaming_market_filter(

market_ids=market_ids)



market_data_filter = streaming_market_data_filter(

    fields=['EX_ALL_OFFERS', 'EX_MARKET_DEF', 'EX_TRADED', 'EX_LTP'])



market_stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter)



market_stream.start(async_=True)



while True:

    market_books = market_queue.get()

    for market_book in market_books:

        print(market_book.streaming_update)```

*Tags: Strategies*

---

**Newbie99** - *10:07:40*

ooops, fixed now :slightly_smiling_face:

*Tags: Errors Debugging*

---

**Newbie99** - *10:21:33*

I just stripped them out, as the problem still arises using the above, so just wanted it to be super simple

*Tags: General Technical*

---

**Mo** - *10:28:00*

When you say problem, you mean that the bet appears in the order book but doesn't disappear when cancelled?

*Tags: General Technical*

---

**Newbie99** - *10:32:58*

on the betfair front end it appears and then disappears, but at my end via the streaming API it appears I miss messages (sometimes), for example, here a bet I placed correctly shows up as placed and cancelled:



```{'id': '1.132099836', 'rc': [{'atl': [[3.3, 2]], 'id': 10317012}]}

{'id': '1.132099836', 'rc': [{'atl': [[3.3, 0]], 'id': 10317012}]}```



But then, trying to replicate, I only receive one side:



```{'id': '1.132099836', 'rc': [{'atb': [[44, 1.99]], 'id': 13231442}]}```



(that one was a £2 back bet @ 44, where 1p got matched), but no cancellation message

*Tags: General Technical*

---

**liam** - *15:25:51*

I don’t understand the problem / see a problem 

*Tags: General Technical*

---

**Newbie99** - *18:22:28*

but just so I don't miss anything, I have the following line:



```logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))```



Does that need to be changed to debug, or is info level okay for what you're looking for (I did try debug initially and obviously saw a ton of messages, but nothing containing conflate = xxx)?

*Tags: Errors Debugging*

---

**liam** - *18:24:30*

Debug will show you everything so if you think there is a problem then set it to debug but I don’t think there is 

*Tags: Errors Debugging*

---

**liam** - *18:55:16*

There can only really be two reasons why, a delayed key or the data is being conflated due to you not reading data off the socket quick enough which could be network or resource issues. If conflated betfair will tell you with a conflated flag which you can see with the debug logs 

*Tags: Errors Debugging*

---

**Newbie99** - *23:17:58*

Just managed to replicate again, placed and cancelled an order, came through instantly via the order stream and nothing on via the market stream :disappointed:



```DEBUG:betfairlightweight.streaming.listener:[Subscription: 1]: UPDATE: {'op': 'mcm', 'id': 1, 'clk': 'ALQGAL4FAMAG', 'pt': 1568585572062, 'mc': [{'id': '1.132099836', 'rc': [{'atl': [[6, 2]], 'id': 10317012}]}]}

DEBUG:betfairlightweight.streaming.listener:[Subscription: 1]: UPDATE: {'op': 'ocm', 'id': 1, 'clk': 'AJ4DAMQDAN0BAJECAKYD', 'pt': 1568585572678, 'oc': [{'id': '1.130856098', 'orc': [{'id': 12832766, 'uo': [{'id': '178731051032', 'p': 2.54, 's': 2, 'side': 'B', 'status': 'E', 'pt': 'L', 'ot': 'L', 'pd': 1568585572000, 'sm': 0, 'sr': 2, 'sl': 0, 'sc': 0, 'sv': 0, 'rac': '', 'rc': 'REG_GGC', 'rfo': '', 'rfs': ''}]}]}]}

{'id': '1.130856098', 'orc': [{'id': 12832766, 'uo': [{'id': '178731051032', 'p': 2.54, 's': 2, 'side': 'B', 'status': 'E', 'pt': 'L', 'ot': 'L', 'pd': 1568585572000, 'sm': 0, 'sr': 2, 'sl': 0, 'sc': 0, 'sv': 0, 'rac': '', 'rc': 'REG_GGC', 'rfo': '', 'rfs': ''}]}]}

{'id': '1.132099836', 'rc': [{'atl': [[6, 2]], 'id': 10317012}]}

DEBUG:betfairlightweight.streaming.listener:[Subscription: 1]: UPDATE: {'op': 'mcm', 'id': 1, 'clk': 'AOYGAO0FAPAG', 'pt': 1568585572693, 'mc': [{'id': '1.130856098', 'rc': [{'atl': [[2.54, 19.72]], 'id': 12832766}]}]}

DEBUG:betfairlightweight.streaming.listener:[Subscription: 1]: UPDATE: {'op': 'mcm', 'id': 1, 'clk': 'AK0HAKcGALMH', 'pt': 1568585573555, 'mc': [{'id': '1.130856098', 'rc': [{'atl': [[2.54, 17.72]], 'id': 12832766}]}]}

DEBUG:betfairlightweight.streaming.listener:[Subscription: 1]: UPDATE: {'op': 'ocm', 'id': 1, 'clk': 'ALwDAOMDAPEBAKUCAMwD', 'pt': 1568585573644, 'oc': [{'id': '1.130856098', 'orc': [{'id': 12832766, 'uo': [{'id': '178731051032', 'p': 2.54, 's': 2, 'side': 'B', 'status': 'EC', 'pt': 'L', 'ot': 'L', 'pd': 1568585572000, 'sm': 0, 'sr': 0, 'sl': 0, 'sc': 2, 'sv': 0, 'rac': '', 'rc': 'REG_GGC', 'rfo': '', 'rfs': ''}]}]}]}

{'id': '1.130856098', 'orc': [{'id': 12832766, 'uo': [{'id': '178731051032', 'p': 2.54, 's': 2, 'side': 'B', 'status': 'EC', 'pt': 'L', 'ot': 'L', 'pd': 1568585572000, 'sm': 0, 'sr': 0, 'sl': 0, 'sc': 2, 'sv': 0, 'rac': '', 'rc': 'REG_GGC', 'rfo': '', 'rfs': ''}]}]}

{'id': '1.130856098', 'rc': [{'atl': [[2.54, 19.72]], 'id': 12832766}]}

```



So I placed and cancelled a £2 bet @ 2.54 on 1.130856098, came through the order stream, but only one instance came through the market stream. Interestingly this time it was the cancel order, previously its only been the placement order.

*Tags: Errors Debugging*

---

**Newbie99** - *23:18:09*

The above is with logging set to DEBUG

*Tags: Errors Debugging*

---

**Newbie99** - *23:19:26*

actually ignore that, having pasted it, I can clearly see both messages in the debug output!

*Tags: Errors Debugging*

---

## 2019-09-16

**Newbie99** - *07:31:46*

yep, I had just stripped out the market bits, the full code is:



```import threading

import logging

from datetime import datetime

import mysql.connector

import queue

from new_functions import calc_live_order_risk, view_runners, view_orders, database_run_sp, getprofitandloss, check_queue, \

    set_initial_market_book, inital_position, create_initial_market_book, calc_initial_risk, check_positions, live_positions, create_initial_risk, combine_stream_results

import auto_hedge

import betfairlightweight

from betfairlightweight.filters import (

    streaming_market_filter,

    streaming_market_data_filter,

    streaming_order_filter,

    market_filter,

    price_data,

    price_projection,

    ex_best_offers_overrides

)

from account_info import accname, accpass, acckey, path, db_host, db_user, db_passwd, db_database

from betfairlightweight.resources.bettingresources import MarketBook, CurrentOrders

from betfairlightweight.endpoints.account import Account

from betfairlightweight.endpoints.betting import Betting

from place_orders import place_order



logging.basicConfig(level=logging.DEBUG)  # change to DEBUG to see log all updates



filters = betfairlightweight.filters



mysql = mysql.connector



logging.basicConfig(level=[http://logging.info|logging.info](http://logging.info|logging.info))



trading = betfairlightweight.APIClient(accname, accpass, acckey, certs=path)

trading.login()



order_queue = queue.Queue()

order_listener = betfairlightweight.StreamListener(output_queue=order_queue)

order_stream = trading.streaming.create_stream(listener=order_listener)

order_filter = streaming_order_filter(

    include_overall_position=True

    # customer_strategy_refs=[""]

    #    partition_matched_by_strategy_ref=None

)

order_stream.subscribe_to_orders(

    order_filter=order_filter)

order_stream.start(async_=True)



market_ids=['1.148222764','1.132099836','1.130856098']



market_queue = queue.Queue()

market_listener = betfairlightweight.StreamListener(output_queue=market_queue)

market_stream = trading.streaming.create_stream(listener=market_listener)

market_filter = streaming_market_filter(

market_ids=market_ids)



market_data_filter = streaming_market_data_filter(

    fields=['EX_ALL_OFFERS', 'EX_MARKET_DEF', 'EX_TRADED', 'EX_LTP'])



market_stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter)



market_stream.start(async_=True)



mb =[]



initial_risk_list = create_initial_risk(trading, filters, market_ids)



def consumer(queue, initial_risk_list):

    while True:

        message = queue.get()

        output = check_queue(message,'market_list', 'order_list', update_time, initial_risk_list)

        live_data = combine_stream_results(output, mb, queue, update_time)





if __name__ == "__main__":

    format = "%(asctime)s: %(message)s"

    logging.basicConfig(format=format, level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO), datefmt="%H:%M:%S")

    pipeline = queue.Queue(maxsize=50)

    c = threading.Thread(target=consumer, args=(pipeline,initial_risk_list))

    c.start()



while True:

    update_time = datetime.now().strftime("%m/%d/%Y, %H:%M:%S")

    order_books = order_queue.get()

    for order_book in order_books:

        print(order_book.streaming_update)

    order_list = {'order_list': view_orders(order_books)}

    pipeline.put(order_list)

    market_books = market_queue.get()

    for market_book in market_books:

        print(market_book.streaming_update)

    runner_list, mb = view_runners(mb, market_books, mysql, db_host, db_user, db_passwd, db_database)

    runner_list = {'runner_list': runner_list}

    pipeline.put(runner_list)

    market_list = {'market_list': mb}

    pipeline.put(market_list)```

*Tags: Errors Debugging, Deployment, Strategies*

---

**Newbie99** - *07:31:48*

I cleaned it up before as this issue persists without all my functions etc, i.e. if the while true section is:



```while True:

    order_books = order_queue.get()

    for order_book in order_books:

        print(order_book.streaming_update)

    market_books = market_queue.get()

    for market_book in market_books:

        print(market_book.streaming_update)```



The issue still persists.

*Tags: General Technical*

---

**liam** - *07:38:45*

Simple fix is to use the same queue for both and you will see it working 

*Tags: Errors Debugging*

---

**liam** - *08:36:58*

```

while True:

    order_books = order_queue.get()

    for order_book in order_books:

        print(order_book.streaming_update)

    market_books = market_queue.get()

    for market_book in market_books:

        print(market_book.streaming_update)

```

*Tags: General Technical*

---

**jhaa** - *16:33:21*

i am getting 'INSUFFICIENT_FUNDS' as an error in PlaceOrders. I try to bet £3 on a horse. I have like £12k available in the account but I have loads of matched and unmatched bets in other sports. What am I missing?

*Tags: Errors Debugging*

---

**jhaa** - *16:44:49*

now I get StatusCodeError 500 . website seems down as well

*Tags: Errors Debugging*

---

## 2019-09-17

**SDhar** - *15:32:16*

Hi - is anyone able to help with how to extract Selection names from a market? Currently Selection IDs are easily available. Alternatively, being able to map selection IDs to names would also be helpful

*Tags: General Technical*

---

**Mo** - *15:32:45*

Streaming or REST API?

*Tags: General Technical*

---

## 2019-09-18

**liam** - *13:40:38*

if you care about ms you shoudn't be using python

*Tags: General Technical*

---

**liam** - *14:16:02*

i record all round trip latency for any requests and log, then aggregate via cloudwatch and datadog and monitor periodically / alerts when above certain thresholds

*Tags: Performance*

---

## 2019-09-22

**ash** - *15:11:42*

quick question, I hope. When i get ```betfairlightweight.streaming.stream:[Stream: 1]: Latency high: 0.8129971027374268``` is that due to latency at my end working through the Que,   or is it warning me that the data I am getting from bet-fair is delayed in some way? Likewise, is market_book.publish_time the time I receive the update or is it the time bet-fair sends it?

*Tags: Performance*

---

**ash** - *15:42:25*

Thankyou. I need to streamline in then, I also need to get better at not being lazy and looking through the betfairlightweight code, apologies for that.

*Tags: General Technical*

---

**Mo** - *15:43:12*

Someone else will have to answer your first question, I don't know myself

*Tags: General Technical*

---

**ash** - *15:52:07*

I think it’s cpu..   its only sporadic so correlates with increased demand due to more frequent stream updates.

isn’t the clock updated prior to calculating the latency?

I am trying to do calculations in the same program as reading the stream which is a bit to much of an ask i think. I need to rethink my overall structure maybe

*Tags: Performance*

---

**liam** - *16:14:42*

Yeah threads are your friend, ideally you don’t want to block the queue, the latency warning is handy in telling you that you have network or cpu issues. It’s based off your clock so common issue is it not being in sync 

*Tags: Performance*

---

## 2019-09-24

**JonJonJon** - *23:33:52*

I started to knock together something very basic with Dash... It can monitor orders by customer strategy ref at the moment. It won't be too hard to add a "delete all orders"button. When it is tidier I can share it if people are interested. Due to Dash limitations I think it can only be very basic, but it should handle my use case.

*Tags: Strategies*

---

## 2019-09-25

**JonJonJon** - *18:18:46*

A good api won't make you money, but it can help stop you losing it. I once worked with a guy who had an algo system that was very hard to turn off. Then he went on holiday butbinsisted we keep it running. Then it started losing money and we didn't have a good way to turn it off.



Anyway, there's no point arguing this. Some people will find a custom UI useful. Some won't.

*Tags: General Technical*

---

**Mo** - *18:36:34*

I think it’s been and continues to be invaluable to debate the different approaches people have to building platforms on here. 



I’m not convinced that a UI is the best way to implement any of the functionality discussed so far, seems like it’s a better use of time to focus on developing a winning strategy when there are off the shelf ways to monitor/interact. But I’m very interested to see what you’re doing with Dash and getting a fresh perspective. 

*Tags: Strategies*

---

## 2019-09-27

**Newbie99** - *13:12:58*

I just wanted to check an assumption, if that's okay...



When cancelling (or placing) an order, the report showing it was successful or failed comes back seemingly before the streaming update.



Therefore I always need to track the position at my end and adjust (if need be) once the streaming update comes?



So for example, in this scenario (assume 0 orders have been placed at the start):



1) Order for 50 @ 20 placed

2) Confirmation order has been placed successfully

3) order count = 0 as no streaming update has been sent yet (unless I adjust locally)

4) streaming update is received

5) order count = 1



and for cancellations:



1) Check order exists

2) If order exists, cancel 30 of the 50

3) Confirmation order has been (partially) cancelled successfully

4) order size_remaining = 50 as no streaming update has been sent yet (unless I adjust locally)

5) Streaming update is received

6) order size_remaining = 20



Hopefully that makes sense and is my logic correct (i.e. that I need to track locally and adjust if need be, post streaming update, I can't treat the placed/cancelled report in the same way as a streaming update)?

*Tags: Errors Debugging*

---

**hugo** - *14:33:03*

Dependencies



    Requests now supports urllib3 v1.25.2. (note: 1.25.0 and 1.25.1 are incompatible)



Deprecations



    Requests has officially stopped support for Python 3.4.

*Tags: General Technical*

---

**JonJonJon** - *15:51:51*

Suppose I place an order.



Then a market book updates via the streaming api.



Is there a chance that the market book won't contain (whether the order is sitting in the order book, or has removed size from the order book)  the order that I just placed? Is there a way to check whether or not my order has reached the book yet?

*Tags: General Technical*

---

**Mo** - *15:55:56*

Unless you use the async parameter then your order is in the book when your call to placeOrders returns.



The only reason a streaming price update received after that would not include your order would be if you had some latency in receiving the updates. 

*Tags: Performance*

---

**hugo** - *15:58:05*

any market update after the order update that was received on the streaming api would definitely contain your order.

*Tags: General Technical*

---

## 2019-09-29

**JonJonJon** - *22:39:46*

With the streaming api, is it possible to filter for orders for a given market?

*Tags: General Technical*

---

## 2019-10-01

**OT** - *08:32:16*

Has anyone ever had betfair stream subscription requests rejected (connection reset by peer) ? I find it works for 12 hours or so and then starts rejecting the requests. I've tried reducing the volume of requests, incase it is some kind of volume-type issue. My software has some architecture flaws, but I'm almost finished with the replacemnt. Just need a quick fix on the old one at the moment ://

*Tags: Errors Debugging, Deployment*

---

**liam** - *08:55:34*

Have a look at the flumine code for a simple way to handle it 

*Tags: General Technical*

---

**OT** - *08:58:59*

thanks, I'll check out flumine

*Tags: General Technical*

---

**OT** - *09:01:12*

basically I am subscribing by markets, I have to regularly push through a list of marketIds. I can't find a way to subscribe more broadly without hitting the data/subscription limit. at least, not with this current package. I've solved it with my new one.

*Tags: General Technical*

---

**liam** - *09:01:50*

Ask betfair to boost it, mentioning the library will help

*Tags: General Technical*

---

**OT** - *09:02:20*

subscription by competitionId would solve all of my problems :slightly_smiling_face:

*Tags: General Technical*

---

**OT** - *09:06:25*

Yes I can see how that would work. resubscribing does occasionally cause some other errors down the line, but nothing as bad as not getting subscribed at all.

*Tags: Errors Debugging*

---

**liam** - *09:38:49*

There were some very poor design decisions from them when it comes to streaming, not including competition id (something due to their internals making it tricky) and naming the filters differently is really annoying 

*Tags: General Technical*

---

**OT** - *09:40:43*

although mollybet api documentation looked promising too, i never got to actually use it.

*Tags: General Technical*

---

## 2019-10-03

**Newbie99** - *19:01:18*

I'm having a similar issue, but I'm hoping its because I don't understand exactly what the async parameter does!



If I cancel an order, I get the Success message back and temporarily store the updated order details locally.



However if I receive a streaming update fractionally after the cancel success message comes back it contains the old order details, which then over writes my local risk calcs...obviously causing plenty of problems!



I saw below the async parameter was mentioned, which I have set as follows:



```order_stream.start(async_=True)

market_stream.start(async_=True)```



If I take out the async_=True then it just seems to hang and never gets past that point in the code.



So, is there something obvious I'm missing here, as I'm a bit confused?

*Tags: General Technical*

---

**JonJonJon** - *23:27:30*

[@UFTBRB3F1](@UFTBRB3F1). There is a published time on the order and market book data. Could that help? I plan on testing those this weekend.

*Tags: General Technical*

---

**JonJonJon** - *23:47:54*

[@UFTBRB3F1](@UFTBRB3F1) regarding why it hangs if you take out, look at [https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/betfairstream.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/betfairstream.py). The code shows that async is False by default, and that the code will enter a loop that keepa going until the socket closes.

*Tags: General Technical*

---

## 2019-10-04

**Mo** - *08:12:05*

I was referring to the async parameter to placeOrders, nothing to do with streaming

*Tags: General Technical*

---

**liam** - *08:48:54*

async in streaming will be going in v2 as it causes a lot of confusion 

*Tags: General Technical*

---

**Newbie99** - *09:41:19*

ah okay cool, so I will ignore async for now...



But how do people get round this?



I did think, as JonJonJon suggests using timestamps, but the market update has a greater level of precision (down to milliseconds), whereas cancel.cancelled_date only appears to show down to seconds, so it doesn't appear that option is viable.



The only options I could come up with are:



I could ignore any streaming updates within 0.5 seconds

Overwrite the next streaming update with the position info post cancellation for just that runner



Neither are ideal though, so does anyone have a smarter solution?

*Tags: General Technical*

---

**liam** - *11:41:49*

i ignore the place/cancel/replace/update response and go off streaming

*Tags: General Technical*

---

**Newbie99** - *11:50:09*

But are your risk calcs not off for a bit then (maybe my logic is flawed here), this is the process flow I have now:





1) Market moves to = y or position size = z

2) Place / cancel order is triggered

3) Place / cancel order success message comes back (tbh arguably this is irrelevant its the next bit thats key)

4) Update local market / order books with risk calcs

5) Streaming update comes and:

a) Is fine as it already includes the updated orders

b) Isn't fine and over writes them (as it was sent milliseconds after/before the place/cancel order request I guess)



If I rely on streaming alone, then when 5b occurs, I would then effectively be trying to place/cancel the order again, as the risk calcs would suggest it was necessary.



Hence my issue, I get the point about ignoring the actual cancel / place message that's fine, but how do you account for a streaming update that doesn't yet have the updated order info in (short of ignoring the first streaming update after placing/cancelling an order or ignoring for 0.x seconds...neither of which are ideal)?

*Tags: General Technical*

---

**Newbie99** - *11:54:59*

maybe I'm being a bit dumb here...but wouldn't the same issue be problematic still?



i.e. if the status is set to pending (which makes sense and I sort of do similar in a far messier way), then an update comes through, how do you determine that update should be ignored? As that first update may already include the pending order (and then the status is no longer pending)...however sometimes it might not, in which case you would want the order to remain as pending until the next update?

*Tags: General Technical*

---

**liam** - *12:01:20*

that is all i do after an execution, i then wait for an order streaming update

*Tags: General Technical*

---

**Unknown** - *12:08:31*

so for that problem i have a strategy runner_context which stores the current state, ie. if an order is live, this would stop an order being placed if in a pending state

*Tags: Deployment, Strategies*

---

**Newbie99** - *12:09:48*

ah okay cool yep that makes sense actually, I will look to do similar, cool tks that has helped a lot!

*Tags: General Technical*

---

## 2019-10-19

**JonJonJon** - *22:22:28*

Does anyone here store the results from list_cleared_orders in a database?

*Tags: General Technical*

---

**JonJonJon** - *22:22:57*

I need something like that to help differentiate the performance of different strategies.

*Tags: Performance*

---

## 2019-10-25

**stefan** - *09:39:26*

[@UBS7QANF3](@UBS7QANF3) [@U4H19D1D2](@U4H19D1D2) Just to let you know that the service should be working again. They have made some changes, and it has been live for the past 2 weeks. During that time I was able to download again via the website without any issues: `[https://forum.developer.betfair.com/forum/developer-program/historical-data/26927-fixed-historical-data-data-for-nov-18-to-date](https://forum.developer.betfair.com/forum/developer-program/historical-data/26927-fixed-historical-data-data-for-nov-18-to-date)`

*Tags: Errors Debugging, Deployment*

---

## 2019-10-27

**ash** - *17:44:35*

Can anybody give me a nudge in the right direction to good ways of dealing with socket.timeout / socket.errors.

excuse If this is simple, but Is there anyway of catching that specific exception and restarting the stream from my main program?

I have tried changing the betfairstrem.py section that throws the exception..

`while self._running and part[-2:] != crlf_bytes:`

`            try:`

`                part = self._socket.recv(self.buffer_size)`

`            except (socket.timeout, socket.error) as e:`

`                if self._running:`

`                    self.stop()`

`                    print('STOP1')`

`                    time.sleep(5)`

`                    self.start()`

`                    # raise SocketError('[Connect: %s]: Socket %s' % (self._unique_id, e))`

`                else:`

`                    return  # 133, prevents error if stop is called mid recv`

but this is erroneous and creating a horrible subscription loop.

*Tags: Errors Debugging*

---

**liam** - *17:47:14*

Wrap start in a try except, catch BetfairError and a normal Exception 

*Tags: Errors Debugging*

---

**liam** - *17:48:11*

[https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L81](https://github.com/liampauling/flumine/blob/master/flumine/flumine.py#L81)

*Tags: General Technical*

---

## 2019-11-03

**Chris** - *16:38:57*

hey does anyone know how to pass s3 credentials to a container if using the flumine with docker?

*Tags: General Technical*

---

**Mo** - *18:08:15*

I believe they should be AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY

*Tags: Deployment*

---

**liam** - *19:03:59*

Are you getting an error?

*Tags: Errors Debugging*

---

**Chris** - *19:04:01*

Im getting botocore.exceptions.ClientError: An error occurred (403) when calling the HeadBucket operation: Forbidden, despite the access key and secret key being correct (just created new ones)

*Tags: Errors Debugging*

---

**Mo** - *19:04:47*

Sounds like a problem with your bucket permissions

*Tags: General Technical*

---

**Chris** - *19:04:49*

but when I change the directory environment variable to anything I still get forbidden error, not the ‘not found’ error

*Tags: Errors Debugging*

---

**liam** - *19:05:23*

It’s hard coded to flumine which I own 

*Tags: General Technical*

---

**Chris** - *19:05:50*

aaah, could I create a bucket called flumine?

*Tags: General Technical*

---

## 2019-11-06

**liam** - *12:01:15*

Tableau when I am being lazy and then python / matplotlib / bokeh 

*Tags: General Technical*

---

**Mo** - *13:22:41*

Tableau although I hate it with a passion. Otherwise jupyter/pandas/seaborn

*Tags: Feature Engineering*

---

**Evaldas** - *14:43:13*

So I pretty much use same tools, but it's such a mess with bokeh, pandas, matplotlib, excel, looking for one sexy solution, some kind of SQL dashboard.. 

Tableau looks nice, but there is much hate for it everywhere :D

Metabase looks interesting, have you tried it?

*Tags: Feature Engineering*

---

## 2019-11-08

**jhaa** - *17:11:47*

I have some unexpected behaviour in list_cleared_orders(). When I specify event_ids or market_ids I get expected results. However when I filter for runner_ids I get an error: Error: {'code': -32602, 'message': 'DSC-0018'}  which implies a mandatory field was not provided. What field would that be or what am I missing?

*Tags: Errors Debugging*

---

**jhaa** - *17:15:13*

unsure how to get the actual request from that line of code

*Tags: General Technical*

---

## 2019-11-10

**keoki** - *03:16:26*

Hi all, I'm new here so apologies for asking questions that I'm sure have been answered before.

- What charges are involved in obtaining a live app key, and using it to execute transactions thereafter?

- Are there throttling limits?

*Tags: Deployment*

---

**Mo** - *08:10:40*

The streaming API has a default subscription limit of 200 markets but you can ask Betfair to remove that

*Tags: General Technical*

---

**keoki** - *08:31:58*

Thank you [@UBS7QANF3](@UBS7QANF3)! That's really helpful, thank you.

*Tags: General Technical*

---

## 2019-11-12

**Leon** - *22:38:46*

I've been trying to set flumine up for the last couple of nights but have got stuck on what is probably a basic error on my part, but I haven't been able to figure it out. The error I get is -

Traceback (most recent call last):

  File "C:/Users/leonf/PycharmProjects/flumine-master/flumine_test.py", line 17, in &lt;module&gt;

    "certificate_login": False,

  File "C:\Users\leonf\PycharmProjects\flumine-master\flumine\flumine.py", line 21, in __init__

    self.trading = self._create_client(settings)

  File "C:\Users\leonf\PycharmProjects\flumine-master\flumine\flumine.py", line 75, in _create_client

    return APIClient(**settings.get("betfairlightweight"))

TypeError: type object argument after ** must be a mapping, not NoneType



I'm presuming the error is being caused by the "certificate_login": False line (I didn't understand why certs were not required, but based it on the sample github code). An extract of where settings are defined is -

flumine = Flumine(

    recorder=MarketRecorder(

        storage_engine=storage_engine,

        market_filter=market_filter,

    ),

    settings={  # passed to betfairlightweight

        "username": "test",

        "password": "test",

        "app_key": "test",

        "certificate_login": False,

    }

)

with test being replaced by my credentials (obviously!).



Any suggestions would be greatly appreciated please.

(My credentials, app_key, etc work fine connecting in other betfairlightweight scripts)

*Tags: Errors Debugging, Strategies*

---

## 2019-11-13

**Mo** - *07:22:17*

`settings` doesn't have a `betfairlightweight` key

*Tags: General Technical*

---

**Mo** - *07:22:35*

So `settings.get("betfairlightweight")` returns `None`

*Tags: General Technical*

---

**Mo** - *07:27:28*

This looks like a mismatch between the documentation and the code to me but I've never used flumine so best to see what [@U4H19D1D2](@U4H19D1D2) says

*Tags: General Technical*

---

**liam** - *08:27:56*

Yeah it should be settings={“betfairlightweight”: {“username......

*Tags: General Technical*

---

## 2019-11-14

**Unknown** - *21:04:40*

Using the python API how can I list all horse markets start time

*Tags: General Technical*

---

**bogdan** - *21:13:15*

market_catalogue_filter = `betfairlightweight`.filters.market_filter(event_ids=['28971066'])



market_catalogues = trading.betting.list_market_catalogue(

    filter=market_catalogue_filter,

    max_results='100',

    sort='FIRST_TO_START'

)



# Create a DataFrame for each market catalogue

market_types_mooney_valley = pd.DataFrame({

    'Market Name': [market_cat_object.market_name for market_cat_object in market_catalogues],

    'Market ID': [market_cat_object.market_id for market_cat_object in market_catalogues],

    'Total Matched': [market_cat_object.total_matched for market_cat_object in market_catalogues],

})



market_types_mooney_valley

*Tags: Feature Engineering, Strategies*

---

**liam** - *21:27:55*

 market_catalogue_filter = betfairlightweight.filters.market_filter(market_type_codes=[‘WIN’], venues=[‘Kyneton’])

*Tags: General Technical*

---

**bogdan** - *21:53:40*

Kyneton_filter = betfairlightweight.filters.market_filter(market_type_codes=['WIN'], venues=['Kyneton'])



Kyneton_venue = trading.betting.list_venues(

    filter=Kyneton_filter

)

#print(Kyneton_venue)

print([v_object.json() for v_object in Kyneton_venue])

*Tags: Strategies*

---

**bogdan** - *22:00:50*

Kyneton_filter = betfairlightweight.filters.market_filter(market_type_codes=['WIN'], venues=['Kyneton'])



market_catalogues = trading.betting.list_market_catalogue(

    filter=Kyneton_filter,

    max_results='100',

    sort='FIRST_TO_START'

)

print([market_cat_object.json() for market_cat_object in market_catalogues])

*Tags: Strategies*

---

**Mo** - *22:08:41*

You should familiarise yourself with the API documentation: [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listMarketCatalogue](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listMarketCatalogue)

*Tags: General Technical*

---

**bogdan** - *22:21:15*

Thank you very much for all your help!

*Tags: General Technical*

---

## 2019-11-15

**PeterLe** - *11:10:19*

Morning All, Just wanted to quickly introduce myself. Ive been trading on betfair since 2007 and I run my own programs (C#) which I had commissioned. I have very limited experience in programming, but very keen and enthusiastic to learn Python. So whilst I cant offer much at this stage, hopefully in the future i will be able to contribute more :slightly_smiling_face:

*Tags: Strategies*

---

**liam** - *16:07:10*

Welcome, I learnt using [https://www.codecademy.com/](https://www.codecademy.com/) and then get stuck straight into the API but looks like they now charge for python3, PyCharm is great to learn on as well

*Tags: General Technical*

---

**Ptolemy** - *20:35:50*

thanks Liam, I'll take a look at it and then get betfairlightweight downloaded at some point.

*Tags: General Technical*

---

**JonJonJon** - *21:42:29*

Are betids always ascending? I need to sort orders by placed date, but am having problems when they are placed in the same second.

*Tags: General Technical*

---

## 2019-11-17

**JonJonJon** - *21:47:34*

The orders do appear to have ascending bet ids from my observations. But I want see if there is any documentation to confirm this,

*Tags: General Technical*

---

## 2019-11-18

**Cagdas Yetkin** - *17:46:42*

Hi! I am new here. I wrote a few small applications already to place bets. I want to bring in the live statistics like number of corner kicks. What method do you prefer for that? I can see some python libraries when i google it. There are also a few paid APIs, like 19 usd/month.

*Tags: Deployment*

---

## 2019-11-19

**Cagdas Yetkin** - *11:25:16*

anyway, thank you for the `betfairlightweight` library. i use it and it works great. aloha

*Tags: General Technical*

---

**Mo** - *11:26:29*

Welcome. Sorry I don’t have a good answer to your question. Getting (good) data feeds is always a problem, primarily the cost

*Tags: General Technical*

---

## 2019-11-22

**bogdan** - *18:18:06*

I tried to find them in the documentation, but I cannot find in the metadata

*Tags: General Technical*

---

## 2019-11-23

**bogdan** - *17:25:59*

[@U4H19D1D2](@U4H19D1D2) thank you for help! :slightly_smiling_face:

*Tags: General Technical*

---

## 2019-11-24

**PeterLe** - *23:30:03*

Hi, for anyone new to this (such as myself), i was just viewing a channel on Youtube - Horsetrader, he only has circa 10 videos, but you may find them useful. Ive just watched his video on login and session management and although Ive been through these steps over the last few days, it would have been easier to have watched that first. All his videos are based on python, so worth a mention.

*Tags: Getting Started*

---

## 2019-11-26

**ash** - *09:20:10*

woke up this morning to a bunch of

`WARNING:betfairlightweight.streaming.listener:[Subscription: 1] status: 503`

not seen that before?

*Tags: General Technical*

---

**liam** - *09:22:09*

I didn’t get any notification either, 503 is the error betfair give when they stop the stream, it carries on when they have fixed things so no user input required 

*Tags: Errors Debugging*

---

**ash** - *09:28:01*

I thought it had caused my program to hang, I just had to restart everything, but that must have been an unrelated problem on my side (I had a Mysql error too.) hmm  ok, thanks!

*Tags: Errors Debugging*

---

## 2019-11-27

**Mo** - *12:04:54*

Get it from the marketDefinition in the streaming data. It has a settledTime field

*Tags: General Technical*

---

**hugo** - *13:58:01*

If I'm dealing with a market that was open for a very long time (an outright market on a world cup for instance) will I find the historical data file for that market in the month that it opened?

*Tags: Data Quality*

---

**hugo** - *17:26:57*

How soon after settling does a market become available on the historical data api?

*Tags: Data Quality*

---

## 2019-11-28

**Jonjonjon** - *21:55:20*

What exactly does includeOverallPosition mean in relation to the order filter in the streaming api? If I turn it off will get I better performance?

*Tags: Performance*

---

## 2019-11-29

**liam** - *09:33:44*

Anyone seeing latency issues on the hour during the day? Not sure if its me or Betfair but started on the 21st which coincides with the SUB_IMAGE update

*Tags: Performance*

---

## 2019-11-30

**Jonjonjon** - *22:52:58*

If I have a Python script that uses a Market Book Stream and Order Book stream, is it important to call .close() on them when the script ends?

*Tags: General Technical*

---

## 2019-12-01

**liam** - *16:09:05*

It’s setup for one when it comes to historical data, obvs the live stream can handle multiple because multiple markets come from betfair and there is only one market per file 

*Tags: Getting Started, Data Quality, Deployment*

---

**Rory** - *16:15:58*

just started looking at flumine (looks great btw) ...

*Tags: General Technical*

---

**Rory** - *16:16:21*

quick question ... is the docker command still accurate

*Tags: General Technical*

---

**Rory** - *16:16:26*

```docker run -d

    -e username='JohnSmith'

    -e JohnSmithpassword='beer'

    -e JohnSmith='morebeer'

    -e STREAM_TYPE='market'

    -e MARKET_FILTER='{"eventTypeIds": ["7"], "countryCodes": ["GB", "IE"], "marketTypes": ["WIN"]}'

    -v /certs:/certs

    -v flumine_data:/data

    liampauling/flumine:latest```

*Tags: General Technical*

---

## 2019-12-02

**bogdan** - *14:32:57*

Hello,



Thanks [@U4H19D1D2](@U4H19D1D2) for the new endpoint.



I have two question instead: where did you find the first part of the link "[https://apieds.betfair.com/api/eds/racing-info/v1](https://apieds.betfair.com/api/eds/racing-info/v1)" using the dev tools in chrome I don't find the "racing-info" part. In the browser I cannot see last races info for horses using [http://betfair.com|betfair.com](http://betfair.com|betfair.com) as a . I can do that only on the android app.



The second question: The link combined with marketId return all the info I need including last races info. I can get this info somehow using betfairlightweight or I have to parse the json returned by this link?



Thanks

*Tags: General Technical*

---

**Mo** - *19:20:40*

My suggestion is to try to get an account manager and use them to get a master/subaccount setup. But I think they don’t really give them out any more. I think their intention is for people to separate strategies using the customerStrategyRef functionality instead. I could be wrong though. [@U4H19D1D2](@U4H19D1D2) do you have any more insight?

*Tags: Getting Started, Strategies, Multi Client*

---

**JonJonJon** - *19:33:09*

Do you know what "turning over a lot" means? I use customer strategy ref, but an actual sub account with a separate balance would be a lot safer, as a bad software bug could empty the account in seconds.

*Tags: Errors Debugging, Strategies*

---

## 2019-12-03

**ash** - *10:37:38*

Sorry for yet another presumably basic question, I do appreciate the opportunity to ask them.

*Tags: General Technical*

---

**liam** - *10:57:39*

If you want to keep things simple, get(block=False, timeout=120) and then handle the EmptyQueue error 

*Tags: Errors Debugging*

---

**Chris** - *17:59:24*

```db = database.cursor()

market_id = db.execute("SELECT match_id FROM bfex_match_ids")

values_list = db.fetchall()



def grabber(market_id):

  market_catalogues = trading.betting.list_market_catalogue(

    filter=filters.market_filter(

      market_ids=[market_id],

  #    in_play_only='false',

    ),

    market_projection=['MARKET_START_TIME', 'RUNNER_DESCRIPTION', 'RUNNER_METADATA'],  # runner description required

    max_results=1

  )



  for market_catalogue in market_catalogues:

    market_books = trading.betting.list_market_book(

      market_ids=[market_catalogue.market_id],

      price_projection=filters.price_projection(

      price_data=filters.price_data(

        ex_best_offers=True

        )

      )

    )



  db = database.cursor()

  sql_insert_query = """UPDATE bfex_match_odds SET selection_odds=%s, selection_back_odds=%s WHERE unique_id=%s"""



  for market_book in market_books:

    for runner in market_book.runners:

      try:

        available_to_back = runner.ex.available_to_back[0].price

        available_to_lay = runner.ex.available_to_lay[0].price

        unique_id = str(runner.selection_id) + market_id

        print(unique_id)

        db.execute(sql_insert_query, (available_to_lay, available_to_back, unique_id))

        print(available_to_lay, available_to_back, unique_id)

      except Exception:

        traceback.print_exc()

        unique_id = str(runner.selection_id) + market_id

        print(unique_id)

        db.execute(sql_insert_query, ("10000.0", "10000.0", unique_id))

        # print("uh-oh")

    database.commit()



def updater(values_list):

  for bfid in values_list:

    try:

      grabber(bfid[0])

    except:

      print("Could not update:")

      print(bfid[0])

      pass



updater(values_list)```

*Tags: Errors Debugging, Strategies*

---

**Chris** - *18:02:07*

And the error is

*Tags: Errors Debugging*

---

**Chris** - *18:02:08*

```Traceback (most recent call last):

  File "./bfex-grabber-3.py", line 52, in grabber

    available_to_lay = runner.ex.available_to_lay[0].price

IndexError: list index out of range```

*Tags: Errors Debugging*

---

**Chris** - *18:06:39*

[@U4H19D1D2](@U4H19D1D2) No, I've adapted most of what I've got from here; [https://github.com/liampauling/betfair/blob/master/examples/](https://github.com/liampauling/betfair/blob/master/examples/) - not a python dev (or a coder at all, really, a sysadmin)

*Tags: General Technical*

---

## 2019-12-04

**PeterLe** - *19:07:39*

Quick question please. Is the non certs option too risky to use (ie Trading.Login_Interactive) I have two step on the account too. Just having some issues with certs which I’ll resolve soon but wondered if it’s ok to use the non certs approach for the time being. Thanks

*Tags: Strategies*

---

**liam** - *21:09:04*

Up to you, are you getting an error?

*Tags: Errors Debugging*

---

## 2019-12-06

**Mo** - *09:38:12*

This feels like a basic question I should really know the answer to... How do I get cancelled orders for markets which haven't settled yet?

*Tags: General Technical*

---

## 2019-12-07

**JonJonJon** - *11:40:40*

Perhaps this is what happened...



I use the market version paramter for my back bets to guarantee they happen before the race starts. I don't use that for exits as I just want to get out. Maybe Betfair had a problem settting the inplay value to true on time, which would invalidate my entry. But then my exir would stillbe valid. But if that is the case, how can I protect myself from these rare situations?

*Tags: General Technical*

---

## 2019-12-11

**bting91** - *12:00:15*

I'm looking to just pull all current odds for all matches and games, but I can't quite see how to do this through the documentation, does anyone know which API calls i'd be looking to make&gt;

*Tags: General Technical*

---

**Mo** - *12:06:11*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

## 2019-12-14

**Ian** - *09:18:34*

Morning - does flumine respect competitionIds passed in the market filter? 



Constructed the filter and it is running but returning markets from all events from event type/country/market type I’ve specified 

*Tags: General Technical*

---

**liam** - *09:19:42*

Competition ids can’t be used in streaming 

*Tags: General Technical*

---

**fjt1973** - *23:08:39*

Evening All... this has been racking by brains for days. Creation and Uploading of a SSL Certificate! I have a Ubuntu server running, created / uploaded and verification of the certificate works just fine. However, Windows 10, Windows Server 2012 and Windows Server 2016 with certificated created by OpenSSL I get the following error when running the Curl Command from the directory the certificated are sat in "{"loginStatus":"CERT_AUTH_REQUIRED"}". I've even tried running the Curl command from root and putting the in the full path...... Is there something I'm missing!

*Tags: Errors Debugging, Deployment*

---

## 2019-12-15

**fjt1973** - *03:41:15*

Ignore the above... it appears even though the curl command doesn't work for me put in in a Python Script and it does :thinking_face::laughing::astonished:

*Tags: General Technical*

---

**Ian** - *13:10:09*

It is really apparent in 3td party manual apps such as Geeks Toy and Bet Angel - using polling or streaming - not just me but every trader in the groups I’m in 

*Tags: General Technical*

---

## 2019-12-16

**liam** - *09:43:49*

[@U7R5CEDAL](@U7R5CEDAL) do you get the 'Latency: xx.xxxs' log? I can't see anything from my logs yesterday

*Tags: Performance*

---

## 2019-12-18

**Newbie99** - *12:56:34*

A very general question, but what do people tend / prefer to use for storing 'live data', by which I mean price data or similar from currently live markets?



I have been using MySQL for my static stuff, but thats way too slow to use for pricing data, so until now I've just created a previous market state in my Python scripts, however I want to get a bit more sophisticated (and possibly tidy up the code too!), so was thinking of an in-memory database like Reddis (which I have seen mentioned on here previously), but was just curious to see what others are using.

*Tags: Performance, Deployment*

---

**Mo** - *13:03:00*

Yeah either Redis or python object depending on your use case

*Tags: General Technical*

---

**Newbie99** - *13:05:46*

Yeah, that I feel would be useful, as I can then play around with the Redis data and analyse a bit more (to refine my models over time).



I don't need super speed, my hardware isn't good enough currently to make that worth worrying about, so not fussed about nano-seconds, but MySQL is just not usable for this kind of a thing, if you have more than a handful of runners the whole thing just grinds to a halt if you're inserting and select querying regularly!

*Tags: Performance, Strategies*

---

**Newbie99** - *13:06:44*

I could refine my code and keep more in memory, but then it gets a lot more complex and I'm not sure whether its the best option for me, if super low latency isn't a huge concern (which it isn't)

*Tags: Performance*

---

## 2019-12-19

**ash** - *14:27:48*

Dose anybody know how an in-memory sqlite database would compare speed wise to redis?  Saving the database to disk when the market completes?

I am not sure about accessing the data between processes either, that should be possible though I presume?

*Tags: Performance*

---

**liam** - *14:28:44*

Why are you bothering? Just store everything in python / classes / dictionaries

*Tags: General Technical*

---

**liam** - *14:29:53*

Recording the raw data through flumine, any order / market data sent to MySQL for analysis and everything else stored in ram 

*Tags: General Technical*

---

**Newbie99** - *14:41:46*

That's a good point actually, simple may well be better, although personally I am quite keen to play around with no-sql databases as I've never tried before!

*Tags: General Technical*

---

**Rory** - *14:57:24*

the NoSQL bandwagon is alive and well in many places (speaking from experience) ... like everything else in tech, there are valid use cases for it but people are too quick to shoe horn it as a solution to every problem

*Tags: Deployment*

---

## 2019-12-20

**Unknown** - *14:56:18*

flumine/logging - how does everyone log flumine - to screen or file? i've been fine with screen but would now like to persist the logs - and include time? at the moment this is what I see:

*Tags: General Technical*

---

**liam** - *15:14:29*

So I use docker and then the cloud watch driver so the logs get pushed to AWS. I see you are using azure have you used this before? 

*Tags: Deployment*

---

**Rory** - *16:51:25*

you could probably infer the score up to a point e.g. if Man City start at 1.12 vs say Brighton, then go to 1.03, 1.02, 1.01 you can assume they have at least scored ... problem is you won't know whether that's 2-0, 3-0, 4-0, 5-0, etc but do you care?

*Tags: General Technical*

---

**Rory** - *16:58:01*

if starting out now, I'd follow this



[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1576860830042900](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1576860830042900)

*Tags: General Technical*

---

## 2019-12-22

**Leon** - *18:39:02*

Does anyone use the form from runner.metadata? An example of how it looks is 8722118 with each digit representing the place the horse finished in each of its last races. I have a few questions about this data I've not been able to find an answer for

1. I assume this is only if they finish 1-9 (otherwise it would be impossible to tell if e.g. 13 meant they finished 13th or 1st and 3rd)

2. I've come across some characters returned that I am unsure what they mean

'0' does this mean they didn't finish or something else?

'-' any ideas?

Any help greatly appreciated

*Tags: General Technical*

---

**Leon** - *18:51:37*

thanks Tom, I didn't think of Wikipedia, I've been searching for betfair specific details. That answers my questions (and also shows that the '-' represents a break in years )

*Tags: General Technical*

---

## 2019-12-23

**Chris** - *13:07:10*

People simulating their strategies - are you modelling cross-matching in your simulator?

*Tags: Strategies*

---

**Ian** - *17:04:16*

Evening all - about to ask a what feels like a super easy q but I'm stumped...

Flumine creates zip files, i've been working with these fine on Windows.

Moved to ubuntu and now it doesn't like them - was getting errors from Python's zip library but even on the shell i get

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *19:18:41*

Have gotten the same error, haven't really looked closely what it was. But good to know that it works on windows.

*Tags: Errors Debugging*

---

**Alex A** - *20:47:09*

So this slack is in general about automated betting on betting exchanges, or specifically about Liam Payne’s betfairlightweight

*Tags: Strategies*

---

## 2019-12-24

**Chris** - *17:25:56*

If you have high latency on a streamer (downloading data with flumine on a server), does this mean it could potentially miss updates? Or is it just saying that it will take longer to process

*Tags: Performance, Deployment*

---

## 2019-12-28

**Marcel** - *12:30:44*

Hi, I am pretty new to betfairlightweight. I would like to run this script:

*Tags: Getting Started*

---

**Marcel** - *12:33:09*

I have two questions. 1) Do I need to have an App key from Betfair to run this script? I already have a Betfair account and I can download historical data manually.

*Tags: Data Quality*

---

**Marcel** - *12:36:13*

2) Can you explain the added functionality from this script compared to manually downloading historical data files? Does it extract the tar - files for example in another format and which format?

*Tags: Data Quality*

---

**Mo** - *12:39:06*

1. Good question, I don't know. As above, ask [@U4H19D1D2](@U4H19D1D2) and he can get you an application key for free

2. There are no tar files involved when using this API. You get a list of individual bzip2 files that you can download

Any reason you want to start with the historic API rather than e.g. this example? [https://github.com/liampauling/betfair/blob/master/examples/exampleone.py](https://github.com/liampauling/betfair/blob/master/examples/exampleone.py)

*Tags: General Technical*

---

**Mo** - *13:08:55*

There's no need to involve 7Zip or any other manual process. Use the `bz2` Python library

*Tags: General Technical*

---

## 2019-12-30

**fjt1973** - *21:31:55*

Hello... is there any documentation around the Endpoints... for example all the options for trading.betting.&lt;option&gt; where option could be "list_competitions", "list_current_orders" or "list_event_types". The links here [https://github.com/liampauling/betfair/blob/master/README.md](https://github.com/liampauling/betfair/blob/master/README.md) do not appear to work

*Tags: Strategies*

---

## 2019-12-31

**fjt1973** - *10:15:36*

[@U4H19D1D2](@U4H19D1D2) when you says its from the website which website do you mean? What i'm trying to understand is how you come up with statements like "horse_racing_event_type_id = trading.betting.list_event_types(    filter=filters.market_filter(        text_query='Horse Racing'    )). i.e Is there any documentation on how to use trading.betting.list_event_types and its variables for example.

*Tags: Strategies*

---

**Mo** - *11:13:51*

Otherwise, you have access to all of the source code, why not just take a look? [https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/filters.py#L71|https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/filters.py#L71](https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/filters.py#L71|https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/filters.py#L71)

*Tags: General Technical*

---

## 2020-01-01

**Rob** - *18:13:24*

Hi guys, no better answer than "use certs" on the question "how to reuse session token when using login interactive?" ? :smile: Thanks a lot

*Tags: General Technical*

---

## 2020-01-02

**Unknown** - *00:59:07*

Hi guys, how can i deal with this error

I used

while time &lt; future time:

....try:

........get updates.

....except:

```stream.stop()

time.sleep(1)

stream.start(async_=True)```

but i doesn't  help. the code stops. i thought if i put a while loop, then when error occurs it will sleep for 1 second and then start the stream again.but still the code stops.  thanks

*Tags: Errors Debugging*

---

**liam** - *10:25:47*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L117|https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L117](https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L117|https://github.com/liampauling/betfair/blob/master/betfairlightweight/baseclient.py#L117)

*Tags: General Technical*

---

**Justin Fisher** - *22:47:17*

:wave:  evening. More related to [@U4H19D1D2](@U4H19D1D2) and #flumine but Im currently using a `delayed stream api key`  and flumine to store some delicious data but... because its delayed Im getting publish times (obviously) corresponding to the  `single message every 3 minutes`  Id like some more granular data and remembered this:

&gt;  I don't know. As above, ask [@U4H19D1D2](@U4H19D1D2) and he can get you an application key for free

*Tags: General Technical*

---

## 2020-01-03

**Chris** - *15:21:05*

does anyone know a way you could identify events using the data flumine provides? Or do you have to just download catalogue data seperately (what im currrently doing).. Would be nice to link external stats to the markets

*Tags: General Technical*

---

**Mo** - *15:27:50*

Isn't flumine just scraping the streaming data? Use the marketDefinition?

*Tags: General Technical*

---

**Chris** - *15:42:13*

You mean using a separate script? completely apart from flumine? Thats what I’m doing at the moment, I was just wondering whether I was missing some functionality from within flumine

*Tags: General Technical*

---

## 2020-01-04

**Alex A** - *18:46:10*

I’m also getting 404 errors for [https://identitysso-cert.betfair.com/api/certlogin|https://identitysso-cert.betfair.com/api/certlogin](https://identitysso-cert.betfair.com/api/certlogin|https://identitysso-cert.betfair.com/api/certlogin)

*Tags: Errors Debugging*

---

## 2020-01-05

**AP** - *12:32:44*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py) - I am messing around with this example code and am attempting to extract the whole order book for each runner to calculate VWAP etc but I am receiving some errors when using runner.available_to_back etc

*Tags: Errors Debugging*

---

**Mo** - *12:33:45*

What errors?

*Tags: Errors Debugging*

---

**AP** - *12:35:31*

```AttributeError: 'RunnerBook' object has no attribute 'available_to_back'```



*Tags: Errors Debugging*

---

**Ian** - *19:40:50*

Hi all - just wondering if I'm missing something insanely simple here, perhaps? I cannot have Python open these files at all, using bz2 or anyother lib. They extract just find in Windows using 7-zip

*Tags: General Technical*

---

**Alex A** - *21:05:48*

I’ve been getting 404 errors all weekend trying a non-interactive login, and at first I thought it was Betfair, as they’ve been having site issues, but surely they wouldn’t be down for non interactive logins all weekend?

*Tags: Errors Debugging*

---

**Alex A** - *21:10:36*

Making a python post request to [https://identitysso-cert.betfair.com/api/certlogin|https://identitysso-cert.betfair.com/api/certlogin](https://identitysso-cert.betfair.com/api/certlogin|https://identitysso-cert.betfair.com/api/certlogin) with my login details and certificate.

*Tags: General Technical*

---

**Mo** - *21:11:28*

Why not use betfairlightweight?

*Tags: General Technical*

---

**Alex A** - *21:12:18*

I’ve been planning to, just figured for now it would be easier to just write my own code to login than go through betfairlightweight.

*Tags: General Technical*

---

**Alex A** - *21:21:19*

Yeah, tried with betfairlightweight and logged in fine.

*Tags: General Technical*

---

**Alex A** - *21:46:57*

At least it gave me an excuse to install and use betfairlightweight.

*Tags: Getting Started*

---

## 2020-01-08

**Kai** - *15:22:23*

I think he means greening up across different selections, not individually on each selection. That is in the general case a rather complicated mathematical problem, especially if some selections have poor liquidity

*Tags: General Technical*

---

**AP** - *16:58:31*

I’ve just been going through all the code and documentation as well as this chat, if you need to store prices when trading (eg what the price was 30 seconds ago for a particular selection), is the idea to use flumine to do this?

*Tags: Strategies*

---

**liam** - *17:03:34*

flumine is a data recorder, allows you to record the data betfair expect you to pay a lot for

*Tags: General Technical*

---

**AP** - *17:14:44*

Ok so for trading live the historical data is stored in memory for the most part 

*Tags: Data Quality, Performance, Deployment, Strategies*

---

## 2020-01-11

**AP** - *14:53:48*

Has anyone attempted to process the historical data in parallel?

*Tags: Data Quality*

---

**Jonatan (skyw)** - *15:48:00*

I "solved" it by opening it in the _init_ method and never closing it...I think it should be taken care of automatically.

In my head it went a little faster. Would be nice if you could report the speedups : )

*Tags: Performance*

---

**AP** - *16:46:22*

I think writing to files/DBs becomes problematic with parallel processing 

*Tags: General Technical*

---

**AP** - *16:48:59*

I/O errors 

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *16:54:41*

nice, are you getting them or assuming, I can not see how it would generate IO errors?

*Tags: Errors Debugging*

---

**AP** - *16:58:30*

```def process_betfair_historical_file(file_path):

    print("Processing: " + file_path)

    

    class HistoricalStream(MarketStream):

    # create custom listener and stream



        def __init__(self, listener):

            super(HistoricalStream, self).__init__(listener)

            self.file_path = file_path.split("/")[-1]

            self.output = open(self.file_path + '.txt', 'w')

            self.output.write('Time,MarketStartTime,MarketType,MarketId,Status,Inplay,SelectionId,VWAP,BestLayPrice,LastPriceTraded,\n')



        def on_process(self, market_books):

            for market_book in market_books:

                for runner in market_book.runners:



                    # how to get runner details from the market definition

                    market_def = market_book.market_definition

                    runners_dict = {(runner.selection_id, runner.handicap): runner for runner in market_def.runners}

                    runner_def = runners_dict.get(

                        (runner.selection_id, runner.handicap)

                    )



                    # calculate lay wap

                    price = 0

                    vol = 0

                    for trade in runner.ex.traded_volume:

                        price += (trade.price * trade.size)

                        vol += trade.size

                    try:    

                        wap = price / vol

                    except:

                        wap = np.nan



                    # get best lay price

                    try:

                        best_lay = runner.ex.available_to_lay[0].price

                    except:

                        best_lay = np.nan



                    self.output.write('%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n' % (

                        market_book.publish_time, market_def.market_time, market_def.market_type, market_book.market_id, market_book.status, market_book.inplay,

                        runner.selection_id, wap or '', best_lay or '', runner.last_price_traded or ''

                    ))



    class HistoricalListener(StreamListener):

        def _add_stream(self, unique_id, stream_type):

            if stream_type == 'marketSubscription':

                return HistoricalStream(self)



    # create listener

    listener = HistoricalListener(

        max_latency=1e100

    )



    # create historical stream, update directory to file location

    stream = trading.streaming.create_historical_stream(

        directory=file_path,

        listener=listener

    )



    # start stream

    stream.start(async_=False)

    

    return "Processed" + file_path```

*Tags: Performance, Strategies*

---

**AP** - *17:13:54*

```---------------------------------------------------------------------------

RemoteTraceback                           Traceback (most recent call last)

RemoteTraceback: 

"""

Traceback (most recent call last):

  File "C:\Users\antpa\Anaconda3\envs\myenv\lib\site-packages\multiprocess\pool.py", line 119, in worker

    result = (True, func(*args, **kwds))

  File "C:\Users\antpa\Anaconda3\envs\myenv\lib\site-packages\multiprocess\pool.py", line 44, in mapstar

    return list(map(*args))

  File "&lt;ipython-input-11-dbf9aa1bac51&gt;", line 4, in process_betfair_historical_file

NameError: name 'MarketStream' is not defined

"""



The above exception was the direct cause of the following exception:



NameError                                 Traceback (most recent call last)

&lt;ipython-input-19-0f3124f0dfdc&gt; in &lt;module&gt;

----&gt; 1 results = pool.map(process_betfair_historical_file, [file_path for file_path in file_paths])



~\Anaconda3\envs\myenv\lib\site-packages\multiprocess\pool.py in map(self, func, iterable, chunksize)

    264         in a list that is returned.

    265         '''

--&gt; 266         return self._map_async(func, iterable, mapstar, chunksize).get()

    267 

    268     def starmap(self, func, iterable, chunksize=None):



~\Anaconda3\envs\myenv\lib\site-packages\multiprocess\pool.py in get(self, timeout)

    642             return self._value

    643         else:

--&gt; 644             raise self._value

    645 

    646     def _set(self, i, obj):



NameError: name 'MarketStream' is not defined```

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *17:40:49*

no problem, hope it goes well!

*Tags: General Technical*

---

## 2020-01-13

**Mo** - *00:08:13*

Wow these are really existential questions

*Tags: General Technical*

---

**Mo** - *00:23:33*

Then you case use listEvents [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L70-L85](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L70-L85) with a market filter [https://github.com/liampauling/betfair/blob/master/betfairlightweight/filters.py#L71-L97](https://github.com/liampauling/betfair/blob/master/betfairlightweight/filters.py#L71-L97) that has competition IDs set

*Tags: Strategies*

---

**Nick P** - *13:28:25*

Hiya - new joiner here. Excellent site by the way.  Just have a basic query around the BF historical data - Can I  get Metadata i.e via the ListMarketcatalogue - need to get historical Jockey/weights/Ratings etc.  Not sure I can but if anyone could  comfirm - thanks !

*Tags: Data Quality*

---

**liam** - *15:42:44*

No, catalogue disappears, your only left with the market definition in the streaming MarketBook. RaceCard goes back a few months though 

*Tags: General Technical*

---

## 2020-01-15

**Robert** - *14:55:43*

Hi, I have betfair historical data and am using your API, I would like to know which horse was paid out by betfair:

I can do runner.status to get 'LOSER' or 'WINNER' but can I be sure this is actually what was paid out and that it was not changed shortly after the race finished?

*Tags: Data Quality*

---

## 2020-01-16

**Mo** - *21:48:51*

Should be in the marketDefinition in the streaming data

*Tags: General Technical*

---

## 2020-01-17

**AP** - *02:13:30*

Should runner name always appear in the market definition section of the streaming API?

*Tags: General Technical*

---

**Leon** - *13:16:57*

does anyone know how to access the OrderSubscriptionMessage through BFLW (in the streaming API)?



```market_filter = streaming_market_filter(

    event_type_ids=['1'], # added football

    event_ids=[29651486],

    market_types=[ 'MATCH_ODDS'],   

)```

 I've guessed it may be through the market_filter (as above) but can't work out the correct syntax - maybe something like order_filter= ? I would like to track unmatched orders to find out when they are matched. Cheers

*Tags: General Technical*

---

**Mo** - *13:43:38*

Use `streaming_order_filter` [https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/filters.py#L41-L55](https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/filters.py#L41-L55) in conjunction with `subscribe_to_orders` [https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/streaming/betfairstream.py#L134-L164](https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/streaming/betfairstream.py#L134-L164)

*Tags: General Technical*

---

## 2020-01-19

**Jonatan (skyw)** - *01:56:30*

[https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listMarketCatalogue](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listMarketCatalogue)



list_market_catalogue in the betfairlightweight

*Tags: General Technical*

---

**Ian** - *07:34:59*

Hi all - just wondered what general methods are utilised to identify strategies. At the moment, my manual trading and time watching the markets (3 years - tiny compared to some, I know) has led to me identifying patterns visually and developing code against that. Now that I have collected months of data (flumine  thank you!) I am begging to explore that. I've built a tool that will let me recreate markets and see flow of money and so on, but clearly that will not work at scale, so will be using scikit - what do you do/use?

*Tags: Strategies*

---

**bting91** - *12:35:56*

i havent signed up for the live data one yet

*Tags: Deployment*

---

**Jonatan (skyw)** - *22:16:46*

If one were to add new market ids to flumine, what is the best way to do it? Right now I do a full restart and add the new markets with the old ones.

*Tags: General Technical*

---

**Jonatan (skyw)** - *23:27:56*

Tried another approach, using the hidden variable _socket in Flumine and use that for resubscribing.

*Tags: General Technical*

---

**Jonatan (skyw)** - *23:38:42*

thought you had streaming API, looks like ex should be exchange

*Tags: General Technical*

---

**bting91** - *23:39:57*

is there any reason to be using streaming instead

*Tags: General Technical*

---

## 2020-01-20

**liam** - *08:24:35*

[@U92CASP1B](@U92CASP1B) restriction of how streaming works, you need to be less restrictive on your filter and just ignore anything you don't want

*Tags: General Technical*

---

**Mo** - *11:55:47*

Whether they will remove it or not is another question :wink:

*Tags: General Technical*

---

**Rory** - *13:47:20*

I subscribe to soccer markets across 11 leagues ... but the best you can do with streaming is either by country code which gives too many markets ... or by just the events you're interested in but of course the issue there is you get no new markets as they appear

*Tags: General Technical*

---

**Rory** - *13:47:39*

support for competition ids on the streaming service would have been ideal

*Tags: General Technical*

---

**bting91** - *15:27:39*

how come you guys use streaming instead of just polling the different endpoints

*Tags: General Technical*

---

**Jonatan (skyw)** - *15:46:07*

With the library it is very easy to start with streaming, it's faster and "standard" if you want to get updates. I think that query different end points has it use cases but if you intend to get updates as they arrive streaming is the way to go.

*Tags: General Technical*

---

**Fab** - *18:06:49*

[@URM3TT0UE](@URM3TT0UE) It depends on what you're building. For example, if your aim is to take a snapshot of horse markets 3 minutes before the off and at post time, polling is a good solution. You'll only hit the Betfair API endpoint 2 times per market.



But if you want to gather, process and react to data more frequently (think in-play horse racing), polling is inefficient and the reason lies in the way HTTP works. Every time you hit a Betfair API endpoint:



- Your machine establishes a new connection with the Betfair server.

- It sends an HTTP request and receives and HTTP response.

- HTTP requests/responses carry headers, so more data is sent/received.



Streaming is more efficient because:



- The connection gets established once.

- From that moment, data keeps flowing from Betfair to your machine.

- There are no data overheads as you would have with polling / HTTP.

*Tags: Deployment*

---

**liam** - *18:21:10*

[@USWDY5P7G](@USWDY5P7G) I am creating a new site for some documentation and I think you have just written the opening paragraph for the streaming part :grinning:

*Tags: General Technical*

---

## 2020-01-21

**Elliott Johnston** - *10:18:51*

Hey guys, I am a front end developer, looking to get some help on an app I’m developing using the Python API. I am doing alright however think it would be good to get a second opinion on how i’m setting stuff up. Send me a message if you think you’d be able to assist

*Tags: General Technical*

---

**ash** - *11:28:31*

Morning, I have been trying out streaming using how it is set up in the new examplestreamingerrhandling.py and I keep getting this error.

everything works fine but if I get a connection issue, for example, I am getting this:

```    raise SocketError('[Connect: %s]: Connection closed by server' % (self._unique_id,))

betfairlightweight.exceptions.SocketError: [Connect: 1002]: Connection closed by server

21/01/2020 11:00,581 GB __main__ INFO Starting MarketStreaming

21/01/2020 11:00,713 GB betfairlightweight.streaming.listener INFO [Connect: 1002]: connection_id: 202-210120110000-541

21/01/2020 11:00,756 GB betfairlightweight.streaming.listener ERROR [Subscription: 1003] INVALID_SESSION_INFORMATION: UnrecognisedCredentials

21/01/2020 11:00,756 GB __main__ ERROR MarketStreaming run error

Traceback (most recent call last):

  File "/Volumes/2/GitHub/OddsTrading/versionTWO.py", line 115, in run

    self.stream.start()

  File "/Users/ash/anaconda3/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 60, in start

    self._read_loop()

  File "/Users/ash/anaconda3/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 198, in _read_loop

    self._data(received_data)

  File "/Users/ash/anaconda3/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 239, in _data

    raise ListenerError(self.listener.connection_id, received_data)

betfairlightweight.exceptions.ListenerError: connection_id: 202-210120110000-541, data: {"op":"status","id":1003,"statusCode":"FAILURE","errorCode":"INVALID_SESSION_INFORMATION","errorMessage":"UnrecognisedCredentials","connectionClosed":true,"connectionId":"202-210120110000-541"}

21/01/2020 11:00,759 GB __main__ INFO Starting MarketStreaming```

This is then stuck in the `@retry` loop raising the same error each time.

*Tags: Errors Debugging, Deployment, Strategies*

---

**Mo** - *11:44:33*

Well one way of dealing with it would be to have a separate thread keeping your session token alive so when the connection is lost and it tries to reconnect with that session token then it’s valid. 



But I think a better way would be to handle the error, relogin and resubscribe

*Tags: Errors Debugging, Deployment*

---

**Rory** - *16:33:51*

quick question re streaming prices ... I'm using a Live key and pulling soccer prices, like this:

*Tags: Deployment*

---

## 2020-01-26

**liam** - *19:19:12*

Ive updated it in the v2 branch 

[https://forum.developer.betfair.com/forum/sports-exchange-api/exchange-api/3425-streaming-bug|https://forum.developer.betfair.com/forum/sports-exchange-api/exchange-api/3425-streaming-bug](https://forum.developer.betfair.com/forum/sports-exchange-api/exchange-api/3425-streaming-bug|https://forum.developer.betfair.com/forum/sports-exchange-api/exchange-api/3425-streaming-bug)

*Tags: Errors Debugging*

---

**William** - *21:38:42*

All im getting out of flumine is

*Tags: General Technical*

---

**William** - *21:38:44*

2020-01-27 08:38:27,977 | INFO | betfairlightweight version: 0.0.0b7 | run.py | run

2020-01-27 08:38:27,977 | INFO | flumine version: 0.7.0 | run.py | run

2020-01-27 08:38:27,977 | INFO | Recorder created 16d0d392 | recorder.py | recorder

2020-01-27 08:38:27,978 | INFO | Starting stream: 1000.0 | flumine.py | flumine

&lt;Flumine [not running]&gt;

*Tags: General Technical*

---

**William** - *22:27:08*

Solved thank you

*Tags: General Technical*

---

## 2020-01-27

**klozovin** - *12:29:52*

How's your experience with Python typing, especially when retrofitting an existing codebase? How useful is it? Do you use editor/IDE that can use typing for intellisense?

*Tags: General Technical*

---

**klozovin** - *12:50:52*

thanks, cool... any bugs/errors you found out just by adding types?

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *13:06:32*

from the betfair  streaming exchange schema

*Tags: General Technical*

---

**liam** - *13:07:51*

Yeah I’ll add it to the list, betfairlightweight is such an abstraction from the schema I don’t think it would work 

*Tags: General Technical*

---

## 2020-01-28

**Mo** - *07:56:38*

I'm not sure I entirely understand your question but I don't see how you can make any assumption about the number of updates as that will depend on activity in the market?

*Tags: General Technical*

---

## 2020-01-29

**Tom** - *09:20:38*

Long shot but don’t suppose anyone has done any work on writing a python function for dutching a market ? 

*Tags: General Technical*

---

**JonJonJon** - *09:39:00*

It's not something I'm currently looking at. I've found that live testing on the exchange is better than doing backtesting on historical data.

*Tags: Data Quality, Deployment*

---

**Fab** - *11:41:30*

```# The function

def dutch_calculator(selections, total_stake):



    book = 0

    for s in selections:

        book += 100 / s["odds"]



    for s in selections:

        s["stake"] = total_stake * 100 / s["odds"] / book

        s["stake"] = round(s["stake"], 2)



    return selections



# How to use it

selections = []

selections.append({"odds": 3.50,})

selections.append({"odds": 4.50,})

selections.append({"odds": 5.50,})

selections = dutch_calculator(selections, 100)



# Now the selection list contains dutching stake for each selection

import json

print(json.dumps(selections, indent=4))```

*Tags: General Technical*

---

**Tom** - *11:49:15*

That’s seems just what I was looking for Fabio - thanks very much for your help ! 

*Tags: General Technical*

---

**bting91** - *15:58:53*

who do we contact to get the limit of 200 moved from the streaming API?

*Tags: General Technical*

---

**bting91** - *20:08:21*

so ive got this from streaming api



{'atb': [[1.34, 147.04]], 'atl': [[1.5, 49.75]], 'id': 47999, 'hc': -0.25}

{'atb': [[1.18, 277.58]], 'atl': [[1.26, 49.75]], 'id': 47999, 'hc': 0.25}

{'atb': [[1.06, 833.08]], 'atl': [[1.09, 49.95]], 'id': 48224, 'hc': 3.75}

{'atb': [[12.37, 4.31]], 'atl': [[17.04, 49.88]], 'id': 47999, 'hc': -3.75}

{'atb': [[4.87, 12.81]], 'atl': [[6.42, 49.75]], 'id': 48224, 'hc': -0.25}

*Tags: General Technical*

---

**bting91** - *20:08:43*

market_data_filter = streaming_market_data_filter(

        fields=["EX_MARKET_DEF", "EX_ALL_OFFERS"],

    )

*Tags: General Technical*

---

**bting91** - *20:15:39*

ash ok so theres a bit of latency

*Tags: Performance*

---

**Mo** - *20:19:19*

There is a lot of useful information in the documentation

*Tags: General Technical*

---

**Mo** - *20:22:14*

Offline streaming documentation: [https://docs.developer.betfair.com/download/attachments/6094862/ExchangeStreamAPI-March2018.pdf?version=3&amp;modificationDate=1534331876000&amp;api=v2|https://docs.developer.betfair.com/download/attachments/6094862/ExchangeStreamAPI-March2018.pdf?version=3&amp;modificationDate=1534331876000&amp;api=v2](https://docs.developer.betfair.com/download/attachments/6094862/ExchangeStreamAPI-March2018.pdf?version=3&amp;modificationDate=1534331876000&amp;api=v2|https://docs.developer.betfair.com/download/attachments/6094862/ExchangeStreamAPI-March2018.pdf?version=3&amp;modificationDate=1534331876000&amp;api=v2) might be easier to search

*Tags: General Technical*

---

## 2020-02-01

**bting91** - *19:10:10*

Trying to print out asian handicap pricing with streaming API, however some of the rows dont have a handicap atttached to them

*Tags: General Technical*

---

**Mo** - *19:17:53*

No problem :+1:

*Tags: General Technical*

---

## 2020-02-02

**Jonatan (skyw)** - *19:02:24*

[@UBS7QANF3](@UBS7QANF3) Do you know what segmented data looks like? Is it just to concat each message until \r\n?, could not find in documentation and have not seen one ...

*Tags: General Technical*

---

## 2020-02-06

**jhaa** - *12:09:19*

I am using the http requests, not streaming

*Tags: General Technical*

---

**Mo** - *12:10:30*

Use streaming to confirm but they're probably just cancelling and reposting

*Tags: General Technical*

---

## 2020-02-08

**JK** - *11:44:23*

Hey guys, love the library. Thanks so much, it is an enormous help. I am just wondering how you guys get started with the stream? I am able to print all the market updates using the example scripts, but am unsure how to start off.



Obviously to begin I need a snapshot of the market. Is the idea that I get a snapshot of the market, and then add the changes I receive from the stream to my snapshot? How would I do this? Also, how do you manage your orders effectively?

*Tags: General Technical*

---

**JK** - *11:44:29*

Cheers, and sorry for the nooby questions

*Tags: General Technical*

---

**JK** - *12:06:14*

Oh i didnt realise that, that's so good! Takes away a lot of the hard work. I ran this example [https://github.com/liampauling/betfair/blob/task/version-2.0/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/task/version-2.0/examples/examplestreamingerrhandling.py)

I'm not quite sure how to get started. My goal is to be able to read in odds, market ids and selection ids from a csv, and then place kill or fill bets within a few mins before start time for racing markets if my odds are different to the market odds. Then, if I dont get matched, to put orders at the top of the queue, and if those dont get matched, to take BSP. I feel like it'd be relatively easy to implement without the stream, but am pretty stuck as to how to implement it with the stream. Would I have a script that runs 24/7 and continuously searches for upcoming markets (say within the next 5 mins), and then if it finds any, adds them to the listener? Does the listener automatically disconnect from markets when they have closed? So many questions! haha

*Tags: General Technical*

---

**Mo** - *12:07:24*

Try looking at this example instead: [https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**JK** - *12:08:13*

I have seen that example, it's really helpful. I am confused about the "clk". What is this?

*Tags: General Technical*

---

**Mo** - *12:21:03*

One important "gotcha" regarding order streaming is that when you first subscribe you only receive data on *unmatched* orders, not matched ones. You would have to use the listCurrentOrders API call if you were interested in these or just make sure that the order stream is running before you place any bets. It doesn't sound like this would be a big problem for your strategy as it sounds like your orders are short lived but it's a surprising limitation of the order stream.

*Tags: Deployment, Strategies*

---

**JK** - *12:26:56*

Ah that's interesting, thanks for that. prob saved me a few hours in the future haha. In production how would you run your bot? Like would I just let a script run 24/7 on an ec2 instance for example? Not a dev so pretty new to all this stuff.

*Tags: Getting Started, Deployment*

---

**Mo** - *12:27:37*

I believe that a lot of us use AWS, yes

*Tags: Deployment*

---

**JK** - *12:29:32*

Thanks for all your help

*Tags: General Technical*

---

**Mo** - *12:30:23*

That's an interesting question. Obviously with e.g. `boto3` you could automate the creation and teardown of an instance that would only need to run between those hours

*Tags: General Technical*

---

**Mo** - *12:31:17*

Which would primarily depend on how powerful an instance type you need to run your strategy

*Tags: Strategies*

---

**Mo** - *12:40:41*

As for anything else, just have a go and let us know if you have any questions

*Tags: General Technical*

---

## 2020-02-09

**Jonjonjon** - *20:21:18*

Apologies for a noob question, but were all GB races cancelled today?

*Tags: General Technical*

---

## 2020-02-10

**Josh** - *15:17:00*

I noticed there were some questions about whether PRO level historical data is handled by the library a while back and I just wanted to point out that I think there may be issues with how the library handles traded volume given [https://support.developer.betfair.com/hc/en-us/articles/360002401937-How-is-traded-volume-represented-within-the-PRO-Historical-Data-files-](https://support.developer.betfair.com/hc/en-us/articles/360002401937-How-is-traded-volume-represented-within-the-PRO-Historical-Data-files-).

*Tags: Data Quality*

---

**liam** - *15:30:07*

I see, fix would be to += rather than replace in the cache but I and many collect their own data direct from the stream where it isn't cumulative. For those using the ADVANCED data (not sure why you would waste your money on it) you will need to create a custom listener/cache

*Tags: Errors Debugging*

---

**Mo** - *19:20:36*

Scraping yourself obviously saves you $$$ but that only helps you going forward and they'll probably come down hard on you if you're not trading enough to justify it

*Tags: Strategies*

---

**klozovin** - *19:28:03*

do you work with lots of historical data? is it gigabytes or terabytes? :slightly_smiling_face:

*Tags: Data Quality*

---

**Alex F** - *20:52:59*

hi, i have managed to run the historic data downloader, however, i am honestly clueless how to make sense of the bz2 files.  i have tried running the historic data streaming, but without success. i set directory to the directory of the folder and get access denied, i target a single bz2 file and receive UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 74: character maps to &lt;undefined&gt;. any help is much appreciated!

*Tags: Errors Debugging*

---

## 2020-02-11

**Alex F** - *18:06:20*

stream = trading.streaming.create_historical_stream, yea

*Tags: Strategies*

---

**Alex F** - *18:14:20*

gotcha. redid it -&gt;     for i in g:



TypeError: 'method' object is not iterable

*Tags: Errors Debugging*

---

**Alex F** - *18:16:24*

```import logging



import betfairlightweight

from betfairlightweight import StreamListener

from betfairlightweight.streaming.stream import MarketStream



"""

Data needs to be downloaded from:

    [https://historicdata.betfair.com](https://historicdata.betfair.com)

"""



# setup logging

logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



# create trading instance (no need to put in correct details)

trading = betfairlightweight.APIClient([mailto:'alexander@gmail.com|'alexander@gmail.com](mailto:'alexander@gmail.com|'alexander@gmail.com)','xxxxxx', app_key='xxxxxx', certs='C:/OpenSSL-Win64/bin')

trading.login()



class HistoricalStream(MarketStream):

    # create custom listener and stream



    def __init__(self, listener):

        super(HistoricalStream, self).__init__(listener)

        with open('output.txt', 'w') as output:

            output.write('Time,MarketId,Status,Inplay,SelectionId,LastPriceTraded\n')



    def on_process(self, market_books):

        with open('output.txt', 'a') as output:

            for market_book in market_books:

                for runner in market_book.runners:



                    # how to get runner details from the market definition

                    market_def = market_book.market_definition

                    runners_dict = {(runner.selection_id, runner.handicap): runner for runner in market_def.runners}

                    runner_def = runners_dict.get(

                        (runner.selection_id, runner.handicap)

                    )



                    output.write('%s,%s,%s,%s,%s,%s\n' % (

                        market_book.publish_time, market_book.market_id, market_book.status, market_book.inplay,

                        runner.selection_id, runner.last_price_traded or ''

                    ))





class HistoricalListener(StreamListener):

    def _add_stream(self, unique_id, stream_type):

        if stream_type == 'marketSubscription':

            return HistoricalStream(self)





# create listener

listener = HistoricalListener(

    max_latency=1e100

)



# create historical stream, update directory to file location

stream = trading.streaming.create_historical_generator_stream(

    directory='C:/Users/alexa/Desktop/repos/bet/betfair/examples/29637259',

    listener=listener

)



# start stream

#stream.start(async_=False)



g = stream.get_generator()



for i in g:

    print(i)

    ```

*Tags: Getting Started, Performance, Strategies*

---

**Alex F** - *18:18:40*

i do not understand the question :confused:

*Tags: General Technical*

---

**Mo** - *18:18:50*

What's the full error you get?

*Tags: Errors Debugging*

---

**Alex F** - *18:19:36*

```

runfile('C:/Users/alexa/Desktop/repos/bet/betfair/examples/examplestreaminghistorical.py', wdir='C:/Users/alexa/Desktop/repos/bet/betfair/examples')

INFO:betfairlightweight.streaming.stream:[Stream: HISTORICAL]: "MarketStream" created

Traceback (most recent call last):



  File "C:\Users\alexa\Desktop\repos\bet\betfair\examples\examplestreaminghistorical.py", line 67, in &lt;module&gt;

    for i in g:



TypeError: 'method' object is not iterable```

*Tags: Errors Debugging*

---

**Alex F** - *18:22:01*

INFO:betfairlightweight.streaming.stream:[Stream: HISTORICAL]: "MarketStream" created

INFO:betfairlightweight.streaming.stream:[MarketStream: HISTORICAL] 1.166912198 added, 1 markets in cache

[&lt;MarketBook&gt;]

*Tags: General Technical*

---

**Alex F** - *18:27:08*

then to be able to analyze it with pandas probs

*Tags: Feature Engineering*

---

## 2020-02-13

**Peter** - *14:34:46*

which will return something like this ...

```{'elapsed_time': 0.18714308738708496,

 '_response': &lt;Response [200]&gt;,

 '_datetime_created': datetime.datetime(2020, 2, 13, 14, 31, 56, 100139),

 '_datetime_updated': datetime.datetime(2020, 2, 13, 14, 31, 56, 100139),

 '_data': {'eventTypeId': 2,

  'eventId': 29697556,

  'score': {'home': {'name': 'Aljaz Bedene',

    'score': '0',

    'halfTimeScore': '',

    'fullTimeScore': '',

    'penaltiesScore': '',

    'penaltiesSequence': [],

    'games': '4',

    'sets': '0',

    'aces': '5',

    'doubleFaults': '2',

    'gameSequence': [],

    'isServing': False,

    'highlight': False,

    'serviceBreaks': 0},

   'away': {'name': 'Stefanos Tsitsipas',

    'score': '0',

    'halfTimeScore': '',

    'fullTimeScore': '',

    'penaltiesScore': '',

    'penaltiesSequence': [],

    'games': '4',

    'sets': '0',

    'aces': '1',

    'doubleFaults': '1',

    'gameSequence': [],

    'isServing': True,

    'highlight': True,

    'playerSeed': 2,

    'serviceBreaks': 0}},

  'currentSet': 1,

  'currentGame': 9,

  'currentPoint': 1,

  'fullTimeElapsed': {'hour': 0, 'min': 37, 'sec': 2},

  'matchStatus': 'Inprogress'},

 'event_id': 29697556,

 'elapsed_regular_time': None,

 'elapsed_added_time': None,

 'event_type_id': 2,

 'match_status': 'Inprogress',

 'time_elapsed': None,

 'time_elapsed_seconds': None,

 'status': None,

 'current_day': None,

 'current_set': 1,

 'description': None,

 'match_type': None,

 'current_game': 9,

 'current_point': 1,

 'full_time_elapsed': &lt;betfairlightweight.resources.inplayserviceresources.FullTimeElapsed at 0x11d1628d0&gt;,

 'score': &lt;betfairlightweight.resources.inplayserviceresources.Score at 0x11d162a50&gt;,

 'state_of_ball': None}```

Though you would need to stitch the scores and odds data together yourself.

*Tags: General Technical*

---

**Alex F** - *17:49:10*

so i can actually build a scraper for the future, but cannot do anything like this for historical data

*Tags: Data Quality*

---

**klozovin** - *19:14:49*

Looking at the documentation and the XML specs for `LimitOrder` type, there's a difference what parameter is mandatory ... in that case, one should go with the XML specs, right?

*Tags: General Technical*

---

## 2020-02-14

**AP** - *22:41:06*

A general question. Using the stream API, what sort of methods do people use to trigger events at certain times?

*Tags: General Technical*

---

**Jonatan (skyw)** - *22:54:50*

Cool, there are some ways to set it up, but someone else might have a solid working production code for it : )

*Tags: Deployment*

---

**Jonatan (skyw)** - *23:07:05*

I don't see how that would be a problem, although I do not know the big  picture : )

Would be cool to hear from anyone else having the same issue solved

*Tags: General Technical*

---

## 2020-02-15

**liam** - *07:14:21*

[@UNQGKT0CR](@UNQGKT0CR) snap might help you [https://liampauling.github.io/betfair/streaming/#snap|https://liampauling.github.io/betfair/streaming/#snap](https://liampauling.github.io/betfair/streaming/#snap|https://liampauling.github.io/betfair/streaming/#snap)

*Tags: General Technical*

---

**Jonjonjon** - *20:20:17*

Has anyone seen anything like this before? The bot generating this error has been running without modification for years, and has suddenly stopped working because of this:



{u'status': u'FAILURE', u'errorCode': u'PERMISSION_DENIED', u'instructionReports': [{u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 23333488, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 20077280, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 11124, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 26016330, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 24986924, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 22034022, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 391245, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 23054357, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 23955673, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 5842556, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}, {u'status': u'FAILURE', u'errorCode': u'ERROR_IN_ORDER', u'instruction': {u'handicap': 0.0, u'orderType': u'LIMIT', u'selectionId': 23955744, u'limitOrder': {u'price': 1.13, u'persistenceType': u'LAPSE', u'size': 400.0}, u'side': u'LAY'}}], u'marketId': u'1.168853098'}

*Tags: Errors Debugging*

---

**Mo** - *21:33:28*

If you log out and login again do you get an error?

*Tags: Errors Debugging*

---

**Mo** - *22:05:12*

You mean you were able to login without an error?

*Tags: Errors Debugging*

---

**Mo** - *22:10:04*

If the theory is correct that this IP is now being recognised as in a restricted location then you will be prevented from logging in with a more descriptive error message than PERMISSION_DENIED

*Tags: Errors Debugging*

---

**Jonjonjon** - *22:12:31*

This is bit embarrasing, but I only have permissions to run scripts on that machine. I don't have UI access or shell access.:expressionless:



I'm now paying the price for using an exceptionally cheap way to get started on my betting career.

*Tags: Errors Debugging, Strategies*

---

## 2020-02-16

**Peter** - *17:24:05*

[@UPMUFSGCR](@UPMUFSGCR) Might not be relevant, but I had a problem a couple of years ago when 1 in 3 of my stream connections were failing. At the time I was using a server in the Netherlands. Managed to track it down to specific servers at the Betfair end (from the connection IDs) and it turned out to be a failing process at the Betfair end, failing to properly replicate acceptable IP addresses to some new validation servers. Once identified they were very quick to sort it out.

*Tags: Deployment*

---

## 2020-02-19

**jb** - *19:04:27*

Hello everyone.  Been lurking for a while, and playing with the great library [@U4H19D1D2](@U4H19D1D2) made (thanks for making it available!).  A couple of noobish questions.



First, I'm trying to use the historic data.  How are you guys consuming the compressed files?



I am unzipping them in 2 stages: `tar -xf data.tar` for the whole archive which gives me files with `.bz2` extensions which I decompress with `bzip2 -d` (individually atm).  But even then I can't `json.load()` them (I get a `JSONDecodeError`, saying `Extra data`.



I imagine there's an easier way!

*Tags: Errors Debugging*

---

**Fab** - *19:07:35*

I haven’t tried to play with historical data using betfairlightweight yet. But from what I recall, each line of an historical data file is a separate json object.

*Tags: Data Quality*

---

**liam** - *19:10:14*

[https://liampauling.github.io/betfair/streaming/#historical|https://liampauling.github.io/betfair/streaming/#historical](https://liampauling.github.io/betfair/streaming/#historical|https://liampauling.github.io/betfair/streaming/#historical) 

*Tags: General Technical*

---

**liam** - *19:10:48*

Any reason you aren’t using bflw? But yeah they are files filled with lines of json

*Tags: General Technical*

---

**Fab** - *19:13:48*

So far I’ve been using Nodejs and have only recently started playing with bflw / Python

*Tags: General Technical*

---

**IamYou** - *19:29:44*

So, n00b question.. How would I get the spread on some runner?

*Tags: General Technical*

---

**IamYou** - *19:30:38*

I have been reading betfair's dev documentation but it is frying my brain.

*Tags: General Technical*

---

**ash** - *19:33:05*

two good starting points? --&gt;

[https://betfair-datascientists.github.io/api/apiPythontutorial/](https://betfair-datascientists.github.io/api/apiPythontutorial/)

[https://github.com/liampauling/betfair/blob/master/docs/quickstart.md](https://github.com/liampauling/betfair/blob/master/docs/quickstart.md)

*Tags: General Technical*

---

**Peter** - *22:00:18*

But as Liam and Mo have said, it's way easier to just use the BFLW package to read them, and there's an example of that in the repo [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**IamYou** - *22:10:35*

If you really need to parse json on each line then maybe check ujson for performance.

*Tags: Performance*

---

## 2020-02-20

**jb** - *11:06:48*

Thanks Peter.  After the inital untar I am indeed now using the python `bz2` to get at them and then BFLW.  I'm used to dealing with the market / event / runner etc objects as json (passing `lightweight=True` ), which is one reason I was trying more direct.  Guess I'll just have to learn to use them properly now..

*Tags: General Technical*

---

**liam** - *13:02:03*

nothing is excluded, creating python objects is slow

*Tags: Performance*

---

## 2020-02-21

**Peter** - *09:20:52*

Although I’ve been using streaming for set and forget type trades, I’ve been using polling for more active trading. I’m now trying to wean myself off this approach and use streaming for all types. However, polling has the advantage that I can get the start of a market and my trades in that market with a single API call, whereas I would need to combine data from a stream with market data and an order stream.



I’m wondering how other people architect their applications to do this. Do you read both streams in a single loop? Or do you run each in its own process and share snapshots via a common cache? Do you initiate market streams for a single event, or leave them running all day choose within your application when you access the data? Do you use messaging to push data from the streams into a trading app, or pull data from a cache or other data store when needed?



Not expecting answers to all those questions … just hoping to get some insight in architectures that work in practice from those a little ahead of me in this area.

*Tags: Strategies*

---

**Mo** - *09:22:44*

Just a quick answer for now but nothing wrong with combining polling and streaming IMHO

*Tags: General Technical*

---

**liam** - *10:13:19*

yep that can be a simple way to do it (its how I do it) [https://liampauling.github.io/betfair/streaming/#snap](https://liampauling.github.io/betfair/streaming/#snap)

*Tags: General Technical*

---

**Mo** - *11:00:56*

Respect the limits and it's hard to see how they can have a problem

*Tags: General Technical*

---

**Peter** - *14:53:25*

Your question did though prompt me to think that for some strategies I could stream the markets and when an interesting situation occurs then do a quick poll on my orders before responding with trades. That would transition some of my strategies in the right direction.

*Tags: General Technical*

---

**liam** - *15:02:39*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

*Tags: General Technical*

---

## 2020-02-22

**Oliver Varney** - *19:38:14*

still not fixed from a month ago when it happened to me

*Tags: Errors Debugging*

---

## 2020-02-23

**Lee** - *10:13:48*

Does anyone buy the betfair historical data or just collect their own?

*Tags: Data Quality*

---

**Mo** - *10:14:31*

Collecting your own only helps going forward

*Tags: General Technical*

---

## 2020-02-25

**Peter** - *17:09:58*

You may find this helpful:

[https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request)

*Tags: General Technical*

---

**Chris** - *19:48:42*

Hi all I was wondering if someone can give me some clarity..  When you get a market book from the queue you have all the back/lay information for the runners, but you also have an attribute called 'streaming_update'

*Tags: General Technical*

---

**jb** - *20:01:45*

My understanding (based on cross-checking between the two) is:

• the `runners` field is correct and up-to-date at the time of publication

• the `streaming_update` field shows the changes since the last update. These changes *have* been incorporated in the `runners` field.

(The update is useful to me as I want to know which runners have changed)

However *I am a noob*, so would be very interested to have this confirmed.

*Tags: General Technical*

---

**liam** - *20:26:32*

Yep that’s correct, chucked the update in there whilst debugging and found it useful so left it 

*Tags: Errors Debugging*

---

## 2020-02-26

**Unknown** - *08:48:40*

Tennis scores API documentation



Corresponds to functions in this module: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/scores.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/scores.py)

*Tags: General Technical*

---

**Mo** - *09:10:20*

It's so long ago I first tried it that I'm not sure if it needs to be specifically activated. Given it's already implemented in betfairlightweight, should be easy to check on your account?

*Tags: General Technical*

---

**Mo** - *09:13:03*

Yes, betfairlightweight has football scores through the undocumented in play service: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py)

*Tags: General Technical*

---

**Lee** - *09:22:23*

my filter is

```market_filter = streaming_market_filter(

    event_type_ids=["1"], country_codes=["GB"], market_types=["WIN"]

)```

*Tags: General Technical*

---

**Chris** - *18:58:44*

hey all, I notice _async has been removed in the recent update.. Is there a way to not have the streamer go into a seperate process so I can debug?

*Tags: Errors Debugging*

---

**liam** - *19:06:15*

Pycharm can do it but tbh I don’t understand the problem as the only change has been now you need to create the thread 

*Tags: General Technical*

---

**Chris** - *19:08:16*

Previously I could just hit debug in pycharm and it would take me to the first breakpoint in the code, but now it just ends so I guess a seperate thread is being created for the streamer right?

*Tags: Errors Debugging*

---

**Chris** - *19:19:28*

Aaaah my bad, I switched to the generator stream by accident while trying to fix something else, thanks :slightly_smiling_face:

*Tags: Errors Debugging*

---

## 2020-02-27

**LK** - *13:55:15*

Question for [@U4H19D1D2](@U4H19D1D2) maybe? Yesterday my app that subscribes to the Orderstream got "Latency high" warnings, latency went as high as couple hundred seconds (!). Then the connection to betfair got disconnected. After reconnecting everything was fine again. I am wondering: could this be a betfair issue? Should I pre-emptively disconnect/reconnect when I get "Latency high" warnings?

*Tags: Performance*

---

## 2020-02-29

**Unknown** - *17:01:10*

With a utilisation on the right, max below 5%, would you expect the number of high latency warnings?

*Tags: Performance*

---

**liam** - *17:03:39*

You can remove the errors by upping the latency on the listener 

*Tags: Errors Debugging, Performance*

---

**Peter** - *19:55:43*

Came a cropper today. Despite Betfair's promise to write to users who would fall foul of the new restrictions on stream connections, they didn't, at least not to me. So this afternoon I started to have new connections rejected. So I'm going to need to rework my soccer streams to aggregate them into a small number of connections. In looking into this I see in the docs that "A maximum consumption limit (exceeding this will result in an error with details of the limit: ErrorCode.SUBSCRIPTION_LIMIT_EXCEEDED)", which I suspect is what caught me out today. But are there any limits on the number of markets types / country codes to which one can subscribe in a single stream? Are there any practical limits that those of you already using more aggregated streams would recommend (e.g. to avoid overloading CPUs for busy markets)?

*Tags: Errors Debugging*

---

**Peter** - *20:00:42*

Yep. But I was streaming multiple markets in individual matches so going over by a lot :rage:

*Tags: General Technical*

---

**Peter** - *20:03:35*

I can understand the 200 markets limit for an APi call where you can calculate it and even specify the markets. But how do people manage this when streaming - I'd imagine you don't want to specify the markets as you want them to be added in as they become available - but when they are that could tip you over the limit. &lt;scratching head&gt;

*Tags: General Technical*

---

**Peter** - *20:11:58*

Noticing in the API documentation that competition Id is available as a market filter and BFLW supports it, but that's absent from the stream documentation. Anybody know if it works there too, or not?

*Tags: General Technical*

---

**Peter** - *20:53:40*

Doesn't help that the needing to be refactored was written within a PHP framework, so will need to be converted from that paradigm to python scripts!

*Tags: General Technical*

---

**jhaa** - *22:07:37*

I get occasional 'INVALID_SESSION_INFORMATION' in list_market_book which for now kills my bot and restarts it. Why do I likely get that error? Is that a server restart that requires a new login?

*Tags: Errors Debugging, Deployment*

---

## 2020-03-01

**Mo** - *08:45:12*

Login sessions last for 8 hours. You need to call the keepAlive endpoint to refresh them: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py)

*Tags: Deployment*

---

**Ian** - *14:23:20*

hi all, on flumine what is the most efficient way to display data/time on the log screen - prepending INFO:, WARNING: ?

*Tags: General Technical*

---

**Lee** - *14:26:06*

Is there any reason you can’t use python’s logging library? Thats what the library uses.

*Tags: General Technical*

---

## 2020-03-02

**liam** - *09:21:32*

v2.1.0 now released with a few improvements and bug fixes (including one I introduced in 2016 that has somehow gone unnoticed..) [https://github.com/liampauling/betfair/blob/master/HISTORY.rst#release-history](https://github.com/liampauling/betfair/blob/master/HISTORY.rst#release-history)

*Tags: Errors Debugging*

---

**Lee** - *09:41:17*

I noticed you also merged 1.0.0 into master for flumine. Thanks!

*Tags: General Technical*

---

**liam** - *09:42:01*

yep and docs for those interested, expect a lot breaking changes so pin to that version when using in production [https://liampauling.github.io/flumine/](https://liampauling.github.io/flumine/)

*Tags: Deployment*

---

**AP** - *15:40:16*

When trying to process multiple historical data files at the same time, is threading more efficient than multiprocessing?

*Tags: Data Quality*

---

**Lee** - *16:05:58*

I was just wondering about flumine, it seems like it creates a separate stream for each strategy, is it possible to use the same stream across multiple strategies?

*Tags: Strategies*

---

**Jonatan (skyw)** - *16:08:53*

I missunderstood, I did not see new version of flumine :slightly_smiling_face:

*Tags: General Technical*

---

**liam** - *17:35:33*

[@UUCD6P13J](@UUCD6P13J) yes you can, just provide the same market filter and it will handle it for you, the logic is here: [https://github.com/liampauling/flumine/blob/master/flumine/streams/streams.py#L21](https://github.com/liampauling/flumine/blob/master/flumine/streams/streams.py#L21)

*Tags: General Technical*

---

**Newbie99** - *21:55:37*

Does anyone know if Betfair get annoyed if you place too many small stake bets below min size (in the docs it says you can place bets under GBP 2.00 if the total return &gt;= GBP 10.00). I wanted to test a new strategy with low stakes and some of them would be very small but at long odds, but obviously I don't want to annoy anyone by placing too many small orders (just for testing I mean, not the long term plan of course)!

*Tags: Strategies*

---

## 2020-03-03

**Peter** - *06:53:21*

[@UFTBRB3F1](@UFTBRB3F1) I've been placing thousands of these a day for a while now without problem. As the documentation implies, the ability to place lower minimum stakes at larger prices was introduced specifically to help with liquidity for longer-priced selections, so what you're describing appears to be a behaviour that they wanted to encourage.

*Tags: General Technical*

---

## 2020-03-05

**Jonjonjon** - *21:59:21*

Hi Liam, I notice that on the Flumine page, it says that a backtesting feature is in development. Do you have any details on how it will simulate fills?

*Tags: Feature Engineering*

---

## 2020-03-06

**Jonjonjon** - *13:54:31*

You make some good points. For some tests a matcher than estimates order book position would be good. For other tests, such as when users are just taking prices, that won't be needed.



I'm just wondering if I should take a stab at my own backtester (I currently have none and don't do any backtesting), or if it would be better to try and do something with Flumine.

*Tags: General Technical*

---

**liam** - *15:23:58*

[@UPMUFSGCR](@UPMUFSGCR) up to you, can't tell you how long it will take to get flumine to that point

*Tags: General Technical*

---

**liam** - *15:32:08*

if its not that or a http error then i will try again, as you place more bets you will encounter errors a lot, on all cancel/update/replace/place

*Tags: Errors Debugging*

---

**Mo** - *15:35:48*

Unless you have best execution turned off



```In normal circumstances the placeOrders is an atomic operation.  PLEASE NOTE: if the 'Best Execution' features is switched off, placeOrders can return 'PROCESSED_WITH_ERRORS' meaning that some bets can be rejected and other placed when submitted in the same PlaceInstruction ```



*Tags: Errors Debugging, Feature Engineering*

---

**Jonjonjon** - *20:12:42*

[@UUM0N2B6W](@UUM0N2B6W) Yes I am using the customer order ref field. I listen for a current orders stream. And when orders come in with those trade refs, I delete them from the Python "set" or hashes that I have stored in memory

*Tags: Performance*

---

## 2020-03-07

**Silver Drifter** - *21:39:06*

I've just taken early, semi-retirement, which basically means I will continue to do odd-jobs here and there.  Having 40 years software development experience means I'm no spring chicken but I do "get" how to develop software for the finance industry in the manner to which they subscribe (long story).  So now, I finally have the time to get stuck into my BF trading bots and I'm "quietly confident" (yes, I understand the irony of being "quiet" whilst announcing my plans").  Anyway, as I've stated in my profile, in theory, I'm happy to consider various collaborations if anyone has a decent idea or maybe something else.  The only potential sticking point is that I am a 100% Microsoft "stack" developer.  Although, I'm seeing Python associated with so many articles to do with modelling, AI and ML, etc., that I MAY be tempted to have a look into the language soon!

*Tags: Strategies*

---

**Silver Drifter** - *21:56:10*

I have recently started to look into AI &amp; ML, purely from the perspective of C# though.  So far, there appears to be quite a lot of useful information available and I'm hopeful of using ML to help me identify patterns, which I may not have been aware of otherwise.  Also interested to the experience of others

*Tags: General Technical*

---

**Jonjonjon** - *21:59:25*

I hope you find something useful to you. I have quite a jaded view, as all the truly effective strategies that I have seen, are the result of human intuition/skills/observation. Not AI/ML models. If you look at the public returns of hedge funds that claim a  heavy use of AI/ML, the real returns don't match the bluster of the promotional material.

*Tags: Strategies*

---

## 2020-03-08

**reload** - *06:54:35*

Hey All



Complete newbie to Betfair, dont know any programming I just work in construction finance.



Would anyone be open to helping our punters get up and running on Betfair? - currently we bet via bookies only.



We are trying to setup database or excel of previous results for horse racing / grey hound racing including opening prices &amp; prices at jump, summarised by jockeys / trainers etc.



Is anyone open to assisting? If so, could you please advise rates? 



Look forward to hearing from you all!



Cheers

*Tags: Getting Started*

---

## 2020-03-09

**D C** - *08:43:31*

I have a question to anyone who is running a profitable bot. My old man is nearing retirement and is constantly telling me he wants to give me money to "invest" in my bots. I am happy to do this but obviously if I take it and make profit and pay the old man his share I am going to end up getting shafted on PC if I ever get that far. To that end I would prefer to give him a bot that he can run himself. My question is, how is this sort of thing viewed by betfair? They would be able to see 2 similarly themed bots running surely. Would they see this as some sort of PC avoidance if it were coming from 2 accounts that were clearly related (same surname living in fairly close proximity etc) ? I am WAY off PC stage and have a lifetime loss of 50K to make back first but I am sure you can see the dilemma. Any thoughts?

*Tags: General Technical*

---

**liam** - *08:48:05*

if you start selling bots they will no longer make money so that isn't a problem :smile:

*Tags: General Technical*

---

**liam** - *08:51:18*

But you will be managing it / logging in etc. That's different to you selling a strategy

*Tags: Strategies*

---

**D C** - *09:04:46*

OK thanks chaps. Your input has been very helpful indeed.

*Tags: General Technical*

---

**D C** - *12:26:29*

I am assuming that most people here are Python devs so maybe a waste of time asking the question here but I will try anyway. I have been a C++ dev on and off for 15+ years but since C++11 and later standards have come out I feel the language is not only dying but is overly complicated now and fast disappearing up its own arse. The C++ space seems to now be filled with pseudo-smartarses who enjoy talking about the language in abstract terms and waxing lyrical about new features, rather than rolling up their sleeves and writing something useful. As a consequence, I am looking to get into another language (not discounting pure C either). In terms of long term career demand, does anyone have any suggestions (everything but C# and Java will be seriously considered)??

*Tags: Feature Engineering*

---

**agberk** - *12:32:41*

Are you saying you're not considering Python for an unstated reason or you're interested in learning another language in addition to it? I'd say there's a pretty strong case for it having long term career demand

*Tags: General Technical*

---

**D C** - *12:40:39*

Golang is probably top of my list. Python next. I am 44 though and all the kids are learning Python. I have dabbled in Python in the past as I do like scripting stuff (my first bots were actually writtin in Perl for the old SOAP API).

*Tags: General Technical*

---

**Alexander Keliris** - *12:46:10*

I love Haskell, have done a few side projects with it and used to attend the Hoodlums meetups at Barclays. But since getting into Rust I found it gives you similar runtime guarantees but with much better performance. More importantly, I found Rust much easier to work with for real projects (better tooling, `cargo` etc.)

*Tags: Performance*

---

**hugo** - *13:13:02*

It's a lisp, so similar to other lisps like Scheme and Common Lisp. It runs on the JVM so has complete access to the Java ecosystem (without this I don't think it's a viable language). Immutable data-structures. You don't have the problem of having thousands of tiny classes because the Clojure of modelling data is essentially "use hashmaps". Macros, although that goes back to it being a lisp. It's functional.

*Tags: Strategies*

---

**Jack Kaminski** - *13:21:46*

Does anyone have any suggestions for academic articles on sports modelling techniques

*Tags: Strategies*

---

**D C** - *13:24:22*

What sports in particular? If you are on twitter and want soccer models you could contact Mercurius - they are quite helpful and sent me some papers a while back

*Tags: Strategies*

---

**Jack Kaminski** - *13:25:40*

I'm unfamiliar with sports modelling techniques, and how this materialises into cash

*Tags: Strategies*

---

**D C** - *13:27:43*

From a stats point of view I think GLMs are the way to go. Multinomial or perhaps ordinal LR. That would be what I would try if I had access to a nice clean fat database

*Tags: General Technical*

---

**Jack Kaminski** - *13:29:33*

What's considered a fat database

*Tags: General Technical*

---

**D C** - *13:30:06*

I would call 10+ years of full horse data a fat database. Something on the proform scale of things

*Tags: General Technical*

---

**Jack Kaminski** - *13:36:35*

I won't bother him until I have a clear understanding of sports modelling

*Tags: Strategies*

---

## 2020-03-10

**Jack Kaminski** - *06:45:44*

Firstly, database creation with those variables

*Tags: General Technical*

---

**Chris** - *21:25:15*

Interesting to see a lot of conversation on ML here, does anyone else work in data science? or making ML models as a day job. Most people seem like software devs on here

*Tags: Strategies*

---

**Chris** - *21:27:51*

I could see simple models being used to aid strategies (predicting time until a race ends for example) but imo building something that would decide what bets to place and when would be extremely difficult, especially for a single person

*Tags: Strategies*

---

**Chris** - *21:34:37*

You need a lot of data for one, and also you need to give the model rewards for its actions as soon as possible.. If it makes lots of decisions in a game but you only see the reward at the end, then the model has no way of knowing which decisons contributed to the end result

*Tags: Strategies*

---

**Chris** - *21:42:02*

Splitting the markets into smaller segments and rewarding the actions in each segment might be a step forward, but then you would need some criteria that decides what reward to give the model at each time step

*Tags: Strategies*

---

**Jack Kaminski** - *23:03:17*

But I know from traditional quant models that the higher the IQ of the model the lower the return

*Tags: Strategies*

---

## 2020-03-11

**Mo** - *09:50:52*

Basic horse racing question... where does a race distance of 6f16y come from? Why bother with the 16 yards?

*Tags: General Technical*

---

**Jonjonjon** - *10:50:06*

[@UBS7QANF3](@UBS7QANF3) I don't know the answer. But if no one else here knows, I've asked similar questions on [https://sports.stackexchange.com](https://sports.stackexchange.com) and got good responses.

*Tags: General Technical*

---

**Oliver Varney** - *12:56:51*

basic question but does the orderSubscriptionMessage return previously fully executed orders when you start it up or is it purely orders that are still unmatched ?

*Tags: General Technical*

---

**Oliver Varney** - *12:58:31*

then the deltas are sent via the streaming option

*Tags: General Technical*

---

## 2020-03-12

**Evaldas** - *16:03:21*

Hi, can anyone give me example of how to convert flumine raw data to marketbooks?

*Tags: General Technical*

---

**Mo** - *16:05:40*

Read it like historic data? [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

## 2020-03-13

**Jack Kaminski** - *07:04:58*

Is there anyway to retrieve the race card data from each selection via streaming? Or do I have to get this data another way

*Tags: General Technical*

---

**Jack Kaminski** - *07:15:59*

Or a database of their values

*Tags: General Technical*

---

## 2020-03-14

**Oliver Varney** - *10:38:25*

your going to struggle to find anyone willing to give there model away pal, not unless they want to shoot themself in the foot

*Tags: Strategies*

---

**Chris** - *23:07:46*

hast anyone not been capturing all events when using flumine to get racing data? Its probably me, but just wondering

*Tags: General Technical*

---

## 2020-03-16

**liam** - *10:24:17*

What edge cases do you foresee? I don't like how the listener is used for market and order data, would probably name the function prime_order_cache so its clear and raise an error if its being used for markets

*Tags: Errors Debugging*

---

**liam** - *10:36:48*

raise error if cache already present for a particular market? i.e. before any order data comes from the stream

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *10:42:55*

I don't think it should raise an error, you should probably subscribe first then do listcurrentorders then apply streamupdates

*Tags: Errors Debugging*

---

## 2020-03-17

**D C** - *12:36:42*

[@UUCD6P13J](@UUCD6P13J) Probably but through isolation boredom more than anything. Problem is that you have another potential unknown to factor in: global financial implosion. Fact is that the betting markets on the other side of this may not be anything like what they were prior to it. Will bookies/exchanges even survive? I am sure they will but really hard to see how life will change after this. May sound like hyperbole but that is how I am looking at it.

*Tags: Strategies*

---

**liam** - *12:49:27*

mental, I suppose its going to be a good time to do a database migration that I have been delaying for about 3 years

*Tags: General Technical*

---

**Alex A** - *16:49:43*

Nice opportunity to do all my TODOs before going into production properly I guess.

*Tags: Deployment*

---

## 2020-03-19

**Jonjonjon** - *08:59:01*

It's definitely interesting. There are some excellent tutorials on deep reinforcement learning from Deepmind on Youtube. However, I  have not managed to used any ML successfully for betting. All my models are based on eyeballed observations.

*Tags: Strategies*

---

## 2020-03-20

**Adam Momen** - *23:42:01*

Hello guys, quick question, is it possible to get these horse runner meta-data properties from the *free* bet-fair exchange API:

• ADJUSTED_RATING

• AGE

• DAYS_SINCE_LAST_RUN

• SEX_TYPE

Because I have been trying to call the API and all I have got is a basic meta-data like:

•  Runner Name

• Handicap

*Tags: General Technical*

---

## 2020-03-21

**Peter** - *04:55:00*

Yes to all those via the race cards API. There's a simple example available in the BFLW examples folder.

*Tags: General Technical*

---

**kense** - *13:49:17*

Hi everyone. I have no experience whatsoever of Betfair trading. A friend of mine uses BetAngel with a variety of Excel spreadsheets, and asked if I would help him out in automating some of his processes. I found this project within a few minutes of Googling, though, and it seems like it makes more sense to skip BetAngel and Excel altogether and replicate the combined functionality with Flumine / Betfair API. Just wanted to introduce myself and apologise in advance for my SFQs!

*Tags: Strategies*

---

## 2020-03-22

**mandelbot** - *08:14:56*

Hi all, I'm new here too. Been meaning to port my process to API for a while and am taking this lockdown as an opportunity to do so. I only started coding a couple years ago (VBA) and am fairly new to Python. So apologies in advance for basic questions I may ask (I promise to google first :)). Many thanks to [@U4H19D1D2](@U4H19D1D2) and everyone who contributed over the years. Looking forward to being a part of this slack!

*Tags: Getting Started*

---

**mandelbot** - *09:38:26*

[@U4H19D1D2](@U4H19D1D2) You've helped me with that already. Thanks again!

*Tags: General Technical*

---

**Rory** - *22:14:41*

just catching up on recent messages ... lots of good questions/discussion around ML ... we've been using it heavily in soccer for the last 2 years .. there is absolutely no doubt that machines are better at spotting patterns in large data sets than humans ... that's just proven beyond doubt by now with most of the publicly available models out there (think AWS image recognition, Google Translate, etc)



but sport by nature is pretty random, so it's a long slog to find the right combination of models to use and the right data to feed them ... we're definitely getting there but it takes a lot of dedication

*Tags: Deployment, Strategies*

---

**Rory** - *22:22:08*

re programming languages, it really doesn't matter which one you choose ... anyone coming from a C++ background will be able to pick up any of the modern languages after a few weeks ... I've been through many years of C, C++, Java in my career and spent the last while learning Python .. I like it as a language .... any of the others mentioned above are equally good candidates ...I wouldn't mind having a look at Rust or Go myself but learning another language is down my priority list right now

*Tags: General Technical*

---

## 2020-03-23

**Johnny** - *22:50:35*

Hi all, been lurking for a while now and suddenly seem to have a bit more time on my hands...  I came across betfairlightweight after doing a load of work writing my own API-NG interface - haven't finished much of the streaming part yet but will probably not bother since Liam's implementation looks so comprehensive!  



Anyway my question is, what is the more common approach here, to collect your own data via recording streams or buying the pro datasets from betfair?  How do you guys browse the data since there are so many markets and events?  I've written my own historical data browser which i suppose all of you backtesters have already done.

*Tags: Data Quality*

---

## 2020-03-24

**liam** - *05:29:59*

[@UU6AE43V0](@UU6AE43V0) many purchase historic data but you can record your own using [https://github.com/liampauling/flumine|flumine](https://github.com/liampauling/flumine|flumine) which is now being developed into a trading framework as well. I store the data in aws s3 and have an API/database which allows me to browse / search markets

*Tags: Deployment, Strategies*

---

## 2020-03-25

**Johnny** - *11:56:14*

Thanks - will have to spend some time on getting to grips with Flumine then!

*Tags: General Technical*

---

**Lee** - *12:51:54*

I’m using flumine too to collect data. Works well. (When there are games on) ha

*Tags: General Technical*

---

## 2020-03-26

**Lee** - *11:20:26*

[https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py)

*Tags: General Technical*

---

**mandelbot** - *16:08:32*

when I try to run the above example I get the following error. Traceback (most recent call last):

  File "C:\Users\Administrator\Desktop\Python\marketrecorder.py", line 8, in &lt;module&gt;

    from strategies.marketrecorder import S3MarketRecorder

ModuleNotFoundError: No module named 'strategies'

am I on an incorrect version or something?

*Tags: Errors Debugging*

---

**Lee** - *17:10:19*

Look at the examples in the repo. Recording a market is implemented as a strategy

*Tags: Strategies*

---

## 2020-03-27

**Oliver Varney** - *09:15:55*

Question, do replace order persist the original customerOrderRef?

*Tags: General Technical*

---

## 2020-03-28

**brightcake** - *15:14:40*

Quick question about betfair historical data - each file that is downloaded contains the history of one market only? Or do you sometimes find data from other markets contained there?

*Tags: Data Quality*

---

## 2020-03-30

**Newbie99** - *13:00:51*

When using listCurrentOrders, if the order count is &gt; 1000, how is this handled, I don't have any markets with that many executed orders to check, but wanted to build this functionality in anyway as its bound to come up eventually.



If I call for market x (and no other parameters) then it will return y CurrentOrders, but lets say the moreavailable flag = True, how to I access these additional records?



Is it a case of checking the number of the last record (i.e.  1 - 1000), then making a second call with 1001 as the start and keep repeating, or is there a slicker way?

*Tags: General Technical*

---

**liam** - *13:02:45*

```    def _get_cleared_orders(self, event, client):

        from_record = 0

        while True:

            try:

                cleared_orders = client.betting_client.betting.list_cleared_orders(

                    bet_status="SETTLED",

                    from_record=from_record,

                    market_ids=[event.market_id],

                    customer_strategy_refs=[config.hostname],

                )

            except BetfairError as e:

                logger.error(

                    "_get_cleared_orders error",

                    extra={"trading_function": "list_cleared_orders", "response": e},

                    exc_info=True,

                )

                time.sleep(10)

                self.equine.account_queue.put(event)

                return



            [http://logger.info|logger.info](http://logger.info|logger.info)(

                "{0} cleared orders found, more available: {1}".format(

                    len(cleared_orders.orders), cleared_orders.more_available

                )

            )



            self.equine.log_control(ClearedOrdersEvent(event.market_id, cleared_orders))

            self.equine.handler_queue.put(

                ClearedOrdersEvent(event.market_id, cleared_orders)

            )



            from_record += 1000

            if not cleared_orders.more_available:

                break

        return len(cleared_orders.orders)```

*Tags: Errors Debugging, Strategies*

---

## 2020-04-02

**Jeff** - *00:34:48*

Hi everyone I am very interested in building applications especially for Betfair. I am a complete noob with any language but I am very interested in Python. I would like to build an app that replays Betfairs horse racing historical data with ladders setup like Gruss or Bet Angel and able to place fictional bets and trade just as if it were real. I realise it's a tall order to build a complicated app with no experiance but I have plenty of time on my hands since I am out of work and have plenty of evenings to start learning. I have downloaded some sample horse racing historical data pro and advanced, Python installed and betfairlightweight and a few other addons. I guess I'd like to know where to start and if someone could point me in the right direction I'd be very greateful. Kindest regards Jeff

*Tags: Getting Started, Data Quality*

---

**liam** - *06:40:00*

Why bother? BA does a very good job already and python is probably one of the worst languages to write any sort of gui applications 

*Tags: General Technical*

---

**Remi** - *10:12:28*

Given that you want to create a GUI it might be a lot easier to create a web app with a python server behind it, even factoring in that you’d have to additionally learn html/css + javascript + a js framework (you could see this as a single skill).

*Tags: Deployment*

---

**liam** - *10:15:22*

+1, in terms of your CV getting experience in a full stack web app is very valuable, using python only to write a GUI will probably kill you

*Tags: General Technical*

---

**Remi** - *10:24:12*

Before you make a big application you have [https://www.ic.unicamp.br/~meidanis/courses/mc336/2009s2/prolog/problemas/|99](https://www.ic.unicamp.br/~meidanis/courses/mc336/2009s2/prolog/problemas/|99) other problems to [https://wiki.python.org/moin/ProblemSets/99%20Prolog%20Problems%20Solutions|consider](https://wiki.python.org/moin/ProblemSets/99%20Prolog%20Problems%20Solutions|consider). I’d say if the first ~30 make you pause then you’re not ready for a project.

*Tags: General Technical*

---

**Mo** - *10:44:10*

Setting aside the idea of using the tool to place fictional bets, the ability to replay and visualise markets is useful for strategy development even if that strategy is going to be fully automated

*Tags: Strategies*

---

**Jeff** - *10:44:14*

I like manual trading and this project will give me something to do and learn about but unfortunately I don't know what to do first. I have Betfair historic horse racing data and Python installed with PyQt but I've hit a brick wall. I don't know where to start first?

*Tags: Getting Started, Strategies*

---

**Mo** - *10:50:01*

Have you used betfairlightweight?

*Tags: General Technical*

---

**Jeff** - *10:51:22*

No not used betfairlightweight before.

*Tags: General Technical*

---

**Mo** - *10:52:29*

1. Using betfairlightweight, get the market book for a race just before it turns in play

2. Using your visualisation framework of choice, display that market book in a table

*Tags: General Technical*

---

**Mo** - *10:53:26*

Have you installed betfairlightweight?

*Tags: Getting Started*

---

**Lee** - *10:54:08*

[@U010RPB4S9Y](@U010RPB4S9Y) It might be worth doing some python tutorials on youtube first

*Tags: General Technical*

---

**Jeff** - *10:55:40*

The python tutorials are okay but there isnt any video tutorials on betfairlightweight

*Tags: General Technical*

---

**Mo** - *10:56:20*

Follow this example for step 1: [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py), then ask questions here

*Tags: General Technical*

---

**Mo** - *10:57:26*

It's 100% out of your league right not and it may or may not be a good idea in the first place but if you focus on things like handling the data in betfairlightweight then you will be developing skills that will apply more generally

*Tags: General Technical*

---

**Lee** - *11:04:47*

There will also be lots of bugs along the way where there won’t be any tutorials, thats the art of programming.

Learning how the language works and how to read libraries helps

*Tags: Errors Debugging*

---

**Mo** - *11:07:52*

No offence because I mean this constructively but I told you very specifically how to get started and you immediately asked how to go about doing it rather than taking the initiative

*Tags: General Technical*

---

**Jeff** - *12:18:56*

On line 66 of example do I need to point it to the historical data file? These are the errors I am getting: C:\Users\Jeff\PycharmProjects\GitHubExample\venv\Scripts\python.exe C:/Users/Jeff/PycharmProjects/GitHubExample/GitHubExample.py

INFO:betfairlightweight.streaming.listener:Register: marketSubscription 0

INFO:betfairlightweight.streaming.stream:[Stream: 0]: "HistoricalStream" created

Traceback (most recent call last):

  File "C:/Users/Jeff/PycharmProjects/GitHubExample/GitHubExample.py", line 71, in &lt;module&gt;

    stream.start()

  File "C:\Users\Jeff\PycharmProjects\GitHubExample\venv\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 295, in start

    self._read_loop()

  File "C:\Users\Jeff\PycharmProjects\GitHubExample\venv\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 301, in _read_loop

    with open(self.directory, "r") as f:

FileNotFoundError: [Errno 2] No such file or directory: '/Users/liampauling/Downloads/Sites 3/xdw/api/c0a022d4-3460-41f1-af12-a0b68b136898/BASIC-1.132153978'



Process finished with exit code 1

*Tags: Data Quality, Errors Debugging*

---

**Lee** - *12:20:45*

The error is telling you what’s wrong 

*Tags: Errors Debugging*

---

**liam** - *13:36:41*

really? [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py#L66](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py#L66)

*Tags: General Technical*

---

**Jeff** - *16:44:12*

I can't make head or tail of it. Best I left Python to the experts.

*Tags: General Technical*

---

**Fab** - *17:04:31*

Jeff, writing software is 10% writing code and 90%:



• thinking of how to structure things

• researching things you don’t know

• figuring out things by experimenting

• banging your head against the wall because of one tiny thing that it’s so obvious and why the f**k doesn’t it work, I mean it’s supposed to work if I write it like this… then realising it just needed a little tweak and the programming language is not stupid, but you are

So, don’t be discouraged if you can’t figure things out in 1 hour

*Tags: General Technical*

---

**Jeff** - *17:08:31*

Yep you're right. This is what I have achieved so far in one week. Installed Python, PyCharm and Betfairlightweight and managed to get my GitHubExample to actually connect to Betfair but that's about it so far. I don't really understand coding even the Liam's examples don't make any sense to me.

*Tags: Getting Started*

---

**Mo** - *17:13:53*

You said you were willing to learn 



It sounds like you need to start with a basic python course before you touch anything to do with betfairlightweight

*Tags: General Technical*

---

**Jeff** - *17:17:07*

Is there any specific basic python courses on youtube you would recommend?

*Tags: General Technical*

---

**Fab** - *17:24:12*

From my experience, we all learn in different ways, so my suggestion is to simply Youtube “python course beginner”, then start looking at courses one by one. If you find it difficult to understand that teacher, move to the next one.

*Tags: Getting Started*

---

**Jeff** - *17:30:31*

I've spent more time in one week trying to get to grips with Python than I did getting my pilots licence. lol

*Tags: General Technical*

---

## 2020-04-03

**liam** - *07:02:14*

[@UBS7QANF3](@UBS7QANF3) how do you deal with ‘1.12345600’, my concern is always that python will suddenly decide to convert that 

*Tags: General Technical*

---

**Mo** - *08:49:52*

Actually, to be clear, I treat them as integers in my database

*Tags: General Technical*

---

**Mo** - *08:50:30*

What would Python try to do with that?

*Tags: General Technical*

---

**Oliver Varney** - *09:38:35*

Is £2 the smallest order amount you can put through? If you have partials left over that are smaller then that will you get an 'INVALID_BET_SIZE' error. Or does that error throw in different circumstances ?

*Tags: Errors Debugging*

---

**liam** - *09:38:56*

2.009 will give you that error

*Tags: Errors Debugging*

---

**liam** - *09:39:52*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/metadata.py#L61](https://github.com/liampauling/betfair/blob/master/betfairlightweight/metadata.py#L61)

*Tags: General Technical*

---

**Oliver Varney** - *09:40:42*

does that error also throw when you put sizes through that that are like 3 decimal places and longer ?

*Tags: Errors Debugging*

---

**Oliver Varney** - *10:52:53*

apologies for the million questions, but does the order stream send updates for partial matches ?

*Tags: General Technical*

---

**PeterLe** - *11:15:11*

[@U010RPB4S9Y](@U010RPB4S9Y) Check out this course on udemy, [https://www.udemy.com/course/the-modern-python3-bootcamp/](https://www.udemy.com/course/the-modern-python3-bootcamp/) I enjoyed doing it, NB: Dont pay full asking price, search for online coupon, you should pay circa £10

*Tags: General Technical*

---

**liam** - *18:53:21*

That looks interesting as it has streaming as well 

*Tags: General Technical*

---

**Lee** - *19:00:23*

Yeah, i had the same thing. Just took me too long to work out how todo simple things.

*Tags: General Technical*

---

**Lee** - *19:16:11*

I do a bit of java and coming back to python is when I miss the types

*Tags: General Technical*

---

**liam** - *19:16:25*

Yeah that’s the challenge, it’s so time consuming, took me months to get it in bflw as I got so damn bored doing it 

*Tags: General Technical*

---

**Jonatan (skyw)** - *19:19:57*

Would it be intresting to add this,  [https://www.python.org/dev/peps/pep-0561/|https://www.python.org/dev/peps/pep-0561/](https://www.python.org/dev/peps/pep-0561/|https://www.python.org/dev/peps/pep-0561/)

*Tags: General Technical*

---

## 2020-04-06

**liam** - *14:29:02*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/stream.py#L130](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/stream.py#L130)

*Tags: General Technical*

---

**liam** - *16:08:48*

Release on bflw to [https://github.com/liampauling/betfair/blob/master/HISTORY.rst|2.3.0](https://github.com/liampauling/betfair/blob/master/HISTORY.rst|2.3.0) which removes c libraries for windows

*Tags: General Technical*

---

**liam** - *16:10:00*

Release on flumine to [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#120-2020-04-06|1.2.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#120-2020-04-06|1.2.0), a lot of breaking changes in preparation for execution integration (live and simulated)

*Tags: Deployment*

---

**Jonjonjon** - *19:15:25*

Is there a problem with Horse Racing on Betfair today? If I click on any of the US or AUS races, I get a 404 error

*Tags: Errors Debugging*

---

## 2020-04-07

**jhaa** - *15:44:25*

How come streaming_market_filter does not have the market_start_time filter but market_filter has it?

*Tags: General Technical*

---

**jhaa** - *15:55:09*

for now I am building the streaming stuff on the horses as that seems to still happen to a certain degree

*Tags: General Technical*

---

**Peter** - *16:08:22*

Having recently switched from a stream each race and each match approach, I've been pleasantly surprised at how lightweight streaming is. While there was enough soccer to justify it, I split my streams by geography, so had separate streams for GB, rest of Europe, Asia and Americas. The 200 market limit seems to apply to the API rather than streams. I did hit a limit on streams but IIRC it was at 1000.

*Tags: General Technical*

---

## 2020-04-08

**liam** - *14:54:22*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/stream.py#L164](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/stream.py#L164)

*Tags: General Technical*

---

## 2020-04-09

**Rob (NZ)** - *05:40:55*

Can I be cheeky and what that kind of functionality helps with, sorry was way over might head but I was guessing your looking at it for a specific reason.

*Tags: General Technical*

---

**liam** - *13:41:15*

[https://liampauling.github.io/betfair/streaming/#snap](https://liampauling.github.io/betfair/streaming/#snap)

*Tags: General Technical*

---

## 2020-04-10

**ClauMor** - *14:03:39*

that means, in the betfairlightweight package, the default max_latency is .5

*Tags: Performance*

---

**Mo** - *14:09:29*

The publish time is divided by 1000 during calculation of the latency

*Tags: Performance*

---

**ClauMor** - *14:11:43*

anyway i get warning with 1.5 seconds latency

*Tags: Performance*

---

**Mo** - *14:30:40*

Consume the stream in the standard way: [https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**Lee** - *22:14:41*

Is it okay to keep the market recorder running in flumine as one of the strategies in production or keep them as two separate services?

*Tags: Data Quality, Deployment*

---

## 2020-04-11

**liam** - *07:17:16*

[@UUCD6P13J](@UUCD6P13J) what do you mean? I run about 5 strategies each collecting data on one instance in production with no problems 

*Tags: Deployment*

---

**liam** - *08:06:43*

So I use flumine for all my data recording and I have a separate framework for strategies. I can’t open source the latter due to their being too much secret sauce tightly coupled to it but I am slowly advancing flumine to be at the same level (if not better / more dynamic / plug and play) 

*Tags: Performance*

---

**Ruben** - *21:31:12*

I have recently installed the betfairlightweight package, it's looking super-promising, thanks a lot to the developer(s)

*Tags: Getting Started*

---

**Ruben** - *21:32:01*

I have one quick question: When trying to log in, I am this exception from the api "AUTHORIZED_ONLY_FOR_DOMAIN_ES"

*Tags: Errors Debugging*

---

## 2020-04-12

**Ruben** - *20:33:56*

Yeah makes sense; despite that, letting everyone access the platform and limiting yourself to collecting a % of the profits does not sound like a bad business model either. But as you say, their current approach is probably more profitable for them

*Tags: Strategies*

---

## 2020-04-13

**liam** - *10:03:43*

open to any other methods though as I don't do much hedging, for me it tends to be: place order -&gt; order matched -&gt; place offset -&gt; cancel offset | wait for offset to be matched -&gt; reset strategy

*Tags: Strategies*

---

## 2020-04-14

**Lee** - *20:10:59*

Just ran my first strategy placing real bets :slightly_smiling_face:

*Tags: Strategies*

---

**Lee** - *20:37:38*

Yeah, it was okay. Lost 26p. Had a few double bets i need to fix. Will fix that and try again tomorrow evening

*Tags: Errors Debugging*

---

**mandelbot** - *20:48:29*

Would someone care to share an example of an implementation of a strategy? Can take the secret sauce out or something that isn't profitable. A non basic example would be really useful for me.

*Tags: Strategies*

---

## 2020-04-15

**Mo** - *07:09:03*

I'm up for this in principle but it would take me a while to put something together.



But if you're not interested in the secret sauce - let's say that's the prediction model and, to a lesser extent, the staking strategy - then what about it are you looking for insight into?

*Tags: Strategies*

---

**mandelbot** - *08:50:56*

i'm more interested in the mechanics of the library and how to use the different methods and classes etc. I'm a novice when it comes to code and find it helps me learn much quicker to work with and try to alter example code. So lets say you get a signal from your prediction model, and you want to place a bet x ticks above best back price when available volume is &gt; 100, odds &lt; 10 and time left till the off is &gt; 3 mins. Just as an example.

*Tags: Strategies*

---

**liam** - *08:51:19*

have you seen flumine?

*Tags: General Technical*

---

**Unknown** - *08:52:45*

yeah it is at the moment due to it being in constant development, this is an example strategy of how it would work

*Tags: Strategies*

---

**mandelbot** - *08:56:44*

thanks, that helps

*Tags: General Technical*

---

**liam** - *09:05:31*

place_order will call validate_order first which will check that the market/runner/strategy is not already 'live'

*Tags: Deployment, Strategies*

---

**liam** - *09:06:08*

using strategy.runner_context, all of which can be overridden or adapted if you want the ability to handle it yourself

*Tags: Strategies*

---

**bogdan** - *14:32:22*

Hello, Can anyone provide me with an example of how to get the account details as 'balance'

*Tags: General Technical*

---

**bogdan** - *14:32:40*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/accountresources.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/accountresources.py)

*Tags: General Technical*

---

**bogdan** - *14:33:09*

I think I have to use something from this source, but I don't figure out how to call the function

*Tags: General Technical*

---

**Lee** - *19:46:46*

I’m just looking for the races with volume and put my strategy against them purely for testing. Figured i can backtest all I like but that doesn’t validate my actual order part of the code

*Tags: Strategies*

---

**jhaa** - *21:05:39*

i vaguely remember asking a similar question a whlie ago and receiving an answer suggesting that I should not use python if I worry about milliseconds

*Tags: General Technical*

---

**klozovin** - *21:10:54*

I vaguely remember Betfair being AWS UK hosted... I get decent pings from AWS and Linode London

*Tags: Deployment*

---

**klozovin** - *21:11:37*

was thinking of spinning up few dozens of AWS instances to see how low does it get... maybe hit a jackpot sometimes :slightly_smiling_face:

*Tags: Deployment*

---

**Jonjonjon** - *22:59:37*

How can we filter out nsw racing using the bflw?

*Tags: General Technical*

---

## 2020-04-17

**Amanda** - *15:39:55*

Are you sure you are streaming using the events and not going round in a loop each time.

*Tags: General Technical*

---

**jhaa** - *17:27:23*

is what I did somewhere to debug at some point

*Tags: Errors Debugging*

---

## 2020-04-18

**fjt1973** - *13:50:06*

While learning my way around I have betfair up at the same time.... it just to makes it easier to compare the output in Python :slightly_smiling_face:

*Tags: General Technical*

---

**fjt1973** - *13:59:04*

As I said a newbie to Python which is becoming used more in my job. So what a better way to learn than doing something you have an interest in. This great library Liam :slightly_smiling_face:

*Tags: General Technical*

---

**liam** - *16:43:05*

I assume your problem is that the order stream does not include any orders that have already been cancelled or matched when you start? Or have I misunderstood 

*Tags: General Technical*

---

**jhaa** - *16:49:39*

```order_filter = streaming_order_filter(include_overall_position=False)```



*Tags: General Technical*

---

**liam** - *16:55:12*

Set logging to debug so you can see all the updates coming through 

*Tags: Errors Debugging*

---

**liam** - *16:57:22*

Debug logs will tell you 

*Tags: Errors Debugging*

---

**Lee** - *18:19:46*

Hi Liam, i was just looking through the dev branch for flumine and was curious what would be some examples of the trading_controls

*Tags: Strategies*

---

## 2020-04-19

**Amanda** - *11:35:47*

Question about the 1000 bets an hour. It doesn't apply to PC payers, is that correct? I never knew about it, is there a charge if you go over it? Do they tell you?

*Tags: General Technical*

---

## 2020-04-21

**Lee** - *15:56:38*

Is there a way currently for me to implement the SimulatedExecution methods in flumine? I’ve installed from the dev branch, sorry for being a bit too keen

*Tags: Getting Started*

---

**Lee** - *15:59:48*

was looking in to how to add the logic for when a order might be matched/filled in a backtest

*Tags: General Technical*

---

**Lee** - *16:42:38*

I would offer help or contributions but not sure i’d be much use currently

*Tags: General Technical*

---

**liam** - *16:45:32*

yeah its certainly not ready for production, the main parts missing are the controls and order.status, once the latter is implemented simulated execution can start to use it

*Tags: Deployment*

---

**mandelbot** - *16:50:34*

Sorry for the dumb question, but what's the diff between flumine and bflw?

*Tags: General Technical*

---

**liam** - *16:51:08*

bflw is a wrapper to the api, flumine is a trading framework (that uses bflw)

*Tags: Strategies*

---

**mandelbot** - *16:52:38*

so flumine is a wrapper for bflw?

*Tags: General Technical*

---

**liam** - *20:31:13*

Lightweight is giving you the raw json from the api, it’s common for the api not to return data for some vars whereas non lightweight will give you a python object regardless 

*Tags: General Technical*

---

## 2020-04-22

**Chris** - *20:17:08*

I'm testing my model on US since GB/IRE isnt available

*Tags: Strategies*

---

**user34** - *23:00:56*

I have just started working with the historical data from Betfair and want to extract the market books at a given time before the event. Does anyone know what functions I should be using for this?

*Tags: Data Quality*

---

## 2020-04-23

**Mo** - *07:44:16*

Have you seen this example? [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**liam** - *08:13:15*

Yeah that seems common, tempting to just cross sell flumine and push people that way

*Tags: General Technical*

---

**Mo** - *08:19:11*

I have no objections to the principle of pushing people towards flumine - I've been following the developments and think it sounds awesome - but I think it should be done as "Here's how you'd do it yourself BUT you should just use flumine" rather than "You should just use flumine"

*Tags: General Technical*

---

**liam** - *09:15:47*

that example now improved with more examples and a lot less boilerplate code [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**Newbie99** - *23:47:35*

I just updated BFLW and now it hangs at:



`INFO:betfairlightweight.streaming.stream:[Stream: 1]: 13 oc added`



```order_stream.start(async_=True)```



*Tags: General Technical*

---

## 2020-04-24

**Jonatan (skyw)** - *00:19:42*

I think async is removed! I also think there are examples on how to do it!

*Tags: General Technical*

---

**liam** - *08:38:17*

:slightly_smiling_face: want to push people in handling the errors or the thread will just die with the user non the wiser

*Tags: Errors Debugging*

---

**Lee** - *12:33:40*

Hi Liam, was just wondering how you was progressing with the simulated execution stuff for flumine?

*Tags: General Technical*

---

## 2020-04-28

**Lee** - *15:53:27*

Assume you’re using betfairlightweight, could you run tests without logging in?

*Tags: General Technical*

---

**Mo** - *16:01:13*

I had the same problem, in the end I just had CircleCI run the unit tests and I run the integration tests locally

*Tags: General Technical*

---

## 2020-04-29

**Cagdas Yetkin** - *09:45:03*

Hello all,



what is the quickest method in `betfairlightweight` to access *in-the-game live odds* for the current live games, for example today Belarus soccer games. I would like to test my application in Belarus soccer games. Any direction could help. Thank you!

*Tags: Deployment*

---

**Mo** - *09:46:14*

Start with this one: [https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**Cagdas Yetkin** - *09:47:19*

Yes I did the totorial once. I could place some bets using that. It was all in `pandas` . I am only reading and writing json files so i will try to avoid `pandas` for the time being. And thanks for the link, I am going there now.

*Tags: Feature Engineering*

---

**Cagdas Yetkin** - *10:19:35*

```"statusCode":"FAILURE",

"errorCode":"NOT_AUTHORIZED",

"errorMessage":"AppKey is not configured for service"```

when I do a test run using the link you gave above, I am getting this kind of error. what to do in this case? is it a paid service?

*Tags: Errors Debugging*

---

## 2020-04-30

**Cagdas Yetkin** - *10:17:07*

[@UBS7QANF3](@UBS7QANF3) I am trying to learn how to setup the stream and get the live odds. I have the delayed streaming service open now.



If I want to convert this sample tutorial into Belarus soccer live odds streaming then should I change the this way? Let’s say today at 12:00 there is a game between *Smolevichi and Energetik* in Belarus League.



```

market_filter = streaming_market_filter(

    event_type_ids=["1"], country_codes=["BLR"], market_types=["MATCH_ODDS"]

)```

thanks for showing some directions

*Tags: Getting Started, Deployment*

---

**Cagdas Yetkin** - *11:06:47*

Thanks for your patience also :slightly_smiling_face: Now I am streaming in some json and i can work! I will do my tests with e-sports i guess.

*Tags: General Technical*

---

## 2020-05-01

**Keith Davey** - *12:45:49*

Hi, I am not great a python, but I got logged in and can query the markets and so on, thanks for the excellent work on this.

*Tags: General Technical*

---

**Keith Davey** - *12:46:10*

I am wondering though, can this be used in an AWS Lambda?

*Tags: Deployment*

---

**liam** - *12:46:41*

yep I have a lot of sls functions that use bflw

*Tags: General Technical*

---

**Jonjonjon** - *21:24:18*

I'm looking at 1.166947655, Jan 4th, which is a free download from Betfair's historical data service

*Tags: Data Quality*

---

**Ruben** - *22:42:37*

```# Create stream

stream = trading.streaming.create_stream(listener=listener)

market_filter = bflw.filters.streaming_market_filter(event_type_ids=["1"], market_types=["MATCH_ODDS"])

market_data_filter = bflw.filters.streaming_market_data_filter(fields=["EX_BEST_OFFERS"], ladder_levels=3)



# subscribe

streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=1000,  # send update every 1000ms

)



t = threading.Thread(target=stream.start, daemon=True)

t.start()



# check for updates in output queue

while True:

    market_books = output_queue.get()



    for market_book in market_books:

        print(

            market_book,

            market_book.streaming_unique_id,  # unique id of stream (returned from subscribe request)

            market_book.streaming_update,  # json update received

            market_book.market_definition,  # streaming definition, similar to catalogue request

            market_book.publish_time,  # betfair publish time of update

        )```

Hi guys, I'm following the streaming tutorial and I am trying to adapt the example code to retrieve the best available lay/back prices, for all soccer games. I would expect this to work, however, market_books seems to be always empty

*Tags: Strategies*

---

**Ruben** - *22:42:49*

I would appreciate any help, thanks a lot!!

*Tags: General Technical*

---

## 2020-05-02

**Unknown** - *07:40:24*

if you include logging (`logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))`) the error tells you the problem, add `EX_MARKET_DEF` to the filter

*Tags: Errors Debugging*

---

**Ruben** - *07:56:33*

that's definitely helpful, thanks a lot [@U4H19D1D2](@U4H19D1D2)

*Tags: General Technical*

---

**Ruben** - *16:28:44*

well its really nice to hear some of the bflw users are profitable!!

*Tags: General Technical*

---

**liam** - *17:35:46*

Come to the next bflw meet-up and there should be at least one, George was going to come to the last but sacked us off for PL football in a box...

*Tags: General Technical*

---

## 2020-05-04

**MacrcoPolo** - *19:02:54*

Quick question, where should I place a vps?

*Tags: General Technical*

---

**MacrcoPolo** - *19:27:01*

Latency

*Tags: Performance*

---

**MacrcoPolo** - *19:27:12*

Solely latency

*Tags: Performance*

---

**PeterLe** - *20:43:37*

How long ago did they start routing via Prolexic mo? Can you recall? I mentioned to Liam the other day; Connection time to betfair for me is currently average 22.98ms (AWS T2 Micro - London), whereas my Home BT Superfast Fibre is 18.83ms. Other than the fact that AWS provides more resilience/fault tolerant etc, is it worth having a VPS these days? (I have screen shot from 2016(?)  and AWS was 12.02ms

*Tags: Deployment*

---

## 2020-05-05

**John Sheppard** - *20:05:20*

...Hello I new here, I came from the BA thread also... not a python guy, but the last post enticed me...

*Tags: General Technical*

---

## 2020-05-06

**liam** - *07:07:22*

Welcome, many (including myself) came from BA or gruss but moved after wanting to do more. For me it was the challenge (can I write this strategy in my own code) as well as the ability to collect my own data and complete flexibility that coding gives you.



However it’s worth noting that it’s a steep learning curve if you are new to programming or don’t have something already ‘working’ using off the shelf software.

*Tags: Getting Started, Strategies*

---

**Bloubliu** - *11:22:26*

Hello all, thank you Liam for the amazing package ! Noobie question : when I am placing multiple orders (without using streaming) on football matches, it appears that I have to wait the 5-7 sec bet delay before moving on to the next order. Apparently the betfair place_order API has an "async" parameter in it to avoid this, but I can't get it to work using the bflw package. Am I missing something basic ?

*Tags: General Technical*

---

**liam** - *11:25:04*

```trading.betting.place_orders(.. async_=True)```

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L483](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/betting.py#L483)

*Tags: Strategies*

---

## 2020-05-07

**Greg** - *07:47:52*

Thanks liam. Off the shelf stuff...yes, I had quite a few things working on horses/tennis  for weeks at a time then they would suddenly turn which made me wonder what was going on. Too many unknowns as possible reasons but when you can't see the code you can't eliminate technical reasons.I guess that answers my question. I have worked in IT for a long time and I'll consider learning python but at nearly 50 I might be too late.Anyway thanks again for the  reply.

*Tags: General Technical*

---

**D C** - *12:30:15*

Is anyone getting errors in market status over the stream API for Hanover racing today? I want to rule out an issue in my code. No other locations affected, just Hanover reporting ALL races suspended. They are not suspended and this is confirmed through site login and API-NG. Anyone else seeing this too?

*Tags: Errors Debugging*

---

**D C** - *12:59:01*

5 minutes ago. NG says OPEN, stream reports OPEN on first image, the suspended immediately afterwards. Thats verified stepping through debugger

*Tags: Errors Debugging*

---

## 2020-05-08

**Nimo22** - *09:20:18*

Hi. New to python and API but very keen to learn. I understand the coding but I struggle with how do you actually use it once written? Do I need a Betfair API tool and somehow add the python code to it? I had found a link to a wordpress tutorial from Liam to help use bflw with Betfair api which is probably what I need but sadly that website is no longer. Does someone has something to point me in the right direction on the basics? I am yet to purchase a API key but want to test and fully understand first. Last thing I want is spam this group with silly questions, if somebody is willing to help a newbee to understand the basics please give me a shout! Thank you

*Tags: Getting Started*

---

**Mo** - *09:21:31*

How new? Do you have Python installed?

*Tags: Getting Started*

---

**Nimo22** - *09:24:34*

Yes installed running with Conda and all required libraries along with bflw installed also

*Tags: Getting Started*

---

**Mo** - *09:26:29*

OK great so I would suggest you start by looking at the bflw examples: [https://github.com/liampauling/betfair/tree/master/examples](https://github.com/liampauling/betfair/tree/master/examples)



[@U4H19D1D2](@U4H19D1D2) can get your application key activated so no need for you to purchase one.

*Tags: General Technical*

---

**Mo** - *09:35:22*

Is that documentation new?

*Tags: General Technical*

---

**Nimo22** - *09:41:01*

Thanks to both of you! So If I understand well need to be working 100% in python only and I wasn’t missing anything. This is impressive! Thanks again :+1:

*Tags: General Technical*

---

**jhaa** - *13:01:16*

I have 'latency high' warnings - all the way up to 5 seconds for a while.  Then they stop for a while and then they come back. The difference between 'published time' and my local time is always below one second though.



What is likely happening here?

*Tags: Performance*

---

**bogdan** - *14:57:26*

thoroughbreds_event_filter = betfairlightweight.filters.market_filter(

    event_type_ids=[7],

    market_countries=['SWE'],

    market_start_time={

        'to': (datetime.datetime.utcnow() + datetime.timedelta(days=1)).strftime("%Y-%m-%dT%TZ")

    }

)

*Tags: General Technical*

---

**bogdan** - *16:34:49*

Thank you for your help!

*Tags: General Technical*

---

**Edoardo** - *17:14:47*

Hello. I'm new and I would like to download some Betfair historical data about soccer. The problem is that I'm from Italy and the operator told me that it is possible to download those data only from UK and Malta. Is there any workaround? Thanks in advance

*Tags: Data Quality*

---

**Mo** - *17:16:32*

Is it dependent on your registered address or your current location? If the latter then you could try via VPN. [@U92CASP1B](@U92CASP1B) I think you have the same problem? Any insight?

*Tags: General Technical*

---

## 2020-05-11

**Oliver Varney** - *09:55:05*

ah okay. Is there specific fields in each of the APIs to look out for. Mainly be looking at the orders stream / market data streaming.

*Tags: General Technical*

---

**Mo** - *09:57:44*

Streaming messages do have a status field but I’m not sure how useful it is

*Tags: General Technical*

---

**liam** - *09:58:29*

yeah you can get a 503, which is basically everything is fucked and we will send you updates when its fixed, bflw handles this

*Tags: Errors Debugging*

---

**liam** - *09:58:56*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/listener.py#L184](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/listener.py#L184)

*Tags: General Technical*

---

**Oliver Varney** - *10:03:30*

Cool, might be useful although probably only any good if the connection issue is your side and not just a general betfair API issue

*Tags: General Technical*

---

**Ian** - *15:55:16*

Hi all, does anyone have any tips for the best way to begin to visualise the market data recorded with flumine - i am planning on following something like this to structure the data in a way I can then feed a plotting library - any comments greatly appreciated.

*Tags: General Technical*

---

**Oliver Varney** - *16:55:08*

I think it mainly comes down to strategy bank size as youve said. just wondered as some of the strategies are not designed to be exposed at racetime

*Tags: Strategies*

---

**James T** - *17:02:50*

Yeah, any strategy which has to close before the off to be profitable is always going to run the risk of exchange crashes. Presumably profit from when it’s working has to outweigh the risk that’s taken. 

*Tags: Strategies*

---

**liam** - *17:09:28*

FYI flumine dev branch can now handle place/cancel simulated execution, added an example strategy, lots likely to change / beware of bugs

*Tags: Errors Debugging, Strategies*

---

**Lee** - *17:20:47*

I'll just be backtesting tonight and will test on live data wed/thursday

*Tags: Deployment*

---

## 2020-05-12

**Jonjonjon** - *08:54:59*

Is that from the example inplay strategy?

*Tags: Strategies*

---

**Lee** - *09:05:30*

Is there a general rule of thumb how much backtesting I should do before testing a strategy on live markets?

*Tags: Deployment, Strategies*

---

**liam** - *09:09:34*

flumine does that + latency

*Tags: Performance*

---

**liam** - *09:10:57*

for me, if I have a strategy that is profitable over a day or two I chuck it straight in and see what happens, leave it for a week or two and then look at the profit vs any variables it used

*Tags: Strategies*

---

**Lee** - *09:13:40*

would you post any data from the blotter to another service to help review what happened, or just use the logs and account statement history?

*Tags: General Technical*

---

**liam** - *09:14:43*

yeah so there is a missing part of flumine which is the logging control, this will output market/trade/order/cleared orders/cleared markets etc This can then be stored in a csv or sent to an API/db for analysis

*Tags: General Technical*

---

**deactivateduser** - *09:27:21*

Hello, i am new in developing. i am trying to create an website using betfair api. can somebody help for how to use betfairlightweight. and i want to know how to create templates over backend. i am stuck at the very first step of "you can use library as follows". where should i write these codes ? import betfairlightweight ?

*Tags: General Technical*

---

**deactivateduser** - *09:29:10*

if anyone suggest me good book or article about this. i'll be very thankful. i am electrical engineer but have interest in learning python

*Tags: General Technical*

---

**Mo** - *09:30:44*

To be clear, you would run these commands in Python

*Tags: General Technical*

---

**Mo** - *09:33:02*

No, that's what I mean when I say you run these commands in Python, as in you enter them into the Python interpreter

*Tags: General Technical*

---

**Mo** - *09:34:50*

If you have questions just ask, we are here to help

*Tags: General Technical*

---

**deactivateduser** - *13:21:38*

Hey guys I am solo in making a betting website. and i am not experienced like you guys. can someone tell me if is there any way to make python backend using betfair api without writing codes your self ? and if possible front end also. where i can just put my information app key username site name and all and rest is already done ?

*Tags: Strategies*

---

**deactivateduser** - *13:26:28*

is there any article i can use to make backend from python and frontend also

*Tags: General Technical*

---

**deactivateduser** - *13:30:57*

thanks Mo you are always helpful

*Tags: General Technical*

---

**Mo** - *13:31:34*

No problem

*Tags: General Technical*

---

**deactivateduser** - *13:31:57*

one more question what's the difference between exchange api and vendor api. if i want to make my own website my own betting system. what should i go for ?

*Tags: Strategies*

---

**deactivateduser** - *13:46:06*

it will be grateful of you. i am electrical engineer and i want to learn and grow in python .

*Tags: General Technical*

---

**deactivateduser** - *14:17:25*

this is a website in india by a friend of mine. can you check if he is using betfair api for data or not

*Tags: General Technical*

---

**deactivateduser** - *14:29:58*

well that's okay if you don't want to tell me i can understand. :sweat_smile: thank you for everything Mo you helped me a lot.

*Tags: General Technical*

---

**deactivateduser** - *14:35:59*

i told you i am just a friend who is helping another friend like you are helping me. the owner is of South america not me. he is not intrested in codding and all so he asked me to help because i have a little knowledge and i am also a bookmaker so.. not that i have this much knowledge like you guys. you guys are experienced :sweat_smile:

*Tags: General Technical*

---

**klozovin** - *14:43:51*

Do people use multithreading with Python/blfw or is it just one big loop of _read socket, update data, update strategies_ in a single thread? Just wondering what are some best practices and how people organize stuff...

*Tags: General Technical*

---

**liam** - *14:51:06*

playing with fire if you handle in the same loop, you need to handle errors from the stream and you would be blocking so risk the queue filling up, do you use the err streaming example? [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

*Tags: Errors Debugging*

---

**klozovin** - *15:05:45*

Haven't used streaming in _blfw_ that much, mostly using it to issue regular requests, but have a separate streaming library I'm messing around with...



Blocking was the reason I was asking, but does it help in Python since it has the GIL?

*Tags: General Technical*

---

**klozovin** - *15:06:14*

I'm not that familiar with threading in Python, but AFAIK due to GIL it's only one thread running at the time...

*Tags: General Technical*

---

**klozovin** - *15:40:46*

although Python usually only lets one thread to run due to GIL, in this case threading should help because when doing a network request the runtime can release the GIL...

*Tags: General Technical*

---

**liam** - *15:43:29*

but tbh you really don't need to worry about the GIL unless you are doing really intensive stuff in which case you are probably using the wrong language :wink: bflw isn't async *yet*

*Tags: General Technical*

---

**klozovin** - *15:45:32*

yeah, I'm not that worried about GIL for performance sake, mostly for understanding how things actually work. I'm working on a Rust client for Betfair so I'm comparing how are things done...

*Tags: Performance*

---

**Lee** - *19:51:36*

is it this line thats blocking? [https://github.com/liampauling/flumine/blob/dev/flumine/execution/baseexecution.py#L95](https://github.com/liampauling/flumine/blob/dev/flumine/execution/baseexecution.py#L95)

*Tags: General Technical*

---

**liam** - *19:58:49*

If it is can try changing wait to False [https://docs.python.org/3/library/concurrent.futures.html|https://docs.python.org/3/library/concurrent.futures.html](https://docs.python.org/3/library/concurrent.futures.html|https://docs.python.org/3/library/concurrent.futures.html)

*Tags: General Technical*

---

**Jonjonjon** - *20:21:44*

[@U4H19D1D2](@U4H19D1D2) Could we update HistoricalGeneratorStream to be able to decompress bz2 files on the fly? It's not a big change:



```    def _read_loop(self):



        self._running = True



        with _open(self.directory) as f:

            for update in f:

                if self.listener.on_data(update) is False:

                    # if on_data returns an error stop the stream and raise error

                    self.stop()

                    raise ListenerError("HISTORICAL", update)

                if not self._running:

                    break

                else:

                    yield self.listener.snap()

            else:

                # if f has finished, also stop the stream

                self.stop()





def _open(filename):

    if filename.endswith('.bz2'):

        [http://logger.info|logger.info](http://logger.info|logger.info)('Opening bz2 file: %s' % filename)

        return bz2.open(filename, 'rb')

    else:

        return open(filename, 'r')```



*Tags: Errors Debugging*

---

**Jonjonjon** - *20:24:12*

It's documented here: [https://docs.python.org/3/library/bz2.html](https://docs.python.org/3/library/bz2.html). Does that mean it's part of the standard library?

*Tags: General Technical*

---

**Jonjonjon** - *20:25:10*

[@UBS7QANF3](@UBS7QANF3) Why would you be against that? The historical data from betfair arrives in that format. So if we don't have it, users will need to decompress all the files anyway.

*Tags: Data Quality*

---

**Jonjonjon** - *20:35:30*

(flumine) jon@jon-VirtualBox:~/PycharmProjectsFlumine/flumine$ conda install smart_open

Collecting package metadata (repodata.json): done

Solving environment: done





==&gt; WARNING: A newer version of conda exists. &lt;==

  current version: 4.8.2

  latest version: 4.8.3



Please update conda by running



    $ conda update -n base -c defaults conda







## Package Plan ##



  environment location: /home/jon/anaconda3/envs/flumine



  added / updated specs:

    - smart_open





The following packages will be downloaded:



    package                    |            build

    ---------------------------|-----------------

    boto3-1.12.39              |             py_0          93 KB

    botocore-1.15.39           |             py_0         3.8 MB

    cachetools-3.1.1           |             py_0          14 KB

    cffi-1.14.0                |   py37he30daa8_1         226 KB

    cryptography-2.9.2         |   py37h1ba5d50_0         626 KB

    google-api-core-1.17.0     |           py37_0          89 KB

    google-auth-1.14.1         |             py_0          58 KB

    google-cloud-core-1.3.0    |             py_0          28 KB

    google-cloud-storage-1.28.0|             py_0          62 KB

    google-resumable-media-0.5.0|             py_1          34 KB

    googleapis-common-protos-1.51.0|           py37_2          69 KB

    idna-2.9                   |             py_1          56 KB

    jmespath-0.9.4             |             py_0          22 KB

    libprotobuf-3.11.4         |       hd408876_0         4.8 MB

    protobuf-3.11.4            |   py37he6710b0_0         711 KB

    pyasn1-0.4.8               |             py_0          58 KB

    pyasn1-modules-0.2.7       |             py_0          63 KB

    pycparser-2.20             |             py_0          93 KB

    requests-2.23.0            |           py37_0          91 KB

    rsa-4.0                    |             py_0          30 KB

    s3transfer-0.3.3           |           py37_0          95 KB

    smart_open-2.0.0           |             py_0          76 KB

    urllib3-1.25.8             |           py37_0         165 KB

    ------------------------------------------------------------

                                           Total:        11.3 MB



The following NEW packages will be INSTALLED:



  boto               pkgs/main/linux-64::boto-2.49.0-py37_0

  boto3              pkgs/main/noarch::boto3-1.12.39-py_0

  botocore           pkgs/main/noarch::botocore-1.15.39-py_0

  cachetools         pkgs/main/noarch::cachetools-3.1.1-py_0

  cffi               pkgs/main/linux-64::cffi-1.14.0-py37he30daa8_1

  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003

  cryptography       pkgs/main/linux-64::cryptography-2.9.2-py37h1ba5d50_0

  docutils           pkgs/main/linux-64::docutils-0.15.2-py37_0

  google-api-core    pkgs/main/linux-64::google-api-core-1.17.0-py37_0

  google-auth        pkgs/main/noarch::google-auth-1.14.1-py_0

  google-cloud-core  pkgs/main/noarch::google-cloud-core-1.3.0-py_0

  google-cloud-stor~ pkgs/main/noarch::google-cloud-storage-1.28.0-py_0

  google-resumable-~ pkgs/main/noarch::google-resumable-media-0.5.0-py_1

  googleapis-common~ pkgs/main/linux-64::googleapis-common-protos-1.51.0-py37_2

  idna               pkgs/main/noarch::idna-2.9-py_1

  jmespath           pkgs/main/noarch::jmespath-0.9.4-py_0

  libprotobuf        pkgs/main/linux-64::libprotobuf-3.11.4-hd408876_0

  protobuf           pkgs/main/linux-64::protobuf-3.11.4-py37he6710b0_0

  pyasn1             pkgs/main/noarch::pyasn1-0.4.8-py_0

  pyasn1-modules     pkgs/main/noarch::pyasn1-modules-0.2.7-py_0

  pycparser          pkgs/main/noarch::pycparser-2.20-py_0

  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0

  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0

  requests           pkgs/main/linux-64::requests-2.23.0-py37_0

  rsa                pkgs/main/noarch::rsa-4.0-py_0

  s3transfer         pkgs/main/linux-64::s3transfer-0.3.3-py37_0

  smart_open         pkgs/main/noarch::smart_open-2.0.0-py_0

  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0





Proceed ([y]/n)? y

*Tags: Getting Started*

---

**Lee** - *20:38:56*

you could do `pip install smart_open[aws]`

*Tags: Getting Started, Deployment*

---

**Jonjonjon** - *20:39:12*

[@UBS7QANF3](@UBS7QANF3) are you curently using the flumine backtester? If so, how do you use it with your multiple cores? I think it only uses a single core at the moment.

*Tags: General Technical*

---

**Mo** - *20:40:47*

No I’m not using flumine

*Tags: General Technical*

---

**Jonjonjon** - *20:44:26*

I can't see a loop. I just want to run the Liam's sample code over a whole folder like this example. but I guess I can just create a new "LowestLayer" for each market inside a separate process:



```folder = '/home/jon/bet_data/ADVANCED/2020/Jan/4/29638355'

markets = os.listdir(folder)

markets = [x for x in markets if x.endswith('.bz2')]

markets = [os.path.join(folder, x) for x in markets]



strategy = LowestLayer(

    market_filter={"markets": markets},

    max_order_exposure=1000,

    max_selection_exposure=10,

    context={"stake": 2},

)

framework.add_strategy(strategy)```



*Tags: Strategies*

---

**Unknown** - *21:31:09*

[@U4H19D1D2](@U4H19D1D2) I'm not sure whether or not you'll want the smart_open and pandas dependencies in there, but this runs on my computer. Takes about 35 seconds on running on 12 cores.

*Tags: Feature Engineering*

---

## 2020-05-13

**Mark** - *15:22:30*

Hello! Like many I came here after downloading some of the free Betfair Historical Data files and after discovering the multiline JSON format was tripping me up, discovered the bflw and flumine packages. Spent the rest of my life reading through the messages! Got to admit, it looks like a steep learning curve so I'm off now to fire up Sublime Text and progress beyond Hello World! :astonished:

*Tags: Data Quality*

---

**Jonjonjon** - *22:15:39*

Suppose I want to chart the best available to back/lay and traded prices for 1.166902450 (Warrenpoint v Crusaders, MATCH_ODDS). I have the free ADVANCED data from Betfair.



Is there an existing way to do this using bflw/flumine? Or would I need to subclass BaseStrategy, and make something similar to MarketRecorder, but that gives me what I want?

*Tags: Strategies*

---

## 2020-05-14

**liam** - *07:50:31*

[@UPMUFSGCR](@UPMUFSGCR) see here for an example [https://github.com/liampauling/flumine/pull/161](https://github.com/liampauling/flumine/pull/161)

*Tags: General Technical*

---

**Vincent Mele** - *10:13:37*

Anyone have any tips on how best to continuously record all horse and greyhound races (with a production API key)? I've modified [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py) with a custom handler to record incoming raw_data (and found flumine only after) with success, but I have a couple questions:

1. Is it correct that new markets are not added to existing streams? e.g., once I set market_filter to "7" and start the thread, it will add all existing horse markets but _not_ add tomorrows' new markets when those markets are published?

2. Because of the 200 market stream subscription limit, what's the best way to manage subscribing to more than 200 automatically? I don't see the logic in flumine and was curious if there was any other source-available methods (otherwise I'll take a stab at it myself of querying for all markets and then building different subscriptions).

*Tags: Deployment*

---

**brightcake** - *12:42:00*

Does the Flumine example market listener require AWS?

*Tags: Deployment*

---

**liam** - *12:45:08*

[@U01093Z1KF0](@U01093Z1KF0) are you looking at the latest version? Moved to a strategy framework rather than market listeners 

*Tags: Strategies*

---

**brightcake** - *12:45:42*

[https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py) was looking at this

*Tags: General Technical*

---

**brightcake** - *12:45:50*

sorry i meant market recorder

*Tags: Data Quality*

---

**brightcake** - *12:49:26*

I'm guessing the S3 is meant for AWS?

*Tags: Deployment*

---

**Jonjonjon** - *23:25:01*

Does anyone do anything to remove this noise from their analyis? I'm trying to plot traded volumes over time, and they are making the charts look unrealistic.



[@U4H19D1D2](@U4H19D1D2) does the Flumine backtester do anything to avoid these?

*Tags: General Technical*

---

## 2020-05-15

**Jonjonjon** - *08:58:13*

This is my hacked up version of your PriceRecorder:



```from collections import defaultdict



from flumine import BaseStrategy

from flumine.utils import get_price



EPS=1e-6



def diff(a: dict, b: dict):

    """

    Return a minus b



    :param a:

    :param b:

    :return:

    """



    result = {}

    for k, v in a.items():

        difference = v - b.get(k, 0.0)

        if abs(difference)&gt;EPS:

            result[k] = difference



    return result





class PriceRecorder(BaseStrategy):

    """

    Example strategy for recording prices

    from historical or live data.

    """



    def __init__(self, *args, **kwargs):

        BaseStrategy.__init__(self, *args, **kwargs)

        self.market_data = defaultdict(list)

        self.runner_data = defaultdict(lambda: defaultdict(list))

        self.runner_names = defaultdict(dict)

        self._prior_traded_volume = defaultdict(dict)

        self._prior_publish_time = {}

        self.traded_volumes = defaultdict(lambda: defaultdict(list))



    def check_market_book(self, live_market, market_book):

        return market_book.inplay==False



    def process_market_book(self, live_market, market_book):



        market_data = self.market_data[market_book.market_id]

        prior_publish_time = self._prior_publish_time.get(market_book.market_id)

        self._prior_publish_time[market_book.market_id] = market_book.publish_time



        if not market_book.market_id in self.runner_names:

            runners = market_book.market_definition.runners

            d = {runner.selection_id:runner.name for runner in runners}

            self.runner_names = d



        is_currency_update = False



        for runner in market_book.runners:

            runner_data = self.runner_data[market_book.market_id][runner.selection_id]



            k = (market_book.market_id, runner.selection_id)

            prior_traded_volume = self._prior_traded_volume[k]

            traded_volume = {ps['price']:ps['size'] for ps in runner.ex.traded_volume}

            traded_volume_diff = diff(traded_volume, prior_traded_volume)

            self._prior_traded_volume[k] = traded_volume

            if traded_volume_diff:

                min_price = min(traded_volume_diff)

                max_price = max(traded_volume_diff)

                volume = sum(traded_volume_diff.values())



                if prior_publish_time.hour!=market_book.publish_time.hour:

                    if len(traded_volume_diff)&gt;1:

                        is_currency_update = True

                        print('Skipping ', market_book.publish_time)



                if min(traded_volume_diff.values())&lt;0.0:

                    is_currency_update = True



                # if market_book.publish_time&gt;=datetime.datetime(2020, 1, 1):

                #     if max_price==4:

                #         print('stpo')

            else:

                min_price = None

                max_price = None

                volume = 0



            is_currency_update=False



            if not is_currency_update:

                atb = get_price(runner.ex.available_to_back, 0)

                atl = get_price(runner.ex.available_to_lay, 0)



                if max_price and (max_price&gt;atl):

                    print('depth was hit')



                runner_data.append(

                    [

                        market_book.publish_time,

                        runner.last_price_traded,

                        atb,

                        atl,

                        max_price,

                        min_price,

                        volume

                        ]

                )



        market_data.append([

            market_book.market_id,

            market_book.publish_time,

            market_book.status,

            market_book.inplay,

        ])```

*Tags: Deployment, Strategies*

---

## 2020-05-18

**Will Morrison** - *04:53:12*

Hi everyone! I'm new to the betfairlightweight package. I already analyzed Betfair historical data using R, parsing the files with a json package and eventually creating rows with things like runner name, batb, batl, etc. I have got the examplestreaming script to generate data for the markets that I am interested in, and if I save market_book.streaming_update as a .txt and then substitute a few things, I can parse it as a json file, but it is missing some of the higher level data that I need, like the runner ids and runner names. When I click around the market_book (object?) in my variable explorer in Spyder, I can see that it looks like all the stuff I need is probably already there. Are there some examples of parsing this data into usable tables, or is there a way that I can retrieve the data to look more like the historical bz2 files that I am used to now? Thanks a bunch for any help, it's super cool to see that there's a community around this!

*Tags: Getting Started, Data Quality*

---

**Mo** - *07:02:55*

Personally I think you should go in the other direction - use betfairlightweight to parse the historic data files and it will generate market books for you 

*Tags: General Technical*

---

**liam** - *07:42:20*

Agreed, or if you want a raw dict you can run streaming in [https://liampauling.github.io/betfair/advanced/#lightweight|lightweight mode](https://liampauling.github.io/betfair/advanced/#lightweight|lightweight mode) by

```listener = StreamListener(max_latency=None, lightweight=True)```

*Tags: Performance*

---

**liam** - *07:43:35*

Have you seen [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py|this](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py|this) example?

*Tags: General Technical*

---

**Will Morrison** - *07:44:49*

Hi Mo! If I had known about betfairlightweight before, I definitely would have done that. Ultimately, I want to use betfairlightweight to place backs and lays according to some analysis, but however I do it, I need to get the data out of the market book object into tabular form. Is there any kind of documentation for working with the market book objects?

*Tags: General Technical*

---

**MrBIN** - *07:49:46*

Hi All. Do you know where is Betfair's matching engine server is located? and what is the best AWS datacenter \ availability zone in terms of latency?

*Tags: Performance, Deployment*

---

**Will Morrison** - *07:49:52*

I hadn't looked at the historicalstreaming example because I assumed it was basically the same as the live streaming one but on historical data. Now I see that it has some writing examples which may be exactly what I need to get started. Thanks Liam!

*Tags: Data Quality, Deployment*

---

**MrBIN** - *07:52:59*

So do you think that the best is to be hosted in the AWS London Data Center?

*Tags: Deployment*

---

**Mo** - *07:53:41*

This is the official documentation: [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-MarketBook](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-MarketBook)



Probably best just to look at the betfairlightweight source and/or click around the objects in Spyder.



[@U4H19D1D2](@U4H19D1D2) - any examples that show getting the best prices for each runner from a market book?

*Tags: Strategies*

---

**Mo** - *07:54:27*

I think it's very hard/pointless to be optimising network latency given this infrastructure

*Tags: Performance*

---

**brightcake** - *21:40:07*

quick q about flumine's backtester - does it only take betfair market data files as input?

*Tags: General Technical*

---

**Jonjonjon** - *22:25:00*

[@U4H19D1D2](@U4H19D1D2) have you had any further thoughts on this proposed change? In the mean time I have been patching builtins.open, but that is causing me problems elsewhere. Also, it's pretty ugly to be importing from unittest.mock in code that isn't for unit testing.

*Tags: General Technical*

---

## 2020-05-19

**liam** - *06:18:57*

[@U01093Z1KF0](@U01093Z1KF0) it takes streaming data yeah, what do you want to give it?

*Tags: General Technical*

---

**liam** - *09:09:49*

would never make pandas a dependency

*Tags: Feature Engineering*

---

**brightcake** - *09:11:31*

that's fair enough, I can always just create something that relates the dataframe to the relevant market streaming files

*Tags: Feature Engineering*

---

**liam** - *09:18:03*

the issue is that you lose the benefit of streaming, quick and small size (relatively)

*Tags: General Technical*

---

**liam** - *09:34:43*

+1, flumine is built on the premise that you just point it to your data source and the same strategies will work on live and historic data with no changes required from the user. As soon as you move from streaming data things get tricky and large

*Tags: Deployment*

---

**brightcake** - *09:36:36*

but having something relating a dataframe to the streaming files shouldn't take up too much extra time right?

*Tags: Feature Engineering*

---

**brightcake** - *09:37:05*

so grabbing the market ids from the df and then opening relating the relevant streaming files for example

*Tags: General Technical*

---

**brightcake** - *09:39:20*

so you keep those market files available on aws?

*Tags: Deployment*

---

**liam** - *09:41:06*

[https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py#L144](https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py#L144)

*Tags: General Technical*

---

**Jonjonjon** - *21:31:01*

I have cloned the repo. And put it into my own branch. But I get this error when trying to push my new branch:



fatal: Authentication failed for '[https://github.com/liampauling/flumine.git/](https://github.com/liampauling/flumine.git/)'



Am I supposed to fork the code, and push to that?

*Tags: Errors Debugging*

---

**Jonjonjon** - *22:08:05*

OK. I worked it out. I've sent the PRs for both Flumine and BFLW.

*Tags: General Technical*

---

## 2020-05-20

**liam** - *06:59:33*

Ah I see the other PR now, we wouldn’t change bflw but change flumine, it currently wants a directory but instead we want to pass a generator and then just loop it. If I understand it correctly your change in bflw just moves the gen creation up a level 

*Tags: General Technical*

---

**brightcake** - *11:50:43*

feel like this might be a silly question but what is the difference between get_price and the price attribute of runner.ex.available_to_lay[0]?

*Tags: General Technical*

---

**liam** - *11:52:47*

error handling, that list could be empty

*Tags: Errors Debugging*

---

**liam** - *11:55:37*

shouldn't, get_price will return None if price isn't available, can you debug?

*Tags: Errors Debugging*

---

**liam** - *12:05:08*

flumine patches it to speed streaming up

*Tags: Performance*

---

**Jonjonjon** - *12:51:25*

Thanks for looking at the PR. My change in bflw allows users to pass generators into



betfairlightweight.streaming.betfairstream.HistoricalStream .



Prior to the change, HistoricalStream expected a directory.



Looking through path of execution for the backtest.py example, if eventually gets to



flumine/streams/historicalstream.py



which creates the HistoricalGeneratorStream class (which is from betfairlightweight, and currently expects the directory, not generator.).



Is that the point at which you believe the change should be made?

*Tags: General Technical*

---

**liam** - *12:52:45*

Yep sorry I wasn’t very clear, passing a generator could probably completely remove the need for historicalstream.py in flumine as the generator could be appended to streams.Streams.streams and the just looped 

*Tags: General Technical*

---

**Unknown** - *13:05:55*

yes but would then need to handle this part because it will already be a generator, always prefer to have a variable as a single type. A question of maybe we remove the string handling completely

*Tags: General Technical*

---

## 2020-05-21

**Lee** - *08:01:19*

On flumine i'm getting some trades where average_price_matched is less than the order price for lay (when backtesting). I've layed at 2.86 and average price matched is 2.75. Is that expected behaviour and would betfair do a similar thing?

*Tags: General Technical*

---

**liam** - *08:02:29*

can you share either order.simulated in a debugger or

```order.simulated.__dict__```

*Tags: Errors Debugging*

---

**Jonjonjon** - *08:55:26*

I had a think last night. Handling of the generator in backtest.py would depend on whether or not iterating over stream_gen() is iterating over Market Books, or strings from the streaming data files. So I'm not sure how to implement it cleanly without uglifying your beautiful code.



At this point, I'm thinking that hard drive space is cheaper than developer time, especially when iterating over every file within a tar archive is slow (my computer averages 0.67 seconds to read a file from a tar archive). Also, using a tar does not save space, as it is the individual members of the tar that are zipped up.



Unzipping the soccer match odds files from Jan2020... The disk usage goes up from 472MB to 4.4GB.

*Tags: Performance*

---

**brightcake** - *09:31:51*

bit of context: i'm trying to deploy a data recorder on aws and want something to be taking the data of all football markets while it runs

*Tags: Deployment*

---

**Mo** - *09:32:09*

It depends on your use case as with streaming it may not be necessary to call list_market_catalogue at all.



But yes, if you do need to use list_market_catalogue you will need to repeatedly call list_market_catalogue to pick up new ones.

*Tags: General Technical*

---

**Mo** - *09:32:47*

You use an appropriate streaming filter and will pick up new markets automatically

*Tags: General Technical*

---

**Mo** - *09:33:08*

But it sounds like you should just be using flumine

*Tags: General Technical*

---

**fjt1973** - *12:37:23*

Morning all... today is a school day and looking at Flumine. QQ. 1 I've been using the PriceRecorder for a specific inplay market. Once I've run the command "framework.run()" how would you end that session gracefully once the market was closed. QQ.2 Once you have a subscription / stream running can you add additional strategies with framework.add_strategy? Just a note I'm running this in Jupyter Lab and the framework.run() cell is still processing until I interrupt / terminate the kernel.

*Tags: Strategies*

---

**liam** - *12:39:57*

So flumine is setup for scripting, i.e. you create a script like example.py and then run it on a server (cron/docker etc) It is not really suited for an IDE like jupyter.

However there is certainly some logic missing to end the run when all markets are closed because you have found it it just continues to run, welcome thoughts on this

*Tags: Getting Started, Deployment*

---

**fjt1973** - *12:48:57*

Thanks Liam, I've been using Jupyter to develop / test before moving into AWS. I've also noticed you're limited on connections available so am I right in thinking you can only run a max of 10? So being able to end a run would be needed before starting the 11th script?

*Tags: Deployment*

---

**fjt1973** - *12:58:03*

I'm also new to Python (6 months) and like the way you can go back and edit a previous cell in Jupyter without having to re-run the whole code again.

*Tags: Getting Started*

---

**fjt1973** - *13:04:35*

I've just answered my own question - it apparently doesn't

*Tags: General Technical*

---

**Jonatan (skyw)** - *20:41:55*

does anyone know if the reference documentation are available in another format then HTML?

*Tags: General Technical*

---

**Andrey Yunoshev** - *21:53:16*

Hi, guys. Nice framework, a great foundation for the future.



I'm just trying to analyze all the greyhound races in one day (and in the future I need a month).

The code is as simple as two pennies - but I never waited for it to finish.

Python eats up 29 gigabytes of memory and everything freezes :)



Is this expected at the moment or am I doing something wrong?



```class PastStrategy(BaseStrategy):

    def start(self) -&gt; None:

        None

    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:

        if market_book.status not in ["CLOSED", "SUSPENDED"]:

            town = market_book.market_definition.venue

            run_time = market.market_start_datetime.isoformat()

            key = "{0} at {1}".format(town, run_time)

            if town and key not in bf_runs_list:

                bf_runs_list.append(key)

                print("start processing new BF run: {0}, total {1} runs".format(key, len(bf_runs_list)))

            return True

        return False



    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:

        None

    def process_orders(self, market: Market, orders: list) -&gt; None:

        None



markets = sorted(list_files_in_folder("./data/bf_uncompress/PRO/2020/May/1", "*.*"))



strategy = PastStrategy(

    market_filter={ "markets": markets},

    max_order_exposure=50,

    max_selection_exposure=10,



)



client = clients.BacktestClient()



framework = FlumineBacktest(

    client=client,

)



framework.add_strategy(strategy)

framework.run()```

*Tags: Performance, Strategies*

---

**Lee** - *22:04:50*

Hi Andrey, yeah it’s a known issue. I have the same problem.

*Tags: General Technical*

---

**Lee** - *22:10:14*

Also appending to bf_runs_list for every market book probably doesn’t help the memory leak

*Tags: Performance*

---

## 2020-05-22

**liam** - *11:35:05*

[@U013R0E7SUV](@U013R0E7SUV) [@UUCD6P13J](@UUCD6P13J) it was a very simple fix! It was basically the gc trying to collect the outout_queue which was just filling up with events, one line fix :wink: [https://github.com/liampauling/flumine/pull/168](https://github.com/liampauling/flumine/pull/168)

*Tags: Errors Debugging*

---

**Lee** - *11:37:21*

how did you debug it, using something like cprofilev?

*Tags: Errors Debugging*

---

**liam** - *11:38:25*

[https://pypi.org/project/memory-profiler/](https://pypi.org/project/memory-profiler/) to get an idea of where the leak was as it was processing, then created just a single generator using bflw (no memory leak) so realised it was how it was implemented in flumine, then noticed the queue being initialised in the base class

*Tags: Performance*

---

**Peter** - *14:22:53*

My experience when I switched to streaming was that it was higher than 200 (which is the limit for the API). There does seem to be a limit in there, but I haven't pin-pointed it yet - and it's attached to your account so may differ from person-to-person. I've not requested an increase, so working from my default, I've seen streams start with upward of 400 markets, and fail with over 1,000.

*Tags: General Technical*

---

**Julio** - *17:52:21*

I am going to use this time at home to try to migrate my personal strategies to flumine.

My understanding is that:

• check_market_book: defines the set of conditions to look at a market (status, time to start, number of runner, venue etc...)

• process_market_book: looks at the prices and volumes, computes indicators and places trades.

• process_orders: checks the order executions (ex: cancellation after x seconds)

The part that I am missing is that how:

• my current orders / positions feed back to process_market_book (as orders not in inputs). For instance my indicator flags a trade, but i do not want to trade each time I process_market_book, I only want to trade if len(orders)=0 

• the maketBook feeds back to order. I might want to cancel my pending order if the price drift away to much from the current price.

I could do self.orders=orders and self.market_book=market_book, but flumine is an efficient way and I'd prefer not introduce memory leakage.

*Tags: Performance*

---

## 2020-05-23

**VT** - *00:18:28*

Hi, I'm in the final stage of developing my BOT and now I need to automate and I still don't know how to do that, it's nothing exceptional just cheap json feed for live statistics + previous performance database + a little scraping.



I don't want to leave my computer on 24 hours a day running the system. Any suggestions for (cheap) servers that work and have no problem with the IP? ( my account is from Brazil).

*Tags: Errors Debugging, Performance, Deployment*

---

**Mo** - *07:36:02*

I (and a lot of others) use Amazon Web Services (AWS). You won’t have a problem with IP as you can select the location to run your servers in. For example, London or Dublin

*Tags: Deployment*

---

**liam** - *13:57:47*

Unit testing then integration testing on a handful of markets (every release), once in production I verify backtesting returns the same profit on every run 

*Tags: Deployment*

---

**Lee** - *13:59:44*

backtesting the production image against local backtest results?

*Tags: Deployment*

---

**liam** - *13:59:47*

Helps me sleep at night 

*Tags: General Technical*

---

**liam** - *14:00:27*

So I have my settings for a strategy which I use for production, I run those exact settings over a set market and assert profit == x on every run 

*Tags: Deployment, Strategies*

---

**liam** - *18:30:42*

Everyone does it a bit differently, I read other people’s code a lot to get a good idea of how to structure (reading code is a great way to accelerate dev knowledge imho) 

*Tags: General Technical*

---

## 2020-05-24

**Sandy Caskie** - *11:27:03*

Has anyone used the following python packages to collect horse racing data: [https://pypi.org/user/justjasongreen/](https://pypi.org/user/justjasongreen/)? Is it useful? If you look at the racing data package it seems to contain some really useful information for creating predictive models. It uses something called MongoDB, I have no experience of this, does anyone else? I have been trying to get it working this morning without any luck.

*Tags: Strategies*

---

**Ruben** - *11:47:58*

does anyone know what the "BSP Auction" is? I keep seeing it mentioned in the documentation, but I have not seen much info about it online

*Tags: General Technical*

---

**brightcake** - *12:07:05*

Yah, i'm learning a fair bit by looking at how flumine and bflw are working under the hood.

*Tags: General Technical*

---

**Sandy Caskie** - *12:42:00*

[@UBS7QANF3](@UBS7QANF3) looking at this package when I call "date = datetime(2016, 12, 1); meets = provider.get_meets_by_date(date)". It returns an empty list. Therefore I am not sure this is maintained. Or it could be something to do with the MongoDB which I do not understand. Looking at MongoDB it is a database system. Though do you know if it is stored locally and set up by yourself or externally and I am merely calling the data? There is also scraper part of this package which makes me think it is stored locally.

*Tags: General Technical*

---

**Julio** - *12:53:22*

I use Heroku. It is easy to set up, free and you can choose your ip location. You def have less functionalities than AWS but I guess it is the trade off between easy/simple vs complex/bestinclass service.

*Tags: Deployment*

---

**Sandy Caskie** - *13:32:13*

[@UFPEU7URG](@UFPEU7URG) ye I was thinking along these lines. In terms of data collection have you used the Betfair timeform data. I had a quick look and it appears to have good data for predictive modeling. Here’s a link to it [https://developer.betfair.com/en/get-started/#timeform-api|https://developer.betfair.com/en/get-started/#timeform-api](https://developer.betfair.com/en/get-started/#timeform-api|https://developer.betfair.com/en/get-started/#timeform-api). Though the associated documentation is not available and it is also £500 per month.

*Tags: Strategies*

---

**Mo** - *13:34:48*

I’m going through the process of building out my horse racing database and I think Timeform is ridiculously expensive for what you can easily scrape from many sources

*Tags: General Technical*

---

**Sandy Caskie** - *15:10:14*

[@U4H19D1D2](@U4H19D1D2) is the app key from globalsportsapi used to interact with betfair? Or do globalsportsapi have their own system? I cannot see any documentation on how to interact with globalsports api.

*Tags: General Technical*

---

**Sandy Caskie** - *15:51:12*

Ok so I went on my account with Global Sports API and use the swagger editor to load data endpoints. This works fine but when I try to load the object names such as  "[https://www.globalsportsapi.com/ODATAv1/HorseRacingSample/performances](https://www.globalsportsapi.com/ODATAv1/HorseRacingSample/performances)?" (I removed my app key) I get an internal service error. Is this a problem at there end?

*Tags: Errors Debugging, Performance*

---

## 2020-05-25

**Ruben** - *07:05:52*

on that same topic, to those that have strategies in production, does it happen often that you see a price available that you want, and when you go take it, it is already gone?

*Tags: Deployment*

---

**liam** - *07:11:42*

I think I only have one strategy that actually takes, 90% of my orders are ‘passive’ before being taken or cancelled  

*Tags: Strategies*

---

**Mo** - *09:47:37*

Do you know what the bottleneck is? Constructing the Python objects/dicts?

*Tags: General Technical*

---

**Lee** - *17:15:14*

does anyone know what streaming_market_data_filter fields the betfair historic data (advanced or pro) includes?

*Tags: General Technical*

---

**Jonatan (skyw)** - *18:54:25*

[@ULDAVFDRP](@ULDAVFDRP)  I use numpy arrays instead of python builtins, not creating new objects just mutating them in place. No sorting use 0 to indicate not present.



For bdatb and batb/l

*Tags: General Technical*

---

**Oliver Varney** - *19:06:03*

ah okay so a numpy array 352x1

*Tags: General Technical*

---

**Oliver Varney** - *19:10:55*

You say you use numpy instead, does that mean you have created a custom cache.py file?

*Tags: General Technical*

---

**Jonatan (skyw)** - *19:13:28*

Yeah, so I am missing out on the flumine part!

*Tags: General Technical*

---

## 2020-05-26

**liam** - *06:22:13*

You want a limit on close order and using this filter add your minimum price [https://github.com/liampauling/betfair/blob/53ade9cb272491574bffead5d089ca430fcf6f83/betfairlightweight/filters.py#L270|https://github.com/liampauling/betfair/blob/53ade9cb272491574bffead5d089ca430fcf6f83/betfairlightweight/filters.py#L270](https://github.com/liampauling/betfair/blob/53ade9cb272491574bffead5d089ca430fcf6f83/betfairlightweight/filters.py#L270|https://github.com/liampauling/betfair/blob/53ade9cb272491574bffead5d089ca430fcf6f83/betfairlightweight/filters.py#L270)

*Tags: General Technical*

---

**Oliver Varney** - *08:46:43*

its really not a biggy. One thing I might look to play around is storing price volumes in numpy arrays for matrix maths but the file processing time was just a little experiment as I will have to rerun them through my processing / database soonish.

*Tags: General Technical*

---

**mandelbot** - *10:11:43*

It certainly won't help. Someone posted on the betangel forum. He/She got an email from betfair, why they didn't send to everyone I have no idea. Typical betfair inconsistency.

*Tags: General Technical*

---

## 2020-05-27

**liam** - *09:22:36*

both, not yet implemented but you could handle it in the strategy when processing orders. Typical use would be:



```trade = Trade(

    market_book.market_id, runner.selection_id, runner.handicap, self,

)

order = trade.create_order(

    side="LAY", order_type=LimitOrder(2, self.context["stake"]),

)

offset_order = trade.create_order(

    side="BACK", order_type=LimitOrder(10, self.context["stake"]),

)

trade.offset_orders.append(offset_order)

self.place_order(market, order)```



*Tags: Strategies*

---

**liam** - *09:23:18*

this is where it would be handled [https://github.com/liampauling/flumine/blob/master/flumine/order/process.py#L64](https://github.com/liampauling/flumine/blob/master/flumine/order/process.py#L64) i.e. the initial order has been matched and offsets now need to be placed

*Tags: General Technical*

---

**liam** - *09:31:03*

yeah that is the idea, I want to clean up the trade/strategy reset logic so its cleaner and easier to work out if you have live trades/pending orders etc. just haven't worked it out in my head yet hence why this hasn't been completed :slightly_smiling_face: I will look to put some warning logs going forward when something is being used which won't do anything

*Tags: Deployment, Strategies*

---

**Newbie99** - *09:48:41*

Out of curiosity is there any GPS integration with BFLW currently, or do those that use GPS run a concurrent, but separate stream (just starting out with the GPS and am curious how people have it set up)?

*Tags: General Technical*

---

**brightcake** - *15:53:54*

quick pycharm question: has anyone ever had 'waiting for REPL response' in pycharm? Seems to be really slowing down code that shouldn't be slow

*Tags: Performance*

---

**Oliver Varney** - *19:18:04*

Quick question, but do the runners remain in the same or in the returned market book through the life of the market? Is it safe to look for the same runner at the same index if your comparing market books?

*Tags: General Technical*

---

**Oliver Varney** - *19:54:01*

not sure how to phase the question well but say I have two market books, one from an 30 seconds ago and one from the latest returned market book. Will all of the runners be at the same index?

*Tags: General Technical*

---

**Lee** - *20:30:16*

I just got one of the exceptions for something which i think works as a good warning. NotImplementedError on execute_replace :slightly_smiling_face:

*Tags: Errors Debugging*

---

**Newbie99** - *21:25:46*

Using flumine, is there a way to only record the last x minutes of a market, or is the approach to record all, then remove what you don't need when processing?

*Tags: General Technical*

---

**liam** - *21:42:08*

The market recorder is setup to record the raw data so that won’t really work but you can modify the price recorder [https://github.com/liampauling/flumine/blob/master/examples/strategies/pricerecorder.py|example](https://github.com/liampauling/flumine/blob/master/examples/strategies/pricerecorder.py|example)



Just change the check_market_book



```if market.seconds_to_start &lt; 600:

    return True```

*Tags: Getting Started, Data Quality*

---

**Newbie99** - *21:45:09*

Ah cool, okay that could work, although now that I've asked the question, I'm thinking to myself there's no downside to recording the complete raw data and then simply using the last x mins, so probably a pointless question now I've asked it, ha!

*Tags: General Technical*

---

## 2020-05-28

**fjt1973** - *11:00:05*

Morning all, QQ when using the back testing within Flumine is this using MarketStream or Datastream Data? Or can you use both?

*Tags: General Technical*

---

**liam** - *11:01:04*

MarketStream, DataStream is raw streaming data so very tricky to make decisions off / doesn't work with any other backtesting logic

*Tags: General Technical*

---

**Derrick** - *16:19:19*

Dumb question but what does `marketCount` actually represent? The number `marketCount` shows doesn't seem to match up for anything for me so not sure if I should just ignore it. For example, it will be like `'marketCount': 2` but I can't seem to see what those 2 markets are. When I try to and get marketId from that I get way more than 2 so just confused on what `marketCount` is. I feel like I am overlooking something or caring about it too much when it may be not releavant to getting information.

*Tags: General Technical*

---

## 2020-05-29

**Mo** - *07:21:28*

The `marketCount` is the number of markets associated with that event and is correct. How are you getting those markets? Because they do not share that event ID. e.g.



```api_client.betting.list_market_catalogue(filter=betfairlightweight.filters.market_filter(market_ids=['1.160316133']), market_projection=['EVENT'], lightweight=True)

Out[10]: 

[{'marketId': '1.160316133',

  'marketName': 'Winner 2019/20',

  'totalMatched': 219.61,

  'event': {'id': '13932380',

   'name': 'Danish Superliga',

   'countryCode': 'DK',

   'timezone': 'Europe/London',

   'openDate': '2018-07-13T16:00:00.000Z'}}]```

*Tags: Strategies*

---

**jhaa** - *17:26:31*

I am getting an internal error because I had exposure moved to a new account. Not sure how relevant this is but maybe this should be caught or is of interest to anyone:



  File "/home/code/venv/lib/python3.6/site-packages/betfairlightweight/resources/bettingresources.py", line 505, in &lt;listcomp&gt;

    self.orders = [RunnerBookOrder(**i) for i in orders] if orders else []

TypeError: __init__() missing 1 required positional argument: 'placedDate'

*Tags: Errors Debugging, Strategies*

---

## 2020-05-30

**Mo** - *13:06:00*

No problem 

*Tags: General Technical*

---

## 2020-05-31

**Steve** - *03:35:16*

I've been having a bit of an issue with latency and I'm trying to figure out what benchmark latency I should be striving for. What is the optimal time I could expect for a strategy to be able to register a change in the orderbook, process the change and have a new order posted back into the orderbook? So essentially, if I'm responding to change in the BF orderbook what latency should I expect between that change being made and my new order being placed in the orderbook?

*Tags: Performance, Strategies*

---

## 2020-06-01

**Will Morrison** - *07:33:20*

Mo and liam, thanks a ton for the help last week, I super appreciate the package and you guys keeping an active community here! I haven't bought an active key yet, so when I run my modified version of the streaming example, I just get an update from the while loop market_books = output_queue.get() every 180 seconds. For now, I just put some loops underneath that to grab the data that I want from the market books and save it as a csv, but this feels like it can't be the right approach once I have a live key, since I would be copying a bunch of unchanged data constantly. I'm guessing that the way the data stream comes in is that it only sends updates to things that have changed, like the json lines in the historical data? Is there an example that would help me see how to record the stream in a smart way? And thanks in advance for your patience, I suspect that for a more experienced programmer this might be a trivial question to figure out.

*Tags: Data Quality, Deployment*

---

**liam** - *07:34:56*

Have you had a look at flumine? It does want you want in regards to recording streaming data [https://github.com/liampauling/flumine](https://github.com/liampauling/flumine)

*Tags: General Technical*

---

**liam** - *19:27:59*

Maintenance [https://forum.developer.betfair.com/forum/developer-program/announcements/31577-planned-maintenance-outage-tuesday-2nd-june-07-30-%E2%80%93-08-30-bst|tomorrow](https://forum.developer.betfair.com/forum/developer-program/announcements/31577-planned-maintenance-outage-tuesday-2nd-june-07-30-%E2%80%93-08-30-bst|tomorrow), 0730-0830 BST, usually a good excuse to test any error handling!

*Tags: Errors Debugging*

---

**JonJonJon** - *21:34:22*

In Flumine, is it possible to place multiple orders at once?

*Tags: General Technical*

---

## 2020-06-02

**liam** - *07:28:54*

You should get a 503 status which will be logged, bflw will then handle the updates once this stops, there should be no need to restart 

*Tags: General Technical*

---

**liam** - *08:00:56*

[https://github.com/liampauling/flumine/blob/master/HISTORY.rst#160-2020-06-02|flumine](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#160-2020-06-02|flumine) v1.6.0 released with handling added for simulated replace/update orders (thanks to [@UUCD6P13J](@UUCD6P13J)) Lots of improvements still to be made, have a look at the [https://github.com/liampauling/flumine/issues|issue](https://github.com/liampauling/flumine/issues|issue) tracker to get a better of idea of the current direction

*Tags: General Technical*

---

**liam** - *09:02:15*

[https://liampauling.github.io/flumine/execution/#simulated|U](https://liampauling.github.io/flumine/execution/#simulated|U)ses django style [https://liampauling.github.io/flumine/execution/#simulated|middleware](https://liampauling.github.io/flumine/execution/#simulated|middleware), its not perfect but surprisingly accurate enough for most cases, works by using the middleware to calculate what has been matched inbetween updates and then the [https://github.com/liampauling/flumine/blob/master/flumine/backtest/simulated.py#L14|Simulated](https://github.com/liampauling/flumine/blob/master/flumine/backtest/simulated.py#L14|Simulated) class does the clever stuff with it

*Tags: General Technical*

---

**liam** - *09:07:35*

welcome a PR to fix that!

*Tags: Errors Debugging*

---

**liam** - *09:09:51*

if you can think of anymore [https://github.com/liampauling/flumine/issues/192](https://github.com/liampauling/flumine/issues/192)

*Tags: General Technical*

---

**Peter** - *09:12:42*

Should double counting of liquidity be "fixed"? In markets with lots of liquidity the effect is usually marginal. In thinner markets, I sometimes take the offered price if it looks like value and immediately see more dripped in (presumably by somebody with an algo that has counter view). So trying to fix double counting may simply add complexity to a situation in order to create results that are sometimes right, sometime wrong and sometimes don't matter.

*Tags: Errors Debugging*

---

**liam** - *09:14:27*

I think this is where the middleware can really help, as you could either remove the `DoubleCountSimulation` deflag it or just write your own but yes I agree it would probably underplay your chance of getting matched

*Tags: General Technical*

---

**liam** - *10:26:38*

Could look to handle a [https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py#L103|list of orders](https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py#L103|list of orders) if you want?

*Tags: Strategies*

---

**JonJonJon** - *10:28:08*

Ah, that is interesting. By "same update", do you mean the same market book update. So that if process_market_book(...) tries to place multiple updates, Flumine waits till process_market_book(...) has finished, and then batches the updates when it has finished?

*Tags: General Technical*

---

**liam** - *10:30:03*

yes it uses OrderPackages, so you may have orders from [https://github.com/liampauling/flumine/blob/master/flumine/baseflumine.py#L130|multiple strategies](https://github.com/liampauling/flumine/blob/master/flumine/baseflumine.py#L130|multiple strategies) being sent at once to try and reduce latency and load, in that code you can see the strategies being processed and then `process_market_orders` being called which handles the pending orders

*Tags: Performance*

---

**JonJonJon** - *10:32:31*

Ah, that is very cool. Thanks for answering my questions so patiently!

*Tags: General Technical*

---

**Andrey Yunoshev** - *14:11:19*

&gt;flumine seems to be getting some users



this is a lot of work, and most importantly is done by a person who understands the subject

I, as usual, wrote the parser myself, and then I caught bugs - and then I decided to google it :)



in general, at the moment, it bothers me a little that everything is slow - it takes 2-3 hours to process a month of greyhounds records

on the other hand, it's almost 10 gigabytes of data :)



I have a couple more questions - you can somehow immediately get the name of the runners?

currently, I call client.betting_client.betting.list_market_catalogue in check_market_book



And second, for backtesting, I do not receive any market with CLOSED status - for getting WIN/LOSE runners status - but need dig more, possible my bug

*Tags: Errors Debugging, Performance, Strategies*

---

**liam** - *14:13:16*

anything closed will get processed in `Strategy.process_closed_market` rather than `process_market_book`

*Tags: Strategies*

---

**liam** - *14:32:34*

Sorry only live, catalogue isn't available in backtesting but you can get the runners names from the [https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L32|marketDefinition](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L32|marketDefinition)

*Tags: Deployment*

---

**Andrey Yunoshev** - *14:33:07*

```    strategy = LiveStrategy(

        market_filter=streaming_market_filter(

            event_type_ids=["4339"],

            market_types=["WIN"],

            bsp_market=True,

            country_codes=["GB", "UK", "AU"],

        ),

        streaming_timeout=0,

        max_order_exposure=20,

        max_selection_exposure=10,

    )



    client = clients.BetfairClient(trading)

    framework = Flumine(client=client)

    framework.add_strategy(strategy)```

*Tags: Deployment, Strategies*

---

**liam** - *14:33:26*

what version of flumine are you using?

*Tags: General Technical*

---

**Andrey Yunoshev** - *14:33:51*

```-&gt; % pip3 show flumine

Name: flumine

Version: 1.5.4

Summary: Betfair trading framework

Home-page: [https://github.com/liampauling/flumine](https://github.com/liampauling/flumine)

Author: Liam Pauling

Author-email: UNKNOWN

License: MIT

Location: /usr/local/lib/python3.7/site-packages

Requires: python-json-logger, betfairlightweight, requests, tenacity

Required-by: ```

*Tags: Strategies*

---

**Andrey Yunoshev** - *14:45:18*

start framework == .run? so, I can run, wait 10s and add strategy after?

*Tags: Strategies*

---

**liam** - *14:46:29*

no you run it normally but there will be 10s where there is no marketCatalogue, its because the streams need time to start first, can maybe look at reducing the 10s if you think it is a problem but i don't see why it would? Just ignore markets where the catalogue is empty

*Tags: General Technical*

---

**Andrey Yunoshev** - *14:51:23*

fix it, now work without extra request. thanks for help

*Tags: Errors Debugging*

---

## 2020-06-03

**Oliver Varney** - *07:04:14*

Morning guys, just been looking through flumine and have a couple of question on where is best to put some code to extend the functionality. Where would I put external data that I would need for a strategy that is specific to a market (Market class?). Secondly where would I put code that is specific to both a strategy and a market (Strategy class?)?

*Tags: Strategies*

---

**liam** - *08:00:36*

You have `Strategy.context` and `Market.context` as a user store that flumine won't touch. When you want it specific to both I guess its up to you. I see this being a crucial part of flumine as external data sources are likely to become more and more important so welcome any thoughts on this.

*Tags: Strategies*

---

**Oliver Varney** - *08:03:39*

I think I might just create a class called StrategyMarket

*Tags: Strategies*

---

**Oliver Varney** - *08:04:32*

and just have a list within  the strategy that retrieves the relevant strategy market within the check_market_book / process_market_book

*Tags: Strategies*

---

**Oliver Varney** - *08:07:32*

the edge case is the combination where you need something that is specific to both a strategy and a market right?

*Tags: Strategies*

---

**liam** - *08:08:48*

Yeah, I normally create a dictionary in the context `Strategy.context = {marketId: xyz}`

*Tags: Strategies*

---

**Oliver Varney** - *08:14:46*

one example from my code is strategy market schedules / events, which can be a list of time based events specific to both the market and strategy. Ive added a list into the strategy to hold strategy markets, which in turn hold strategy market schedules. this then opens up the possibility for strategy market schedules to have specific associated trades

*Tags: Strategies*

---

**Oliver Varney** - *08:17:50*

which I guess is just the same as holding in the Strategy.context which the value of the market_id being a class

*Tags: Strategies*

---

**liam** - *10:15:20*

stay well away from dockerhub, the uptime is terrible, the builds takes ages, sometimes they will just stop randomly and you have to wait for it to timeout, after a painful migration I now use github actions + aws ecr

*Tags: Deployment*

---

## 2020-06-04

**Ruben** - *08:03:13*

I am wondering, what is are the best practices to be able to deploy a python project in any server and avoid it causing too much pain? using conda environments?

*Tags: Deployment*

---

**Mo** - *08:21:39*

I install as a Python package to a virtual environment

*Tags: Getting Started*

---

**AP** - *09:58:02*

Do the docker images get deployed to an EC2 instance (or similar)?

*Tags: Deployment*

---

**liam** - *10:18:15*

I use github actions (test/build) -&gt; AWS ECR -&gt; pulled into ec2 instances (mixture of scripts)

*Tags: Deployment*

---

**JonJonJon** - *10:30:09*

Does anyone know if this book is a good starter for learning Docker?

[https://www.amazon.co.uk/Practical-Docker-Python-Release-Distribute/dp/1484237838](https://www.amazon.co.uk/Practical-Docker-Python-Release-Distribute/dp/1484237838)

*Tags: General Technical*

---

**liam** - *12:16:49*

I just use boto3

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1591266684275500](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1591266684275500)

*Tags: General Technical*

---

**Newbie99** - *19:46:06*

I think you mean this:



```market_filter = streaming_market_filter(

    turn_in_play_enabled=True)```



*Tags: General Technical*

---

**Remi** - *19:49:19*

I am trying to get high resolution pricing data on available to back/lay. This is the call that I am using:

```list_market_book(market_ids=[market_id],

                                                       price_projection=filters.price_projection(

                                                           rollover_stakes=True,

                                                           ex_best_offers_overrides=filters.ex_best_offers_overrides(

                                                               rollup_model="STAKE",

                                                               best_prices_depth=3,

                                                               rollup_limit=0,

                                                           ),

                                                           price_data=filters.price_data(ex_best_offers=True, ),

                                                       ),```

But the price data that I am getting is not more precise than the minimum bet size.



Is this as expected or is there a way to get higher resolution (other than scraping the website)?

*Tags: Strategies*

---

**Mo** - *19:51:58*

`rollup_model='NONE'`

*Tags: Strategies*

---

**Remi** - *19:55:25*

`rollup_model='NONE'` doesn’t do it. E.g. [https://www.betfair.com/exchange/plus/e-sports/market/1.170652449](https://www.betfair.com/exchange/plus/e-sports/market/1.170652449) I don’t see the available to lay 2€ at 1.38 but I see 13.99 at 1.39

*Tags: Strategies*

---

**Mo** - *19:58:03*

[https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Enums#BettingEnums-RollupModel](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Enums#BettingEnums-RollupModel)

*Tags: Strategies*

---

## 2020-06-05

**Julio** - *14:31:30*

Hello all,

For all of you who are not (yet) recording data, Betfair has an offer:

*Free Betfair Exchange Historical Data Offer!*

We are offering the following historical data free of charge:

*January 2020– May 2020 – ADVANCED data - All Sports – (normally £2615)*

*April 2020 – May 2020 – PRO data – All Sports – (normally £2180)*

*Tags: Data Quality*

---

**JonJonJon** - *21:54:25*

From looking at a flumine Order object, is it possible to extract the times of individual fills?

*Tags: General Technical*

---

## 2020-06-06

**Will Morrison** - *09:28:11*

Does anyone have an example of calling the MarketRecorder class in the flumine examples (or similar)? What I mean is something like the marketrecorder.py example where it creates an S3MarketRecorder object:

strategy = S3MarketRecorder(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB", "IE"],

        market_types=["WIN"],

        # market_ids=["1.169056942"],

        # event_ids=[29671376]

    ),

    stream_class=DataStream,

    context={

        "local_dir": "/tmp",

        "bucket": "fluminetest",

        "force_update": False,

        "remove_file": True,

    },

)

but for the MarketRecorder class instead.

I'm relatively comfortable with writing small scripts and functions, but I'm a bit overwhelmed trying to figure out what arguments I do and don't have to hand to a big class like MarketRecorder which inherits the BaseStrategy class when I instantiate it. Is learning to use the inspect module the right path for me to get more comfortable with this problem in the future?

*Tags: Strategies*

---

**liam** - *09:42:01*

Need to improve the docs on this but here is a simple example:



```strategy = MarketRecorder(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB", "IE"],

        market_types=["WIN"],

    ),

    stream_class=DataStream,

    context={

        "local_dir": "/tmp",

        "force_update": False,

        "remove_file": True,

    },

)```

*Tags: Strategies*

---

**Will Morrison** - *10:49:55*

Worked like a charm, now I'm using flumine for the first time! If the following is too much "teach me Python", I apologize, and please feel free to tell me to just go learn more basics: I'm trying to make my first example custom strategy object, where I would like to modify the flumine example.py class to try to place a BACK on a single player if his batb is above a certain threshhold. In betfairlightweight, I was able to get these values by looping through the market book and using lines like

market_book._data["runners"][x]["ex"]["availableToBack"][lvl]["price"], where x and lvl are loop variables

which I was able to figure out by using the variable explorer in my Spyder IDE too dig into the market_book object after running one loop of examplestreaming.py. Now in flumine I don't know how to get a similar thing to investigate. I want to take a conditional like this from the example -

if (

                runner.status == "ACTIVE"

                and runner.last_price_traded

                and runner.selection_id == 11946248



and instead of runner.last_price_traded, say something like

                and runner.ex.availableToBack[0]["price"] &gt;= 3.0



Is there an easy way for me to figure out what all the runner.[things] available are named?

*Tags: Getting Started, Strategies*

---

**Will Morrison** - *11:05:38*

By running just

client = clients.BetfairClient(trading)



framework = Flumine(client=client)



strategy = ExampleStrategy(

    market_filter=streaming_market_filter(market_ids=["1.170643189"]),

    streaming_timeout=2,

)

at the end, but not the framework.run(), I think I was able to get what I needed!

*Tags: Strategies*

---

**liam** - *11:35:36*

Recommend backtesting when testing strategies, debugger is helpful in investigating objects, have a search as we were discussing it the other week 

*Tags: Errors Debugging*

---

**Ruben** - *17:11:19*

The code I am using is equivalent to examplestreaming.py

*Tags: General Technical*

---

**Ruben** - *17:11:43*

any help is more than welcome

*Tags: General Technical*

---

**Jonatan (skyw)** - *17:17:52*

Try just printing market Id  and see what latency you get.

*Tags: Performance*

---

**Jonatan (skyw)** - *17:21:18*

Yeah, that's the latency I'm referring to :)

*Tags: Performance*

---

**Ruben** - *17:34:51*

```stream = trading.streaming.create_stream(listener=listener)

streaming_unique_id = stream.subscribe_to_markets(market_filter=market_filter,

                                                  market_data_filter=market_data_filter,

                                                  heartbeat_ms=500)



t = threading.Thread(target=stream.start, daemon=True)

t.start()



event_db = {}

market_db = {}

before_queue_get = time.time()

count = 0

while True:

   count += 1

    market_books = output_queue.get()

    current_time = time.time()

   print(f"Average time: {(current_time - before_queue_get)/count}")```



*Tags: Strategies*

---

**brightcake** - *19:29:19*

I seem to get hit with a 'TOO_MUCH_DATA' error code

*Tags: Errors Debugging*

---

**brightcake** - *19:32:04*

alright cheers, that solved it for me

*Tags: General Technical*

---

**Mo** - *19:33:06*

Back in my day you had to actually read the API documentation :wink:

*Tags: General Technical*

---

**Mo** - *19:36:08*

No judgement, I think it's interesting that the accessibility of betfairlightweight means that you can potentially use the API without ever looking at the documentation

*Tags: General Technical*

---

**brightcake** - *19:39:12*

The documentation isn't exactly a bed time read so it's quite nice to not have to do that :smile:

*Tags: General Technical*

---

**liam** - *19:44:03*

And if you want to measure your latency you should be comparing the time to `MarketBook.publish_time`

*Tags: Performance*

---

**JonJonJon** - *19:48:37*

Hmmm... Not sure if I explained what I meant clearly, as I'm not sure if it's related to the market book.



So I am using flumine to place limit orders. The orders sometimes get matched. I'm trying to find the times of those matches.

*Tags: General Technical*

---

## 2020-06-07

**Ruben** - *01:14:41*

I agree that this is a much better way to measure latency

*Tags: Performance*

---

**Will Morrison** - *08:43:02*

I'm testing the MarketRecorder example strategy, and it seems like it saves a list of all trades on each runner under trd, with price - volume pairs. Is there a way to identify the time of each trade?

*Tags: Strategies*

---

**liam** - *09:21:01*

This was raised yesterday, I can look at it tomorrow [https://github.com/liampauling/flumine/issues/205|https://github.com/liampauling/flumine/issues/205](https://github.com/liampauling/flumine/issues/205|https://github.com/liampauling/flumine/issues/205)

*Tags: General Technical*

---

**Will Morrison** - *09:39:03*

Thanks both - I'll see if I can figure out how to look at the raw stream update.

*Tags: General Technical*

---

## 2020-06-08

**Jonatan (skyw)** - *11:00:13*

Specific library for that, it's not supported by python by default?

*Tags: General Technical*

---

**Lee** - *11:02:51*

I've come across some of the problems it mentions in the readme from using pyyaml

*Tags: General Technical*

---

**liam** - *12:06:04*

Is this what you are after? [https://github.com/liampauling/flumine/pull/208](https://github.com/liampauling/flumine/pull/208)

*Tags: General Technical*

---

**Martin Alley** - *16:28:17*

I'm looking to be able to use the Race Status API via betfairlightweight. Is this possible? Thanks Martin

*Tags: General Technical*

---

## 2020-06-09

**liam** - *06:20:29*

Nope, no problem with that and yeah should be much quicker!

*Tags: General Technical*

---

**liam** - *09:32:02*

I am aware it is a bottleneck, an LRUCache might be simpler to start off with [https://github.com/liampauling/flumine/blob/master/flumine/utils.py#L134](https://github.com/liampauling/flumine/blob/master/flumine/utils.py#L134)

*Tags: General Technical*

---

**John Sheppard** - *14:27:06*

Thankyou Mo, you are very helpful.  Seems like a strange way to do it to me (from Betfair's perspective). I didn't realise Betfair had such a thing and I got a bit excited after seeing MartinAlley mention it....but not yet ready to try and apply it's use...now its on the todo list :slightly_smiling_face:

*Tags: General Technical*

---

## 2020-06-10

**jhaa** - *16:30:57*

06/10 16:30:50 [Stream: 1]: Latency high: 0.6275410652160645

*Tags: Performance*

---

**jhaa** - *16:31:27*

Which one gives me the latency waarning?

*Tags: Performance*

---

**liam** - *16:31:45*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/streaming.py#L24](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/streaming.py#L24)

*Tags: General Technical*

---

**jhaa** - *16:32:57*

ahh ok I was in streaming/stream.py

*Tags: General Technical*

---

**jhaa** - *17:39:25*

If Corbin would have won all issues with British telecom would have been solved by now

*Tags: General Technical*

---

## 2020-06-12

**gprokisch** - *17:04:15*

Where I can find historical data of Greyhound race for use in backtesting?

*Tags: Data Quality*

---

**liam** - *17:05:37*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1591363890287900|https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1591363890287900](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1591363890287900|https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1591363890287900)

*Tags: General Technical*

---

## 2020-06-13

**Sandy Caskie** - *18:17:23*

I am now moving over to the blw. I have set up my certificate key as outlined here [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA).



I have uploaded a .pem file to *Automated Betting Program Access* on the Betfair security page.



I then use the following code to access my account using blw:



```# Change this certs path to wherever you're storing your certificates

certs_path = r'/Users/Sandy/Desktop/CodingLightWeight/TestApp1.pem'



# Change these login details to your own

my_username = "XXXXX"

my_password = "XXXXX"

my_app_key = "XXXXX"



trading = betfairlightweight.APIClient(username=my_username,

                                       password=my_password,

                                       app_key=my_app_key,

                                       certs=certs_path)



trading.login()```

The path is to the .pem file I created. Though I get the following error:



```APIError: None 

Params: None 

Exception: Certificate folder not found in /Users/Sandy/Desktop/CodingLightWeight/TestApp1.pem```

I appear to be missing something due to the above error. Can anyone help?

*Tags: Errors Debugging, Strategies*

---

**Sandy Caskie** - *18:30:52*

When I created the certificate both a .key and .pem file were created. These have been put in the same folder, which is I have given a path to as variable certs_path. Though this error persists.

*Tags: Errors Debugging*

---

**Lee** - *18:31:25*

whats the new error?

*Tags: Errors Debugging*

---

**Sandy Caskie** - *18:32:13*

```APIError: None 

Params: None 

Exception: Certificate folder not found in /Users/Sandy/Desktop/CodingLightWeight/Certificate```

*Tags: Errors Debugging*

---

**Sandy Caskie** - *18:43:49*

Ok I've now created the .crt file. Though I now get the following error:



```APIError: None 

Params: None 

Exception: [('PEM routines', 'PEM_read_bio', 'no start line'), ('SSL routines', 'SSL_CTX_use_PrivateKey_file', 'PEM lib')]```



*Tags: Errors Debugging*

---

**Sandy Caskie** - *18:53:24*

Yes, that is the new error I am getting:



```APIError: None 

Params: None 

Exception: [('x509 certificate routines', 'X509_check_private_key', 'key values mismatch')]```



*Tags: Errors Debugging*

---

**Sandy Caskie** - *18:58:13*

Ahh I get it. Its the match between myself and the file I upload to betfair to ensure it is me that interacting with it via bflw?

*Tags: General Technical*

---

**Sandy Caskie** - *19:02:14*

Ok I'll run through that again thanks for your help. Just so I fully understand.



1. You generate a .key file using the procedure outlined in [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA) 

2. You generate a .crt file using the procedure outlined in [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA) 

3. Locate these in the same folder

4. Upload your .crt file to Betfair security

Is this correct?

*Tags: General Technical*

---

**Lee** - *19:02:54*

no problem

*Tags: General Technical*

---

**PeterLe** - *20:58:08*

[@U014W8KJGNL](@U014W8KJGNL) A few of us have got stuck on this part..check out this link [https://betfairlightweight.slack.com/archives/C4H05ML2E/p1590763027445700](https://betfairlightweight.slack.com/archives/C4H05ML2E/p1590763027445700)

*Tags: General Technical*

---

## 2020-06-14

**Sandy Caskie** - *12:17:54*

Thanks for your help. I can see that you have 4 files in your folder. Though you only need 2 is that correct; them being your .crt file and .key file? Also it is the .crt file which is uploaded to betfair?

*Tags: General Technical*

---

**Sandy Caskie** - *12:44:47*

Got it! I think anyway lol. No error and returned &lt;LoginResource&gt;



These things never work when I stay up late and try do them.



I think what happened was the .key file was not associated with .crt file when I created them. Therefore the error key values mismatch was returned.

*Tags: Errors Debugging*

---

## 2020-06-16

**Mo** - *14:50:28*

Pretty clear in the documentation

*Tags: General Technical*

---

**Disco** - *20:02:29*

For Chrome, if I remember correctly, there was an extension which allowed to debug websockets easier

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *20:03:43*

look interesting, I missed that one looked at two other python libraries.

*Tags: General Technical*

---

## 2020-06-18

**gprokisch** - *05:26:02*

Hi guys, I had deployed a script using Google cloud but I'm getting this error [https://stackoverflow.com/questions/25817167/betfair-api-betting-restricted-location-when-logging-in-from-google-app-engine|BETTING_RESTRICTED_LOCATION](https://stackoverflow.com/questions/25817167/betfair-api-betting-restricted-location-when-logging-in-from-google-app-engine|BETTING_RESTRICTED_LOCATION) cause google ip address, in my local computer it's works fine.

*Tags: Errors Debugging, Deployment, Strategies*

---

**gprokisch** - *05:48:17*

Thats is the problem. It's US ip based.

*Tags: General Technical*

---

**gprokisch** - *05:49:57*

Before I try to move to AWS or Azure I want to know if someone here had success to deploy in API on those services.

*Tags: Deployment*

---

**Mo** - *05:50:22*

Yes lots of people here use AWS, myself included

*Tags: Deployment*

---

**Edoardo** - *14:35:42*

Hi, I'm new and I'm just playing around with the APIs. I have a doubt about the selection id. Let's say I have a football match and I get the market 'under/over 2.5' then I will have two selection IDs, one for under and one for over. How do I get the selection id associated with 'under'? Any help is appreciated

*Tags: General Technical*

---

## 2020-06-19

**brightcake** - *09:40:44*

so what would normally be runners in historical data are treated as seperate markets in stream?

*Tags: Data Quality*

---

**Mo** - *15:01:48*

No, not sure how you got to that? The runners are listed in the market definitions in the live streaming data but there are no names

*Tags: Deployment*

---

## 2020-06-20

**Jonatan (skyw)** - *08:12:21*

Have they hinted about this before? Kinda horrible to say the least breaking users applications in one day without any notice, and with no real documentation...

*Tags: General Technical*

---

**Stefan** - *13:43:10*

I hope your python library is not so buggy implemented, I think you know who I am on betfair forum.

*Tags: Errors Debugging*

---

**liam** - *13:50:07*

Bflw is just a wrapper so up to user in how to implement a min bet order, flumine doesn’t have the ability yet so no bugs here :wink:

*Tags: Errors Debugging*

---

**Andrey Yunoshev** - *14:18:51*

```REPLACE PRICE, FROM 9.0 TO 8.6

{'bet_id': '202759245511',

 'customer_order_ref': 'da32eac47a66a-138119515919016440',

 'handicap': 0,

 'id': '138119515919016440',

 'info': {'average_price_matched': 0.0,

          'side': 'BACK',

          'size_cancelled': 0.0,

          'size_lapsed': 0.0,

          'size_matched': 0.0,

          'size_remaining': 2,

          'size_voided': 0.0},

 'market_id': '1.170903586',

 'order_type': {'bet_target_size': None,

                'bet_target_type': None,

                'min_fill_size': None,

                'order_type': 'Limit',

                'persistence_type': 'LAPSE',

                'price': 9.0,

                'size': 2,

                'time_in_force': None},

 'selection_id': 28741430,

 'status': 'Executable',

 'status_log': 'Pending, Executable',

 'trade': {'id': UUID('cae6e176-b2f7-11ea-8807-d0034bcc9d7d'),

           'market_notes': '8.2,8.6,9',

           'notes': '8.6,8.2,9.0,Henlow,2020-06-20 13:17:00,Layton Jim,4',

           'orders': ['138119515919016440'],

           'status': &lt;TradeStatus.LIVE: 'Live'&gt;,

           'strategy': &lt;__main__.LiveStrategy object at 0x10ffbf4d0&gt;}}

INFO:flumine.order.order:Order status update: Replacing```

*Tags: Deployment, Strategies*

---

**Andrey Yunoshev** - *14:20:31*

Here is order info and flumine record - but I do not see, Replace Confirm or another status confirming replace? I do something wrong?

*Tags: General Technical*

---

**Stefan** - *15:00:13*

All betfair libraries are just wrappers, but not mine, it is betfair bot framework. Yes, [@U4H19D1D2](@U4H19D1D2) I know I checked your library, I can program in python, and am actually here to learn more about python, I prefer to use it only in ML.

*Tags: General Technical*

---

**Andrey Yunoshev** - *15:45:54*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1592659131481300](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1592659131481300)



here, I have back order with price 9.0 and size 2, and trying replace for new price 8.6



and where i accually new order creating?



I see following code



```    def replace(self, new_price: float) -&gt; None:

        if self.order_type.ORDER_TYPE in [OrderTypes.LIMIT, OrderTypes.LIMIT_ON_CLOSE]:

            if self.order_type.price == new_price:

                raise OrderUpdateError("Prices match")

            elif self.status != OrderStatus.EXECUTABLE:

                raise OrderUpdateError("Current status: %s" % self.status)

            self.update_data["new_price"] = new_price

            self.replacing()

        else:

            raise OrderUpdateError(

                "Only LIMIT or LIMIT_ON_CLOSE orders can be replaced."

            )



```



Its just change order status, but can not find where replacting accually happens

I use 1.7.0

*Tags: Errors Debugging*

---

**gprokisch** - *17:36:39*

Does anyone have an example of streaming odds? These example in the documentation seems is not working:

*Tags: General Technical*

---

**liam** - *17:49:12*

That’s a screenshot of the order streaming example, I assume you want market?

*Tags: General Technical*

---

**liam** - *17:49:43*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py|https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py|https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**liam** - *18:01:48*

[https://liampauling.github.io/betfair/streaming/|https://liampauling.github.io/betfair/streaming/](https://liampauling.github.io/betfair/streaming/|https://liampauling.github.io/betfair/streaming/)

*Tags: General Technical*

---

**gprokisch** - *18:17:03*

Sorry man.. For the silly question. I'm new to this betting world. This API still confusing me a bit.

*Tags: Getting Started, Strategies*

---

## 2020-06-21

**liam** - *07:56:03*

You can however do `market.replace_order(order, new_price)` which is the same as can be seen [https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py#L115|here](https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py#L115|here)

*Tags: Strategies*

---

**MrBIN** - *08:52:33*

Hi all. I have a question regarding the Betfair's SP (Starting Price). I can see that some users add sp orders right before the actual SP is published (seems like a few milliseconds before). I wondered if there is any indication (hopefully through API) to when the SP is published.

*Tags: General Technical*

---

**Stefan** - *10:44:40*

[@UT2P36CLW](@UT2P36CLW) It is more about different strategies people try to execute using trading software, not all of them offer simple way to place sp bet in automation. SP is calculated from back/lay offers of already placed SP bets, so you cannot estimate it exactly, well, yes betfair api offers estimates of those prices through api, but as the moment of race turning in-play is not accurate, even race status api is plus minus 2-3 seconds behind, and not all races starts from starting stalls.



So if you read on forum, that people try to place bets at SP, it is not actually they place SP bets, but try to place a bet as soon as possible after race start, and as close as possible to sp price.

*Tags: Strategies*

---

**Stefan** - *12:27:22*

[@UFTBRB3F1](@UFTBRB3F1) just google text from such strategy:



I want to back the second horse to trade below 2 and if the selection should drift I would want to lay the same selection say &gt;16.

*

In a similar vein can you please tell me how to back the favourite at Betfair SP. I am having problems when the favourite changes late in betting.



Of course we developers can do anything with api, and do so, but common people come with such strategies.

*Tags: Strategies*

---

**yuv** - *14:07:43*

Hi, I saw in the documentation that the SP’s far price and near prices only updates every 60 seconds (through API), Do you know if the SP sizes do updates in realtime (‘backStakeTaken’ and ‘LayLiabilityTaken’) ?

*Tags: General Technical*

---

**gprokisch** - *16:06:36*

Hi guys. Any idea how I can fix this error: INVALID_SESSION_INFORMATION

*Tags: Errors Debugging*

---

## 2020-06-22

**liam** - *08:03:11*

[https://github.com/liampauling/betfair/blob/master/HISTORY.rst#250-2020-06-22|v2.5.0](https://github.com/liampauling/betfair/blob/master/HISTORY.rst#250-2020-06-22|v2.5.0) released, thanks to [@UUCD6P13J](@UUCD6P13J) for the `file_path` improvement :thumbsup: Strongly recommend the use of github actions for testing/deployment, I don't need to anything other than approve to get things tested / deployed / docs updated

*Tags: Deployment*

---

**Newbie99** - *08:10:47*

`ERROR: flumine 1.7.0 has requirement betfairlightweight==2.4.0, but you'll have betfairlightweight 2.5.0 which is incompatible.`

*Tags: Errors Debugging*

---

**Evaldas** - *15:29:46*

Hi,

If I have 2-3k markets in market/order stream cache, could that take significant amount of memory in bflw?

And if so, is there a way to clean that cache on market close?

*Tags: Performance*

---

**Evaldas** - *15:39:48*

maybe it's just the way python reserves memory. I run mine non-stop, and it gains 200-300MBs over 2weeks

*Tags: Performance*

---

**liam** - *15:40:25*

That running bflw on its own? 

*Tags: General Technical*

---

**liam** - *16:29:51*

Implying that it might be your code leaking rather than bflw 

*Tags: General Technical*

---

## 2020-06-23

**Johnny** - *22:10:38*

Hi guys, very basic question here.  I see the Asian Handicap (football) markets have arbitrary pricing, i.e. not aligned to the normal Betfair tick prices.  Do any other markets have thi sas well?  How do you deal with the many additional possibilities in your models?

*Tags: Strategies*

---

**Mo** - *22:29:34*

Not sure why you would need to do anything special in your model

*Tags: Strategies*

---

## 2020-06-24

**Mo** - *07:02:35*

Yeah in the cold light of morning I could see how if you have some price driven model you might want to do something with ticks but maybe it's possible for you to do something with probability instead which can then be mapped to either price ladder

*Tags: Strategies*

---

**yuv** - *08:38:47*

Hi guys, there is something I can’t figure out regarding the SP bets. I look at the “stakes and liabilities taken” published after the SP is settled and it never sums up to be the sp bets I collected from the streaming API while constantly listening before the race. It seems like there are sp bets that I don’t get through the streaming API. For example let’s say, that in the last update I got from the API on selection X the total sp LayLiabilityTaken for that selection was 1,000 Pound and when the final sp details were published there were 3,000 Pounds (and that happens pretty much all the time).

*Tags: General Technical*

---

**MrBIN** - *10:55:24*

with regard to [@U015R4ZU8G5](@U015R4ZU8G5) question - Do you know if every update of new SP orders is pushed to the stream (assuming a subscription with *MarketDataFilter that includes* SP_TRADED field). Or maybe only a regular book update is sent to the stream callback (with the current state of SP sizes)?

*Tags: General Technical*

---

**Jorge** - *13:33:19*

[@U4H19D1D2](@U4H19D1D2) Wow, okay then I need to figure out what the problem is with my machine, since my resources are more than enough. I guess it has been a long time since the last time I restarted it

*Tags: General Technical*

---

## 2020-06-25

**liam** - *10:05:51*

1gb for just python?? That’s huge 

*Tags: General Technical*

---

**Jorge** - *16:18:39*

Yes [@U4H19D1D2](@U4H19D1D2). I just use Python for API calls and I read small csv files apart from that...

*Tags: General Technical*

---

**Stefan** - *17:31:21*

@[https://app.slack.com/team/UT2P36CLW|MrBIN](https://app.slack.com/team/UT2P36CLW|MrBIN), you have got nearPrice and farPrice, estimations of SP prices. The first one includes any bets placed, the last one only SP bets. What kind of strategy do you want to execute?

*Tags: Strategies*

---

## 2020-06-26

**steve** - *15:53:15*

hey guys. im using python to download in game odds from betfair. any links to where i can find how to do this from START to END. i have json files and completely stuck

*Tags: General Technical*

---

**Mo** - *15:54:43*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**Mo** - *15:55:19*

[https://liampauling.github.io/betfair/streaming/#historical](https://liampauling.github.io/betfair/streaming/#historical)

*Tags: General Technical*

---

**Mo** - *15:56:03*

Good luck, any questions let us know

*Tags: General Technical*

---

**steve** - *17:53:06*

thanks again for the help ive got an example game exporting data to a file, so should be ok from here

*Tags: General Technical*

---

**Pete** - *21:00:11*

Hello everyone. I'm new here.

Sorry if this is an FAQ and I've missed it.

I'm new to Python. How do I satisfy myself that the Flumine/BFlightweight code doesn't share my credentials anywhere but with Betfair?

I'm not paranoid...just careful.

*Tags: Getting Started*

---

**liam** - *21:28:27*

Yeah the source is all on GitHub, the only thing it does share is the user agent on requests `betfairlightweight/version` but betfair don’t actually analyse this 

*Tags: General Technical*

---

## 2020-06-27

**Rich** - *12:43:17*

Does anyone know of the top of their head what a horse non-runner would look like in the streaming api? I'm guessing a new marketDefinition is sent?  Do I cache the number of runners then compare it to the new number of runners?

*Tags: General Technical*

---

**Mo** - *16:52:35*

But this is where it comes from: [http://www.totalperformancedata.com](http://www.totalperformancedata.com)

*Tags: Performance*

---

**Mo** - *16:56:27*

Assuming you mean the live data

*Tags: Deployment*

---

## 2020-06-28

**fjt1973** - *11:50:24*

[@U4H19D1D2](@U4H19D1D2) is BFL compatible with AWS Lambda? I've uploaded BFL as a package into AWS Layers but I'm getting the following error.

*Tags: Errors Debugging, Deployment*

---

**fjt1973** - *11:50:30*

Exception: join() argument must be str or bytes, not 'tuple'

Traceback (most recent call last):

  File "/var/task/lambda_function.py", line 37, in lambda_handler

    trading.login()

  File "/opt/python/lib/python3.7/site-packages/betfairlightweight/endpoints/login.py", line 31, in __call__

    self.url, session=session

  File "/opt/python/lib/python3.7/site-packages/betfairlightweight/endpoints/login.py", line 53, in request

    raise APIError(None, exception=e)

*Tags: Errors Debugging, Strategies*

---

**sartux** - *12:50:38*

I'm having several difficulties in making a cash out function, can someone help me?

*Tags: General Technical*

---

**fjt1973** - *13:01:53*

So this is my handler, very basic to return the balalce of my account.



def lambda_handler(event, context):

    # TODO implement

    trading = betfairlightweight.APIClient(username=my_username,

                                       password=my_password,

                                       app_key=my_app_key,

                                       certs=certs_path)



    trading.login()

    account_funds = trading.account.get_account_funds()

    balance = account_funds.available_to_bet_balance

    return balance

    #return {

    #    'statusCode': 200,

    #    'body': json.dumps('Hello from Lambda!')

    #}



And this is the execution results



Response:

{

  "errorMessage": "None \nParams: None \nException: join() argument must be str or bytes, not 'tuple'",

  "errorType": "APIError",

  "stackTrace": [

    "  File \"/var/task/lambda_function.py\", line 33, in lambda_handler\n    trading.login()\n",

    "  File \"/opt/python/lib/python3.7/site-packages/betfairlightweight/endpoints/login.py\", line 31, in __call__\n    self.url, session=session\n",

    "  File \"/opt/python/lib/python3.7/site-packages/betfairlightweight/endpoints/login.py\", line 53, in request\n    raise APIError(None, exception=e)\n"

  ]

}



Request ID:

"f4b6780c-fb3a-485f-881d-3fc75f1cfddd"



Function Logs:

START RequestId: f4b6780c-fb3a-485f-881d-3fc75f1cfddd Version: $LATEST

[ERROR] APIError: None

Params: None

Exception: join() argument must be str or bytes, not 'tuple'

Traceback (most recent call last):

  File "/var/task/lambda_function.py", line 33, in lambda_handler

    trading.login()

  File "/opt/python/lib/python3.7/site-packages/betfairlightweight/endpoints/login.py", line 31, in __call__

    self.url, session=session

  File "/opt/python/lib/python3.7/site-packages/betfairlightweight/endpoints/login.py", line 53, in request

    raise APIError(None, exception=e)

END RequestId: f4b6780c-fb3a-485f-881d-3fc75f1cfddd

REPORT RequestId: f4b6780c-fb3a-485f-881d-3fc75f1cfddd	Duration: 19.17 ms	Billed Duration: 100 ms	Memory Size: 128 MB	Max Memory Used: 85 MB	Init Duration: 782.62 ms

*Tags: Errors Debugging, Performance, Strategies*

---

**liam** - *13:56:59*

Yeah it’s in the market definition in streaming 

*Tags: General Technical*

---

**Mo** - *14:13:40*

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/racecard.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/racecard.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/racecard.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/racecard.py)

*Tags: General Technical*

---

**D C** - *14:15:13*

OK thanks. Ill look at that python and convert it to C++. Many thanks

*Tags: General Technical*

---

**Mo** - *14:22:54*

Sorry you didn’t get any replies to this yet, there has been a lot of chat about this recently. I think [@U013MLED3V1](@U013MLED3V1) and [@U0155J92A7Q](@U0155J92A7Q) were involved. Can you guys help?

*Tags: General Technical*

---

**sartux** - *15:14:23*

I'm wasting time on something that will definitely be a banal.

in part with

trading.betting.list_current_orders () .__ dict__

I get the dictionary of all open orders.



my intention would be to take the size and initial odds to calculate the exit.



therefore in fact a for loop with all the orders that, when it arrives at the market (example 1.XXXXXXX), takes the data calculates the size and executes the opposite bet.



the problem is very simple, I cannot cycle the elements obtained from trading.betting.list_current_orders () .__ dict__  using them as an array

*Tags: Strategies*

---

**sartux** - *15:38:12*

In documentation example, I am unable to transform the _data into an array and therefore I cannot read the odds and the size



```trading.betting.list_current_orders(customer_strategy_refs=['back_the_fav']).__dict__

Out[311]:

{'_data': {'currentOrders': [{'averagePriceMatched': 0.0,

    'betId': '142384852665',

    'bspLiability': 0.0,

    'customerStrategyRef': 'back_the_fav',

    'handicap': 0.0,

    'marketId': '1.150038686',

    'orderType': 'LIMIT',

    'persistenceType': 'LAPSE',

    'placedDate': '2018-10-26T00:46:46.000Z',

    'priceSize': {'price': 7.0, 'size': 5.0},

    'regulatorCode': 'MALTA LOTTERIES AND GAMBLING AUTHORITY',

    'selectionId': 21283271,

    'side': 'BACK',

    'sizeCancelled': 0.0,

    'sizeLapsed': 0.0,

    'sizeMatched': 0.0,

    'sizeRemaining': 5.0,

    'sizeVoided': 0.0,

    'status': 'EXECUTABLE'}],

  'moreAvailable': False},

 '_datetime_created': datetime.datetime(2018, 10, 26, 2, 14, 56, 84036),

 '_datetime_updated': datetime.datetime(2018, 10, 26, 2, 14, 56, 84036),

 'elapsed_time': 1.327456,

 'more_available': False,

 'orders': [&lt;betfairlightweight.resources.bettingresources.CurrentOrder at 0x23e0e7acd30&gt;],

 'publish_time': None,

 'streaming_unique_id': None,

 'streaming_update': None}```



*Tags: Strategies*

---

**Jorge** - *19:45:43*

Hey guys, does anyone know what is /usr/bin/fail2ban-server ?? I had problems with Python scripts using a lot of RAM memory and I have found out it is actually because of some executions of this....

*Tags: Performance, Deployment*

---

**Jorge** - *19:48:14*

Aha, and why would I have 5 processes running *python /usr/bin/fail2ban-server -xf start* in my server?

*Tags: Deployment*

---

**Mo** - *19:49:45*

I mean is it in a cloud service like AWS?

*Tags: Deployment*

---

**fjt1973** - *19:50:46*

Thanks for commenting.. I'll try uploading the libraries with my code as package to the function as opposed to using layers for my libraries. AWS is just something I've just starting to look at.

*Tags: Deployment*

---

**Jorge** - *19:51:50*

Ah so is it a security problem that I am getting people to try to connect to my VPC? I don't use AWS, it's a smaller provider

*Tags: Deployment*

---

**Mo** - *19:54:32*

I don't know what kind of capabilities your host provides but for example on AWS you can have a security policy that only allows access to the box from certain IP addresses

*Tags: Deployment*

---

**sartux** - *20:12:44*

```td = trading.betting.list_current_orders(lightweight=True)



for i,dd in td['currentOrders']:

    print(dd['marketId'])```

Where is the error?

ValueError: too many values to unpack (expected 2)

*Tags: Errors Debugging, Strategies*

---

**sartux** - *22:59:59*

do a check on the cycles you use, surely you can optimize the code by avoiding reading the same data several times.

help yourself using a database or simply text files and above all decrease the number of lines when you do the for, a technique could be to use a lot of filters

*Tags: General Technical*

---

**Newbie99** - *23:06:22*

Its not as simple as that, in that there is are multiple ways to 'cash out' depending on exactly what you're looking to do. There has been a bit of chat about being able to place an API call to the actual Betfair cash out (private) API, but its not in the docs, so there isn't too much info on it (other than a bit on this chat a few days ago).



It may be worth having a quick search as you can see some of the recent chats about cash out logic and then hopefully someone will have attempted the same approach as you and will be able to help.

*Tags: General Technical*

---

## 2020-06-29

**Jorge** - *09:28:56*

Anything more I can help regarding cashing out?

*Tags: General Technical*

---

**sartux** - *10:06:37*

```url = "[https://cashout-service.betfair.it/cashout-service/transactional/v1.0/cashout](https://cashout-service.betfair.it/cashout-service/transactional/v1.0/cashout) "

values = {'appKey': '#######',

          'marketId': marketId,

          'quotePercentage': 100,

          'quoteValue': 0}



data = urllib.parse.urlencode(values)

data = data.encode('ascii')  # data should be bytes

req = urllib.request.Request(url, data)

with urllib.request.urlopen(req) as response:

    the_page = response.read()

    print(the_page)```

  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py", line 649, in http_error_default

    raise HTTPError(req.full_url, code, msg, hdrs, fp)

urllib.error.HTTPError: HTTP Error 500: Server Error

*Tags: Errors Debugging, Deployment*

---

**sartux** - *10:06:40*

I tried and received the same error as yesterday (500 generic error)



I sent the POST variables via urllib, where am I wrong?

*Tags: Errors Debugging*

---

**sartux** - *10:23:29*

same problem with requests



{"faultcode":"Server","faultstring":"DSC-0002","detail":{}}

*Tags: Deployment*

---

**sartux** - *10:45:58*

at the moment I solved with the cashout formula, it is not accurate to the cent but it is not a problem, it is obvious that if cashout via POST worked it would be the best solution

*Tags: General Technical*

---

**sartux** - *16:44:47*

I have searched the documentation but cannot find an answer to the problem.



Is there a method that returns the percentage of the book to me or do I have to calculate it?

*Tags: General Technical*

---

## 2020-06-30

**sartux** - *09:38:52*

[@U0155J92A7Q](@U0155J92A7Q) I inspected the request in case of cash out and I saw that the operation can also be closed with GET variables, there is only one problem ...



[https://cashout-service.betfair.it/cashout-service/transactional/v1.0/cashout?_ak=######&amp;alt=json&amp;currencyCode=EUR&amp;marketId=1.170987782&amp;quotePercentage=100&amp;quoteValue=0](https://cashout-service.betfair.it/cashout-service/transactional/v1.0/cashout?_ak=######&amp;alt=json&amp;currencyCode=EUR&amp;marketId=1.170987782&amp;quotePercentage=100&amp;quoteValue=0)



it works only if you are logged in via browser. how could we solve it?

*Tags: General Technical*

---

**Unknown** - *10:47:45*

this is the problem!!!

*Tags: General Technical*

---

**sartux** - *11:46:44*

I found the error, I have to use 'averagePriceMatched'



can you tell me the types of orderType with the differences? I can't find the documentation

*Tags: Errors Debugging*

---

**Jono** - *12:05:58*

Hey everyone, just a quick one this time. I was wondering if it was possible to check games on the exchange for event_id, selection_id, ko time/date etc without certain betfair account details. Ive seen in a few of the examples in the lightweight repository that some operations dont require valid usernames, passwords, appKeys or sessionTokens to be entered (specifically examplestreaminghistorical.py and exampleracecard.py). Is it possible using betfairlightweight to alter the scripts I have already written to retrieve the above mentioned game details such that I no longer have to enter any credentials? Any help is greatly appreciated.

*Tags: General Technical*

---

**Jono** - *13:07:23*

Inside the racecard example on line 9 &amp; 10 where the trading instance is created it says that "don't need username/password" so was under the impression this was an example of connection to live data without credentials. As for why, i want to share some of my scripts with a friend and it would be convenient if there was a way of me sending him logic that would run without requiring inputs from them

*Tags: Deployment, Strategies*

---

**Jorge** - *14:54:47*

I am not following your questions at all, sorry, maybe others can help

*Tags: General Technical*

---

**Newbie99** - *17:21:09*

Sometimes, I get an error similar to the below:

*Tags: Errors Debugging*

---

**Newbie99** - *17:22:49*

`File "D:\Python37\webpages\racing_functions.py", line 1175, in &lt;listcomp&gt;`

    `runner_name = [runner.runner_name for runner in market_catalogue.runners if runner.selection_id == place_instruction_report.instruction.selection_id]`

`AttributeError: 'NoneType' object has no attribute 'selection_id'`



Appreciate that's not much use to anyone...but I think it is to do with when a runner is removed, so just wanted to do a logic check.



If a runner is removed, is the selection_id then removed from the market_catalogue, but remains in the market_book as non-active (well I know the second part, but its the market_catalogue behaviour I just wanted to check)?

*Tags: Errors Debugging*

---

**liam** - *17:53:26*

I don't think that is the problem, as a removed runner will stay in the book and catalogue, that error is in reference to your placeInstructionReport not having an instruction

*Tags: Errors Debugging*

---

**liam** - *19:20:03*

Big [https://forum.developer.betfair.com/filedata/fetch?id=32354|changes](https://forum.developer.betfair.com/filedata/fetch?id=32354|changes) in transaction charges 



```Transaction Charge Changes

Over the years, the Exchange has processed an increasing number of transactions at an ever-increasing cost to infrastructure and stability.

Therefore, from 1st September 2020, Betfair will be changing the thresholds of the Transaction Charge. The changes are as follows:

• The new limit of “free” transactions will be set at 5,000 transactions per hour. o A “transaction” shall include all bets placed and all failed transactions.

• Every transaction that exceeds the limit will incur a cost of £0.002 (a fifth of a penny). o This cost will be offset by a user’s commission generated.

• There will be no exemptions from this charge.

We have modelled the new charge to disincentivise wasteful transactional behaviour and promote platform stability on a level playing field. It is possible to trade multiple events and markets without ever incurring any of these charges.

Your account manager will be in touch if you would have incurred these charges in the past, giving you an opportunity to update trading strategies and fix bugs before the charge goes live on 1st September.```

*Tags: Errors Debugging, Deployment, Strategies*

---

**Pete** - *19:33:46*

and I can't even get someone on their help desks who knows what they are, or where I can see them on my account

*Tags: General Technical*

---

**liam** - *19:34:25*

Speak to Neil, bdp or account manager, never go through the help desk 

*Tags: General Technical*

---

**sartux** - *20:10:49*

but in this way I miss the withdrawal and therefore cause me the problem. I, on the other hand, would like to book the quota I choose and remain on hold, as I do manually

*Tags: General Technical*

---

**Dave Simonds** - *21:04:53*

I have zero problem with this if all the revenue generated goes towards making their infrastructure a bit more stable. Nothing sucks more than their servers/service being up and down like a hookers knickers

*Tags: Deployment*

---

## 2020-07-01

**Mo** - *14:13:31*

That won't help and you should ask [@U4H19D1D2](@U4H19D1D2) to get your key activated for free. But best price execution can and will still happen and it is still a positive thing for you

*Tags: General Technical*

---

**Mark** - *14:34:55*

I'm currently editing the examplestreaminghistorical.py file in BFLW to output a little more data. I've figured out most of what I need, but can't for the life of me find the last traded value. Of course the last_price_traded for each runner is already there, but where do I find the actual amount traded? I think it corresponds to the TRD value in the BF historical data file.



```runner_def = runners_dict.get(

                    (runner.selection_id, runner.handicap))

                output.write(

                    "%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s\n"

                    % (

                        market_book.market_id,

                        market_book.publish_time,

                        market_book.status,

                        market_book.inplay,

                        market_book.total_matched,

                        runner.selection_id,

                        runner.ex.available_to_back[0].price,

                        runner.ex.available_to_back[0].size,

                        runner.ex.available_to_lay[0].price,

                        runner.ex.available_to_lay[0].size,

                        runner.total_matched,

                        runner.last_price_traded or "",

                    )

                )```



*Tags: Data Quality*

---

**Jonatan (skyw)** - *14:59:46*

Don't know if it's the bflw solution but you can access the prices traded in the stream update. I think its available in each market book.

*Tags: General Technical*

---

**liam** - *15:01:21*

If you use your debugger, you can see it in

```runner.ex.traded_volume```

*Tags: Errors Debugging*

---

**Jonatan (skyw)** - *20:04:06*

Yeah I just can't see how that would help me if they limit the number of logins :)

*Tags: General Technical*

---

## 2020-07-02

**FlyingFish** - *12:19:57*

Hi everyone,

decided to switch from R to python with BFLW and trying a very basic bot.

What is the best approach to place bets near the off on a selection of races?

In R i use taskScheduler but was wondering if there is a more efficient way to accomplish this only with BFLW/Flumine.

*Tags: General Technical*

---

**Newbie99** - *14:29:05*

[@U0147CTRM8E](@U0147CTRM8E) do you mean streaming by chance, you can then kick things off in the morning and leave it running all day (or longer)?

*Tags: General Technical*

---

**Oliver Varney** - *14:44:56*

Question on flumine, can strategies be added / removed after run has been called on live markets?

*Tags: Deployment*

---

**liam** - *16:25:55*

[@U0147CTRM8E](@U0147CTRM8E) you can use either, flumine obviously does a lot of the heavy lifting for you but streaming makes this easy as you can just subscribe to your markets and place when requierd

*Tags: General Technical*

---

**Oliver Varney** - *16:54:19*

I guess the use case is how do I add a new strategy without stopping the process and restarting it? What would happen if I had positions against a strategy? I havent checked the code but would these reload?

*Tags: Strategies*

---

**FlyingFish** - *18:02:46*

Managed to do it with looping the markets list and time.sleep() but looking at it now and the streaming really looks like a better option.

Streaming won't be static, as you mentioned, and will allow for expansion to other bots in the future.

*Tags: General Technical*

---

**Mo** - *18:04:34*

Yes, streaming is the way to go

*Tags: General Technical*

---

**liam** - *18:05:37*

There are some limitations but you can restart and the live order will be pulled into the strategy on restart 

*Tags: Deployment, Strategies*

---

## 2020-07-03

**Mo** - *07:32:42*

Is the Bot Executor tool implemented in flumine?

*Tags: General Technical*

---

**liam** - *10:01:51*

[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1591956822252700|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1591956822252700](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1591956822252700|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1591956822252700)

*Tags: General Technical*

---

**Eswyn** - *10:11:03*

Hi all, I'm working on a script that will run on a server 24/7. Streaming_market_filter doesn't accept market_start_time, so what is the best way to filter the stream to get only the races starting in the next 24 hours at any given time?

*Tags: Deployment*

---

**Lee** - *10:16:45*

(when i'm trying to word these questions it's giving me ideas of places i might be going wrong / things to test)

*Tags: General Technical*

---

**Eswyn** - *10:16:58*

Absolutely, but the pre-race strategy I want to use is just for the 24 hours before the race starts, hence why I'm trying to filter the stream

*Tags: Strategies*

---

**liam** - *10:17:37*

You would have to filter by market or eventId but streaming is so cheap I wouldn't bother, just ignore the ones you don't want

*Tags: General Technical*

---

**Eswyn** - *10:19:08*

Right, but how do I identify the ones I _do_ want? Can it be done with flumine?

*Tags: General Technical*

---

**liam** - *10:20:16*

[https://github.com/liampauling/flumine/blob/master/flumine/markets/market.py#L98|yep](https://github.com/liampauling/flumine/blob/master/flumine/markets/market.py#L98|yep)

```market.market_start_datetime```

*Tags: General Technical*

---

**Eswyn** - *10:30:09*

And flumine will check the condition for every stream update, right?

*Tags: General Technical*

---

**Mo** - *15:29:58*

Use the undocumented inplayservice endpoint: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py)

*Tags: General Technical*

---

**PeterLe** - *18:41:39*

Ive been doing something similar this afternoon looking at the results of back bets for a revision of a strategy

Id be interested in getting the opinion of someone who knows more about stats than I do.

If you were to compare your actual results across a number of bets (in my case there were 739 back bets, lets call this season 1) and then run it in a monte carlo simulation for a 1000 seasons (uses a random generated number and compares that against the IP of the odds matched)

Isnt this a good way of determining if you are generally getting obtaining value?

In my case, my 'season' was better than the Monte Carlo results 93% of the time which is a very good indication of value and might fit what lee is trying to achieve?

(PS I cant take the credit for this method as it was introduced to me by JamesT but i use it a lot now, Lee happy to share my spreadsheet with you if it would help you? You would need a decent sample size though)

*Tags: Strategies*

---

**Michael** - *19:33:43*

If I understand your situation correctly then you are trying to evaluate a semi-hypothetical strategy – in other words what would have happened if you had done things differently. If this is the case then rather than looking for statistical tests it might be be better to flip the problem around and evaluate your methodology for generating the revised strategy. Come up with a simple estimate of how likely your methodology was to produce a false result and take your confidence to be 1 – that. For example if you somehow filtered your past results until they showed a profit then your confidence in that result should be very low because your methodology was extremely likely to show a false result. The more different filters you tried the less likely it is that you've come to a representative result so you'd adjust accordingly. Likewise an incidental discovery should be treated with great scepticism. On the other hand if you understood the situation in some new way, had one new idea and an evaluation of that idea appeared to confirm it then you're on much better ground.

*Tags: Strategies*

---

**Michael** - *19:45:17*

I think people sometimes want to use statistical tests in situations when they're neither necessary nor helpful. Before you apply a test (especially one you don't really understand) I think you should stop and ask yourself: Will the outcome of this test make any difference to my future actions regarding this strategy? If you've made a profit after 1000 markets then you're going to keep going whatever the tests say and if you haven't then you're not going to keep going for much longer whatever the tests say. So what are the tests for? So that you can feel more confident? So that you can kid yourself into raising your stakes when you already know perfectly well that you shouldn't? If you're betting at normal odds then 1000 markets is usually plenty to get a good feeling of the efficacy of your strategy. Job done.

*Tags: Strategies*

---

**PeterLe** - *22:23:57*

Thanks Michael, Yes some of that does resonate with me. Ive been doing this successfully now of 14 years and Ive never used statistics to form a strategy. Ive always used small stakes (on a sensible theory) then test, hone and refine as I go along. What the MC simulation is good at is for confirmation. How many times do you see people showing charts that have run for many events with a nice upwards equity curve only to find out further down the line that it doesn't work after all.

*Tags: Strategies*

---

## 2020-07-04

**Michael** - *07:17:15*

Just for the avoidance of doubt - I'm not saying that statistical tests are never helpful, just that sound thinking should be applied to what techniques are appropriate to the situation,

*Tags: General Technical*

---

**Michael** - *12:38:41*

_"How many times do you see people showing charts that have run for many events with a nice upwards equity curve only to find out further down the line that it doesn't work after all."_ The reason for this is usually that the 'winning' run was some kind of back test and the downturn is when they start betting. It happens because of the way they generate their strategies. What people tend to do is trawl every permutation they can think of until they get something that looks like a win. If you do this for long enough you will inevitably find something that looks like a solid winning strategy when all you're really seeing is a random bit of variance. You find what you look for. What I suggest above about examining ones methodology would rid one of this delusion but who wants to hear it? In my experience good strategies win from the start and just keep winning or else peter out gradually. Unless something fundamental in the markets you're operating in changes they don't go bad all of a sudden. The best way to have confidence in your strategies is to have well founded confidence in yourself. If your last strategy was good then your next one has a good chance, and if it wasn't, well, don't be too confident.

*Tags: Strategies*

---

**Michael** - *12:39:54*

Also - no one who knows a feckin thing posts a graph of their new strategy's winning run.

*Tags: Strategies*

---

**Michael** - *22:30:11*

Any type of back-testing will tend to exaggerate the edge that you will get on real bets because the strategies that look best from the back-test are also likely to be the ones that benefited from random variation in that data set. There are techniques to minimise that and more data always helps but you can never fully escape it. You may well have had better ideas that you discounted because they had an unlucky run in your back-test data but how would you ever know? If you're going to generate a strategy purely out of back testing then you need to make sure it has much more edge than you need to make a profit in order to cover that discrepancy.

*Tags: Strategies*

---

## 2020-07-05

**sartux** - *09:26:23*

Do betfair APIs also provide match statistics? (shots on goal, total shots, possession of the ball, etc.)



If they don't provide them, what do you recommend that I can use with the bot?

*Tags: General Technical*

---

**sartux** - *09:28:08*

the controls are very simple, 2-digit rounding and, for Italy, a minimum bet of € 2



so with an IF and a Round you solve the problem

*Tags: General Technical*

---

**SrFabio** - *09:58:56*

Hey that doesn't answer my question :wink:

*Tags: General Technical*

---

**SrFabio** - *10:01:38*

You can have a look at betfair exchange historical data ([https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home)). If you are looking for inplay stats then you'd have to use a third-party like opta (I've not used any myself so can't really recommend)

*Tags: Data Quality*

---

**sartux** - *10:01:48*

why do you have to face this problem if you can solve it from the beginning?

*Tags: General Technical*

---

**sartux** - *10:54:49*

yes, but the problem is solved trivially by increasing the size or



IF coverSize &lt;minBook:

   coversize = minBook



it's a no problem :)

*Tags: General Technical*

---

**Mo** - *14:29:13*

The documentation explicitly states direct strikes to the tracker from the whip can generate those kind of readings

*Tags: General Technical*

---

**D C** - *14:30:24*

Any chance you could point me in the direction of that documentation? I've not come across any reference to that at all so far. I wonder what else I am missing now when it comes to the "quirks" of the system.

*Tags: General Technical*

---

**Mo** - *14:32:02*

As someone who is relatively new to the feed, it was amazing how many times a new piece of documentation suddenly materialised that contained critical information

*Tags: Getting Started*

---

**Mo** - *14:35:49*

Ah that piece of information came from the GitHub repo README: [https://github.com/TotalPerformanceData/gmaxfeed](https://github.com/TotalPerformanceData/gmaxfeed)



&gt; Whip strikes can cause sudden spikes in the data, velocities hitting near 50m/s and skewing the X,Y way off the track, there's nothing we can do about this as the trackers are padded as much as the weight/size guidelines will allow.

*Tags: Performance*

---

**D C** - *14:39:45*

Ah right that explains why I have never seen this. I never knew of the existence of this github but I guess that is what happens when you go via intermediaries. Wonder what else I am missing. Many thanks indeed for that info though [@UBS7QANF3](@UBS7QANF3) that helps explain quite a few of the weird things I have seen on the feed.

*Tags: General Technical*

---

**sartux** - *15:14:28*

I know, but for my size i dont have problem

*Tags: General Technical*

---

## 2020-07-07

**birchy** - *19:48:03*

Hi all,

I'm new to bflw/flumine but not new to betfair api/python. Have been running bots since around 2004 - back then I had VB6 and a carefully crafted web scraper. Here in 2020, and after using my own library ([http://BespokeBots.com|BespokeBots.com](http://BespokeBots.com|BespokeBots.com)) for many years, I've decided it's time to get to grips with this new-fangled streaming API malarkey. Was going to code my own library but there is no good reason for me to do so when [@U4H19D1D2](@U4H19D1D2) has already created this marvellous resource. I'm on a bit of a learning curve at present because Liam has written "modern" Python code, whereas mine is very old fashioned and only uses very basic implementations. Having said that, my simple library has served me well for many years and has more than paid for itself.



Moving forward, I've run a couple of the bflw/flumine examples (copy/paste code!) and have login/etc working as expected. Planning on mining some data initially to get familiar with the syntax, pitfalls, etc. Can anyone suggest the best way to go about mining data with this library? Also of interest, what format is required for backtesting?



Regarding data mining, has anyone considered combining data as a pooled group? i.e. if "N" people were all mining data, the combined efforts would help to eliminate those "missing" sections that we encounter when we lose connection. Also, if we were mining ALL available markets at a sensibly throttled speed, the combined data would achieve a much better granularity, simulating something close to what we would get if we were concentrating on specific markets at balls-out speeds. This would require a pre-agreed structure, but is easily solved if all members use the same framework. Is this do-able, or does it contravene some clause in Betfair's T&amp;C's?

*Tags: Getting Started, Performance*

---

**Mo** - *19:52:58*

It sounds like you might be conflating the term mining data with scraping, in which case your comments on throttled speed are irrelevant with streaming because there is no polling involved

*Tags: Performance*

---

**Mo** - *19:54:13*

If you're starting from scratch (in a Python sense), flumine is the way to go for both recording data and backtesting but I've personally never used it so I'll let others comment on that

*Tags: General Technical*

---

**birchy** - *19:59:06*

I've been polling data for the last 16 years, so please forgive my stupidity. :grinning:

TBH, I've not read the streaming docs in any detaiI, but was of the impression that "conflate_ms" was basically a self-defined throttle?

Flumine seems to be the way forward for a "new" bot and I know for a fact that my code libraries are too far detached to be able to integrate easily, so I'll be starting from scratch with this new "tool".

*Tags: General Technical*

---

**liam** - *20:00:25*

Hi Birchy, I used your library when I first started so thanks for sharing that :grinning: Regarding backtesting flumine takes streaming data you record yourself and anything you purchase from betfair 

*Tags: General Technical*

---

**birchy** - *20:03:32*

[@U4H19D1D2](@U4H19D1D2) I *thought* you did, but wasn't sure. I remember us all discussing betting shit on that crappy Lefora forum many years ago. I'm still admin of that site but only because I don't know how to abandon it. lol.

Nice work by the way, well impressed. In fact so impressed that I don'y know WTF I'm doing with it.

*Tags: Strategies*

---

**liam** - *20:09:27*

Yeah I certainly visited your forum, bflw can be a bit complicated when reading the code as it’s designed to be easy to use whilst matching the api endpoints. If you are used to polling then streaming is a complete breeze! Flumine is very much in beta so welcome any feedback, I know the docs need to be improved but there isn’t that much code to read through if you want to work out how it works 

*Tags: General Technical*

---

**birchy** - *20:17:27*

Yeah, Flumine certainly makes life easy! I implemented streaming in a matter of minutes, whereas adding it to my library would of taken a few days of coding/testing. TBH, it wasn't obvious at first what Flumine actually did when reading the docs. I initially thought/assumed that it was a mining library based on the name: flu-mine.

*Tags: General Technical*

---

**birchy** - *20:24:10*

Plus my understanding of Python is very basic as I've never had any formal training, so only used what I actually needed. I'm such a dinosaur, that I only looked into threading about a week ago!

*Tags: General Technical*

---

**liam** - *20:52:34*

Great, yeah readme can be improved on what it does and doesn’t do, I think the real selling point is being able to move from backtesting to live with no changes to your strategy (betfair are still giving 2020 q1 data away for free if you want to try it out) Classes can certainly help abstract away the low level stuff, I find reading code is the best way to find new ways of doing stuff (and sometimes how not to do it!) 

*Tags: Deployment, Strategies*

---

## 2020-07-08

**PeterLe** - *10:50:01*

[@U016TGY3676](@U016TGY3676), I think many of us have visited your website over the years, so looking forward to your participation on here

I read your comments on being a fan of £2 bets rather than larger bets, Im the same and wondered if others had found this too.



I run multiple accounts with a slight variation of a theme and recently I was contemplating whether to simply create more accounts (and effectively clone the strategies) and stick to lower stakes £2 stakes (In play for the back strategies) or increase the stakes on my existing accounts. 

Around 2011 to 2013/14 I was using stakes circa 17 times bigger than I do today and the bulk of my lifetime profits are from this period.

I know you have been around a while; I was just curious was to find out how your staking had changed (if any) over the years. Interested to hear others comments too, Thanks (PS if the answer is “Backtest with Flumine, Im trying to implement!! :grinning:)

*Tags: Multi Client*

---

**liam** - *11:00:10*

[@UQL0QDEKA](@UQL0QDEKA) you don't need to use different accounts just duplicate the strategy but with the same size stakes, even ms in differences in placing/cancelling orders is enough to limit the market impact of a strategy inplay

*Tags: Strategies*

---

**PeterLe** - *11:06:59*

Thanks Gents, Yes, Ive found that too [@U4H19D1D2](@U4H19D1D2), running the exact same strategy, same settings, can have different results at the end of the day (there is no clear pattern when you compare the betid’s against accounts). Just thinking out loud really; Im looking at the options available to me to make best use of the master/sub account set up. I guess that’s what keeps us all interested, because there is always something else to test :grinning:

*Tags: Strategies*

---

**Mo** - *11:08:26*

But nevertheless you could expect the logic of the strategy on the other side to react differently

*Tags: Strategies*

---

**Oliver Varney** - *11:42:27*

morning, question on flumine, whats the best way to check exposure match and unmatched orders for a runner

*Tags: General Technical*

---

**Oliver Varney** - *12:34:55*

another question also, is the variable stop in trade meant to be used as a trade size variable?

*Tags: General Technical*

---

**liam** - *12:43:53*

It’s open to abstraction, you could handle this either in the trade itself .create_order (check others or raise error) or maybe in strategy.validate_order 

*Tags: Errors Debugging, Strategies*

---

**birchy** - *18:07:39*

[@UQL0QDEKA](@UQL0QDEKA) Interesting that you mentioned your main profits were in the period 2011-2013, because I experienced exactly the same thing! I was averaging £3K a month profit for about 15 months in 2011/2012 from a single £2 strategy that was basically placing Lay bets at silly prices on low liquidity markets. I did once lose around £600 in a space of 2 hours (at £2 a bet!) and it turned out that someone had worked out how to exploit my strategy on Aussie greyhound races at 4am when everyone else, including myself, was fast asleep. Soon patched that up and continued to profit for several months after, but then the bot slowly started to decline and I pulled the plug when my P&amp;L graph started to flat-line. Using a graph to plot the P&amp;L showed a definite change in the markets. Tried lots of fine-tuning, etc but never got back to that golden period. Betfair launched the Sportsbook in 2012, so I can only assume that the "mugs" I was profiting from were attracted by the Lucky 15's, 6-fold accumulators, etc.



Regarding the £2 thing, I've found that 5x £2 bets are more likely to get matched than one £10 bet. Doesn't make much sense because £10 is a tiny drop in the ocean, although it may just be a psychological thing and in fact £10 is £10, whether it be in one lump or 5 parts...

*Tags: Performance, Strategies*

---

**birchy** - *18:33:05*

[@U4H19D1D2](@U4H19D1D2) you mentioned the free historical data last night...now I've added everything available to my "purchases" but am having a ball-ache downloading it. Seems like they're throttling the upload and my calcs suggest it's going to take about a week to download it all through Chrome browser. Is there a quicker way to download and/or save/resume?

*Tags: Data Quality*

---

**liam** - *18:37:43*

You mean your not using bflw? [https://github.com/liampauling/betfair/blob/master/examples/examplehistoricdata.py|https://github.com/liampauling/betfair/blob/master/examples/examplehistoricdata.py](https://github.com/liampauling/betfair/blob/master/examples/examplehistoricdata.py|https://github.com/liampauling/betfair/blob/master/examples/examplehistoricdata.py)

*Tags: General Technical*

---

**user34** - *18:43:12*

Did anyone find any particularly useful resources for how to get betfairlightweight running on AWS? I've been looking through the documentation provided by Amazon and am a bit lost.

*Tags: Deployment*

---

**Unknown** - *18:49:59*

[@U016TGY3676](@U016TGY3676) [@U4H19D1D2](@U4H19D1D2) I remember that they trialed Cross matching in March 2010 and then they switched it on for Lingfield on Tuesday 23 rd Nov 2010. I remember the day very clearly as I thought it would be the end of the gravy train :grinning:, I scaled my stakes down to the minimum on Lingfield that day and kept a screen shot of the very first race :grinning:, so it wasnt XM that started the decline for me. Birchy, if it helps inspire you, they are some of the guys/girls on here that are making 5 figures and even 6 figures a month, so stick with it :grinning: (Cant believe that was ten years ago?)

*Tags: General Technical*

---

**user34** - *19:01:58*

I have a script that is triggered a certain amount of time before an event starts and then places my bets if the appropriate conditions are met. I currently just collect trigger times in advance and then leave python running on my PC to process the data and place the bets. I guess I would need AWS Lambda for the triggering, something (Elastic Beanstalk?) for the processing, some way to interact with Betfair using my API key, and some way to tie it all together. Sorry that this is such a broad question.

*Tags: Deployment*

---

**liam** - *19:03:06*

Just run an ec2 server 

*Tags: Deployment*

---

**birchy** - *19:15:23*

Ditto the above...a server/cloud is just another pc. The code you run on it is no different than what you run on your pc at home.

On a EC2 Linux instance, launch the script from a command terminal with '&amp;' at the end, so that it detaches from the terminal, allowing you to close the terminal and logout.

*Tags: Deployment*

---

**birchy** - *20:05:48*

[@UQL0QDEKA](@UQL0QDEKA) was just thinking...you mentioned earlier that you use multiple accounts. I have 2 accounts, but only because I use one for testing and the other for my production bots. I don't have any issues with multiple bots placing bets on the same markets. It used to be beneficial to use one account as it boosted the commission discount, but of course they have the 2% package now. Do you not use the strategyRef parameter to identify bots?

*Tags: Deployment, Strategies, Multi Client*

---

**PeterLe** - *20:18:51*

No I dont use strategyRef parameter, without getting too deep.. I have the option of creating many systems with each account (the gent who created this for me Ive known a long time now and he is a brilliant programmer c#). So for each account, say I have five strategies, the first maybe £2.00 stakes, the second £2.01, the third £2.02 etc...So its easy to see what's working and whats not.  Id strongly agree with Liam's sentence above - "...just play the numbers game".  Im here because I want to learn this for myself and Python seems easier than c#, plus there are many generous people on here keen to help, hopefully I can contribute more in the future

*Tags: Strategies*

---

## 2020-07-10

**Robert** - *10:58:55*

Hi all, I’m a software developer at a sports algo trading company which uses this slack group and the betfair library. We’re currently hiring and I thought this might be of interest to some of you.  Based in Cambridge, UK. You can find out more at [http://www.seamlessml.com|www.seamlessml.com](http://www.seamlessml.com|www.seamlessml.com) or email me at [mailto:robert@seamlessml.com|robert@seamlessml.com](mailto:robert@seamlessml.com|robert@seamlessml.com) if you have questions.

*Tags: Strategies*

---

## 2020-07-12

**Oliver Varney** - *09:33:22*

Liam, might be incorrectly reading the code, but just looking at the flumine example on the github home/front page, I think you must have added market: Market as an additional param to cancel /update / place in BaseStrategy. the example passes in order in the first param position, which is now the market.

*Tags: Strategies*

---

**Oliver Varney** - *10:03:38*

in BaseStrategy

*Tags: Strategies*

---

## 2020-07-13

**Evaldas** - *13:10:28*

Hi,

Does anyone know someone on betfair, that could help to speed up reopening of my closed account?



Not doing anything against rules here, but my account got closed on Thursday, left me with 3k+ of exposure that day, funds now suspended, no explanation, no warnings, and didn't get a single replay to emails since Thursday..

Helpdesk explained that I need to provide proof of funding, which is laughable as I'm fulltime bf trader and was about to became PC2 payer this month. I did withdraw all my funds during quarantine, and then slowly deposited it back, which is I guess what triggered closure. But I'm being ignored now, and no idea what to do.. Already sent all my statements, showing deposits to betfair, cards photos etc..

*Tags: Performance*

---

**liam** - *13:20:40*

yeah Neil probably wont be able to help, you could try the generic premium mailbox [mailto:priority@betfair.com|priority@betfair.com](mailto:priority@betfair.com|priority@betfair.com)

*Tags: General Technical*

---

**Oliver Varney** - *17:11:22*

[@U4H19D1D2](@U4H19D1D2) am I right in thinking non runner adjustments in flumine are only not implement for simulated markets only? Orders on live markets should update?

*Tags: Deployment*

---

## 2020-07-14

**Mo** - *14:28:54*

`HistoricalGeneratorStream` calls `self.listener.snap()` which means the `streaming_update` field of the `MarketBook` is `None`. Is there a canonical way to ensure that these updates are on the `MarketBook`s?

*Tags: General Technical*

---

**Mo** - *15:10:19*

I've done this:



```class StreamingUpdateListener(StreamListener):

    streaming_update = None



    def snap(self, market_ids: list = None) -&gt; list:

        result = super().snap(market_ids)

        if result:

            if self.lightweight:

                result[0]['streaming_update'] = self.streaming_update

            else:

                result[0].streaming_update = self.streaming_update

        return result



    def _on_change_message(self, data, unique_id):

        self.streaming_update = data['mc'][0]

        super()._on_change_message(data, unique_id)```

*Tags: General Technical*

---

**liam** - *15:15:52*

lol I forgot a new listener is created on each run, so was questioning the use of `[0]`

*Tags: General Technical*

---

**liam** - *15:19:53*

`streaming_update = None`

*Tags: General Technical*

---

**liam** - *15:23:01*

if that was set as a list you have a problem but again a new listener is created each time so shouldnt be a problem

*Tags: General Technical*

---

**liam** - *15:27:26*

yeah class variables are dangerous, can be hard to debug

*Tags: Errors Debugging*

---

## 2020-07-15

**Oliver Varney** - *16:22:08*

looks like BaseControl._on_error call order.violation, maybe a good place to add a param in order?

*Tags: Errors Debugging*

---

**liam** - *16:24:39*

[https://github.com/liampauling/flumine/blob/master/flumine/baseflumine.py#L143](https://github.com/liampauling/flumine/blob/master/flumine/baseflumine.py#L143)

*Tags: General Technical*

---

## 2020-07-16

**JC** - *12:52:08*

Hi guys, pretty new here and really enjoying getting to learn the ropes of working with the API. Thanks to the brilliant libraries of bflw and flumine and previous queries in this chat I have managed so far to implement an S3 market recorder running on an EC2 instance. I am now looking to capture as much other available data about the market and runners as possible i.e. metadata available via `list_market_catalogue` , as well as some summary data such as total amount traded at start and end, so that I can query and filter the files on S3 using this. I would also like to record a stream for in-play scores data for tennis. Seems like I should set up a simple script for the former on a time loop which can update and feed my database. For the scores streaming, is there a way to do this in flumine? Any advice would be very welcome on either of these. Thanks again for these libraries and for the active community here :grinning:

*Tags: Data Quality, Deployment*

---

**Mo** - *13:51:04*

For tennis scores, you have two options. For ATP and WTA main tour events, Challengers and Grand Slams excluding Australian Open you can use the “official” scores endpoint: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/scores.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/scores.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/scores.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/scores.py)



For everything else such as ITF, exhibition matches, Hopman Cup, Davis Cup etc you can use the undocumented inplayservice: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py)

*Tags: General Technical*

---

**Misha** - *14:02:40*

Last question on the scores: only a JSON RPC endpoint? No REST?

*Tags: General Technical*

---

**Misha** - *14:05:05*

Different app key than streaming?

*Tags: General Technical*

---

**JC** - *14:49:00*

So do you think it would be best to put a marketCatalogue data request as part of the flumine set up or just as a separate process? How do you access the betgenius feed?

*Tags: General Technical*

---

**Mo** - *15:36:51*

That's right. I think for licensing reasons, they are not allowed to make the official endpoint streaming. I have never got a clear answer from Neil how frequently we're allowed to poll it but I would assume every 0.2 s which is the limit for listMarketBook.



The website hits the inplayservice endpoint every 10 seconds so polling it more frequently than that could be considered risky. I've personally not had any push back doing it once a second.

*Tags: General Technical*

---

## 2020-07-17

**Oliver Varney** - *07:47:24*

morning chaps, I have a variety of the pro files in both zip file format and bz2 formats. How do people go about opening these / patching without extracting in bflw /flumine. Is there some kind of param im missing or libraries work using ?

*Tags: General Technical*

---

**sartux** - *08:45:27*

the "problem" of the best price has been solved, I wanted to create a function that simulates the betfair cashout. Then an exit cycle until the win / loss is balanced in all selections



how do you cash out?

*Tags: General Technical*

---

**Oliver Varney** - *08:46:02*

[https://help.smarkets.com/hc/en-gb/articles/115001431011-How-to-calculate-a-hedge-bet](https://help.smarkets.com/hc/en-gb/articles/115001431011-How-to-calculate-a-hedge-bet)

*Tags: General Technical*

---

**sartux** - *09:19:40*

I'm a very scrupulous programmer, maybe it's just my mental problem hahahah

*Tags: General Technical*

---

**sartux** - *09:59:41*

you're absolutely right :slightly_smiling_face:



can I ask you one last thing?

I am automating the generation of the daily report, which method should I call?



I tried with these 4 lines but I can't get the market result in economic terms



On every single game run between 20 and 30 bets and I just wanted to load the list of the day and then make an aggregate and add



get_account_statement gives me different information but not the outcome



```'idx','avgPrice','betSize','betType','betCategoryType','commissionRate','eventId','eventTypeId','fullMarketName','grossBetAmount','marketName','marketType','placedDate','selectionId','selectionName','handicap','startDate','transactionType','transactionId','winLose','deadHeatPriceDivisor','avgPriceRaw'```



`four_days_ago = (datetime.datetime.utcnow () - datetime.timedelta (days = 4)). strftime ("% Y-% m-% dT% TZ")`

`acct_statement_date_filter = betfairlightweight.filters.time_range (from_ = four_days_ago)`



`account_statement = trading.account.get_account_statement (item_date_range = acct_statement_date_filter)`



`class_data = [json.loads (account_statement.account_statement [i] .item_class_data ['unknownStatementItem'])`

     `for i in range (len (account_statement.account_statement))]`

*Tags: Strategies*

---

**Mo** - *12:08:55*

This is what I do:



1. Maintain a database table for each bet status (SETTLED, CANCELLED, VOIDED, LAPSED)

2. Every hour, for each bet status, find the most recent settledDate from the database table

3. Use listClearedOrders to get all orders from (and including!) settledDate onwards

4. Upsert the orders into the database

*Tags: General Technical*

---

## 2020-07-18

**Unknown** - *12:30:00*

I ask you a trivial question, is there a function to know the current profit on a market?

Example

*Tags: General Technical*

---

**Dave** - *13:21:03*

[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1592159363298400](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1592159363298400)

*Tags: General Technical*

---

**Dave** - *14:10:14*

(or to the mid if the spread is too wide, but figuring out the "true" mid is a complex problem in itself)

*Tags: General Technical*

---

**Dave** - *19:31:30*

I personally have not used it, sorry! This message ( [https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1592148816290700|Click here](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1592148816290700|Click here) ) has some of the raw endpoints as well, but really all you need to do is replicate what the browser is doing. I'm not sure if it only works with the default website appKey though. I'll let others who've used it chime in though, hopefully they can point you in a better direction.

*Tags: General Technical*

---

## 2020-07-19

**Mo** - *08:11:20*

[@U4H19D1D2](@U4H19D1D2) - have you considered using `TypedDict`s? [https://www.python.org/dev/peps/pep-0589/](https://www.python.org/dev/peps/pep-0589/)

*Tags: General Technical*

---

**Unknown** - *11:18:25*

[@UBS7QANF3](@UBS7QANF3) exit problem solved :slightly_smiling_face: I won the battle against rounding ahhahah

*Tags: General Technical*

---

**liam** - *14:20:52*

Used to have separate but slowly moving to a single app via [https://github.com/liampauling/flumine|flumine](https://github.com/liampauling/flumine|flumine) 

*Tags: Performance*

---

**Ruben** - *23:23:58*

won't Python's GIL make it indifferent to log in the same thread vs in another one?

*Tags: General Technical*

---

**Misha** - *23:28:02*

I don't use Python (but find this group a useful resource), but if you try to log a lot of data to disk, say for example streaming updates, the disk IO can hold up normal processing. For small writes its find to do in sequence, for larger/more frequent writes its better to do in another thread

*Tags: General Technical*

---

## 2020-07-20

**liam** - *11:48:08*

[@ULDAVFDRP](@ULDAVFDRP) got a branch that can handle non runners, voids and updates prices, you able to give it a test your side? [https://github.com/liampauling/flumine/pull/239](https://github.com/liampauling/flumine/pull/239)

*Tags: General Technical*

---

**Michael** - *12:50:58*

Both things happen frequently - runners are removed in the last few minutes with suspension and re-forming and others are removed from the market after the race after having refused to start or caused a problem of some kind. This is not new.

*Tags: General Technical*

---

**liam** - *12:55:16*

[@U016535QCJ2](@U016535QCJ2) I think you may be misunderstanding the problem i am trying to solve, this is in regards to backtesting / simulated order matching and pricing after a removal

*Tags: General Technical*

---

**Oliver Varney** - *14:29:34*

[@U016535QCJ2](@U016535QCJ2) problem is if your simulating a hedge against a non adjusted entry trade then your numbers are way off. Also if you have a concept of min price you will accept in a strategy that also needs adjusting.

*Tags: Strategies*

---

**liam** - *18:24:29*

Btw paper trading is now implemented so you can simulate trading against live markets, makes debugging matching a lot easier as you can use the betfair site 

*Tags: Errors Debugging, Deployment, Strategies*

---

**steve** - *18:43:39*

does anyone know of an app to run python on mobiles? need to web scrape too

*Tags: General Technical*

---

**Mo** - *18:50:54*

Pythonista

*Tags: General Technical*

---

**steve** - *19:08:14*

[@UBS7QANF3](@UBS7QANF3) pythonista looks perfect but seems to be ios only. any suggestions for android? thanks in advance

*Tags: General Technical*

---

**Twatter** - *20:27:27*

Hi All.. Stumbled upon the betfairlightweight library recently, and so just starting out trying to write my own Betfair bot. Slowly trawling through the history of these slack channels to pick up some info. Quick Q - I got an idea for a strategy that involves intra market trading across a horse races Win/EW/Place(s) markets. I'm confused as to whether I should be coding it with Flumine, or Betfairlightweight? What's the guide on choosing Flumine or just rely on betfairlight?

*Tags: Performance, Strategies*

---

**Mo** - *20:29:42*

If you’re starting out go with Flumine

*Tags: General Technical*

---

**liam** - *20:31:27*

Yeah need to improve the readme on flumine, bflw is a wrapper for the API and flumine is a fully functioning trading framework which happens to use bflw 

*Tags: Strategies*

---

**JonJonJon** - *21:18:08*

Is there problems viewing P&amp;L today? I can't see anything more than 3 hrs ago

*Tags: General Technical*

---

**birchy** - *22:22:34*

I'm just getting to grips with Flumine in my (limited!) spare time and have got MarketRecorder running. Just wondering what the recommended method is for filtering markets such as "if tv &gt; 50K"? Or would you generally capture ALL data? Or perhaps save only data 15 mins til start?

*Tags: General Technical*

---

## 2020-07-21

**Unknown** - *10:31:16*

[@U016TGY3676](@U016TGY3676) +1 on s3, here is my monthly bill for a shit load of streaming data (compressed)

*Tags: General Technical*

---

**JT** - *13:04:10*

Hi Mo, sorry to be jumping on this thread. I have been trying to get scores on tennis matches that are showing scores on the website, but seem to be getting 503 / connection errors. I am using the

*Tags: Errors Debugging*

---

**Mo** - *14:27:13*

What version of bflw?

*Tags: General Technical*

---

**JT** - *14:30:13*

Just realised the problem, for some reason, the url it was querying was v1.1. Changing it to v1 resolved it.

I am

*Tags: General Technical*

---

**JT** - *14:31:47*

It seems the current library is setting it to 1.1

Line 133 ([https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py))

@property

def url(self) -&gt; str:

return "[https://ips.betfair.com/inplayservice/v1.1/](https://ips.betfair.com/inplayservice/v1.1/)"

*Tags: General Technical*

---

**JT** - *14:35:10*

Ok. That works as well. I probably need to upgrade my bflw version. Thank you!

*Tags: General Technical*

---

**azevedo** - *19:39:59*

[!here](!here) Hey guys! A question with regards to new transaction charges from 1st September. [@U4H19D1D2](@U4H19D1D2) posted about this on 30th June. see link below.



would this stuff really apply to the bigger guys? i.e. are those limits enforced per master accounts or only per sub accounts? (because you can have 10s of sub accounts and you could just distribute your activity appropriately to avoid any transaction charges)



[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1593541203173200|https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1593541203173200](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1593541203173200|https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1593541203173200)

*Tags: General Technical*

---

**liam** - *19:40:33*

Master account and zero exceptions is the line from my AM

*Tags: Errors Debugging*

---

**liam** - *20:35:19*

Zero exceptions, I and about 300 others don’t pay above 1000

*Tags: Errors Debugging*

---

**birchy** - *21:57:30*

Quick question regarding AWS S3...are you guys using the Basic, Developer or Business Plan?

*Tags: Deployment*

---

## 2020-07-22

**Misha** - *08:05:25*

I think the P&amp;L page has had some problems in the last 24-48 hours. Not sure that this is your issue, but on another group there have been a few complaints

*Tags: General Technical*

---

**D C** - *12:52:09*

This aimed at GPS users - I was watching the first Yarmouth race in my render software and the final suspension inplay occurred when my stuff clocked an eighth of a furlong left to go. Of course this could be errors in my stuff or betfair being trigger happy with early suspend, but just a word of warning to anyone using GPS. Unfortunately I cannot check the lag times easily at present but if the same happens in the next I will be killing my GPS bot for the rest of the day. Any confirmation of this from other users would be very welcome

*Tags: Errors Debugging*

---

**D C** - *13:00:30*

I guess I can try and detect an excessive lag and block placement of bets if a certain threshold is exceeded. Truth be told most of the problems I get are positional errors rather than lag problems. Thanks for advice anyway

*Tags: Errors Debugging*

---

**Michael** - *13:01:54*

If I were you I wouldn't quit for the day - if you're worried then maybe block Yarmouth and keep betting at Bath? These problems often sort themselves out after a couple of markets.

*Tags: Strategies*

---

**Mo** - *13:12:49*

[http://www.totalperformancedata.com/|http://www.totalperformancedata.com/](http://www.totalperformancedata.com/|http://www.totalperformancedata.com/) is the original source but Betfair have a limited number of licenses they give out

*Tags: Performance*

---

**azevedo** - *13:33:15*

I noticed the TPD also offer an inplay price feed on their website. Like a bookie product I guess. Anyone used/seen that feed? Any comments on the quality of the model behind those prices?

*Tags: Strategies*

---

**Mo** - *13:36:07*

The model is based on machine learning of sectional times

*Tags: Strategies*

---

**Mo** - *13:39:40*

The TPD CEO mentions the model in his interview on the business of betting podcast: [https://m.soundcloud.com/businessofbetting/ep-107-ceo-of-total|https://m.soundcloud.com/businessofbetting/ep-107-ceo-of-total](https://m.soundcloud.com/businessofbetting/ep-107-ceo-of-total|https://m.soundcloud.com/businessofbetting/ep-107-ceo-of-total)

*Tags: Strategies*

---

**D C** - *13:40:04*

That is very interesting. Personally I think IP really needs a exchange model but I suppose bookie punters will bet on anything at times

*Tags: Strategies*

---

**steve** - *17:31:13*

is anyone having trouble with their models atm? im having trouble with one of mine and not sure what to do with it. one segment is just the polar opposite of what the last 3 years of stats were. what do you do when this happens?

*Tags: Strategies*

---

**birchy** - *18:19:02*

I'm late to the party as usual. Interesting reading about the GPS feeds, I didn't even know they existed! To the professionals among us: would you say that 3rd party data sources are essential nowadays or can decent profits be made from simple strategies based on nothing but the Betfair API data?

*Tags: General Technical*

---

**Mo** - *18:33:02*

I’ll answer it tangentially by saying in my experience good data is way more important than the model 

*Tags: Strategies*

---

**steve** - *18:37:07*

my model has 3 segments. 2/3 segments are ok but the other is well off

*Tags: Strategies*

---

**steve** - *18:38:20*

not sure how to approach this situation

*Tags: General Technical*

---

**steve** - *18:44:23*

i have a model that splits into very high prob 90%+, mid prob 70-90+ and 50-70% prob, and its been quite consistant over the last 3 years, but this year (half way thru) has shown my 70-90% segment to be significantly off

*Tags: Strategies*

---

**steve** - *18:48:39*

if we assume the model has changed for that specific segment. what would be the first thing you look at? i understand statistically its possible for this to happen, but the SD over the last few years has been quite consistant

*Tags: Strategies*

---

**birchy** - *19:03:41*

[@UBS7QANF3](@UBS7QANF3) [@UGV299K6H](@UGV299K6H) I've been running various reincarnations of 15+ year old "value" strategies for....15+ years. All simple mathematical strategies that earn pocket money rather than a salary. I took the decision to distance myself from studying form, sport stats, etc about 10 years ago because my conclusion was that those are already built into the betfair prices/bookie odds and are being compiled by sophisticated learning algorithms. I've never had enough time or intelligence to get into the depths of machine learning to a level where I would be competitive.

TBH, I'm now very surprised that I make any profit at all with my dumb "if...if...if...bet...goto next market" type strategies!

*Tags: General Technical*

---

**birchy** - *19:07:18*

[@U016GUPM2UR](@U016GUPM2UR) Totally agree with [@UBS7QANF3](@UBS7QANF3). 20 bets is nowhere near meaningful. You could achieve similar or better results with a completely random strategy. Personally, I don't even check my P&amp;L until 1000+ markets and even then, I'm fully aware that it could all go pear-shaped with a handful of "unlucky" bets. In particular, watch out for the bigger wins as they can make a massive difference to the ROI. For example, if you were backing 100/1 shots and the first bet won, you'd have a massive ROI initially but it's highly likely that the next 99 will lose...leaving the strategy -EV after commission.

*Tags: Strategies*

---

## 2020-07-23

**Misha** - *11:31:27*

A quick question on API access: There is a REST endpoint and an RPC endpoint. Where there is both, the requests and responses are identical. There are some calls (the scores endpoint) that are only on the RPC endpoint. Is there any difference in performance or reliability? Is it possible for one to be up and the other down during a partial outage? I have always used the REST endpoint but have recently updated code to switch between either because I will be using the scores endpoint at some stage

*Tags: Performance*

---

## 2020-07-24

**birchy** - *23:27:42*

Actually, now that I've written that down, I'm questioning my own ability. I've not watched a betfair market for at least 5 years, so as such I'm developing strategies blindly, using nothing but educated guesses. I do test them on historical data before going live, but in reality everything is based on assumptions that I've accumulated over the last 30 years, mixed with a handful of new ideas I've read on various forums and betting blogs.

*Tags: Data Quality, Deployment, Strategies*

---

## 2020-07-25

**Michael** - *12:15:49*

I don't think betfair markets have changed in any fundamental way in that time. Liquidity waxes and wains on this or that sport but basic market mechanics seem fixed to me.

*Tags: Errors Debugging*

---

**Michael** - *12:18:30*

I'm all for educated guesses too. You never know how a strategy is  going to do until you get some bets on so I just check that what I'm thinking isn't daft then give it a go.

*Tags: Strategies*

---

**eduard** - *12:55:30*

:wave: hi everyone, got a quick question about vendor web app streaming api access ? Are we meant to create a connection for each user ? They seem to mandate on behalf of user headers for the connection session token

*Tags: General Technical*

---

**Jorge** - *16:34:17*

Hi guys! How do you querie for the marketStartTime? Is it possible to get this information with a call to listMarketBook endpoint? I want my strategy to finish some minutes before the market goes "inplay" as I have the feeling that odds get worse when it does, but sometimes marketStartTime can be changing

*Tags: Strategies*

---

**liam** - *16:37:30*

It’s in the marketCatalogue or in the marketBook.market_definition (streaming only) [https://github.com/liampauling/betfair/blob/737e17f8d864c8bfae0a995f69a13b9550f1e78d/examples/exampleone.py#L45|https://github.com/liampauling/betfair/blob/737e17f8d864c8bfae0a995f69a13b9550f1e78d/examples/exampleone.py#L45](https://github.com/liampauling/betfair/blob/737e17f8d864c8bfae0a995f69a13b9550f1e78d/examples/exampleone.py#L45|https://github.com/liampauling/betfair/blob/737e17f8d864c8bfae0a995f69a13b9550f1e78d/examples/exampleone.py#L45)

*Tags: General Technical*

---

**Jorge** - *16:42:03*

I'm interested for the marketBook one, how can I get it to display market_definition? I am currently not using streaming but my own endpoint calls

*Tags: General Technical*

---

**Mo** - *16:45:42*

The market definition is for streaming data only

*Tags: General Technical*

---

**Jorge** - *16:46:39*

Aha I see, so I would need to make 2 calls per iteration in my strategy: listMarketBook for prices and listMarketCatalogue for start_time updates :L

*Tags: Strategies*

---

**Mo** - *16:47:35*

Streaming is much better

*Tags: General Technical*

---

**Jorge** - *16:48:07*

Maybe it's time for me to move to betfairlightweight and streaming:slightly_smiling_face:

*Tags: General Technical*

---

**liam** - *16:48:59*

[https://liampauling.github.io/betfair/streaming/|https://liampauling.github.io/betfair/streaming/](https://liampauling.github.io/betfair/streaming/|https://liampauling.github.io/betfair/streaming/)

*Tags: General Technical*

---

**Jorge** - *16:51:28*

I'm just not a big fan of threading in Python. My main concern is error handling in the daemon thread

*Tags: Errors Debugging*

---

**Mo** - *16:52:43*

Here you go then: [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

*Tags: General Technical*

---

**liam** - *17:07:32*

Yep, there are some nuances with current orders and the market book has extra stuff like the definition, worth reading the bf + bflw docs 

*Tags: General Technical*

---

## 2020-07-26

**qwerty.nat** - *01:35:27*

howdy all, what is the general consensus, when placing an order to do it async and subscribe to the orders streaming feed, or synchronously and wait for reply?  And infact anyone done any work on response times between streaming order responses vs the api ng endpoints return times?

*Tags: General Technical*

---

**Misha** - *02:12:48*

My view is that it's a lot better to just use the streaming as it has all the bets in one place and doesn't rely on A) waiting for a bets response, and B) parsing the bets response. The only issue you can have is that when you initially subscribe it doesn't return your current matched orders

*Tags: General Technical*

---

**liam** - *09:16:05*

I have never actually tested it but streaming is always going to be faster over a large sample, with API-NG you would have to get lucky on your request to receive the response before the stream. But thinking about it a socket is always going to be quicker than http

*Tags: General Technical*

---

**PeterLe** - *10:31:39*

Morning Quick Question please. Will a certificate that i created on my windows PC run OK on my macbook too? ie is it associated in any way with the OS, or is t just a digital certificate. Thanks in advance

*Tags: General Technical*

---

**birchy** - *17:33:04*

Have just setup AWS Lightsail and S3. Just wondering what everyone else does regarding their working environment? I've been using Tagadab and basically network mount a remote folder on my local machines and put all my source code into there. This allows me to pick up the source code from multiple machines/devices. Pycharm is OK with this setup as it's effectively a local drive. Sometimes I do quick edits from my phone via SSH and Nano. Are any of you doing something similar with S3? I'm currently investigating how to mount S3 as a network drive. Are there any known issues? Will also be looking at mounting the S3 in the Ubuntu instance on Lightsail to save having to SCP source code. Will be interesting to see how others setup their environments.

*Tags: Getting Started, Deployment*

---

**jfo** - *17:58:50*

The best solution is one you are comfortable with and can use :smile:. I personally prefer to use docker but that comes with its own overhead. I have also just pushed code to git and pulled it down onto aws lightsail machines before

*Tags: Deployment*

---

**birchy** - *18:20:11*

Never used Git, is it secure enough for production code?

*Tags: Deployment*

---

**Dermot** - *18:56:26*

Whatever suits yourself. I use one project with a mix of python and go, but if you prefer to separate then go for it. There’s no wrong way, except for your current method :wink:

*Tags: General Technical*

---

**Mo** - *22:09:46*

I think you’re conflating three separate issues. 



You 100% should have all of your code under version control, whether that is with GitHub or some other service.



As far as synchronising work in progress across multiple devices, I personally like to use Dropbox.



As far as deployment of production code to your VPC, I like to use ansible but I know that Docker is a popular option. 

*Tags: Deployment*

---

**Mo** - *22:18:31*

So I have copies of my GitHub repos checked out to a Dropbox folder and any changes I make to my working copies are automatically synced with Dropbox. 



I push my commits to GitHub about once a day to make sure my collaborators have access in case I get hit by a bus etc. Or I’ll push if the changes need to be deployed to production. 



Managing infrastructure with ansible means that the setup of my servers is described in code that can similarly be version controlled. Deploying trading code to production is a case of running ansible tasks that will pull the relevant code from GitHub. 

*Tags: Getting Started, Deployment, Strategies*

---

**birchy** - *23:23:47*

I work as a PLC programmer in my full time job and we use Dropbox for everything, so I'm familiar with using it, which is why I replicated a similar setup with my own VPC.

I totally agree that I need to update my way of thinking and start using a proper version control system as I now have a lot more projects. For my needs, a centralised system seems to be the best fit.

All I need to do now is work out the easiest way to:

a) get Pycharm to automatically sync to subversion or similar when using my main dev machine

b) do simple edits/bug fixes from an Android device with automatic syncing

c) pull the code into my VPS and launch the bot. I normally use a script for this as I compile to pyc, copy to ~/ and launch, so I could potentially use that to pull the latest version before launching

d) be able to carry out minor edits from within the VPS when logged in via SSH

Does that sound feasible, or am I still pissing in the wind?

*Tags: Getting Started, Errors Debugging*

---

## 2020-07-27

**Oliver Varney** - *09:50:10*

[@U4H19D1D2](@U4H19D1D2) With flumine order status, does pending mean sent to betfair but no confirmation message (i.e. no bet_id)? Im guessing that many market books could be processed before the bet response comes back from betfair?

*Tags: General Technical*

---

**liam** - *09:52:50*

are you overriding the strategy validate order functionality?

*Tags: Strategies*

---

**liam** - *10:46:06*

for streaming? flumine waits and uses the http response so it should be fine

*Tags: General Technical*

---

**Oliver Varney** - *10:48:52*

I was thinking that the order status would still be in pending right ? so even if orders have been sent and they are having latency issues, as long is a check against the order.status then no orders / hedges will be refired

*Tags: Performance*

---

**liam** - *10:50:51*

yes the order status would stay in a pending state on a http error, flumine will try and make the request again (max 3 times) before giving up

*Tags: Errors Debugging*

---

**liam** - *10:53:02*

logic is here [https://github.com/liampauling/flumine/blob/master/flumine/execution/betfairexecution.py#L207](https://github.com/liampauling/flumine/blob/master/flumine/execution/betfairexecution.py#L207)

*Tags: General Technical*

---

**Oliver Varney** - *11:02:01*

bflw this is, not sure if flumine has a way

*Tags: General Technical*

---

**liam** - *11:04:02*

happy to add it to the listener via bflw as long as it doesn't slow things down (too much), BSP offload would be up to the user wouldn't want to default that behaviour

*Tags: Performance*

---

**Oliver Varney** - *11:04:53*

yer was talking about myself personally on that part. some strategies bankrolls are not set up to cope with bets riding for me personally.

*Tags: General Technical*

---

**liam** - *11:11:51*

```_error_handler```

*Tags: Errors Debugging*

---

**Oliver Varney** - *11:15:18*

so if error_handler gets called on every new raw data, exposes  status here shouldnt add any overhead?

*Tags: Errors Debugging*

---

**Twatter** - *15:18:28*

Quick question - I'm slowly getting to grips with Betfairlightweight, and figuring out how to use Flumine, and looking at the BaseStrategy class, does this work on one market book at a time? e.g. process_market_book(self, market, market_book) - would I have to overwrite the BaseStrategy class if I wanted to get it working with two separate markets as an input ?

*Tags: Performance, Strategies*

---

**liam** - *15:24:26*

so its event driven, processing one market at a time however if you want to access another market you can:

```# event markets contains a dictionary of all event related markets

event_markets = market.event



# all markets (need to create a property for this)

all_markets = self.flumine.markets  # &lt;flumine.markets.Markets&gt; object```

*Tags: General Technical*

---

**birchy** - *15:52:21*

Thanks for the tips [@U0136FHM84T](@U0136FHM84T), I'll be back with more stupid questions when I get time to investigate further. :grinning:

*Tags: General Technical*

---

## 2020-07-28

**PeterLe** - *09:49:15*

Quick question please for those using Lightsail…it seems counterintuitive to me; but am I right in thinking that you are still better keeping the server running even when its not being used to increase the CPU burst capacity? (It appears that it resets if you stop the instance?), do you just leave it running 24x7 even if it's not being used for the full period? Thanks

*Tags: Deployment*

---

**PeterLe** - *10:29:54*

I haven’t tried it yet Liam, I created it last night (to compare against EC2). So will be testing today. If its a problem, I may move a few of the non time dependent strategies to my home PC. it just seems much cheaper and on the face of it seems to do everything I want. Yes it starts at zero credit and resets to zero if the instance is stopped/restarted (I think :grinning:)

*Tags: Deployment*

---

## 2020-07-29

**Robert** - *10:16:34*

Hi, I have a question about market catalogues. We request market catalogue at about 9-9.30 in the morning from betfair and use that throughout the day. I have noticed that sometimes events are added to the market catalogue at later times and this causes us issues.  Does anyone know the rules about when new events are added or removed from the market catalogue, is there any guarantees about time?

*Tags: General Technical*

---

**Robert** - *10:17:45*

at the moment its to make deployment safer -- betfair can go down temporarily

*Tags: Deployment*

---

**Robert** - *10:21:14*

it sort of doesnt change the question even if I update it throughout the day -- I need to know how it changes because maybe half way through the day they change the market catalogues and I need to know what sort of changes they can make

*Tags: General Technical*

---

**Robert** - *10:26:21*

for now I will just start later since I used to start it an hour later since I used to and didnt see this problem before -- if issue still happens I will poll it

*Tags: General Technical*

---

**Robert** - *10:31:39*

a related question -- I know sometimes betfair will remove a market book -- does it ever remove and then add back in a market book?

*Tags: General Technical*

---

**Robert** - *10:35:38*

thanks for the help all

*Tags: General Technical*

---

**Misha** - *11:16:08*

I view a selection id as an internal Betfair identifier used within a market. Their may be a pattern of the use of selection ids across markets for the same event, but I have never seen documentation that guarantees this, so I don't assume it

*Tags: General Technical*

---

**JC** - *11:37:51*

Hi guys, - question for those of you who are using the undocumented inplayservice to record/analyse tennis scores. I have set up my recorder and I am noticing often that scores only seem to become available about 30 minutes into the match so quite often my recordings are starting at 4-4 or 3-4 etc. Is this normal or is there something wrong in my code?

*Tags: General Technical*

---

**Mo** - *12:39:33*

It's hard to say if your code is a problem without knowing exactly how you're doing it

*Tags: General Technical*

---

**Oliver Varney** - *16:39:11*

Is the logging in flumine just there so people can set the level, debug and print to the console. Am I right in assuming that the expectation is to define output files, levels, format etcs ?

*Tags: Errors Debugging*

---

## 2020-07-30

**Unknown** - *10:57:59*

[@U016535QCJ2](@U016535QCJ2) [@UBS7QANF3](@UBS7QANF3) this is an example of an event from 2 days ago where my recorder seemed to start approximately 30 mins late. In this case one whole set had been completed by the time recording commenced. I think that if this is a problem with my code, it's that it considers the market to be settled if a `list_market_catalogue` response is empty for the event id twice in 2 minutes, which triggers an upload to s3 and removal of the local file. This might be a premature way of considering the event to be over, and if the code starts tracking the game once it becomes available again, it will just overwrite the first part that was recorded when it uploads the new version to S3. Or it's just a problem with the inplayservice endpoint lol...

*Tags: General Technical*

---

**Mo** - *11:15:48*

Right, I don't think that's implemented in betfairlightweight but you're not using it are you?

*Tags: General Technical*

---

**Newbie99** - *17:18:28*

something like this:



```market_stream = se.Streaming(trading, market_filter, market_data_filter, conflate_ms=conflate)```



*Tags: Strategies*

---

**Newbie99** - *17:21:02*

have you got the streaming class example from github?

*Tags: General Technical*

---

**Newbie99** - *17:22:08*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

*Tags: General Technical*

---

**Oliver Varney** - *17:22:11*

yes I had a older version of streaming code running, but updated to that new code and couldn't remember which / if any parameter controlled it

*Tags: General Technical*

---

## 2020-07-31

**Mo** - *08:43:10*

How generally are you talking? Just poll it every X minutes and upsert into a database

*Tags: General Technical*

---

**Lee** - *08:50:10*

Using flumine. There is already a poll worker which gets the market catalogue. Ideally i'd like to dump that data to a file in a similar way it does with the market_book and load it in on the historical stream. I'm also assuming that's the idea/plan [@U4H19D1D2](@U4H19D1D2) had.

I could save it to a db but seems like it’s added to the market book for a reason

*Tags: General Technical*

---

## 2020-08-01

**birchy** - *00:17:44*

True to my word...

I now have GitHub setup with a private repo. As you suggested, doing minor edits in the browser is fine for those occasional quick fixes. Regarding deployment on vps, I would prefer to only get the latest source when a bot is (re)started. The VPS is purely a deployment machine, so all edits will be done externally.

So I created a SSH key and loaded it into GitHub. I then ran a git clone from the VPS to download my source code ready for deployment. Bot deployed, slippers fitted, scrumpy poured. All good so far...

Following a new commit, I decide to restart a bot. Git clone fails due to folder existing already, so I delete the folder, git clone and start the bot.

It's all working as expected, however the git clone over ssh feels a bit hacky, particularly having to delete the local folder in order to allow a clone. Also, having to start ssh-agent service and then adding the key in order to connect to GitHub feels like a workaround given that I normally create a SSH config file and simply do "ssh lightsail" to gain access to the server.



Is there a nicer way of doing this?

*Tags: Getting Started, Errors Debugging, Deployment*

---

**birchy** - *12:09:03*

Thanks again [@UBS7QANF3](@UBS7QANF3). I'll plan out what I actually need and then make a decision cos I've got too many questions right now. :joy:

*Tags: General Technical*

---

## 2020-08-03

**Mo** - *08:32:35*

The documentation states the connections are closed after 3 minutes

*Tags: General Technical*

---

**liam** - *08:33:30*

Got a link? I have this [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Optimizing+API+Application+Performance](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Optimizing+API+Application+Performance)

*Tags: Performance*

---

**liam** - *08:38:06*

linux/docker, whatever python is using

*Tags: General Technical*

---

**liam** - *08:40:07*

in regards to your setup do you close anything older than 3 and create new? Do you then take the hit on the first round trip or prime it first? (in regards to latency sensitive requests such as place/cancel requests)

*Tags: Getting Started, Performance*

---

**Mo** - *08:42:12*

And yeah avoiding the SSL handshake is critical if you care about latency 

*Tags: Performance*

---

**liam** - *08:43:06*

hmm my problem is a sudden burst where i can have 10+ session in use, theses then get added to a queue LIFO resulting in some becoming stale

*Tags: General Technical*

---

**liam** - *08:53:02*

i just wonder if this problem has been solved

*Tags: General Technical*

---

**liam** - *09:48:55*

[@UUCD6P13J](@UUCD6P13J) this work for you? [https://github.com/liampauling/flumine/pull/249/commits/ece3e463488c7e3ced2f2d76afd7282844e6b914](https://github.com/liampauling/flumine/pull/249/commits/ece3e463488c7e3ced2f2d76afd7282844e6b914)

*Tags: General Technical*

---

**liam** - *09:52:41*

your not python though? I don't think you can send a keep-alive using python/requests

*Tags: Deployment*

---

**Misha** - *09:56:46*

No - so not sure about Python threading. But keep-alives are pretty fast so worthwhile doing them often

*Tags: Deployment*

---

**Lee** - *09:57:10*

looks great, thanks! within the strategy i should just manually read the file right?

*Tags: Strategies*

---

**liam** - *10:22:53*

i currently make a GET to `[https://api.betfair.com](https://api.betfair.com)` to prime on creation but it just seems wasteful, with python i want more control over the socket sometimes

*Tags: General Technical*

---

**Misha** - *10:26:06*

[@U4H19D1D2](@U4H19D1D2) - yes. You should be able to do this in Python as well

*Tags: General Technical*

---

**liam** - *10:32:03*

yep, my understanding is that python uses the os for this, i have attempted to use a custom adapter but that caused all sorts of connection issues

*Tags: General Technical*

---

**Lee** - *11:49:19*

slightly odd, 1.10.2 was on pypi 10mins ago

```$ pip install flumine==1.10.2

ERROR: Could not find a version that satisfies the requirement flumine==1.10.2 (from versions: 0.1.1, 0.1.2, 0.2.0, 0.2.2, 0.2.3, 0.3.0, 0.5.0b0, 0.5.1b0, 0.5.2b0, 0.5.3b0, 0.6.0, 0.6.1, 0.7.0, 0.8.0, 0.8.1, 0.9.0, 1.0.0b1, 1.0.0b2, 1.0.0b3, 1.0.0b4, 1.0.0b5, 1.0.0b6, 1.0.0b7, 1.0.0, 1.1.0, 1.2.0, 1.3.0, 1.4.0, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.6.6, 1.6.7, 1.6.8, 1.7.0, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0, 1.10.1)```

*Tags: Getting Started, Errors Debugging*

---

**liam** - *11:49:42*

yeah having the same problem, seen this before and it normally sorts itself out

*Tags: General Technical*

---

**Pete** - *21:20:00*

[@U4H19D1D2](@U4H19D1D2) Sorry to bother you with this, but do you have a decent contact in the VIP desk? I can't get anywhere with the eejits at the normal help desk.

*Tags: General Technical*

---

## 2020-08-04

**Oliver Varney** - *08:12:39*

[@U4H19D1D2](@U4H19D1D2) probably some code myside but have youdone some work on flumine / bflw listMarketCatalogue polling

*Tags: General Technical*

---

**Oliver Varney** - *08:13:33*

I did just update my package and had to make some code edits elsewhere but im getting an exception in the poll_market_catalogue

*Tags: Errors Debugging*

---

**Oliver Varney** - *08:14:58*

Error: {'code': -32099, 'message': 'ANGX-0003', 'data': {'APINGException': {'requestUUID': 'ie2-ang21b-prd-05190827-00895cdc7c', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0003', 'data': {'APINGException': {'requestUUID': 'ie2-ang21b-prd-05190827-00895cdc7c', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}

*Tags: Errors Debugging*

---

**liam** - *08:24:16*

can you turn debug logs on and see if any logout requests are being made?

*Tags: Errors Debugging*

---

**SrFabio** - *08:32:39*

Does anyone have any (unofficial) documentation for the betfair's CashoutService? Looking specifically at possible response body. The cashout endpoint seems to return 200 status code even when the cashout fails

*Tags: General Technical*

---

**Michael** - *16:57:54*

From TPD: Hmm .... we had an issue I thought had been resolved. Apologies. Will report back ASAP.

*Tags: General Technical*

---

**Kai** - *18:31:55*

lightsail is basically meant to be a simple cheap alternative for people who don't like the complexity of AWS and just want a virtual server IMO.

An alternative to the simple web based interface of digitalocean.

If you create and destroy your machines often then I wouldnt use it.

*Tags: Deployment*

---

**PeterLe** - *18:38:40*

Hi Kai, Thanks. Ive been running EC2 for many years, but thought Id give lightsail a go as it was cheaper. I thought Id try the $40 month package, but  it was overkill so now running on $20/Month.  only use it for my bots, so suits me just fine :+1:

*Tags: Deployment*

---

**Oliver Varney** - *19:16:03*

[@U4H19D1D2](@U4H19D1D2) If flumine process is restart, is there a process to load back in strategy orders ? Also is customerStrategyRef implemented ?

*Tags: Strategies*

---

**Oliver Varney** - *19:18:58*

okay, hmm what function/ whereabouts in the code should I look for debugging?

*Tags: Errors Debugging*

---

**Jonjonjon** - *21:53:58*

In Flumine, when backtesting, we don't get the streaming_update when process_market_book is called. Would it be easy to add?

*Tags: General Technical*

---

## 2020-08-05

**liam** - *06:32:11*

Do you not? Should be `market_book.streaming_update` [https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/streaming/cache.py#L222|https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/streaming/cache.py#L222](https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/streaming/cache.py#L222|https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/streaming/cache.py#L222)

*Tags: General Technical*

---

**Oliver Varney** - *06:39:55*

[@U4H19D1D2](@U4H19D1D2) Am I right in seeing that customer strategy ref is determined by the oshostname?

*Tags: Strategies*

---

**Oliver Varney** - *06:49:56*

I might be missing something but, could we not make customer_strategy_ref a param for the base_strategy or some variation of account + base strategy?

*Tags: Strategies*

---

**liam** - *07:41:58*

Yep so customer_strategy_ref is used to differentiate different running instances hence hostname rather than strategies, this allows multiple instances to run using the same strategies. It then uses the customer_order_ref to store the strategy name



```@property

def customer_order_ref(self) -&gt; str:

    return "{0}-{1}".format(self.trade.strategy.name_hash, self.id)```



*Tags: Strategies*

---

**Jono** - *13:32:25*

Hey guys is it possible to get a print out of an accounts appKey through just the corersponding bf account username and password using bflw?

*Tags: General Technical*

---

**birchy** - *22:27:48*

What's the best way to achieve interprocess control between bots in python? I basically have a strategy that locks into one market until closure and then moves on to the next available market. If I run multiple instances of the same bot, what's the easiest way to avoid them locking onto the same market? I currently pickle the market ids and check the pickle file to confirm that no other bots are already working on the market but it's a bit clunky and I'd like to use something less error prone.

*Tags: Errors Debugging, Strategies*

---

**Jonjonjon** - *22:37:33*

Unless my Python is out of date, you'll need the multiprocessing version of the Queue: [https://docs.python.org/3/library/multiprocessing.html](https://docs.python.org/3/library/multiprocessing.html)

*Tags: General Technical*

---

## 2020-08-06

**Mo** - *06:54:15*

My trading platform is made up of multiple components each of which is a separate Python process. For example, a Betfair price feed, a market catalogue feed, and each event traded is its own process. Communication between them is done primarily using Redis pubsub but other features are useful - e.g. storing the market catalogues for easy lookup by the different components. For your use case a Redis Set would be perfect for keeping track of which markets you're trading

*Tags: Feature Engineering, Strategies*

---

**Misha** - *08:09:22*

The system can monitor &gt;5000 markets concurrently (and execute any strategy) using only a 4 CPU VM

*Tags: Strategies*

---

**Misha** - *08:10:42*

On version 5 now after being in development for 10 years. Has been used in production for most of that time for huge clients. Every 2-3 years I review and update architecturally

*Tags: Deployment*

---

**Misha** - *08:11:42*

BTW, in production it does run on a hosted server :wink:

*Tags: Deployment*

---

**liam** - *08:18:25*

So there are guys on here that care about milliseconds, have code that is very close to the metal and operate at the same level as those in the equity markets. Many (including myself) focus on the nuances of the matching engine and have execution models to be able to take advantage with a view to maximise pnl / matching rates.

Others are running ML models in real time based on multiple data sources across numerous domains feeding into a single system, behind these models are teams of people.

No offence but being able to process 5000 markets is not sophisticated, its a pre requisite.

*Tags: Strategies*

---

**Misha** - *08:20:42*

Architecturally it's fairly simple to explain. Two parts, a monitor and a trader. The monitor basically just monitors whatever I need (Betfair, other exchanges, tennis APIs, etc) and through a pipeline of "micro-services" pushes data into a database which is exported as a tree of JSON files with URL links accessible via simple HTTP requests. The trader takes this market information, streaming info, other price info, scores, and converts all of them to push feeds, aggregated and matched, so everything runs via object references in memory. The trader doesn't access a database

*Tags: Performance*

---

**Misha** - *08:23:45*

Previous client was probably the most successful on UK racing. I only supplied the operational system that took inputs from their model. Was making an eye-watering amount. I was the developer, so none of that went to me unfortunately

*Tags: Strategies*

---

**Misha** - *08:30:13*

I partner with other people who develop models so I concentrate solely on developing and refining the operational system

*Tags: Strategies*

---

**Peter** - *08:43:14*

Well 40 years ago I was programming linear regression models. But as I was doing so on a mainframe using punch cards, I don't think I'll be claiming that as a huge influence on what I do now :slightly_smiling_face:

*Tags: Strategies*

---

**Twatter** - *09:21:09*

[@U016535QCJ2](@U016535QCJ2) I don't quite understand why you haven't been using it yourself since you started - esp. if it was showing to make lots of money? Was it a case that although you had the code, you didn't have access to the models it ran off?

*Tags: Strategies*

---

**Misha** - *09:39:53*

A number or reasons: 1) I am a professional developer so a nerd (into sports) and not a risk taker: 2) I get paid a reasonably good amount to do this with no risk; 3) I don't have a big pot of money behind me; 4) I need a good model which I'm not as comfortable with

*Tags: Strategies*

---

**Misha** - *09:42:32*

So after 10 years, when my last client exited Australia, I thought why not have a go. So I have teamed up with a modeller and we are about to go live. Have spent the last 7 months updating the system in preparation. I specialise in the execution engine (my code) which I then update for clients and they can use it. When I part ways I just take the generic bits and keep developing

*Tags: Deployment, Strategies*

---

**Misha** - *09:44:01*

When you work in the industry for the big guys you see what is possible if you have a good model. But the really successful ones have been at it for 20 years. Easy enough to fill a niche (which would be good money), but very hard to get to their level

*Tags: Strategies*

---

**Misha** - *09:44:45*

So I won't go into racing and football, which is where the big syndicates go. They make all their money on statistical models pre-play

*Tags: Strategies*

---

**Misha** - *09:51:19*

I wouldn't pay myself to develop my own model :joy: (even though I reckon I could make a good go of it). So I have split the system in two; I fetch and clean all the data, the modeller takes that and generates probabilities, and I then take that and build the execution engine. My partner has only one job

*Tags: Strategies*

---

**Misha** - *09:53:23*

BTW, I'm on here because I am always looking to learn, and everyone has a different background, so it's good to see what others are doing. Always gets me thinking about how to improve what I do

*Tags: General Technical*

---

**Twatter** - *10:17:13*

I joined here so I can get an idea of what everyone else is doing for Betfair bots and how they using Liam's BFLW/flumine and to get help whilst developing.... to be honest over the history of what i've been reading i've been completely blown away by how sophisticated all your trading set-ups are. I was expecting a few hobbyists like my self trying to write a bot for a bit of side income - and instead found a bunch of people using bits of AWS that i've never come across, ML, real time GPS feeds from races that I didn't even know existed, and god knows what else going on. Crazy...

*Tags: Deployment, Strategies*

---

**liam** - *11:10:45*

I am happy to do one on either flumine setup/use or future/roadmap if there is interest

*Tags: Getting Started*

---

**Jonjonjon** - *11:12:55*

[@U4H19D1D2](@U4H19D1D2) Would you be able to throw in some content on how to use Docker with Flumine?

*Tags: General Technical*

---

**Twatter** - *11:13:25*

Haha - My infrastructure so far is a laptop. With internet. And power supply. Sometimes use a mouse when at a desk. Still putting together just the very basics of grabbing market prices on horses for the strategy I need. Not even sure whether the strategy I use will translate into Betfair markets as it depends on how efficient all the pricing is inter market. It's all an interesting learning experience though so far as first time properly using Betfair APIs etc

*Tags: Getting Started, Strategies*

---

**Twatter** - *11:17:05*

Flumine setup and use sounds great - might sound basic for a lot of people but would be great for newbies like myself. So far, although you guys recommended to start plugging my strategy and stuff into flumine, i've started using trying to do it all in betfair lightweight, but still at the pre-requisite stages of simply grabbing market data and prices that I need and trying to analyse them. The way I see it is that since BFLW is so close to the Betfair API, it's giving me a great understanding of how the Betfair API itself works and how it returns data, and then I can look at how Flumine then wraps more of the trading and strategy around BFLW..

*Tags: Getting Started, Strategies*

---

**Twatter** - *11:27:48*

I haven't used docker myself, but wouldn't that just make it easy to deploy a working bot? i.e. if you've already got it running on a server, and it's a bot/strategy that won't benefit from deploying the same code to a 2nd/3rd server etc, then I would have thought that containerising your code and deployment into docker would only help with speed of deployment and speed of recovery if the server fell over?

*Tags: Performance, Deployment, Strategies*

---

**PeterLe** - *11:42:36*

[@UBS7QANF3](@UBS7QANF3) [@U4H19D1D2](@U4H19D1D2) The last meet up we had was great, but i was thinking that for a future one, maybe we could all throw a £10 in and hire a large meeting room at a local hotel, we could bring laptops and the more experience guys could help the less so. ie have it as a workshop drop in and then beer afterwards :slightly_smiling_face: just an idea

*Tags: General Technical*

---

**Misha** - *12:07:10*

My advice is to keep it as simple as you can. You only ever learn from experience so it isn't till you actually get going that you start to get an idea of what you need. The syndicates making money have surprisingly simple setups beyond using hosted servers (none of the ones I worked for used AWS services at the time). I know because I ran a few of them. It's the model that makes the money, and the operational system can increase the profits (but can never overcome a poor model)

*Tags: Getting Started, Deployment, Strategies*

---

**Ruben** - *13:27:42*

One quick question about the Event.time_zone attribute; does this refer to the timezone of event.start_date, or to the timezone in which the event will take place? In which case I would assume all event start dates are in UTC

*Tags: General Technical*

---

## 2020-08-07

**Newbie99** - *14:17:22*

Does anyone have the generic betfair help desk e-mail (not the API one), I need to send them a screenshot of something?

*Tags: General Technical*

---

**liam** - *15:41:13*

Regarding the bflw talk, any objection to Betfair being involved? Potential for Neil and/or Betfair head of liquidity to join

*Tags: General Technical*

---

## 2020-08-08

**Oliver Varney** - *08:31:32*

Morning guys, not too sure if I had too many beers last night and am having a blonde moment this morning, my understanding is that betfair times are UTC, although in flumine it appears the market_book.publish_time is UTC (although no timezone info set on the datetime object) and publish_time_epoch is BST (or one hour ahead). Should int(market_book.publish_time.timestamp()*1000) == market_book.publish_time_epoch

*Tags: General Technical*

---

**liam** - *09:11:20*

[https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/resources/bettingresources.py#L586|https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/resources/bettingresources.py#L586](https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/resources/bettingresources.py#L586|https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/resources/bettingresources.py#L586)

*Tags: Strategies*

---

**liam** - *09:12:06*

And then [https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/resources/baseresource.py#L35|https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/resources/baseresource.py#L35](https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/resources/baseresource.py#L35|https://github.com/liampauling/betfair/blob/a639787bee11e0cbd135bf0e7bfeb171217f78d7/betfairlightweight/resources/baseresource.py#L35)

*Tags: General Technical*

---

**john walsh** - *10:35:30*

I am getting this message for each entry "[&lt;betfairlightweight.resources.bettingresources.PriceSize object at 0x0B4D7A30&gt;]".  Does anybody know how to fix it?

*Tags: Errors Debugging, Strategies*

---

**liam** - *10:38:15*

Looks like you are using non [https://liampauling.github.io/betfair/advanced/#lightweight|lightweight](https://liampauling.github.io/betfair/advanced/#lightweight|lightweight) mode so you are getting python objects

```traded_volume = [&lt;PriceSize&gt;]

for traded in traded_volume:

    print(traded.price, traded.size)```

*Tags: General Technical*

---

## 2020-08-10

**Amanda** - *16:40:51*

Quick question, anyone else unable to access the ‘Exchange Profit &amp; Loss’ option from the Betfair Exchange web page?  Just says ‘There were no results for that request’

*Tags: General Technical*

---

**Peter** - *17:00:02*

I'm having the problem that [@UV6F64HPG](@UV6F64HPG) described. Was working fine earlier today, but now it's not returning any results,

*Tags: General Technical*

---

**PeterLe** - *17:30:54*

Hi,

A very quick question please..(im still learning :slightly_smiling_face:) in the code below, is there a simple way to replace the parameter in the market start time such that it always defaults to today?

ie listing all eventids for today (GB)

Many thanks in advance



def todaysmarkets():

    racing_filter = filters.market_filter(

        event_type_ids=[7],

        market_countries=["GB"],

        market_type_codes=["WIN"],

        market_start_time={"to":"2020-08-10T22:45:00Z"},

    )

    results = trading.betting.list_market_catalogue(

        market_projection=[

            "RUNNER_DESCRIPTION",

            "RUNNER_METADATA",

            "COMPETITION",

            "EVENT",

            "EVENT_TYPE",

            "MARKET_DESCRIPTION",

            "MARKET_START_TIME",

        ],

        filter=racing_filter,

        max_results=100,

    )

*Tags: Strategies*

---

**PeterLe** - *17:44:16*

Thanks for that, I also had to add in :

```from datetime import datetime```

but I get this error: market_start_time={"to": "{0}T23:45:00Z".format(datetime.date.today())}

AttributeError: 'method_descriptor' object has no attribute 'today'

*Tags: Errors Debugging*

---

**PeterLe** - *17:49:07*

Thanks fixed it now (I replaced from datetime import datetime with :

```import datetime```

much appreciated

*Tags: Errors Debugging*

---

**Douglas Hickling** - *19:43:14*

Hi,

I have no experience with streaming API’s and I am trying to work out how to use the Betfair one. I was wondering if anyone could point me in the direction of any material that may help me understand it?

Also I have been using Jupiter notebooks for the snapshot API, however I understand that isn’t suitable for the streaming api, what do you use?

Thanks for the help.

*Tags: General Technical*

---

**Jonjonjon** - *19:56:13*

[@UV6F64HPG](@UV6F64HPG) I've had a problem with the "Exchange Profit and Loss" page for a few weeks.



I can see my P&amp;L for the last day or 2. But if I try to view the P&amp;L for the last 7 days or more, it tells me "There were not results for that request" too.

*Tags: General Technical*

---

**Jonjonjon** - *19:57:28*

In Chrome, I get the same problem as you. But in Firefox it is fine

*Tags: General Technical*

---

**liam** - *20:09:53*

Have you seen the [https://github.com/liampauling/betfair/tree/master/examples|examples](https://github.com/liampauling/betfair/tree/master/examples|examples)? It might be that you want to process historical data first to get a feel however I recommend making some requests as per the [https://liampauling.github.io/betfair/quickstart/|QuickStart](https://liampauling.github.io/betfair/quickstart/|QuickStart) to get a better understanding of the API and responses 

*Tags: Data Quality*

---

## 2020-08-11

**birchy** - *22:57:09*

I now have a load of data stored on S3 using Flumine S3MarketRecorder class, so am wondering how best to integrate this data into backtesting? I've got some basic backtesting code from the Flumine examples which works with a downloaded file after manually unzipping it. What I want to do in Flumine is:

a) download data from S3 to AWS Lightsail on the fly

b) automatically unzip the file

c) run backtest

d) goto a)

Are there any examples on how to do this?

*Tags: Deployment*

---

## 2020-08-12

**Lee** - *13:18:49*

in that example open is being patched with smart_open, in the readme there is an example of how to read from s3 without downloading before

*Tags: General Technical*

---

## 2020-08-14

**Lee** - *11:50:39*

ladder_levels is only set to 3 but hope it helps

*Tags: General Technical*

---

**birchy** - *18:33:28*

from smart_open import s3_iter_bucket                   

                                                 

bucket = 'mybucket'                           

                                                 

for key, content in s3_iter_bucket(bucket, worke$

    print(content)                               

    exit()



This returns the file bytes. The data was saved using Flumine  S3MarketRecorder which creates .zip files. Reading the docs, it appears that smart_open doesn't natively support zip, so is there a way to force unzipping?

*Tags: General Technical*

---

**birchy** - *18:37:42*

Bummer, I would normally use bz2 inline with the historical data but the Flumine example uses zip, so I left it alone... assuming it was a suitable format for backtesting.

*Tags: Data Quality*

---

**Mo** - *18:40:54*

Yeah I don't use flumine for scraping. Doesn't make sense to me to compress single files as zips but maybe [@U4H19D1D2](@U4H19D1D2) had his reasons?

*Tags: General Technical*

---

## 2020-08-15

**Mo** - *08:06:20*

Off the top of my head, with `awscli` OS package installed and configured



```aws s3 sync s3://&lt;bucket_name&gt;/&lt;path_to_directory_containing_zip_files/ /tmp/betfair/

find -type f /tmp/betfair -exec sh "unzip {} ; rm {}" \;

find -type f /tmp/betfair -exec bzip2 {} \;

aws s3 sync --delete /tmp/betfair/ s3://&lt;bucket_name&gt;/&lt;path_to_directory_containing_zip_files/```

I wouldn't run the last command - which will delete everything in the S3 directory that now doesn't exist in /tmp/betfair - without checking you're happy with what /tmp/betfair contains

*Tags: Getting Started, Deployment*

---

## 2020-08-16

**Lee** - *18:14:16*

Out of interest, what does elapsed_time include and does the following time seem slow?

```{

	"message": "execute_place",

	"trading_function": "place",

	"elapsed_time": 1.1396217346191406

}```

I mainly ask because a few of my offered prices did not match in live and if I increase `PLACE_LATENCY`  in simulatedexecution.py it seems to match live more accurately.

*Tags: Performance, Deployment, Strategies*

---

**Lee** - *18:19:58*

so based on that the PLACE_LATENCY to match in backtesting would be 0.139 (if i was being pedantic)

*Tags: Performance*

---

**mlpanda** - *20:35:37*

Hi [@U4H19D1D2](@U4H19D1D2), is there any take-profit/stop-loss functionality in Flumine? I'm currently planning to implement it in the strategy under `process_orders()` , but it would be cool to have an order type where you could define take-profit and stop-loss odds which would then trigger a hedge

*Tags: Strategies*

---

## 2020-08-17

**birchy** - *13:11:04*

I'm away from home this week and the only device I have is my android phone. Nevertheless, I thought I'd look at getting this S3 streaming working but am having a hard time. Having converted all the saved files from .zip to .bz2, I'm still not having any joy. As far as I understand from the smart_open docs, it should auto decompress the bz2 files but that doesn't seem to be happening. In the following code, "content" is a byte object and I can't do anything with it. `.decode("utf-8")`, `smart_open.open()`, etc all fail or return the original bytes. Getting really frustrated with this now as it shouldn't be THIS difficult.



`import smart_open`



`bucket_name = 'mybucket'`



`for key, content in smart_open.s3_iter_bucket(bucket_name): workers=1):

    print(content)

    exit()`



Also tried:



`import boto3`



`bucket_name = 'mybucket'`



`s3 = boto3.resource('s3')

bucket = s3.Bucket(bucket_name)

for obj in bucket.objects.all():`

    `body = obj.get()['Body'].read()`

    `print(data)

    exit()`

But this is what `smart_open.s3_iter_bucket()` does in the background, so the results are the same.



I simply want to run my backtest code in the Lightsail instance as I can then leave it to run and check results later. As you know, I am a noob to the whole aws thing, so I'm wondering what I am failing to understand? 



Logically, I am assuming that the workflow is simply a case of:

a) save data using the S3MarketRecorder example (.zip)

b) stream/read each market from S3 using `smart_open`

c) process market



I hate asking for spoon feeding but I am starting to think I am too stupid for this. :flushed:

*Tags: Deployment*

---

**liam** - *13:43:42*

This works for me

```import smart_open

import zipfile

from io import BytesIO



bucket_name = 'flumine'



for key, content in smart_open.s3_iter_bucket(bucket_name, prefix="marketdata/streaming/7", workers=0):

    print(key)

    with zipfile.ZipFile(BytesIO(content)) as zf:

        for contained_file in zf.namelist():

            with zf.open(contained_file) as f:

                print(f.readlines())

    exit()```

*Tags: Errors Debugging*

---

## 2020-08-18

**Mo** - *12:16:40*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1594903864009900?thread_ts=1594903864.009900](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1594903864009900?thread_ts=1594903864.009900)

*Tags: General Technical*

---

**birchy** - *13:11:38*

Thanks [@UBS7QANF3](@UBS7QANF3) and [@U4H19D1D2](@U4H19D1D2), have it working now. Hard work on a phone cos copy/paste goes wanky in the nano terminal. What's the easiest way to push this into the strategy market_filter as a path?

*Tags: Strategies*

---

**liam** - *14:05:15*

```There's been a power failure at the Data Centre on which the Gmax systems depend, this means that until their issues are resolved the live GPS stream and other APIs will be unavailable.



[https://status.exn.uk/](https://status.exn.uk/)

[https://status.giga.net.uk/incidents/3zcfz8s8g43h?u=syfy92mhvnh0](https://status.giga.net.uk/incidents/3zcfz8s8g43h?u=syfy92mhvnh0)



Apologies for the inconvenience```

*Tags: Deployment*

---

## 2020-08-19

**Peter** - *14:42:24*

True, But I think Misha's point is addressing the accuracy of GPS as a technology in general. Using the signals from multiple satellites to triangulate the position of a moving object in real-time necessarily comes with a margin of error that tends to be glossed over a bit in the marketing materials of those selling the systems.

*Tags: Errors Debugging*

---

**Twatter** - *15:22:55*

Fascinating area of research.. I wonder how accurate a set of cameras could be at tracking the horses - if there was one or two fixed cameras at certain positions to track horses and instaed of facial recognition perform pattern+colour recognition on the jockeys.. although I'm guessing the delay of processing the feed would be a bit pointless..

*Tags: Errors Debugging*

---

## 2020-08-20

**D C** - *10:41:11*

I agree regarding the speed and complexity. Look at something like Hawkeye to capture motion of cricket ball which is a VERY simple problem by comparison. You basically just need to fit an ellipse to the outputs of some edge detection. Cameras positioned accurately to optimise that. Not sure if they do that in real time or just run it after the ball has been delivered. Imagine a foggy February day over the fences trying to use image recognition in a 20 strong field. Really can't see it being feasible.

*Tags: Performance, Deployment*

---

**Twatter** - *14:47:16*

[@U0187TRCGAZ](@U0187TRCGAZ) Does the same Market IDs work via the Betfair API website?

*Tags: General Technical*

---

**john walsh** - *15:32:46*

Obviously there is not an easy solution in Betfairlightweight or somebody could tell me what I was doing wrong.  So, obviously it is not my mistake.  And yes, there are alternatives but Betfairlightweight is an excellent piece of code so it would be nice to use it.

*Tags: General Technical*

---

**john walsh** - *15:45:10*

I have never had a problem with horse racing.  It brings back the data whether it is UK, Ireland or US every time.  In fact, the Timeform  data Betfair produces is an excellent way of betting against the favourite because the horses they choose are always nominated (unless non-runners) as favourites by all the press and the number of favourites they choose which actually win in UK is abysmally small.

*Tags: Strategies*

---

**john walsh** - *18:09:21*

Since nobody seems to have the answer to my query, I will plough the lonely furrow and of course I can use "R" as an alternative to Python which has no problem handling the greyhounds.  I have also developed a very good football application in VIsual Studio which I am sure can be adapted.

*Tags: General Technical*

---

**Mo** - *18:18:58*

I guess I wasn’t clear: you are not getting any data because the data does not exist _on the race card endpoint,_ regardless of what you might see on the website. Using R instead of Python won’t make the slightest difference. 

*Tags: General Technical*

---

**Mo** - *19:03:01*

It’s an undocumented endpoint used internally by Betfair so they are the only people who could answer your question as to why it is not on there

*Tags: General Technical*

---

**john walsh** - *19:18:16*

All I am happy about is that it was not a coding error by me. Onward and upward.  Plenty of other things work to keep me happy.

*Tags: Errors Debugging*

---

**Jonjonjon** - *21:03:53*

Suppose I want to hedge/green a matched back bet with a lay. The lay is in the order book. I want to move the lay order (either up or down), while adjusting the size so that it will still hedge/green the back bet correctly. What is the best way to do that in Flumine?

*Tags: General Technical*

---

**liam** - *21:16:42*

Use the market version to prevent the order executing inplay (not implemented in flumine yet)

*Tags: General Technical*

---

**Oliver Varney** - *21:28:23*

I think it comes down to the strategy. Ill always argue for hedge over bet to win purely because of bank size, but there does become a point where you move the market too much, weaker horses can be a nightmare

*Tags: Strategies*

---

**Oliver Varney** - *21:33:52*

bank size can be like 5-6 times smaller when trading odds and hedge vs bet to win. For me its worth giving up 1-2% on the strategy, for smaller bank roll with higher consistent strike rate.

*Tags: Strategies*

---

## 2020-08-21

**Unknown** - *08:46:52*

[@UUE6E1LA1](@UUE6E1LA1) data are available, so what is the problem here?

*Tags: General Technical*

---

**D C** - *10:33:48*

[@U013MLED3V1](@U013MLED3V1) I thought that the problem was that to be that people are not able to get that data all the time. If you have the solution then maybe you can explain how to get this data to [@U0187TRCGAZ](@U0187TRCGAZ). Perhaps I have misunderstood things.

*Tags: General Technical*

---

**Stefan** - *10:41:13*

[@UUE6E1LA1](@UUE6E1LA1) Data are available, so my reply was why you did not put them to python code making john happy.

*Tags: General Technical*

---

**D C** - *10:42:13*

[@U013MLED3V1](@U013MLED3V1) Because I am not a python programmer.

*Tags: General Technical*

---

**john walsh** - *10:46:19*

I tried to do something that I logically thought would work.  It did not, so I wanted to know was it my fault or was it not a function of betfairlightweight. I accepted others opinion that it was not a function of betfairlightweight and was happy with that so "onwards and upwards" to other things.  It is not the only thing I have done with betfairlightweight which work so I am happy to forget about greyhounds for the moment and work on other things.

*Tags: General Technical*

---

**Stefan** - *11:23:21*

[@UUE6E1LA1](@UUE6E1LA1) in what programming languge are you programming these day? Historically I went from 8080 assembler (well yes that is not programming language), basic, c, visual basic, pascal, c++, lisp, php, C#, Objective-C, F#, python, and many more but I count only those programming languages I made some app used by others. Yes I am not expert in python, and frankly to say it will not happen language of my choice.

*Tags: General Technical*

---

**Peter** - *11:33:13*

[@U013MLED3V1](@U013MLED3V1) Given that list (over the course of 40 years I've acquired a similar one) Then Python would surely be some way from the bottom of your preferences.

*Tags: General Technical*

---

**Stefan** - *12:25:14*

[@U9JHLMZB4](@U9JHLMZB4) we are old ones here, well I am only 54 :slightly_smiling_face: [@UUE6E1LA1](@UUE6E1LA1) node.js is not type safe, maybe if you use typescript. Just from my experiences working with clients coming from your world (node.js, php), there is always a lot of problems in their code.

*Tags: General Technical*

---

**Stefan** - *12:27:38*

Yes, you are right, but what I meant was really not about adapting new language, a learning curve, because if you program for some time, it is actually no problem, you can switch to any programming language in reasonable time.

*Tags: General Technical*

---

**D C** - *13:22:40*

I've used it for close to 20 years now and I would say that you can advertise it as type safe but any language with an operation labelled reinterpret_cast (never mind having to support the C style cast) just can't be. Maybe in the hands of the perfect programmer who knows every line of the ever changing language standards AND never makes an error you could consider it type safe, but in practical terms it is not. For sure it is an improvement on C though I agree there but the fact that is has to support C leads to issues. You can access private members directly from a pointer to object - and this is directly attributable to having to support C language features. I guess we won't end up agreeing on this - it's just my opinion on writing my own bad code and reviewing other people's bad code over the years.

*Tags: Errors Debugging, Feature Engineering*

---

**john walsh** - *17:26:58*

Here is another anomaly.  I coded in the [https://betfair-datascientists.github.io/api/apiPythontutorial/](https://betfair-datascientists.github.io/api/apiPythontutorial/)

*Tags: General Technical*

---

**Lee** - *17:33:32*

```&gt;&gt;&gt; competition_filter = betfairlightweight.filters.market_filter(event_type_ids=[1], market_start_time={'to': datetime_in_a_week})

&gt;&gt;&gt; competition_filter

{'eventTypeIds': [1], 'marketStartTime': {'to': '2020-08-28T16:31:02Z'}}

&gt;&gt;&gt; competitions = trading.betting.list_competitions(filter=competition_filter)

&gt;&gt;&gt; import pandas as pd

&gt;&gt;&gt; soccer_competitions = pd.DataFrame({

...     'Competition': [competition_object.competition.name for competition_object in competitions],

...     'ID': [competition_object.competition.id for competition_object in competitions]

... })

&gt;&gt;&gt; 

&gt;&gt;&gt; soccer_competitions[soccer_competitions.Competition.str.contains('English Premier')]

               Competition        ID

48  English Premier League  10932509

&gt;&gt;&gt; soccer_competitions

                 Competition        ID

0        Swedish Allsvenskan       129

1       Spanish Copa del Rey     12801

2                   Specials   2608550

3     Danish Women's Matches  12016693

4               Czech 3 Liga    892425

..                       ...       ...

82           French National   1081960

83    Irish Premier Division  12203971

84  Belgian First Division A     89979

85            UEFA Euro 2020  11997260

86         Icelandic 1 Deild  12010570



[87 rows x 2 columns]```

*Tags: Feature Engineering, Strategies*

---

**Lee** - *17:33:42*

what error message do you get?

*Tags: Errors Debugging*

---

**john walsh** - *18:52:02*

Thank you for that.  When I run your code pandas does not automatically print out the dataframes and I have to print them.  When I do that with your code it does not produce the line "48 English Premier League 10932509" but the whole list as per yours after that entry.  I am using Python 3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)] and I have installed Pandas 1.0.5

*Tags: Getting Started, Feature Engineering*

---

## 2020-08-23

**Unknown** - *09:29:02*

To anyone that uses FlumineBacktest... I've been experimenting with a slightly modified version of [https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|backtest_multi.py](https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|backtest_multi.py) using simple strategies and running them on a low spec AWS Lightsail instance (1 CPU, 2gb RAM).



Iterating 100 markets takes about 3.5 minutes - is that normal?

*Tags: Deployment*

---

**birchy** - *10:20:41*

I've got workers=multiprocessing.cpu_count(), which in this case is 1. When testing the same strategy against pickled data gathered by my bespokebots polling library at 1 second intervals, backtesting 100 markets completes in a few seconds, which is why I'm wondering if 3.5 minutes is normal. The bigger issue is when I test against 1000+ markets as it's going to take hours rather than minutes. :thinking_face:

*Tags: Strategies*

---

**Misha** - *11:04:57*

Just for my own curiosity, how many "streaming price updates" do you process per second (roughly)?

*Tags: General Technical*

---

**birchy** - *11:23:08*

In Flumine or my lib?

*Tags: General Technical*

---

**Misha** - *11:57:49*

For historical data. The number of updates live is limited by how many there are at the time :wink:

*Tags: Data Quality, Deployment*

---

**Misha** - *12:37:17*

If you aren't "trading" so much as checking the price at certain intervals, having a second-by-second summary is very useful. I do this for tennis as I am only interested in the price at a particular time for back testing/modelling

*Tags: Strategies*

---

**Misha** - *12:38:30*

In fact I process the historical stream for tennis in two phases. Firstly I pre-parse the full stream to get a list of one-second summaries for all markets and all times. Then I can process this to get specific snapshot prices and put them into a database

*Tags: General Technical*

---

**birchy** - *13:18:30*

All of my strategies are based on some form of "value" betting, so as such my bots already have a pre-calculated "true price" they work from. Obviously the trigger prices change as the market develops/new info is acquired, etc. For that reason, I'm only really backtesting if the bets would of been matched and for that, 1 second intervals is sufficient. 

I want to investigate trading and for that, I think streaming is pretty much essential and it seems pointless writing my own when bflw/flumine exists.

*Tags: Strategies*

---

**liam** - *15:18:47*

So streaming data contains all/any updates which is probably 50-100x more data to process over a markets lifetime. This doesn’t include the limiting to 30minutes, however you can limit the data to process in flumine with the following on the market filter `”... listener_kwargs”: {“seconds_to_start”: 600, “inplay”: False}`

*Tags: General Technical*

---

## 2020-08-24

**ricky** - *09:15:37*

All, I am using streaming API, i was able to get match odds and traded volume data, now i want to get additional "OVER_UNDER_25" Back/Lay price and traded volume.

I used below filter to set market_types to "OVER_UNDER_25", but no "OVER_UNDER_25" data return, Any suggestions?



market_filter = streaming_market_filter(

    event_type_ids=["1"], market_types=["MATCH_ODDS", "OVER_UNDER_25"], market_ids=["1.172086373"]

)

*Tags: General Technical*

---

**ricky** - *10:10:38*

[@UUCD6P13J](@UUCD6P13J) Instead of use market_id i changed to country code (otherwise i got error SUBSCRIPTION_LIMIT_EXCEEDED above 200), but nothing change.

I still can not see "OVER_UNDER_25" data when i print everything from market_books

*Tags: Errors Debugging*

---

**liam** - *10:13:59*

```market_filter = streaming_market_filter(

    event_type_ids=["1"], market_types=["MATCH_ODDS", "OVER_UNDER_25"], event_ids=[29958270]

)```

*Tags: General Technical*

---

**liam** - *10:57:41*

Tried to explain it [https://liampauling.github.io/flumine/trades/#parameters|here](https://liampauling.github.io/flumine/trades/#parameters|here), `place_reset_seconds` is seconds since the last order was reset i.e. complete where as `reset_seconds` is just seconds since the last place on that runner

*Tags: General Technical*

---

**Lee** - *18:31:54*

Yeah, i just asked them to increase and no questions were asked

*Tags: General Technical*

---

**Unknown** - *22:49:45*

Dumb q to help maintain my sanity - What do EX and SP stand for?

Context: market_book --&gt; runners list --&gt; runner --&gt; EX, SP

*Tags: General Technical*

---

## 2020-08-25

**Jonjonjon** - *09:17:42*

Is anyone having issues placing orders this morning? I have an insufficient funds error, trying to place £50 of liability on an account with over £1k of unused exposure.

*Tags: Errors Debugging*

---

**liam** - *09:20:02*

I havent had any errors

*Tags: Errors Debugging*

---

**Jonjonjon** - *10:39:33*

Thanks. It's for a bit of my bot that isn't migrated to bflw yet :sweat:

*Tags: General Technical*

---

**Twatter** - *13:31:01*

Just a general question out of curiosity - do most of you guys do betfair automated trading as part of your job (i.e. you work for companies/hedge fund type companies that do sports trading), or are you doing it for side profit or is it your personal job/sole source of income etc?

*Tags: Strategies*

---

**Twatter** - *14:36:01*

[@UUCD6P13J](@UUCD6P13J) Lack of time to properly get coding on my strategy.... so haven't been able to paper trade it yet (not sure how it differs to most other strategies around here but it's intra market based, so I'm comparing prices across 3 maybe 4 markets per race). But I like how BFLW makes it easy to grab the data and stuff.

*Tags: Strategies*

---

**Twatter** - *14:38:54*

Suppose what I'm worried about is implementing it, and then finding that the markets per race are too efficient (which I sort of suspect they are), and so there's no value in my strategy - and that the value only lies at the bookies... at which point I need to start again!

*Tags: Strategies*

---

**Twatter** - *14:50:05*

The main problem is that bookies know that the EW bets we taking are mis priced and they ban/restrict you to pennie

*Tags: General Technical*

---

**Twatter** - *14:50:23*

i.e. it's a short lived strategy (well across a few years)

*Tags: Deployment, Strategies*

---

**Twatter** - *14:55:14*

Well.. I'm giving it a shot and suppose will be interesting to see what comes of it. Worst case scenario is I'm learning how to put a flumine/BFLW bot together so if I uncover something else then I can implement it quicker. But yeah... if I wasn't restricted to about 50p EW bet at Bet365 on a 7.0 odds horse, then I probably won't be here!

*Tags: General Technical*

---

## 2020-08-26

**PeterLe** - *09:28:24*

Morning, A similar background for me; Electrical/Electronic engineer; Vax/PDP Systems engineer (80’s/90’s). Account Manager (At one time I worked for a a large American corp and was the account manager to Betfair :grinning:). Spent time at Hammersmith/Gibraltar, still in Account Management. Although I did Contemplate going full time at one point, I still enjoy my job. Money not really the sole motivator for me, Ive given around 40% of my lifetime profits to help others in my family and a couple of close friend who fell on hard times. Can’t describe that feeling of helping others, a pound is a lot when you don’t have it. Just love the challenge of going toe to toe with some of the sharpest minds out there. See you on the back straight in a couple of hours :grinning:

*Tags: General Technical*

---

**Jorge** - *11:20:47*

I'm asking because I'm starting my career as a developer at a quant hedge fund and being playing with Betfair API for a year now, thinking about dedicating more time to BF

*Tags: General Technical*

---

**Misha** - *11:24:01*

There is no equivalent to building a statistical results model in financial trading (which is the way I do it). A good model will make good money for years. And you can just recirculate the money as the return is near instant

*Tags: Strategies*

---

**Jorge** - *11:29:02*

[@U016535QCJ2](@U016535QCJ2) I guess a quant model can suddenly stop working in both places: sports trading and stock market

*Tags: Strategies*

---

**Misha** - *11:29:17*

In terms of scale it's really down to how deep your pockets are and how good is your model. I reckon the upper limit on Betfair might be somewhere in the order of £20M/year profit

*Tags: Strategies*

---

**Misha** - *11:31:03*

Based on a statistical model, yes, because the fundamentals of the sport don't change much, or at least they take a long time to change (years)

*Tags: Strategies*

---

**Misha** - *11:34:42*

Not sure if it will take time for things to settle down for those type of models

*Tags: Strategies*

---

**JC** - *16:44:55*

Hi guys - also a maths graduate (specialising in pure maths). Been employed as an analyst/data scientist at a start up this year. Recently left my job due to starting a masters in computational neuroscience in October. Using this time in between to learn sports trading having had an interest in matched betting (like how many people start) as an undergrad. Hoping to support my studies with betfair trading this year, failing that to at least learn more about automation and modelling! :sunglasses:

*Tags: Strategies*

---

**Twatter** - *17:07:10*

The thing that baffles me is where to start when building a model - seems that a lot of docs/blogs/tutorials and such surrounding it online either boil it down into simple terms (Step1 collect data, step 2 bunch of things in R, Step3 profit!) or dive into a stupid amount of maths straight off.. Anyone know of any resources (text books/uni courses) that kind of explains "what" goes into a model?

*Tags: Strategies*

---

**liam** - *17:07:46*

95% of my strategies are if statements, no modelling as such

*Tags: Strategies*

---

**Twatter** - *17:11:39*

[@U4H19D1D2](@U4H19D1D2) Yeah strategy i'm trying to start with won't require a model either - just interested in knowing where to start

*Tags: Strategies*

---

**Michael** - *17:13:47*

You can forget about online guides, books or whatever. No one who knows how to do it is going to tell you. Use your common sense and maths that you understand - and get betting. Until you've got bets you've got nothing.

*Tags: Strategies*

---

**Twatter** - *17:15:47*

hahaha- Problem is with the way my luck goes, when I start betting I'll have less than nothing!

*Tags: Strategies*

---

**liam** - *17:18:40*

I like to break down like Mike with simple maths, either by modelling the market and finding value or simply placing orders where you think there might be value. The former requires some stats which gets boring very quickly and the latter requires a lot of trial and error and after the time analysis which I prefer 

*Tags: Errors Debugging, Strategies*

---

**birchy** - *18:34:32*

Refreshing to see that [@U4H19D1D2](@U4H19D1D2) is having success with simple "if this condition" type strategies as that's exactly what I do. I basically have a set of functions that check various market conditions and then combine the return values to make a decision. In the early days I would create a strategy based on my understanding of the markets and then go straight to live testing at minimum stakes. I still do that occasionally but mostly gain some confidence first by backtesting against historical data. The one thing you really have to understand is the power of big numbers... i.e. 1000 bets or markets is a reasonable starting point but even that can sometimes be showing a loss for a strategy that is actually profitable over a much larger sample. Variance is both our enemy and our best friend.



There was a time when I would write off bots if they lost £100 at £2 stakes (which was day one in some cases!), but learnt quite a few years ago that you realistically need a bank of at least 1000 bets just to cover the variance.

*Tags: Data Quality, Deployment, Strategies*

---

## 2020-08-27

**InvestInHorses** - *04:57:01*

hi, I joined this Slack channel as I've purchased Betfair historical data and want to build a price cache for back testing my own strategies and creating my own tools for research

*Tags: Data Quality*

---

**InvestInHorses** - *04:58:17*

I'm a developer and I think I know where to start.. But there is a lot of data and hopefully this is a helpful community. Look forward to participating :slightly_smiling_face:

*Tags: General Technical*

---

**Ruben** - *19:49:33*

when processing a market_book with flumine (i.e. process_market_book()), is it possible to access the other markets?

*Tags: General Technical*

---

**liam** - *20:20:31*

It certainly is, so `market.event` will give you a dictionary of all markets for the current market eventId, or you can access the full market list with `self.flumine.markets`

*Tags: General Technical*

---

**birchy** - *21:25:13*

I have a noob question regarding the python logging module... I pretty much copy &amp; pasted [https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|market recorder.py](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|market recorder.py) to an AWS Lightsail instance a month ago and launched as a detached process. I've just discovered that despite the process still running, no data has been saved for over a week. I've not previously used the logging module, so where is the logging info going? I assume there's a file somewhere, otherwise it's not actually "logging" anything?!

*Tags: Data Quality, Deployment*

---

## 2020-08-28

**Unknown** - *07:49:50*

[@U016TGY3676](@U016TGY3676) if you run your code in docker you can use the [https://docs.docker.com/config/containers/logging/awslogs/|CloudWatch](https://docs.docker.com/config/containers/logging/awslogs/|CloudWatch) handler on top and everything from stdout gets piped through. There is also a CloudWatch agent to do the same thing via files but thats more work

*Tags: Deployment*

---

**birchy** - *08:07:32*

[@U4H19D1D2](@U4H19D1D2) that looks pretty nifty! Not using docker, but probably should. Still learning aws, bflw, flumine, and a big list of python modules I've never used before. All a bit overwhelming to be honest. :flushed:

*Tags: Deployment*

---

**Jono** - *08:48:32*

Morning guys, simple question. When filling out the size field for a LAY bet is this value the stake  or liability to be placed?

*Tags: General Technical*

---

**Jono** - *11:16:44*

# login

trading.login_interactive()

# event_type_ids_list = [1]

event_ids = [29975922]





live_events_with_scores = trading.scores.list_available_events(event_ids=event_ids)

for live_event in live_events_with_scores:



    print(live_event.__dict__)

    # event_ids.append(live_event.event_id)





# score request (provide list / returns list)

scores = [http://trading.in|trading.in](http://trading.in|trading.in)_play_service.get_scores(event_ids=event_ids)

print(scores)

for score in scores:

    print(

        score,

        score.description,

        score.status,

        "%s-%s" % (score.score.home.score, score.score.away.score),

    )  # view resources or debug to see all values available

*Tags: Errors Debugging, Deployment, Strategies*

---

**Jono** - *11:23:53*

ah right ok so scores isnt useful in my case. For the in_play_service is just dependent on whether betfair have live data being fed for that specific game? Are there some instances where bf dont bother to have a feed of each teams scores?

*Tags: Deployment*

---

**Taking Value** - *20:42:32*

Hi everyone, first off - thanks to whoever built this library. As a novice python and mongodb coder it has made my life a lot easier.

*Tags: General Technical*

---

## 2020-08-29

**birchy** - *17:21:46*

To all you Python experts...have any of you got a nice solution to importing from a folder/package which is several directories higher than the python script doing the import? e.g. package is located at:

`/home/birchy/betfair/botpkg`

and bots are located at something like:

`/home/birchy/betfair/bots/laying/laybot/laybot.py`

`/home/birchy/betfair/bots/backing/backbot/backbot.py`

etc...

Currently using a symlink but it's not the prettiest solution...

*Tags: General Technical*

---

**birchy** - *17:30:54*

it's the botpkg that I'm importing into each bot as it contains a large number of helper functions and I prefer to manage them as a single source for all bots rather than have multiple copies. With the symlink, it's simply `import botpkg` in each bot. It works, but I'm wondering if there is a nicer way?

*Tags: General Technical*

---

**birchy** - *18:48:23*

The only downside I can see with making it an installable package is that I'd have to recompile every time I edit the botpkg, although that's not frequent. My `botpkg/__init__.py` file looks like this:

```import os

import botpkg.betfair.api

import botpkg.betfair.racecard_api

import botpkg.betfair.price_mod

import botpkg.logger

import botpkg.testbed

import botpkg.trading

import botpkg.compile

import botpkg.filing

import botpkg.utils

import botpkg.credentials



path = os.path.dirname(os.path.dirname(__file__))

api = botpkg.betfair.api.API()

pricemod = botpkg.betfair.price_mod

logger = botpkg.logger.Logger()

testbed = botpkg.testbed.TestBed()

trading = botpkg.trading.Trading()

racecard_api = botpkg.betfair.racecard_api.API()

compile = botpkg.compile.Compile()

filing = botpkg.filing.Filing()

utils = botpkg.utils.BotFunctions()```

The main reason I took that route is because I like minimal code and wanted to avoid having lots of imports at the top of each bot. With this method, each bot only needs `import botpkg` and everything becomes available. I know it's a bit weird, but it's also very convenient. Obviously, that is the way I do things in my own library, but now I'm looking at bflw/flumine, I'm researching the best way to implement something similar.

Regarding making the package installable, which method do you recommend?

*Tags: Getting Started, Strategies*

---

**Mo** - *18:52:09*

As for how to make it installable, I’d suggest just looking at the setup.py file in a package like bflw

*Tags: Getting Started*

---

**birchy** - *19:14:34*

Yeah, I've never needed to make any python code installable before, but just had a quick Google and it looks like the best way would be to create a single "botting" venv for all bots and then `python setup.py install --user` for my package? Or do you use `pip` in local mode?

This? [https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs](https://pip.pypa.io/en/stable/reference/pip_install/#editable-installs)

*Tags: Getting Started*

---

## 2020-08-30

**Taking Value** - *14:23:50*

Can I just check, I am not getting very frequent updates via the listeners. I think its because I am only using the demo API key at the moment. Is there a way to captures all updates to market data using the demo key or must you have the live key. I am happy to pay BF for the live key but there is a bit of a questionnaire to fill in first. Is this something to be taken seriously, have there been cases where people have lost access to BF because they didn't bet frequently enough or in a large enough volume?  I was going to do nothing but record the data for half a year first.

*Tags: Deployment*

---

**Taking Value** - *14:52:58*

Yea I realised its missing key data e.g. traded volume. Very few of the market updates come through too. I can still plan my schema for the database using it though and I will get that script written fir the two quid bets before I go live so that I don't piss them off. Thanks for the help guys.

*Tags: Deployment*

---

**Unknown** - *18:26:45*

I am still on a trial key until next week or so but am getting traded volume so you may have a different problem here as well. Make sure you are requesting the fields

```fields=[

    "EX_ALL_OFFERS",

    "EX_TRADED",

    "EX_TRADED_VOL",

    "EX_LTP",

    "EX_MARKET_DEF",

    "SP_TRADED",

    "SP_PROJECTED",

]```

[https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Exchange+Stream+API](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Exchange+Stream+API)

*Tags: General Technical*

---

**Ruben** - *21:20:46*

[@U4H19D1D2](@U4H19D1D2) might be able to help you with getting your live key activated

*Tags: Deployment*

---

**Mo** - *21:21:40*

[@U4H19D1D2](@U4H19D1D2) can _definitely_ help with getting your live key activated

*Tags: Deployment*

---

## 2020-08-31

**Angelos** - *12:57:35*

hi there,



i just started looking into betfair libraries and i’m so glad i found this community



got stuck with a few things, could you give me some pointers please?



i just switched from the traditional api to the streaming one but i get a

```data: {"op":"status","id":2,"statusCode":"FAILURE","errorCode":"NOT_AUTHORIZED","errorMessage":"AppKey is not configured for service"```

i’m currently using the delayed key just to play for now

*Tags: Errors Debugging*

---

**Mo** - *13:01:40*

You need to ask BDP for your app key to be enabled for streaming 

*Tags: General Technical*

---

**Mo** - *13:02:34*

If you use betfairlightweight then you’ll be able to access streaming data in the same structures you’d get back from using listMarketBook or listCurrentOrders

*Tags: General Technical*

---

**Angelos** - *13:05:41*

yea that makes sense!



but i have just discovered this library, so in the past i created my own client wrapper.



so unless betfairlightweight didn’t do anything special with the api responses i should be safe!

*Tags: General Technical*

---

**ricky** - *16:12:38*

Hi there,

I am using streaming API, I have two questions,

1) When i receive all three (home, away, draw) back/lay prices from runner books, How do i check which array index is for home game?  I observed in football game the order of return value (home, away, draw) from each runner books are changed per match. (e.g Match 1 runner_book return: Away, Draw, Home; Match 2 runner_book return: Home, Draw, Away;)

2) If i create filter with two market types "MATCH_ODDS" and "OVER_UNDER_25" , when new streaming data get update, how can i check this update is from which market type?

*Tags: General Technical*

---

**Mo** - *16:45:27*

If you are using the streaming API with betfairlightweight

*Tags: General Technical*

---

**Mo** - *16:46:43*

You can find the definition here: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py)

*Tags: General Technical*

---

## 2020-09-01

**liam** - *09:15:15*

I personally use AWS lambda for all cron type work

*Tags: Deployment*

---

**Mathias Tejs** - *09:18:53*

Depending on your setup (e.g. if you're jobs are in python), celery can be a good choice. For instance, it's got flower for monitoring job status in a GUI out of the box, and it handles automatic restart on errors, storing task results and so on.

*Tags: Getting Started, Errors Debugging*

---

## 2020-09-02

**liam** - *09:00:34*

Looking for beta testers for [https://pypi.org/project/betfairlightweight/2.8.0b0/|2.8.0b0](https://pypi.org/project/betfairlightweight/2.8.0b0/|2.8.0b0) going to beta first because of the change to orjson (was ujson) PR [https://github.com/liampauling/betfair/pull/330|here](https://github.com/liampauling/betfair/pull/330|here)

*Tags: General Technical*

---

**Taking Value** - *09:49:43*

Can I just check when collecting data, storing it in a database and operating bots would it be advisable to use a cloud based solution or to buy/build some basic servers. What do others here do?

*Tags: Deployment*

---

**Mo** - *09:51:42*

Combination of AWS and dedicated servers

*Tags: Deployment*

---

**Misha** - *09:57:33*

Spin up a server from any provider (I use Vultr). I just need the machine - I don't need any AWS services, so that might sway your decision. I definitely don't run any servers on location any more

*Tags: Deployment*

---

**liam** - *10:16:51*

I store all streaming data in AWS s3 (object storage) and then use AWS rds for db, all managed and easy to setup with terraform

*Tags: Getting Started, Deployment*

---

**liam** - *10:53:24*

Cheers, one test fails on flumine but its because the transaction limit has changed from 1k to 5k

*Tags: General Technical*

---

**liam** - *12:59:33*

So sentry will tell me if an error has been caught or unhandled, if it’s really bad I get a slack notification. I use CloudWatch for day to day logs / checking on things 

*Tags: Errors Debugging*

---

**Taking Value** - *13:07:26*

Thanks for all the input guys. I will go for AWS probably, depends on the cost.

*Tags: Deployment*

---

**Misha** - *13:08:36*

That's not bad (I have customised software that I developed myself), but my main question is: how do you "eyeball" the data generated by the system? So not error data, actual system captured data (like you store on AWS)

*Tags: Errors Debugging, Deployment*

---

**liam** - *13:16:06*

Not sure what you mean [@U016535QCJ2](@U016535QCJ2), I like many are just processing streaming data as it comes and making decisions based off it. The only system generated data I have is all in cloudwatch such as latencies / request and response logs and strategy specific details. Market / Order data I log but also store in a db through an API for later processing 

*Tags: Strategies*

---

**Chris** - *19:10:07*

What does the ORDER_PACKAGE event represent? I am trying to self-serve here but I don't see it listed in the documentation at [https://liampauling.github.io/flumine/design/](https://liampauling.github.io/flumine/design/). Thanks in advance

*Tags: General Technical*

---

## 2020-09-03

**Taking Value** - *09:54:01*

I'm storing whole race as a single document. Bit of meta data before hand to make the document identifiable but then all the runner changes sit in an array in a single key value pair. I must admit I didn't think it would come close to 16MB for a race but Mongo is spitting the error at me. I guess I could easily store each runner change as its own document, they are tagged with the race id after all.

*Tags: Errors Debugging*

---

**Mo** - *09:54:56*

I don’t think many of us are in the habit of storing price data in a database

*Tags: General Technical*

---

**Taking Value** - *09:55:24*

If you don't have teh price data in a database how do you back test?

*Tags: General Technical*

---

**Oliver Varney** - *09:57:08*

I do for model training but limit it to 60 seconds

*Tags: Strategies*

---

**Mo** - *09:57:36*

I’ll treat you as the exception that proves the rule 

*Tags: Errors Debugging*

---

**Taking Value** - *10:46:28*

Thanks for the input all. Will probably spend most of the day considering what to do. The Amazon S3 option is a curve ball, didn't know anything about it before today. I had just assumed everyone would be operating an SQL or NoSQL database with a self built GUI on the front. At least I have some new topics to research, always fun.

*Tags: Getting Started*

---

**Lee** - *10:47:44*

[@U019HMPCQT0](@U019HMPCQT0) you also could use flumine, there's an example how to record the streaming data to files and upload to S3 [https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|here](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|here)

*Tags: General Technical*

---

**ricky** - *16:25:11*

Anyone know how traded volumed calcuated in betfair?

I place a £3 back bet @ 3.30, it shows the traded volumed changed is £6.

I place a £3 Lay bet @3.9, it shows the traded volumed changed is 6.

But when I place a another £5 draw Lay bet @3.95, it shows the traded volumed changed is 5, i can see at the same time home and away traded volumed changed.



My question: traded volume is alway double the normal bet/Stake value? Why lay traded volume sometime show double sometime not?

*Tags: General Technical*

---

**Chris** - *18:31:16*

Is there an easy way to see what new data was sent that ends up triggering a CURRENT_ORDERS event? Does the CURRENT_ORDERS event only get called if I receive new order data?

Context: I am streaming data for a single market and am sending sample trades for a single runner in that market. The test trade that I sent is not getting filled (it is a lay order at 1.01). Despite that, I am getting ~10 CURRENT_ORDERS events for ever one MARKET_BOOK event.

*Tags: General Technical*

---

**liam** - *18:42:27*

On an update or every 0.25s (default) if there are executable orders in the market [https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/streams/orderstream.py#L51|https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/streams/orderstream.py#L51](https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/streams/orderstream.py#L51|https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/streams/orderstream.py#L51)

*Tags: General Technical*

---

**liam** - *18:42:52*

If you want less change the streaming_timeout or change that code 

*Tags: General Technical*

---

## 2020-09-04

**Peter** - *04:19:57*

A flat file is just a file. Flat indicates that it has no hierarchy (unlike say a database which is normally a collection of related files), so if you dump json to a file that's "flat" since the file itself has no special structure (even if it's content does).

*Tags: General Technical*

---

## 2020-09-05

**Oliver Varney** - *15:04:40*

Can anyone help me with more information on the error code ERROR_IN_ORDER. All the docs say is, "The action failed because the parent order failed". Probably something silly im doing but not sure where to start

*Tags: Errors Debugging*

---

## 2020-09-06

**liam** - *06:25:08*

If you set logging to debug you can see the updates / heartbeats 

*Tags: Errors Debugging*

---

**liam** - *07:20:06*

Why? Are you using the production example? [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py|https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py|https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

*Tags: Deployment*

---

**Dylan** - *07:42:51*

Yes, I am using that production example. I have it running as a Daemon, and Ideally it would be to prevent multiple threads being spawned unnecessarily if it was invoked more than once

*Tags: Deployment*

---

**ricky** - *12:35:33*

Hi everyone, i am trying to collect "MATCH_ODDS" and "OVER_UNDER_25" football match data into a single txt file.  I am streaming from a single thread, however, after sometime the program stop with error. (i tried set both

"output_queue = queue.Queue()" and "output_queue = None" without luck)



I can stream separate "MATCH_ODDS" or "OVER_UNDER_25" market without error, My question is do i need two threads to handle each market streaming data? Any existing example handle multiple thread streaming?

*Tags: Errors Debugging*

---

**liam** - *12:36:48*

Whats the error?

*Tags: Errors Debugging*

---

**liam** - *12:37:17*

/ you need to handle betfair and network errors like the example above does

*Tags: Errors Debugging*

---

**ricky** - *12:46:05*

The error happened after steaming udpate, i read each items from process_runner_books(runner_book).

Log shows "pandas/core/series.py. line 871, __getitem result = self.index.get_value(self, key)"

Key Error:2

*Tags: Errors Debugging, Feature Engineering*

---

**Mo** - *12:47:28*

You’re going to have to share your code, it’s something to do with how you’re transforming the data using pandas, nothing to do with bflw itself

*Tags: Feature Engineering*

---

**Mo** - *12:58:24*

No, you need to fix your code. Looks like you’re trying to access the third runner in a two runner market. But that’s all I can say without seeing it

*Tags: Errors Debugging*

---

**ricky** - *13:00:18*

Sure, i will check, thank you for your help.:+1:

*Tags: General Technical*

---

**ricky** - *20:28:00*

[@UBS7QANF3](@UBS7QANF3) Problem solved, you are absolutely right. Previously i got my market_type value wrong. thanks.

*Tags: General Technical*

---

## 2020-09-08

**Oliver Varney** - *09:35:29*

morning all, probably a very basic question, but wit the list_cleared_orders more available, once ive got the initial paginated results whats the best way to access the next results?

*Tags: General Technical*

---

**liam** - *09:36:15*

how i do it [https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/worker.py#L165](https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/worker.py#L165)

*Tags: General Technical*

---

**D C** - *11:52:10*

That is quite a big question [@U013MLED3V1](@U013MLED3V1) and depends exactly what you mean by a peak. It can have a very simple answer or a very complex one.

*Tags: General Technical*

---

**Unknown** - *12:56:52*

[@UUE6E1LA1](@UUE6E1LA1) I have just tried to implement zscore smoothed algorithm, and applying that for historical prices traded on selection or just last couple of points my app recorded. Have a look at screenshot, on the left side you have got my code and test chart, on the right side algorithm applied on betfair data. My question is what other devs here use? Or if there was anyone trying to use ML, or really simply naive algorithm, just to compare couple of last time traded prices and from std  to detect up or down signal when detecting peak or negative peak. I am looking for any feedback, experience others have got.

*Tags: General Technical*

---

**Misha** - *22:42:24*

[@U013MLED3V1](@U013MLED3V1) - no general algorithm will work. Some prices are forced one way by the way money is put on the prices. In-play is influenced by scores. You can't come up with a good general solution when you are solving different problems

*Tags: General Technical*

---

## 2020-09-09

**Chris** - *01:11:05*

Hopefully a quick two-parter regarding live orders  (Market --&gt; Blotter --&gt; Live Orders) and market/runner exposure. My goal here is to calculate exposure at the runner level so I can handle inventory at that level (ex. if I already have X exposure on a runner then don't buy/sell any more [I don't know where to get the X number])

1. Is an order only considered live if it has at least some unmatched size in the market?

2. Once an order has been completely filled is it then considered where can I access them? Do I need to use market.blotter.strategy_orders(strategy) and then parse it down to the runner / market level?

*Tags: Deployment, Strategies*

---

**D C** - *09:56:56*

[@U013MLED3V1](@U013MLED3V1) There are a multitude of ways you could deal with this but only you know what precisely defines a peak and you need to work on that basis. For example, you say in that example the middle point is the peak but only you know why that plot has just ONE peak instead of 3 or 0. Also depends on what you are trying to do with it and if you need it in real time (in which case you are restricted to causal filter/analysis) or a retrospective analysis. . If I were dealing with this problem myself the first thing I would do is pin down parameters that will define your peaks and go from there. I guess that depends on the problem you are trying to solve too - you might need to try something different if you are looking to identify a cricket sniper error or a fat finger order than if you were looking for the top of a price oscillation in a pre-race horse market.

*Tags: Errors Debugging*

---

**Unknown** - *12:07:06*

Thanks [@UUE6E1LA1](@UUE6E1LA1) but my question was really simple, a name of algorithm, if it is common one, for instance from ML, or stock indexes, any one here is using. From answers, no one here is using peak detection, at least from those ones replying. I implemented 3 types of algorithm, from screenshots is clear which one I will use.

*Tags: General Technical*

---

**D C** - *12:13:23*

[@U013MLED3V1](@U013MLED3V1) Sorry - I used to work as a researcher in surface metrology so to me your question is very deep and the answer depends on scale, goals, resource restrictions and assumptions. But I understand you just want a fast answer and maybe something quick and "dirty". But ML is surely overkill for such a problem.  FWIW I would look at applying some type of discrete wavelet transform but again that is probably overkill depending on your needs. Hope you find an answer that matches your requirements.

*Tags: General Technical*

---

**Misha** - *12:49:09*

A more pertinent question is "is your peak detection algorithm accurate enough to be useful"? Very easy to implement an algorithm, much harder to determine whether the results are useful

*Tags: General Technical*

---

**liam** - *14:07:41*

I don't recognise those docs as it doesn't even match up with the streaming API but the 200 limit is a 200 limit and you would need to get it raised by emailing bdp

*Tags: General Technical*

---

**Newbie99** - *14:08:16*

In theory you do subscribe to everything by doing that...but in reality your connection is immediately killed and you get an error

*Tags: Errors Debugging*

---

**Newbie99** - *14:09:24*

```{"asctime": "2020-09-09 13:09:03,511", "levelname": "ERROR", "message": "[Subscription: 2002] SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 9243 markets whereas max allowed number was: 200"}```

*Tags: Errors Debugging*

---

**liam** - *14:36:43*

1. Yes an order is live if betfair consider it EXECUTABLE ie. unmatched 

2. All orders will be in the blotter, you can get your strategy orders from that function you have mentioned or the exposure using `selection_exposure` the plan is to add more helpful functions [https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/markets/blotter.py#L96|here](https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/markets/blotter.py#L96|here) so welcome any ideas

*Tags: Deployment, Strategies*

---

**Jonjonjon** - *16:01:46*

[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1591873545205700?thread_ts=1591861538.194100&amp;cid=CTPL3R3FU](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1591873545205700?thread_ts=1591861538.194100&amp;cid=CTPL3R3FU)

*Tags: General Technical*

---

**Jonjonjon** - *16:32:01*

As part of my code, I have a function that gets the parameters that I want to run. The parameters predominantly choose the size per bet and max position. e.g. I might trade in clips of £2, up to £10 of risk. Before I run a strategy on a market, I randomly choose 1 of 2 sets of parameters. I use the customer_strategy_ref (might be a typo there) field to tag a trade to a strategy. e.g. doggies_2_10, or doggies 10_40.

*Tags: Strategies*

---

**Jonjonjon** - *16:32:39*

At a later point I can download my orders, and use pandas to aggregate the results by customer_strategy_ref and market.

*Tags: Feature Engineering, Strategies*

---

**liam** - *16:36:09*

your using flumine yeah?

*Tags: General Technical*

---

**Jonjonjon** - *16:47:38*

Alas for that bit of my code base it is using BFLW only. I'm currently using Flumine for my backtesting, but that has yet to bear any fruit.

*Tags: General Technical*

---

**Jonjonjon** - *16:49:26*

Wasting lots of time tinkering with my new machine. My Ryzen 9 3950X  can backtest 1,000 markets, using Flumine, in 45 seconds.

*Tags: General Technical*

---

**Jonjonjon** - *19:50:26*

For the backtests in question, I was only looking at the last 10 minutes before a race.

*Tags: General Technical*

---

## 2020-09-10

**Lee** - *11:35:55*

Some of my API calls in `_process_order`  LoggingControl failed to post to an API successfully (a bug i've now fixed), but is there a way to fill these in? I was trying to use `list_cleared_orders` but as it's missing the flumine trade property i'm not sure how i'll know which strategy placed that order.

*Tags: Errors Debugging, Strategies*

---

**Chris** - *17:04:06*

Do matched and unmatched orders that existed in a market before starting flumine ever get added to the blotter after starting flumine? From some light testing yesterday I didn't see them getting added but I could be mistaken.

*Tags: General Technical*

---

**liam** - *20:17:35*

If they were created by flumine then yes

*Tags: General Technical*

---

**Unknown** - *21:37:54*

I have matched orders that were originally sent from flumine but I don't see them getting added to the blotter. Is there a different way that I am supposed to access them?

Included screenshot to provide context. Mainly, the blotter length is two, which represents the two active orders but does not include the previous matched orders that have net exposure. Note that the matched orders were sent by flumine session that was killed and they potentially filled while flumine was not listening (not sure if this makes a difference)

*Tags: General Technical*

---

**Lee** - *21:40:09*

Were the previously matched orders sent by flumine on the same machine? I believe it filters the orders by hostname

*Tags: General Technical*

---

**Chris** - *21:41:28*

Yes, they were sent by flumine on the same machine with the same vpn session

*Tags: General Technical*

---

## 2020-09-11

**nthypes** - *00:21:59*

Something strange is happening. The "mcm" message is not streaming _trd_ deltas. Is this normal ?



This is my filter:



```      marketDataFilter: {

         fields: ["EX_BEST_OFFERS_DISP","EX_BEST_OFFERS","EX_ALL_OFFERS","EX_TRADED","EX_TRADED_VOL","EX_LTP","EX_MARKET_DEF","SP_TRADED","SP_PROJECTED"],

         ladderLevels: 2

      }```

*Tags: General Technical*

---

**Chris** - *02:07:08*

Can you give more info here?

I am using the default filter and received trd messages in the mcm messages earlier today.

Default Filter:

```DEFAULT_MARKET_DATA_FILTER = filters.streaming_market_data_filter(

    fields=[

        "EX_ALL_OFFERS",

        "EX_TRADED",

        "EX_TRADED_VOL",

        "EX_LTP",

        "EX_MARKET_DEF",

        "SP_TRADED",

        "SP_PROJECTED",

    ]

)```

Example Data:

`{"op": "mcm", "pt": 1599764467703, "clk": null, "mc": [{"id": "1.172246112", "rc": [{"atl": [[12, 350.85]], "trd": [[12, 7019.34]], "tv": 18660.5, "id": 48317}], "tv": 181019.99}]}`

*Tags: General Technical*

---

**Unknown** - *14:05:41*

[@U4H19D1D2](@U4H19D1D2) or others - Got it, fully matched orders don't come through the stream as orders given they are execution_complete. However, I see the resulting net position information coming in the door in an order change message within the matched bets field.



How do I access this information at the top level flumine.run() loop?



There's a reasonable chance I am just looking in the wrong place but I have searched numerous objects for it (the blotter, the market, the strategy, etc. and now I am looking into the runner). Note that I am using the same machine/strategy that the orders were originally sent from.



ocm message:

```{"op":"ocm","id":1001,"initialClk":"FMDN+IcDFfqk/oADFYmTtIUDFNWit4oDFP+m7oMD","clk":"AAAAAAAAAAAAAA==","conflateMs":0,"heartbeatMs":5000,"pt":1599791822669,"ct":"SUB_IMAGE","oc":[{"id":"1.172246112","orc":[{"fullImage":true,"id":56323,"smc":{"DESKTOP-PUCIJ46":{"mb":[[1.31,9],[1.3,9]]}}}]}]}```

Here is a view of the runner a few updates after the above message was received, which shows nothing matched (the blue boxed are where I would intuitively expect this information to be stored):

*Tags: Strategies*

---

**liam** - *15:18:04*

[@UUCD6P13J](@UUCD6P13J) that is only for backtesting, if you want BPE off you need to manually do it on the site at the account level, need to add an error / validation on this 

*Tags: Errors Debugging*

---

**Chris** - *15:57:46*

I see the correct oc / mb message make it in the door and the corresponding market (1.72246112) get added to the OrderStream and Stream. However, I don't see that market get added to flumine.baseflumine until a later mc message that gets processed _after_ the oc message. Is it possible that the implementation depends on the mc message arriving first?



2020-09-11 14:44:08,606 - flumine.baseflumine - INFO - Adding trading control ORDER_VALIDATION

2020-09-11 14:44:08,606 - flumine.baseflumine - INFO - Adding trading control STRATEGY_EXPOSURE

2020-09-11 14:44:08,607 - flumine.baseflumine - INFO - Adding client control MAX_ORDER_COUNT

2020-09-11 14:44:08,607 - flumine.baseflumine - INFO - Adding strategy NaiveMarketMakerStrategy

2020-09-11 14:44:08,607 - flumine.streams.streams - INFO - Creating new &lt;class 'flumine.streams.marketstream.MarketStream'&gt; (2000) for strategy NaiveMarketMakerStrategy

2020-09-11 14:44:08,607 - flumine.baseflumine - INFO - Starting flumine

2020-09-11 14:44:08,609 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): [http://identitysso-cert.betfair.com:443|identitysso-cert.betfair.com:443](http://identitysso-cert.betfair.com:443|identitysso-cert.betfair.com:443)

2020-09-11 14:44:09,372 - urllib3.connectionpool - DEBUG - [https://identitysso-cert.betfair.com:443](https://identitysso-cert.betfair.com:443) "POST /api/certlogin HTTP/1.1" 200 87

2020-09-11 14:44:09,377 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): [http://api.betfair.com:443|api.betfair.com:443](http://api.betfair.com:443|api.betfair.com:443)

2020-09-11 14:44:10,085 - urllib3.connectionpool - DEBUG - [https://api.betfair.com:443](https://api.betfair.com:443) "POST /exchange/account/json-rpc/v1 HTTP/1.1" 200 176

2020-09-11 14:44:10,088 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): [http://api.betfair.com:443|api.betfair.com:443](http://api.betfair.com:443|api.betfair.com:443)

2020-09-11 14:44:10,653 - urllib3.connectionpool - DEBUG - [https://api.betfair.com:443](https://api.betfair.com:443) "POST /exchange/account/json-rpc/v1 HTTP/1.1" 200 158

2020-09-11 14:44:10,656 - flumine.worker - INFO - BackgroundWorker keep_alive starting

2020-09-11 14:44:10,656 - flumine.worker - DEBUG - BackgroundWorker keep_alive executing

2020-09-11 14:44:10,656 - flumine.worker - INFO - BackgroundWorker poll_account_balance starting

2020-09-11 14:44:10,656 - flumine.worker - INFO - BackgroundWorker poll_market_catalogue starting

2020-09-11 14:44:10,657 - flumine.worker - INFO - BackgroundWorker poll_cleared_orders starting

2020-09-11 14:44:10,657 - flumine.streams.streams - INFO - Starting streams..

2020-09-11 14:44:10,657 - flumine.streams.orderstream - INFO - Starting OrderStream

2020-09-11 14:44:10,657 - flumine.streams.orderstream - INFO - Starting output_thread &lt;Thread(OrderStream_output_thread, initial daemon)&gt;

2020-09-11 14:44:10,658 - betfairlightweight.streaming.listener - INFO - Register: orderSubscription 1001

2020-09-11 14:44:10,658 - betfairlightweight.streaming.stream - INFO - [Stream: 1001]: "OrderStream" created

2020-09-11 14:44:10,658 - flumine.streams.marketstream - INFO - Starting MarketStream

2020-09-11 14:44:10,658 - flumine.streams.marketstream - INFO - Starting output_thread &lt;Thread(MarketStream_output_thread, initial daemon)&gt;

2020-09-11 14:44:10,659 - betfairlightweight.streaming.listener - INFO - Register: marketSubscription 2001

2020-09-11 14:44:10,659 - betfairlightweight.streaming.stream - INFO - [Stream: 2001]: "MarketStream" created

starting strategy 'ExampleStrategy'

2020-09-11 14:44:11,333 - betfairlightweight.streaming.betfairstream - DEBUG - [Subscription: 2002] Sending: '{"op": "authentication", "id": 2002, "appKey": "bOP9A5Vmp8n8K48d", "session": "XIlFe2j97G/m11sC7MXvyXllXn+/pQA3q/6nRSlqnf8="}\r\n'

2020-09-11 14:44:11,333 - betfairlightweight.streaming.betfairstream - DEBUG - [Subscription: 2002] Sending: '{"op": "marketSubscription", "id": 2001, "marketFilter": {"marketIds": ["1.172246112"]}, "marketDataFilter": {"fields": ["EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP", "EX_MARKET_DEF", "SP_TRADED", "SP_PROJECTED"]}, "initialClk": null, "clk": null, "conflateMs": null, "heartbeatMs": null, "segmentationEnabled": true}\r\n'

2020-09-11 14:44:11,335 - betfairlightweight.streaming.listener - INFO - [Connect: 2001]: connection_id: 103-110920144408-1944934

2020-09-11 14:44:11,335 - betfairlightweight.streaming.betfairstream - DEBUG - [Subscription: 1002] Sending: '{"op": "authentication", "id": 1002, "appKey": "bOP9A5Vmp8n8K48d", "session": "XIlFe2j97G/m11sC7MXvyXllXn+/pQA3q/6nRSlqnf8="}\r\n'

2020-09-11 14:44:11,335 - betfairlightweight.streaming.betfairstream - DEBUG - [Subscription: 1002] Sending: '{"op": "orderSubscription", "id": 1001, "orderFilter": {"includeOverallPosition": false, "customerStrategyRefs": ["DESKTOP-PUCIJ46"], "partitionMatchedByStrategyRef": true}, "initialClk": null, "clk": null, "conflateMs": null, "heartbeatMs": null, "segmentationEnabled": true}\r\n'

2020-09-11 14:44:11,336 - betfairlightweight.streaming.listener - INFO - [Connect: 1001]: connection_id: 104-110920144408-1957086

2020-09-11 14:44:11,455 - betfairlightweight.streaming.listener - INFO - [Subscription: 1002]: SUCCESS (9 connections available)

2020-09-11 14:44:11,455 - betfairlightweight.streaming.listener - INFO - [Subscription: 1001]: SUCCESS (9 connections available)

2020-09-11 14:44:11,463 - betfairlightweight.streaming.listener - INFO - [Subscription: 2002]: SUCCESS (8 connections available)

*2020-09-11 14:44:11,475 - betfairlightweight.streaming.listener - DEBUG - [Subscription: 1001]: SUB_IMAGE: {'op': 'ocm', 'id': 1001, 'initialClk': 'FI60s4gDFZzFy4EDFcDZ+IUDFMup9YoDFLfurYQD', 'clk': 'AAAAAAAAAAAAAA==', 'conflateMs': 0, 'heartbeatMs': 5000, 'pt': 1599835448156, 'ct': 'SUB_IMAGE', 'oc': [{'id': '1.172246112', 'orc': [{'fullImage': True, 'id': 56323, 'smc': {'DESKTOP-PUCIJ46': {'mb': [[1.31, 9], [1.3, 9]]}}}]}]}*

*2020-09-11 14:44:11,475 - betfairlightweight.streaming.stream - INFO - [OrderStream: 1001] 1.172246112 added, 1 markets in cache*

*2020-09-11 14:44:11,476 - betfairlightweight.streaming.stream - INFO - [Stream: 1001]: 1 oc added*

2020-09-11 14:44:11,479 - betfairlightweight.streaming.listener - INFO - [Subscription: 2001]: SUCCESS (8 connections available)

2020-09-11 14:44:11,503 - betfairlightweight.streaming.listener - DEBUG - [Subscription: 2001]: SUB_IMAGE: {'op': 'mcm', 'id': 2001, 'initialClk': 'whrdibCSB8wa//HplwfFGo7WiZkH', 'clk': 'AAAAAAAA', 'conflateMs': 0, 'heartbeatMs': 5000, 'pt': 1599835448182, 'ct': 'SUB_IMAGE', 'mc': [{'id': '1.172246112', 'marketDefinition': {'bspMarket': False, 'turnInPlayEnabled': True, 'persistenceEnabled': True, 'marketBaseRate': 5, 'eventId': '29969294', 'eventTypeId': '1', 'numberOfWinners': 1, 'bettingType': 'ODDS', 'marketType': 'MATCH_ODDS', 'marketTime': '2020-09-12T16:30:00.000Z', 'suspendTime': '2020-09-12T16:30:00.000Z', 'bspReconciled': False, 'complete': True, 'inPlay': False, 'crossMatching': True, 'runnersVoidable': False, 'numberOfActiveRunners': 3, 'betDelay': 0, 'status': 'OPEN', 'runners': [{'status': 'ACTIVE', 'sortPriority': 1, 'id': 56323}, {'status': 'ACTIVE', 'sortPriority': 2, 'id': 48317}, {'status': 'ACTIVE', 'sortPriority': 3, 'id': 58805}], 'regulators': ['MR_INT'], 'countryCode': 'GB', 'discountAllowed': True, 'timezone': 'GMT', 'openDate': '2020-09-12T16:30:00.000Z', 'version': 3356134545, 'priceLadderDefinition': {'type': 'CLASSIC'}}, 'rc': [{'atb': [[10, 422.04], [10.5, 556.23], [9.8, 141.82], [9.4, 2], [1.01, 73767.48], [9.6, 20], [1.04, 18699.25], [1.02, 62403.5], [1.03, 44624.06], [2, 1.47], [1.1, 10000], [1.05, 14000], [1.09, 10000], [1.08, 10000], [1.07, 14000], [1.06, 14000]], 'atl': [[11, 120.65], [11.5, 603.9], [12, 511.84], [12.5, 568.7], [15.5, 17.5], [13.5, 749.87], [1000, 7.9], [14, 422.21], [13, 490.67], [15, 117.5], [16, 2]], 'trd': [[11, 3916], [10.5, 1267.23], [11.5, 7465.42], [13.5, 60.63], [13, 168.73], [12.5, 5668.28], [12, 9072.82], [10, 22.17], [9.6, 10.32], [9.8, 29.56]], 'ltp': 11, 'tv': 27681.16, 'id': 48317}, {'atb': [[1.32, 8095.3], [1.29, 36848.2], [1.3, 27113.82], [1.31, 9545.35], [1.01, 148763.48], [1.27, 30772.3], [1.28, 34136.61], [1.04, 18699.25], [1.24, 30010.9], [1.22, 25824.06], [1.02, 137403.5], [1.25, 50201.35], [1.23, 20937.03], [1.26, 50513.1], [1.03, 59624.06], [1.21, 25], [1.09, 10100], [1.1, 10000], [1.08, 10000], [1.07, 14000], [1.06, 14000], [1.05, 14000]], 'atl': [[1.33, 1193.27], [1.34, 1675.23], [1.35, 2113.48], [1.4, 20], [1.38, 600], [1.36, 1436.29], [2, 11.85], [1.45, 6.53], [1.8, 3.66], [1000, 5.05], [10, 82]], 'trd': [[1.32, 13620.02], [1.33, 42973.12], [1.34, 1499.87], [1.31, 36034.48], [1.29, 31452.81], [1.3, 64317.62], [1.27, 3095.66], [1.28, 22549.18]], 'ltp': 1.32, 'tv': 215542.76, 'id': 56323}, {'atb': [[6.6, 103.43], [6.4, 449.32], [6.2, 499.94], [5.8, 2], [5.9, 139.33], [6, 464.6], [1.01, 43.45], [1.03, 2], [3.5, 5]], 'atl': [[6.8, 391.26], [7.2, 1193.74], [7, 1406.89], [7.8, 708.38], [1000, 7.9], [8, 677.37], [7.4, 1667.71], [7.6, 892.78], [950, 2.2], [13.5, 2]], 'trd': [[6.6, 4780.12], [6.8, 11467.19], [6.4, 1194.02], [6.2, 26.38], [7, 12286.69], [7.4, 1110.62], [7.2, 3942.68], [7.6, 127.79], [6, 8], [7.8, 0.2]], 'ltp': 6.6, 'tv': 34943.69, 'id': 58805}], 'img': True, 'tv': 278167.61}]}

2020-09-11 14:44:11,503 - betfairlightweight.streaming.stream - INFO - [MarketStream: 2001] 1.172246112 added, 1 markets in cache

2020-09-11 14:44:11,505 - betfairlightweight.streaming.stream - INFO - [Stream: 2001]: 1 mc added

2020-09-11 14:44:11,506 - flumine.baseflumine - INFO - Adding: 1.172246112 to markets

*Tags: Errors Debugging, Deployment, Strategies*

---

**Chris** - *16:56:39*

Alright - I am holding that the exposure info is not propagated into any of the objects in a way that is easily routinely accessible. Will proceed by calling all of the current orders via bflw (trading.betting.list_current_orders()) and then writing that info down in the strategy

*Tags: Strategies*

---

**jhaa** - *17:25:30*

I have a newish bot running that uses the streaming. It runs fine for a few days without any flaws and then it goes beserk and trades markets inplay even though it is build to not to do that and it places bets multiple times because it assumes that they were not places yet.



I had the bet placement issue previously and sort of fixed that by removing ujson.



Any idea what might cause this? It sort of looks like something is wrong further down the software stack or I get wrong stuff through the stream

*Tags: Errors Debugging, Deployment*

---

**liam** - *18:38:49*

I believe it will be what [@UFTBRB3F1](@UFTBRB3F1) is seeing with the queue filling up due to low ram, see issue [https://github.com/liampauling/flumine/issues/269|269](https://github.com/liampauling/flumine/issues/269|269)

*Tags: General Technical*

---

**liam** - *19:11:55*

So 7k isn’t going to help things, 15s, I doubt the snap function took that long, I assume you mean from the snap to actually processing the data?



This is a problem with using queues, you have a slow down due to an increase in updates that then clogs up your ram, you then max out your CPU in trying to clear the queue, the queue then starts to exponentially increase in size before the dreaded memory fault  

*Tags: Performance*

---

**jhaa** - *19:12:11*

i usually get like 10ms-20ms. I used to use ovh but they messed up their geo location conf and now I used quickhostuk. most people use aws in here i think

*Tags: Deployment*

---

**liam** - *19:12:27*

Yep AWS here 

*Tags: Deployment*

---

**Chris** - *21:08:05*

Alright - thanks I will take a look at both. I am leaning toward AWS as I believe [@U4H19D1D2](@U4H19D1D2) uses some of their auxiliary services as well but would be interested to hear if there are specialty host services specifically for trading on the exchange

*Tags: Deployment, Strategies*

---

**Chris** - *21:10:58*

Does anyone know where the bf servers are actually located? I found a thread from 2014 that seemed to imply you could get 5 ms ping with some instances of aws

*Tags: Deployment*

---

**liam** - *21:15:12*

[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1595766099328400|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1595766099328400](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1595766099328400|https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1595766099328400)

*Tags: General Technical*

---

**Chris** - *21:24:48*

good point - not super worried but mine is bad now so I definitely have to buy server space somewhere.



I know if you are really trying to squeeze out performance you have to move to a compiled language like C++ and that's what most HFT firms use but do you have any idea how big of a difference it actually makes? For example, do you have stats on your average time difference between receiving a message and trading on that info? Is it &lt;10 ms or is it more in the neighborhood of 50 ms? I was assuming it was &lt; 10 ms but I am genuinely not sure

*Tags: Performance, Deployment, Strategies*

---

## 2020-09-12

**Chris** - *02:11:23*

Also, I think it is more dependent on strategy than player type and the strategy is potentially indicative of player type. If you are betting outright on the market then it probably doesn't matter, if you are making markets then it probably matters a lot more

*Tags: Strategies*

---

**Misha** - *08:04:12*

My general view is to never have a strategy that relies on you being the fastest - because that is a tenuous proposition at best. I know in-play horse racing is a very different market than pre-play, but my former client made huge amounts on UK racing (possibly more than anyone else) and their strategy was to rely on probabilities that were generated some hours in advance and just betting late (in terms of minutes, not seconds)

*Tags: Strategies*

---

**Misha** - *08:46:38*

In terms of consistency, their model was brilliant, and consistently so. In theory it's simple - get better probabilities than the market, and when you bet, bet as big as you can (in multiple chunks). UK racing has very high liquidity, so works well for UK racing

*Tags: Strategies*

---

**PeterLe** - *10:49:47*

I think that anyone who is relying on speed as the major factor  in their strategy is on a long and tortuous road these days, especially if you are offering money to the market. (not to say that it hasn't worked in the past). There is no point being first to a party only to find that there is no one else to talk to....and just as you leave, someone else turns up. You could for example still get there first, but wait around a little longer..or any other combination

*Tags: Performance, Strategies*

---

**Michael** - *12:13:30*

I'd like to know whether there is a 'Bolt' at that game - or just a ton of wannabees and each of them flukes it sometimes and not others. It's such an obvious strategy that there must be a lot of people having a crack.

*Tags: Strategies*

---

**Michael** - *12:42:10*

Is there documentation on how that works anywhere?

*Tags: General Technical*

---

**Michael** - *12:43:44*

To be honest if they are filling up the 1.01s I couldn't say I'd be that bothered. If that's the best you can do as a strategy I don't think you deserve much.

*Tags: Strategies*

---

## 2020-09-13

**Remi** - *10:19:56*

I have seen this mentioned a few times but I fail to see why this would be an obviously profitable strategy

*Tags: Strategies*

---

**Taking Value** - *12:19:13*

For any one using AWS  - Do you know is there a way to move a file from EC2 to S3 using python but using IAM roles as opposed to boto3 where the key pair has to be hard-coded. I was going down the boto3 route but read its best practice to use IAM roles (which I can do via console but don't know how to do it via python).

*Tags: Deployment*

---

**Taking Value** - *12:25:10*

If speed is this crucial, is it not better to write all the code in a lowerlevel language rather than have a higher level abstraction such as python? Does the extra layer not add a significant time lag in code execution?

*Tags: Performance*

---

**D** - *12:29:25*

Hard-coding the key pair is quite a crude way of passing them to boto3, there are several other ways that are better. Following on from Mo's comment, if you type *`aws configure`* at the command prompt you can set up the credentials once only.

*Tags: Deployment*

---

**Taking Value** - *12:36:19*

[@UBS7QANF3](@UBS7QANF3) the boto3 example code I am considering implementing assigns the public and private keys to variables.Obviously I don't want to do that. I have configured s3 using I am roles so my ec2 instance has access and I can use console commands to get it to send files across. I wanted to write a python script to do it but basically don't know how or where I should be looking to learn.

*Tags: Deployment*

---

**Mo** - *12:39:02*

I’m a bit confused - sounds like you can already use awscli with it picking up credentials from the credentials file so you should already be able to use boto3 without setting the credentials in code

*Tags: Deployment*

---

**liam** - *12:45:13*

The speed of being able to develop / deploy in python is more important to me 

*Tags: Performance, Deployment*

---

**D C** - *13:08:56*

On the topic of AWS can anyone suggest a really good idiot's guide to it? When I say idiot, I mean someone who is comfortable with develpoment and sysadmin but no experience with cloud whatsoever. Something short and sweet that explains what EC2, S3, Lambda etc are. I know I can pick the bones of it starting with amazon pages but as there are lots here who use this stuff I wondered if anyone can suggest a better starting point? I have put this off for way too long how and its time to roll my sleeves up and get hands dirty.

*Tags: Deployment*

---

**Taking Value** - *13:13:29*

Tonne of good youtube videos on it. I had no idea what AWS was a week ago. I'm not far off running data collection and maintenance on it now.

*Tags: Deployment*

---

**Mo** - *13:15:49*

I really feel you have to learn by using it. The documentation is shit and there are a vast number of services they offer so I think you just need to get stuck in

*Tags: General Technical*

---

**D C** - *13:18:02*

OK I will start with youtube. I agree with you [@UBS7QANF3](@UBS7QANF3) - this is how I usually go about things but I need to know WHAT I need to sign up for before doing so. I think all devs are used to shitty documentation. I guess maybe I start with a few youtube videos for overview then sign up and get stuck in.

*Tags: General Technical*

---

**Taking Value** - *13:23:04*

My first concern was -If I an signing up to this how much does it cost? But actually you can do the whole thing for free and only incur costs when you have stored  data (even then its super cheap) or if you want a better equipped ec2 instance than the free tier (I can't see why you would)

*Tags: Deployment*

---

**D C** - *13:26:58*

I need an even more basic idea than that though. What actually IS EC2 (OS, fancy name, product tier label etc)? Do you do access like ssh login? Is it linux based environment? Are there OS type choices like VPS? Someone told me I need Lambda to run a bot - no idea what that is or if it is true. I need the real idiot's guide introduction and I guess youtube is best place to start. I just favour written resources but in this instance maybe a video is best.

*Tags: Deployment*

---

**Taking Value** - *13:29:04*

Yea you are where I was at a week ago (had no idea what EC2 meant - its elastic cloud compute or something like that)

*Tags: Deployment*

---

**Mo** - *13:29:29*

EC2: Elastic cloud compute. It lets you provision servers on demand. They can be Windows, Linux, with multiple distro options, or your own saved configuration. The hardware resources (CPUs and memory) can be as small or large as you like. There are specialist options for e.g. GPUs, FPGAs etc.



S3: Simple storage service. A way to store files for quick and easy access.



Lambda: lets you run serverless code e.g. a scraping process that runs hourly and saves some data to S3

*Tags: Performance, Deployment*

---

**D C** - *13:31:40*

Cheers [@UBS7QANF3](@UBS7QANF3) . That is a great concise explanation. So could I effectively just use EC2 as it if were a remote linux server and install whatever I need on it and go from there?

*Tags: Getting Started, Deployment*

---

**Taking Value** - *13:34:40*

[https://www.youtube.com/watch?v=WE303yFWfV4](https://www.youtube.com/watch?v=WE303yFWfV4) Was also useful - walk through of creating an ec2 instance and uploading files to it.

*Tags: Deployment*

---

**Ruben** - *20:21:35*

seeing as the market subscription limit is giving me some trouble, is it possible with flumine, to have a strategy resubscribe to a new stream? i.e., update the streaming_market_filter of the strategy

*Tags: Strategies*

---

## 2020-09-14

**agberk** - *13:30:40*

I've read plenty of criticism with this page and while some might be technically correct, I feel like this is a good guide to explain to someone in your position what the different building blocks are actually for even if there's more nuance once you get acquainted: [https://expeditedsecurity.com/aws-in-plain-english/](https://expeditedsecurity.com/aws-in-plain-english/)

*Tags: Deployment*

---

**Peter** - *15:39:43*

Betfair times can be a bit confusing. Suspend time doesn't indicate a delay. Based on my observations I believe that it is the time that Betfair expects the markets to be suspended prior to going in-play (though the actual time that this happens will depend on the punctuality at the venue). So basically the same as startTIme, which is usually the same as marketTime - though very occasionally I've seen differences and haven't been able to work out whether they're meaningful or errors.

*Tags: Errors Debugging*

---

**Misha** - *22:23:51*

It will be similar to this - [https://help.tab.com.au/s/article/Tabcorp-Schedule-of-Deductions-for-Fixed-Odds-Racing-Betting](https://help.tab.com.au/s/article/Tabcorp-Schedule-of-Deductions-for-Fixed-Odds-Racing-Betting)

*Tags: Errors Debugging, Strategies*

---

**birchy** - *23:03:26*

[@U016535QCJ2](@U016535QCJ2) looks very similar, so probably another closely guarded secret.

I find this interesting: `Each horse is allocated a reduction factor, based on its *likely chance of winning the race*`

[https://support.betfair.com/app/answers/detail/a_id/408/c/483/](https://support.betfair.com/app/answers/detail/a_id/408/c/483/)

I would _like_ to think that Betfair are using some sophisticated algorithm to calculate this value as it might effectively be a "true" price. What I don't understand is why Betfair use a fixed value for the reduction factor as surely it would be far more efficient to use the actual price of the runner when it gets withdrawn?

*Tags: Errors Debugging*

---

## 2020-09-15

**Oliver Varney** - *07:39:34*

[@U016TGY3676](@U016TGY3676) one problem with updating it constantly is that if you can get the information ahead of time you can manipulate the market (not sure if this somewhat happens currently)

*Tags: General Technical*

---

**D C** - *08:38:40*

[@U016TGY3676](@U016TGY3676) don't worry - even I have heard of this one so it's not a secret. I was actually going to write a tool to parse the BHA twitter account for up to date NR but that link Liam just provided. Not to do what people do with the NR to create value on another runner, but to detect last minute ones so my inplay bots ignore the pulled runner. Seems to happen a fair bit right at the last minute recently and too soon for BF to actually pull it from the market. This is one area that my prehistoric desktop setup IS a help because I can manually deselect a pulled runner just after the off. Trouble is I want to be outside doing the garden not watching the screen all afternoon.

*Tags: Getting Started*

---

**Misha** - *08:52:31*

Fixed agencies seemed to offer good value early though

*Tags: Errors Debugging*

---

**D C** - *08:52:43*

Surely early prices could provide the biggest opportunities? Admittedley the volumes are not great but it you have got a highly accurate model and the morning price is way off why would you not take it while it is available?

*Tags: Strategies*

---

**Michael** - *08:57:07*

You don't need a model for this it's basic maths.

*Tags: Strategies*

---

**Misha** - *09:04:01*

Exceptions to this are big races where volume is there early and you can bet with size early and not move prices (e.g. the Melbourne Cup in Australia)

*Tags: Errors Debugging*

---

**liam** - *09:04:10*

it being the potential problem

*Tags: General Technical*

---

**D C** - *09:04:38*

For me, I am operating in a more crude manner. I don't yet have a decent pricing model. You guys are using scalpels and I am using a bread knife at present. I find it hard when the sensor feed dies - you can infer a NR from it but also dodgy sensor. I block any race action if I get a dodgy sensor because I cannot guarantee that its a NR and could get into trouble.

*Tags: Strategies*

---

**D C** - *09:13:24*

I still don't get it. AFAIK you can't lay on the tote. If your model price is any good then the current price should converge to it by offtime. If the current price is 10-1 and your model is 6-1 then your price should only be coming in - in which case ignoring the 10-1 is not sensible. Even if you are just taking £2 potshots of what is available on the exchanges I can't see how that would reduce your profits betting late. I don't know maybe there is much more to it but refusing value early to bet late at shorter odds I don't understand - unless you have something that predicts that price will get bigger. But if you believe in CLV then I don't understand it.

*Tags: Strategies*

---

**D C** - *09:24:02*

It's funny. As a mathematician I spend all my time thinking about how to generate an accurate value for a runner probability. Then you realise that even if you get there you have to contend with lots of other practical issues around how to optimise profits based on things like that.

*Tags: General Technical*

---

**Misha** - *09:29:37*

I start with a target, then work out what I need to do to reach that target. In-play allows you multiple bets per event so in some respects it's easier to turnover a higher volume, but often there is less of a statistical model to rely upon

*Tags: Strategies*

---

## 2020-09-16

**Dave** - *20:56:33*

evening all - looking to set up some higher-freq strategies in the coming months so have deployed the classic flumine market recorder. Qq about restarting on failure - is there any issue with writing the MD to the same file it was writing to prior to any crash/failure? I noticed the example flumine market recorder creates a new directory each time it starts up, so could be a pain when it comes to stitching MD from a previous run with MD recorded from the subsequent run of the recorder (for the same given market of course). I imagine there'll just be extra market definition entries, but other than that no issue?

*Tags: Data Quality, Deployment*

---

**liam** - *21:40:50*

Dave the issue is you will get a full image on restart, this probably isn’t an issue as it seems the betfair data is full of it but you need to be aware and hence the new directory. But you can change the recorder to not do this if you want, easy to test  



I just start it off and forget about it, it will handle any betfair errors or network issues, only time you need to stop is to add more strategies or upgrade 

*Tags: Errors Debugging*

---

**Ruben** - *22:39:33*

When backtesting with Flumine, since -I believe- market files are processed one after another, am I right in assuming that, in process_market_book(), market.event will contain only the market that we are processing?

*Tags: General Technical*

---

## 2020-09-17

**Jono** - *09:10:12*

Morning everyone, i was wondering how to correctly make use of the historical streaming service. Can the .bz2 files outputted from the examplehistoricdata script be used immediately in the examplestreaminghistorical script?

*Tags: General Technical*

---

**Jono** - *09:10:38*

"stream = trading.streaming.create_historical_generator_stream(

        directory="new_historic/tmp/1_169317568.bz2", listener=listener,

    )



    # create generator

    gen = stream.get_generator()



    # print marketBooks

    for market_books in gen():

        for market_book in market_books:

            print(market_book)"

*Tags: Strategies*

---

**Jono** - *09:12:40*

I am receiving an error  at the "for market books in gen()" line regarding a unicode error: 'charmap' codec can't decode byte 0x8f in position 159: character maps to &lt;undefined&gt;. So i was wondering if any processing has to be performed on the downloaded files prior to executing this script and/or parsing historical data?

*Tags: Data Quality, Errors Debugging*

---

**D C** - *09:31:44*

[@U4H19D1D2](@U4H19D1D2) Can you expand on this "the ability to know if the market has moved right at the point of matching is very valuable.. HFT guys would kill for this" ? I am somewhat confused as to how this is helpful. I mean if you match at a larger price than you ask for you assume the market has drifted and if you don't get matched immediately (or after IP lag) then you know the market price has come in. If subject to a IP delay, the stream will update you on the market movement during that delay. I get the feeling I am missing something and I am not sure if it's something obvious or something very subtle?

*Tags: General Technical*

---

**liam** - *09:31:56*

[https://github.com/liampauling/betfair/blob/63b119ddda855572a4f436810e719e4b70dd22bb/betfairlightweight/resources/bettingresources.py#L587|publish time epoch](https://github.com/liampauling/betfair/blob/63b119ddda855572a4f436810e719e4b70dd22bb/betfairlightweight/resources/bettingresources.py#L587|publish time epoch)

*Tags: Strategies*

---

**Jonjonjon** - *10:09:28*

Thanks DC. This is almost making my heard pound. I think it only really started for me a few weeks ago. I had a trading error in my favour for £5x, amongst many other trivial trades, but the total for the day only showed £4x, which definitely did not agree.

*Tags: Errors Debugging, Strategies*

---

**Jonjonjon** - *10:13:13*

At the end of the day, does your actual account balance look correct? I know this is a stupid question, but my whole setup is a mess and some of my stuff isn't in bflw, so it isn't that easy working out my total profit/loss manually.

*Tags: Getting Started*

---

**D C** - *10:17:47*

I have some node scripts that I use that allows me to extract profits by event type/strategy ref and pattern match by event names so I can drill down very easily. I use the event type ID one for daily P&amp;L. I don't use bflw either but its just basic API listClearedOrders operation so you should be able to code it yourself easily. I can give you my scripts if you want but its rather clunky (6 CLI args) and just dumps to console so you might be better off writing one as part of your own setup.

*Tags: Getting Started, Strategies*

---

**Jonjonjon** - *10:19:56*

Thanks for the offer, but I'm not a node guy. I will just knock something up myself using bflw and pandas. It won't take long.

*Tags: Feature Engineering*

---

**D C** - *13:54:40*

I've always had some issues with the commission it seems to return the entire market commission even when you split by strategy ref. So I can see a losing race with respect to particular strategy ref but still have positive commission reported. I personally calculate commission myself per strategy as otherwise you think you are worse off than you actually are if you look at it at face value.

*Tags: Strategies*

---

**D C** - *13:56:24*

I usually pull the overall profit/loss by event type ID for each market and store that as an absolute then take if off the strategy aggregate

*Tags: Strategies*

---

## 2020-09-18

**jp** - *07:05:52*

Note that if you aggregate commission by EVENT or EVENT_TYPE, there is a bug in the computation of commission from Betfair's side. This is reported to the BDP team but still not resolved.

*Tags: Errors Debugging*

---

**Jorge** - *11:30:13*

Hey guys, one general question, does Betfair act as a Market Maker providing liquidity in the markets?

*Tags: General Technical*

---

**Jorge** - *13:07:10*

Anyway, my question regarding Betfair acting as a Market Maker was directed to see if there is room for market making strategies

*Tags: General Technical*

---

## 2020-09-19

**Taking Value** - *09:35:55*

Does anyone here work on Betdaq too? If so is their a similarly excellent python library that is the equivalent of betfairlightweight that you would recommend.

*Tags: General Technical*

---

## 2020-09-22

**Newbie99** - *13:10:19*

...unless everyone else adjusts their strategy too :wink: Then presumably people may not bet in the same scenarios, so your fill rate could change (as an example)

*Tags: Strategies*

---

**liam** - *13:12:51*

obviously very strategy specific but you can't make accurate decisions on certain courses / race types without a lot of data 1 year+ minimum in my view

*Tags: Strategies*

---

**D C** - *16:20:33*

Winner was almost at the back. Looks like one of those offset errors again.

*Tags: Errors Debugging*

---

## 2020-09-23

**Taking Value** - *00:02:28*

My script that was collecting horse racing market data stopped executing Tuesday (or late Monday evening). Wondered if anyone recorded something similar (as in was it a BF issue or was it client side)? In situations like this what method do people use to re-connect and continue recording? I have a fix but it might involve spamming BF with re-subscription requests which I'm guessing might piss them off in a scenario where their API is actually down.

*Tags: Errors Debugging*

---

**Mo** - *05:37:43*

Error handling example: [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

*Tags: Errors Debugging*

---

**Lee** - *08:59:00*

I use flumine and it handles all the network errors for you 

*Tags: Errors Debugging*

---

**Jono** - *09:08:26*

Currently having some success getting to grips with parsing historical data from bf but was wondering if an events associated competition id is available in the data from the basic plan. Ive had a look at the official documentation and the info parsed from a such a file and cant seem to find an events comp id. Is this id available in some resource i am not looking at? or in the market_definition returned? Thanks

*Tags: Data Quality*

---

**Jono** - *10:05:41*

thanks for the clarification fellas, has anyone ever had success actually trying to gain more data from betfair directly? I imagine bf does store this info as its invaluable for model training and data analysis

*Tags: Strategies*

---

**D C** - *10:16:05*

[@U0154JA98TH](@U0154JA98TH) if it is third party data, they are possibly not allowed to sell it on but why would they store it themselves anyway. Are betfair in the business of modelling markets themselves? Do they actually do modelling for their sportsbook or buy the prices from a place such as bet genius?

*Tags: Strategies*

---

**JonM** - *12:27:53*

Hi all, just wondering how people tend to end their Flumine frameworks once kicked off with framework.run()?  I'm wondering about more graceful ways than using a keyboard interrupt or killing the process!

*Tags: General Technical*

---

**JC** - *13:02:26*

True, nice find. So this is how betfair update their visualisations etc... do you think they also use the info in this feed to input into models in their sportsbook?

*Tags: Strategies*

---

**Twatter** - *13:31:16*

This URL seem to list all eventIds for Betfairs Horse racing, but haven't figured out the URL to get data per Event yet:

[https://betfair.betstream.betgenius.com/betstream-view/getMappedFixtures/v1/product/betgeniushorseracing/sport/horseracing.json](https://betfair.betstream.betgenius.com/betstream-view/getMappedFixtures/v1/product/betgeniushorseracing/sport/horseracing.json)

*Tags: Errors Debugging*

---

**Twatter** - *13:32:27*

Interesting little reverse engineering and stuff - github has a number of code entries for the text "betstream-view", but ultimately I'm not sure of how to capitalise on this data! Looks helpful to others though...

*Tags: General Technical*

---

**liam** - *16:44:02*

if you want race data / results use the [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/racecard.py|raceCard](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/racecard.py|raceCard) endpoint that the site uses

*Tags: General Technical*

---

## 2020-09-24

**Taking Value** - *01:03:21*

Can I just check, is the base listener a lot faster than stream listener? I switched to the error handling code that [@UBS7QANF3](@UBS7QANF3) very kindly pointed me in the direction of. Its great but it uses the stream listener and it keeps flashing latency warnings at me. I am only using it for recording data at the moment so its not an issue but presumably it is an issue when its being used to place bets.

*Tags: Errors Debugging, Performance*

---

**river_shah** - *09:31:53*

can orders inserted via api / flumine be visualised in tools such as betangel ladders and manually cancelled if the need ever arises?

*Tags: General Technical*

---

**liam** - *09:33:41*

Yep but flumine will work off that cancel event and may reset -&gt; place new orders dependant on how it has been setup

*Tags: Getting Started*

---

**liam** - *10:11:58*

no you get an error

*Tags: Errors Debugging*

---

**liam** - *10:44:14*

If you have BPE disabled, you get an error if BPE is used to get you matched, e.g. you ask for a BACK at 5.0 but the best price is now 5.2 you would get an error with the size fully lapsed regardless of the amount available at 5.2

*Tags: Errors Debugging*

---

**agberk** - *13:58:01*

redis probably the easiest to set up, especially with python; if you have some very specific performance requirements you might want to consider zeromq but depending on what you care about most it may not really make a difference

*Tags: Performance*

---

**liam** - *14:17:05*

Why bother? Betfair already does this for you through kafka -> streaming

*Tags: General Technical*

---

**liam** - *14:17:34*

Using bflw you can subscribe to all racing and it won't even touch your CPU

*Tags: General Technical*

---

**Oliver Varney** - *14:19:13*

Think [@U4H19D1D2](@U4H19D1D2) right, streaming is cheap. My use case is not just to store the current state from the stream

*Tags: General Technical*

---

**Dave** - *14:20:22*

It is good to know it's not CPU intensive tho (running flumine has proved that tbh) - less cores to pay for in the end!

*Tags: General Technical*

---

**liam** - *14:25:00*

Betfair distributed event stream using kafka -&gt; streaming client (nice and simple per client and scalable as Betfair handle that)



Betfair distributed event stream using kafka -&gt; streaming -&gt; redis -&gt; client (not simple and involves redis scaling)



You would basically be replicating the problem betfair has already solved and then trying to solve it again yourself

*Tags: General Technical*

---

**Misha** - *14:30:04*

I'm with [@U4H19D1D2](@U4H19D1D2) on this one. Just use the streaming API internally to each client. Why complicate things

*Tags: General Technical*

---

**agberk** - *14:34:20*

We actually do the republishing with redis; for a component which just wants to consume particular data I don't want there to be a dependency with the Betfair API

*Tags: General Technical*

---

**Dave** - *14:42:34*

I am at a point where I want to transition from a dedicated process that collects my tick data (your epic flumine client [@U4H19D1D2](@U4H19D1D2) ) to something that just dumps whatever the strategies would be recieving, so the data collection is really just a small component in the grand scheme of things (and one use case for pub/sub). Thanks all!

*Tags: General Technical*

---

**river_shah** - *14:44:51*

```from flumine import FlumineBacktest, clients



client = clients.BacktestClient()

framework = FlumineBacktest(client=client)



strategy = ExampleStrategy(

    market_filter={"markets": ["/tmp/marketdata/1.170212754"]}

)

framework.add_strategy(strategy)



framework.run()```

isn’t it more hassle to spin up own data recorders? I guess depends on use case

*Tags: Strategies*

---

**Chris C** - *15:40:48*

Hi all,

quick question on the max number of connections / sockets with betfair. For my account it's currently set to 10. Has anyone had success increasing the number of max connections?

*Tags: General Technical*

---

**Jono** - *15:57:20*

hey all quick one this time, ive got some purchased historical data and looking for the exchange odds values and volumes but struggling to locate where they are. Should i be able to see them in the market_definition or runner objects or somewhere entirely different? help is greatly appreciated

*Tags: Data Quality*

---

**klozovin** - *20:28:53*

that's 10 streaming connections, regular request/response HTTP API is not affected by that?

*Tags: General Technical*

---

## 2020-09-25

**Jono** - *16:17:13*

is there anyway of differentiating if last traded price from Basic historical data was on a BACK or LAY selection?

*Tags: Data Quality*

---

## 2020-09-26

**Michael** - *16:07:05*

The question of whether you can keep yourself out of a higher PC bracket in the future is more complicated than it first appears too.

*Tags: General Technical*

---

**azevedo** - *17:00:01*

hey guys! a quick question on football. does bf ever change (at in-play) the line ranges it offers for handicaps (-4.0 to +4.0) and total goals (0.5 to 8.5)? Im talking about rare high-scoring cases, have there been precedents where they added extra lines say 9.5, 10.5 total goals etc. ?

*Tags: General Technical*

---

**Unknown** - *21:48:12*

But....



Suppose one thousand £2.00 bets were placed at 2.00 (assuming that is fair value).



The 2% payer would expect to win £980. Lose £1000.



Adding that to some scenarios in my spreadsheet, I can see it beating 5%/50%, while helping to keep the Total Charges% above %.



But I guess the ability to bet at 2.00 fair value isn't really possible.

*Tags: General Technical*

---

## 2020-09-28

**river_shah** - *11:13:24*

```client = clients.BacktestClient()

framework = FlumineBacktest(client=client)



_market = "/XXX/betfair_data/ADVANCED/2020/Jan/31/29677355/1.168130637"



market_filter = streaming_market_filter(market_ids=[_market])

strategy = ExampleStrategy(market_filter=market_filter)

framework.add_strategy(strategy)



framework.run()```



*Tags: Strategies*

---

**river_shah** - *11:13:54*

```{"asctime": "2020-09-28 10:04:00,062", "levelname": "INFO", "message": "Adding market middleware &lt;flumine.markets.middleware.SimulatedMiddleware object at 0x1116272e8&gt;"}

{"asctime": "2020-09-28 10:04:00,062", "levelname": "INFO", "message": "Adding trading control ORDER_VALIDATION"}

{"asctime": "2020-09-28 10:04:00,062", "levelname": "INFO", "message": "Adding trading control STRATEGY_EXPOSURE"}

{"asctime": "2020-09-28 10:04:00,062", "levelname": "INFO", "message": "Adding client control MAX_ORDER_COUNT"}

{"asctime": "2020-09-28 10:04:00,063", "levelname": "INFO", "message": "Adding strategy ExampleStrategy"}

{"asctime": "2020-09-28 10:04:00,063", "levelname": "WARNING", "message": "No markets found for strategy ExampleStrategy"}```



*Tags: Strategies*

---

**liam** - *11:14:49*

```strategy = ExampleStrategy(market_filter={"markets": [_market]})```

*Tags: Strategies*

---

**darren b** - *12:08:08*

Hey, I'm just starting out. I've downloaded and extracted some historic data for a market. Is there a simple way to view this as a line chart using python? Or would I need to carry out some pre-processing of the data to get it to work properly? Thanks

*Tags: General Technical*

---

**darren b** - *16:29:51*

This is to use the betfair .bz2 price files right? I'm running it but can't get past an 'inplay' KeyError. Any suggestions on what might be causing it? Thanks

*Tags: Errors Debugging*

---

**Mo** - *16:39:05*

Yes, it's intended for use on Betfair historic data or files scraped using flumine

*Tags: General Technical*

---

## 2020-09-30

**liam** - *09:09:11*

Yeah pending is an internal flumine status to say it is going through execution 

*Tags: General Technical*

---

**liam** - *09:09:41*

And yep you can’t cancel because it doesn’t have an Id, it should error?

*Tags: Errors Debugging*

---

**river_shah** - *09:10:31*

cool, thanks for clarifying and I will check if I can force an error.

*Tags: Errors Debugging*

---

**klozovin** - *21:56:57*

there's no need to auth and/or send session token when using _inplayservice_? (not using the Python wrapper, just trying something out in the console)

*Tags: General Technical*

---

## 2020-10-01

**Dave** - *14:46:14*

Qq - are there any negative implications to subscribing to the same set of markets twice from separate clients? Am I using up any alloted quota with the second subscription data-wise (bandwidth perhaps)? Still reconsidering re-publishing market data internally vs subscribing to it directly from all consumers. Don't want to end up throttled on Betfair side if I can just republish internally with negligible latency.

*Tags: Performance*

---

**Misha** - *14:50:28*

I run each application using their own streams, but I can pretty much run everything in one application (any number of strategies can be implemented in the same engine). My rationale is that the streaming API is very efficient, so I see no reason to duplicate that functionality to "share" streams. You are only limited by the number of streaming connections, which is 10, but each streaming connection can have its market limit increased from 200 upwards

*Tags: General Technical*

---

## 2020-10-03

**Dave** - *00:37:41*

FWIW - I tried republishing market data internally over zmq pub/sub and latency impact is negligible. So if you wanted a setup where you had say, one process per market, then you can't use a dedicated streaming session per market if you plan on trading more than 10 markets concurrently (for the reason Misha mentioned above). But republishing internally is a way around that, and the added latency is dwarfed by external network latency. Decided not to opt for Redis now given zmq is sufficient

*Tags: Getting Started, Performance, Strategies*

---

## 2020-10-04

**Ruben** - *14:44:04*

I'm trying to understand the design of flumine a bit better: how come there is a distinction between `Trade`  and `Order` ? Is it not enough to model only individual orders?

*Tags: Strategies*

---

## 2020-10-05

**ricky** - *13:08:36*

I only begin to use Flumine, I want to do price recorder for Football MATCH_ODDS, OVER_UNDER_25 and more market type per match. I prefer to save each football match price data to separate txt file.



My question:

1) If i want to record all premier League match today, what is the recommend way to do it, should i create new strategy for each match (e.g each strategy has different market filter and file name?)

2) What is the max number of strategy i can add into framwork?

3) What is the max number of instance i can run Flumine?

thanks

*Tags: Strategies*

---

**Lee** - *13:34:06*

I'll answer based on how i use Flumine

1. For recording I use the S3 market recorder from the examples and use the same recorder to record all sports. So for football i'd run it using something like below. This will record all GB football matches to individual files and upload to S3. I personally wouldn't restrict by date as the streaming will subscribe to new markets to the recorder as and when they become available.

```framework.add_strategy(

    S3MarketRecorder(

        market_filter=streaming_market_filter(

            event_type_ids=[1],

            market_types=[

                "MATCH_ODDS",

                "OVER_UNDER_25",

            ],

            country_codes=["GB"],

        )

        stream_class=datastream.DataStream,

        context={//add config here}

    )

)```

2. There's no limit (afaik) in Flumine so just depends on what your strategies are doing and how long they take to process each update.

3. There's also no limit on how many instances of Flumine you can run, you're only limited by betfair streaming connections which is 10.

*Tags: Data Quality, Strategies*

---

**ricky** - *14:19:58*

[@UUCD6P13J](@UUCD6P13J) Thank you for your message, by default, everything save to one output.txt file? do you know how to configure each football matches save to individual files?

How you manage each files ( or market id) with corresponding event name?

*Tags: General Technical*

---

**ricky** - *15:17:29*

Thanks for your information, i can see MarketRecorder class did save each markets in individual files.

I have two more questions:

1) I use data visualization tools to view the live price data per matches, is it any way to identity which group of market id (files) have the same event name?

2) i can see there is background worker pulling list_market_catalogue every 60s, if i want to get event name from market_catalogues, how can i access market_catalogues data from class PriceRecorder? ( e.g maybe inside process_market_book)

*Tags: Deployment*

---

**Ian** - *15:39:33*

Hi all, quick question -I’ve had a scan of the BF api docs and would I be correct in thinking that the only the runnerId is available from the stream and to collect additional meta (e.g. runner name, trainer - for horse markets) the API-NG needs to be used?

*Tags: General Technical*

---

**ricky** - *18:19:32*

[@UUCD6P13J](@UUCD6P13J) "market.market_catalogue" i got error said undefined variable "market" inside in the PriceRecorder class.

*Tags: Errors Debugging*

---

**liam** - *18:32:32*

cheers, btw 1.13.0 released today, includes a [https://github.com/liampauling/flumine/commit/edd6735de54c46a9c91c76f96dcd6de8ca811b36|change](https://github.com/liampauling/flumine/commit/edd6735de54c46a9c91c76f96dcd6de8ca811b36|change) to backtesting that may have an impact on differences you were seeing between live as well as much improved [https://github.com/liampauling/flumine/pull/283|control](https://github.com/liampauling/flumine/pull/283|control) thanks to [@UPMUFSGCR](@UPMUFSGCR)

*Tags: Deployment*

---

**ricky** - *23:48:26*

I previous used lightweight steaming API, now switch to Flumine for price recording.

I did a period of 90 mins ( football in-play) performance testing between these two, steaming API give me around 20000 data points, Flumine give around 25000 data point. For trade volume, i can see slight diffentence between these two.

Any thought, which framework is more accurate in term of steaming?



Flumin version: 1.12.1

betfairlightweight version 2.8.0

*Tags: Performance*

---

## 2020-10-06

**liam** - *06:30:48*

Flumine uses bflw so should be the same but the amount of points / conflation is based on how quickly you can pull the data off the socket 

*Tags: General Technical*

---

**Alessio** - *08:36:48*

i was looking into the historical data..(the free PRO and ADVANCED ones) maybe somebody did it as well.. i was trying to see how the market moves while in play for some soccer or horse racing market.. but it seems there's only a few seconds between the market gets defined as 'in play' and it gets closed.. is it me reading the data wrong or is it actually like this? (i'm using bflw histostream of course)

*Tags: Data Quality*

---

**river_shah** - *11:18:57*

for bflw how may I get the stream generator to only print market_books when 10 minutes left for start please?

```stream = trading.streaming.create_historical_generator_stream(

    file_path='horse-racing-pro-sample',

)



g = stream.get_generator()



for market_books in g():

    print(market_books)```



*Tags: Strategies*

---

**liam** - *11:24:47*

[https://github.com/liampauling/betfair/blob/63b119ddda855572a4f436810e719e4b70dd22bb/examples/examplestreaminghistorical.py#L34](https://github.com/liampauling/betfair/blob/63b119ddda855572a4f436810e719e4b70dd22bb/examples/examplestreaminghistorical.py#L34)

*Tags: General Technical*

---

**Jonjonjon** - *14:20:11*

I have a strategy that trades in and out of positions.



It can back or lay to enter positions.



After it has backed or layed, it can place an order to exit at my estimate of "fair value".



Without storing any state inside my Python code, is there a way to tag orders, to identify whether or not they are "entry" or "exit trades"?



I'm aware of the "customerStrategyRef" field, and would like that to be the same for both all orders.



The customerOrderRef field is used by Flumine, and uses up all 32 permitted characters.



Is there anything else I could try?

*Tags: Strategies*

---

**Mo** - *14:22:13*

In what way does Flumine use it?

*Tags: General Technical*

---

**Jonjonjon** - *14:26:44*

I've had difficulties in the past storing stuff in databases. I'm not a full-time pro like yourself. So in choosing to use a MySql database a few years ago, my system is now very hard to migrate.

*Tags: General Technical*

---

**Jonjonjon** - *14:27:05*

So I'm trying to mak  my new bflw/flumine code stateless.

*Tags: General Technical*

---

**Jonjonjon** - *14:33:25*

I'm wondering....



Will the gods of clean code and Python damn my soul if I hack this:



```self.id = str(uuid.uuid1().time)```

And take the first 4 characters to store my own string of data

*Tags: General Technical*

---

**Jonjonjon** - *14:34:06*

e.g.



new_order.id = my_prefix + new_order.id[:-len(my_prefx)

*Tags: Errors Debugging*

---

**Mo** - *14:34:14*

Depends if flumine relies on that reference being unique

*Tags: General Technical*

---

**Oliver Varney** - *14:36:16*

[@UPMUFSGCR](@UPMUFSGCR) I changed the customer_order_ref to include strategy, trade and order id components

*Tags: Strategies*

---

**Newbie99** - *14:39:54*

hmmm, one other approach (not saying its optimal, but it could work) is to seperate out back and lay orders by strategy, i.e. strategy A only ever backs on entry, strategy B only ever lay's on entry, so you then know the an opposite trade was an exit.

*Tags: Strategies*

---

**Newbie99** - *14:40:33*

(as in 1 strategy, but split into 2 strategies in flumine terms)

*Tags: Strategies*

---

**Oliver Varney** - *15:31:35*

off memory I did something like strategy_hash-internal_trade_id-order_id, with it being 6-6-18

*Tags: Performance, Strategies*

---

**Oliver Varney** - *15:34:50*

might of been slightly less but essentially I stole some of the characters from the strategy_name_hash

*Tags: Strategies*

---

**Oliver Varney** - *15:35:31*

```@property

def customer_order_ref(self) -&gt; str:

    return "{0}-{1}".format(self.trade.strategy.name_hash, self.id)

@property

def name_hash(self) -&gt; str:

    return create_cheap_hash(self.name, 13)```



*Tags: Strategies*

---

**Oliver Varney** - *15:36:01*

in flumine that is. I have changed it to suit my requirements

*Tags: General Technical*

---

**Jonjonjon** - *15:48:31*

Using this code:



```import datetime

import uuid





def from_uuid_time(uuid_time):

    return datetime.datetime(1582, 10, 15) + datetime.timedelta(microseconds=uuid_time//10)





print('Modifying 3rd digit')

d0 = from_uuid_time(13842839081240720)

d1 = from_uuid_time(13942839081240720)

print(d0)

print(d1)

print(d1-d0)```

I get this output:



`1626-08-26 19:18:28.124072`

`1626-12-20 13:05:08.124072`

`115 days, 17:46:40`



So it looks like if I use the 1st two characters of the order id for my own prefix, then I should be safe from clashes for 115 days.:grinning:

*Tags: Errors Debugging*

---

**river_shah** - *17:09:10*

for less liquid runners not receiving many streaming updates this `get_price(runner.ex.available_to_back, 0)` can remain None for a while, despite there being valid prices for the selection. is it possible to get the state snapshot using api-ng when the framework / strategy comes up?

*Tags: Strategies*

---

**liam** - *17:09:59*

the book will be up to date if streaming

*Tags: General Technical*

---

**Misha** - *22:54:51*

It is the ONLY guaranteed way to match a bet you place and the bet updates coming through the streaming API, and doesn't rely on any state management in your own software (as long as you store the bet requests, or give that field enough information to uniquely identify any bet)

*Tags: General Technical*

---

## 2020-10-07

**ChickenMAn** - *03:53:51*

Sup everyone. Python developer and football lover. Think I'm in the right place.

*Tags: General Technical*

---

## 2020-10-08

**liam** - *09:39:50*

[https://github.com/liampauling/betfair/blob/63b119ddda855572a4f436810e719e4b70dd22bb/betfairlightweight/resources/bettingresources.py#L304](https://github.com/liampauling/betfair/blob/63b119ddda855572a4f436810e719e4b70dd22bb/betfairlightweight/resources/bettingresources.py#L304)

*Tags: Strategies*

---

**river_shah** - *09:41:13*

have looked at the comment as well. sorry trying to understand further. why is direct attribute access not working (stackoverflow was not helpful).

*Tags: General Technical*

---

**liam** - *09:41:34*

Ahh is this within flumine?

*Tags: General Technical*

---

**liam** - *09:42:11*

We patch it flumine because people complained it wasn't fast enough  [https://github.com/liampauling/flumine/blob/b1717c7b13909d15fae865c88ecfd2108a4786d6/flumine/patching.py#L2](https://github.com/liampauling/flumine/blob/b1717c7b13909d15fae865c88ecfd2108a4786d6/flumine/patching.py#L2)

*Tags: General Technical*

---

**liam** - *09:42:21*

[https://github.com/liampauling/flumine/commit/64ae76f0cb5571a040ce9904febf35df733931c4](https://github.com/liampauling/flumine/commit/64ae76f0cb5571a040ce9904febf35df733931c4)

*Tags: General Technical*

---

**liam** - *09:45:16*

The issue you are seeing is that the inspector hasn't detected the patch, not sure we can fix that?

*Tags: Errors Debugging*

---

**river_shah** - *09:47:03*

thanks for commits, taking a look. no worries I will fix the inspector warnings (on my local end). i see correct usage now.

*Tags: Errors Debugging*

---

**birchy** - *21:38:02*

Probably being a twat here...but when backtesting with Flumine, I'm seeing orders in the blotter with status `EXECUTABLE` and a `simulated.profit != 0.00` which seems wrong to me. I know bets can be partially matched, but that should give us one part `EXECUTION_COMPLETE` and one part `EXECUTABLE` with a zero P&amp;L as it hasn't been matched?

*Tags: General Technical*

---

## 2020-10-10

**ricky** - *15:44:29*

Question on the max number of connections / sockets in flumine,

i used flumine for price recording, becasue i want to manage customised price calculation for different market, i need create few instance of PriceRecorder as example below:



my_event_id = [30050835]

strategy1 = PriceRecorder(

    market_filter = betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["1"], market_types=["MATCH_ODDS", "OVER_UNDER_25"], event_ids=my_event_id

    ),

    market_data_filter = betfairlightweight.filters.streaming_market_data_filter(

        fields=["EX_BEST_OFFERS", "EX_MARKET_DEF", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP"], ladder_levels=3

    ),

)



my_event_id = [30050836]

strategy2 = PriceRecorder1(

    market_filter = betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["1"], market_types=["MATCH_ODDS", "OVER_UNDER_25"], event_ids=my_event_id

    ),

    market_data_filter = betfairlightweight.filters.streaming_market_data_filter(

        fields=["EX_BEST_OFFERS", "EX_MARKET_DEF", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP"], ladder_levels=3

    ),

)



framework.add_strategy(strategy1)

framework.add_strategy(strategy2)

...



When i tested with 10 or more strategies, I got exceed max number of connection error, expect 10 got 11. I thought there is not limitation in terms of number of strategies? in this example, Did each strategies create separate socket connection to befair? any possible work around?

*Tags: Errors Debugging, Strategies*

---

**Lee** - *15:50:49*

ah that would probably be the reason, could you remove the event_ids filter from there and filter within the strategy instead in `check_market_book`?

*Tags: Strategies*

---

**ricky** - *16:23:04*

market_filter=streaming_market_filter(

        event_type_ids=[1],

        market_types=[

            "MATCH_ODDS",

            "OVER_UNDER_25",

        ],

        country_codes=["GB"],

    ),



If above is the high level market filter, Any example how to filter only "premier league" in check_market_book?

*Tags: General Technical*

---

**ricky** - *16:30:53*

[@UUCD6P13J](@UUCD6P13J) Looks good, i will give a try, thank you for your help.

*Tags: General Technical*

---

**Lee** - *16:31:01*

no problem

*Tags: General Technical*

---

## 2020-10-11

**mandelbot** - *20:04:49*

Is there a way to place a lay bet at SP at a fixed bet size regardless of liability?

*Tags: Errors Debugging*

---

## 2020-10-12

**river_shah** - *07:21:47*

Have a strategy that trades `Over/Under 0.5 Goals` and `Over/Under 2.5 Goals`. I need to be able to consume both markets together as the trading decision for one market is dependent on the other. If I provide a list of the historic markets. the data is run through one by one and the market streams are not merged together. To counter this issue I just provide the event_id and file for the backtest to be run on. But this is causing a huge amount of log spamming as soon as the first goal gets scored and the market is closed. What is best way to handle this please? `logging.getLogger('flumine').setLevel(logging.WARNING)` is not an option as it kills my visibility into what flumine is doing. however the following logs are also very disruptive (event_file: `ADVANCED/2020/Jan/19/29637723/29637723`). the displayed log spamming continues till the end of the match. the first goal happens in the first 15 minutes.

```2020-10-12 07:13:53.094 INFO  Market closed

2020-10-12 07:13:53.104 INFO  Market 1.167019680 closed

2020-10-12 07:13:53.104 INFO  Market closed

2020-10-12 07:13:53.105 INFO  Market 1.167019598 closed

2020-10-12 07:13:53.105 INFO  Market closed

2020-10-12 07:13:53.107 INFO  Market 1.167019670 closed

2020-10-12 07:13:53.107 INFO  Market closed

2020-10-12 07:13:53.132 INFO  Market 1.167019681 closed

2020-10-12 07:13:53.132 INFO  Market closed

2020-10-12 07:13:53.141 INFO  Market 1.167136003 closed

2020-10-12 07:13:53.141 INFO  Market closed

2020-10-12 07:13:53.150 INFO  Market 1.167019680 closed

2020-10-12 07:13:53.150 INFO  Market closed

2020-10-12 07:13:53.151 INFO  Market 1.167019598 closed

2020-10-12 07:13:53.151 INFO  Market closed

2020-10-12 07:13:53.153 INFO  Market 1.167019670 closed

2020-10-12 07:13:53.153 INFO  Market closed

2020-10-12 07:13:53.178 INFO  Market 1.167019681 closed

2020-10-12 07:13:53.179 INFO  Market closed

2020-10-12 07:13:53.189 INFO  Market 1.167136003 closed

2020-10-12 07:13:53.189 INFO  Market closed

2020-10-12 07:13:53.199 INFO  Market 1.167019680 closed

2020-10-12 07:13:53.199 INFO  Market closed

2020-10-12 07:13:53.200 INFO  Market 1.167019598 closed

2020-10-12 07:13:53.200 INFO  Market closed

2020-10-12 07:13:53.202 INFO  Market 1.167019670 closed

2020-10-12 07:13:53.202 INFO  Market closed

2020-10-12 07:13:53.230 INFO  Market 1.167019681 closed

2020-10-12 07:13:53.230 INFO  Market closed

2020-10-12 07:13:53.239 INFO  Market 1.167136003 closed

2020-10-12 07:13:53.240 INFO  Market closed

2020-10-12 07:13:53.249 INFO  Market 1.167019680 closed

2020-10-12 07:13:53.249 INFO  Market closed

2020-10-12 07:13:53.250 INFO  Market 1.167019598 closed

2020-10-12 07:13:53.250 INFO  Market closed```



*Tags: Strategies*

---

**river_shah** - *07:22:38*

flumine does not remove closed markets in backtest as otherwise it will remove blotter objects and will lose simulated pl etc

*Tags: General Technical*

---

**river_shah** - *08:12:29*

I am passing the event historic file. `market = "xxx/betfair_data/football/ADVANCED/2020/Jan/19/29637723/29637723"` is replicable with `ExampleStrategy`

*Tags: Strategies*

---

**Ruben** - *08:34:16*

I thought it wasn't possible to backtest several markets simultaneously with flumine?

*Tags: General Technical*

---

**liam** - *09:04:16*

bflw [https://github.com/liampauling/betfair/blob/master/HISTORY.rst#290-2020-10-12|2.9.0](https://github.com/liampauling/betfair/blob/master/HISTORY.rst#290-2020-10-12|2.9.0) now released, minor change to only call the clear stale cache on every new market rather than every update (thanks @synapticarbors) Any issues let me know

*Tags: General Technical*

---

**river_shah** - *14:43:20*

for historic market catalogue support, I intend to use aws and maintain tables by querying every few minutes and upserting into rds. [@U4H19D1D2](@U4H19D1D2) [@UBS7QANF3](@UBS7QANF3) I believe you are doing this. would it be possible to add any example of this usage into flumine/examples please. still coming to grips with betfair market catalogue fields and want to ensure that i capture all essential catalogue data. thank you

*Tags: Deployment*

---

**Mo** - *14:44:25*

Yes I scrape the market catalogues I an interested in on an hourly basis and upsert into RDS. However, I don't use flumine

*Tags: General Technical*

---

**liam** - *15:32:32*

it took them 5 years to get to position to sell streaming data

*Tags: General Technical*

---

**Ruben** - *17:14:04*

Is there any example script on how to merge individual market files recorded with the PriceRecorder into an event file, so as to backtest several markets of an event simultaneously?

*Tags: General Technical*

---

**liam** - *17:21:48*

The way flumine monkeypatches datetime that will cause all sorts of issues, there will be a clean way to do this but I can't think right now however its on my todo list

*Tags: General Technical*

---

## 2020-10-13

**Jonjonjon** - *15:52:31*

Does flumine simulate the inplay delay when backtesting?

*Tags: General Technical*

---

**Lee** - *15:53:47*

[https://github.com/liampauling/flumine/blob/master/flumine/backtest/backtest.py#L135](https://github.com/liampauling/flumine/blob/master/flumine/backtest/backtest.py#L135)

*Tags: General Technical*

---

**azevedo** - *18:08:57*

flumine MarketRecorder handles network issues already right? just wanted to ask what happens on restart. say your connection was down for 15mins, would you miss the data/updates that happened during that period?

*Tags: General Technical*

---

**river_shah** - *18:15:46*

There is no buffering, you'll lose the updates. Hence record on robust infra (AWS)

*Tags: Deployment*

---

## 2020-10-14

**AP** - *02:43:12*

When using flumine, is there a method to extract how many seconds has elapsed since a market went in-play?



I'm guessing it's not: `elapsed_seconds_closed`

*Tags: General Technical*

---

**river_shah** - *12:49:19*

is there much desire to integrate this into flumine? seems like a good safety feature with not much overhead.

*Tags: Feature Engineering*

---

**river_shah** - *12:57:03*

cool, I’ll see how to do it.

*Tags: General Technical*

---

## 2020-10-15

**Charlie 303** - *00:03:39*

BF API n00b here; Getting thing started, I applied and received live app key. Then discovered searching through BF forums that you can increase market subscription limit (200-&gt;1000) by simply asking BF support, which is free of charge.  Didnt see any official documentation on that, but that indeed worked.  Curious if there are other features/limits that can be requested/applied for, either free or premium?

*Tags: Feature Engineering, Deployment*

---

**river_shah** - *13:37:56*

how can a strategy know if `place_order` successfully met all validation checks please? currently this returns `None`

```def place_order(self, market: Market, order) -&gt; None:

    runner_context = self.get_runner_context(*order.lookup)

    if self.validate_order(runner_context, order):

        runner_context.place()

        market.place_order(order)```

want to put some additional logic to handle order placement validation failures, please let me know how best to handle. thanks

*Tags: Strategies*

---

**river_shah** - *14:43:00*

what would be the impact of moving validation forward? lot more redundant validation checks? when / which code section is the earliest a strategy can know after calling `place_order` that a `VIOLATION` has been generated

*Tags: Strategies*

---

**river_shah** - *15:06:49*

`python -m cProfile -s cumtime launch_racing_strategy.py` very helpful in profiling the code, no flumine hotspots (found some nasty things in mine though)

*Tags: Strategies*

---

**Jonjonjon** - *16:09:50*

When backtesting with flumine, the `market_book` and `live_market` are received on each update by `process_market_book`.



In that data, how can I tell whether or not:



• the market will go inplay? This is different to the `inplay` flag that defines whether or not it is actually inplay.

• whether or not it the bsp is available?

*Tags: Deployment*

---

**Seabass** - *16:18:55*

Hi, I'm a beginner with betfair but familar with tick trading data. I'am trying to create essentially a time and sales file using the Pro historical data. But when I merge the order book data and the time and sales file I generate, the results look a little strange. Generally, you can see the exact traded size removed from the order book. But occasionaly, there seems to be a trade that was adjusted later on (size doesn't match order book) and occasionally I see exactly half of the reported trade size being removed from the book. To calculate the trades I just take the difference from the current trd cache and the newest trd cache. What is the correct way to calculate the amount that was traded? And am I overlooking something in my calculation?

*Tags: Getting Started, Data Quality, Strategies*

---

**Seabass** - *18:25:32*

Okay, I think that makes sense. It's similar to implied orders on a derivatives exchange. Okay, I will check it out in more detail, thank you for the help [@U4H19D1D2](@U4H19D1D2) and [@UBS7QANF3](@UBS7QANF3)!

*Tags: General Technical*

---

**Seabass** - *18:30:14*

This makes it a lot more complicated to generate the time and sales file. Got it, thank you for the help!

*Tags: General Technical*

---

## 2020-10-16

**Rob (NZ)** - *09:35:38*

```#Look at all NZ and Aussie Markets for the next day and bring back Market Descriptions

market_catalogue_filter = betfairlightweight.filters.market_filter(market_countries=['NZ','AU'],event_type_ids=['7'],market_start_time={ 'to': (datetime.datetime.now() + datetime.timedelta(days=1)).strftime("%Y-%m-%dT%TZ") },market_type_codes = ['WIN'])



market_catalogues = trading.betting.list_market_catalogue( filter=market_catalogue_filter, max_results='100', market_projection = ['MARKET_DESCRIPTION','RUNNER_METADATA','MARKET_START_TIME'], sort='FIRST_TO_START' )



#Create a DataFrame for each market catalogue

market_types_markets = pd.DataFrame({ 'Market Name': [market_cat_object.market_name for market_cat_object in market_catalogues], 'Market ID': [market_cat_object.market_id for market_cat_object in market_catalogues], 'Start Time': [market_cat_object.market_start_time for market_cat_object in market_catalogues], })



market_types_markets```

outputs 100 rows but im just wondering if there is a way to get the total days racing

*Tags: Feature Engineering, Strategies*

---

**Rob (NZ)** - *09:36:47*

when I put it at 500 it was coming back with a error

*Tags: Errors Debugging*

---

**liam** - *09:42:22*

Take a list and chunk it [https://github.com/liampauling/flumine/blob/master/flumine/worker.py#L107](https://github.com/liampauling/flumine/blob/master/flumine/worker.py#L107)

*Tags: General Technical*

---

**Rob (NZ)** - *11:05:01*

any good resources I could read up on how to run the flumine (still a python newb) is it something i can pip install like lightweight

*Tags: Getting Started*

---

**Lee** - *11:06:11*

there's some good examples [https://liampauling.github.io/flumine/](https://liampauling.github.io/flumine/) and there's also a couple of examples in the repo

*Tags: General Technical*

---

**dan2002** - *12:08:28*

Hi Liam,  I have a strategy generate a signal for an order, so i place an order, only after the order get filled i place an hedge order. How can i keep track of which order is hedged(can i tag "headged" to an order? or do i have to keep a table in my strategy to know which order is hedged). Thanks.

*Tags: Strategies*

---

**dan2002** - *12:10:24*

another question is that in example.py,do process_market_book and process_orders run on different threads?

*Tags: General Technical*

---

**Mai** - *16:04:49*

Hello all! New to betfairlightweight (and betfair) and it's great to find this community. Thanks to the developers for making it open!

*Tags: Getting Started*

---

## 2020-10-17

**Rob (NZ)** - *11:19:08*

is there any good example tutorial videos of setting up and running things like Flumine ,

*Tags: General Technical*

---

**Ruben** - *20:01:38*

question about order logging to flumine users: those that log the status of your orders, do you do it because it is the only way to rebuild your position in case of a failure? for posterior data analysis? because you don't trust betfair?

*Tags: General Technical*

---

## 2020-10-18

**Mo** - *10:37:18*

I assume because you didn't set a customerStrategyRef on that bet

*Tags: Strategies*

---

**qwerty.nat** - *10:53:43*

No don't think so, looking at the Swagger streaming def file, smc = StrategyMatchChange which is an object (dict in python land)  with 2 keys 'mb' and 'ml' which are nullable. It seems they are sending an object withing an object that doesn't confirm to their own definition? just wondering if it's just me or everyone

*Tags: Strategies*

---

**Mo** - *10:55:31*

It's a map where the keys are the strategy name

*Tags: Strategies*

---

**Alessio** - *12:00:46*

funnily, flumine does that, but it doesn't send that info to the strategies

*Tags: General Technical*

---

**Alessio** - *12:07:05*

(didn't want to speak bad of flumine :P)

*Tags: General Technical*

---

## 2020-10-19

**James Norman** - *02:48:11*

Hello all! Thanks for letting me join your Slack group :slightly_smiling_face: I've been betting full time for 3 1/2 years. My wife and I moved to Australia at the start of the year. It's always been an ambition to automate some strategies and I've used Lockdown (Still in it here) to learn Python. The lightweight wrapper looks awesome! Great job Liam! Although I'm struggling to log in with it... I went through the BetAngel forum and found some helpful information but saving the key in a .key format is currently alluding me! Probably a rookie question but any advice / direction would be appreciated. Many thanks again James

*Tags: Strategies*

---

**Mo** - *07:08:17*

Certificate creation instructions are here: [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Non-Interactive+%28bot%29+login#Non-Interactive(bot)login-CreatingaSelfSignedCertificate](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Non-Interactive+%28bot%29+login#Non-Interactive(bot)login-CreatingaSelfSignedCertificate). What problems specifically are you having?

*Tags: General Technical*

---

**Mo** - *08:20:51*

Not sure I can be of much further use to you then. I think [@UQL0QDEKA](@UQL0QDEKA) might have some tips, I seem to remember him having and solving similar problems

*Tags: General Technical*

---

**river_shah** - *09:49:22*

hope everyone had a good weekend. trying to look at starting prices, have tried a dozen or so races but SP object is always empty pre inplay. I am using betfair advanced data. do I need to change anything to data filter fields please? my understanding is that `sp.near_price` should start displaying expected starting price before race goes inplay

```selection_id = {int} 12126337

sp = {SP} &lt;flumine.patching.SP object at 0x7fe1187e0cc8&gt;

 actual_sp = {NoneType} None

 back_stake_taken = {list: 0} []

 far_price = {NoneType} None

 lay_liability_taken = {list: 0} []

 near_price = {NoneType} None

status = {str} 'ACTIVE'

total_matched = {float} 700.52



fields = ["EX_ALL_OFFERS",

          "EX_MARKET_DEF",

          "SP_AVAILABLE",

          "SP_TRADED",

          "EX_TRADED"]```



*Tags: General Technical*

---

**Jonjonjon** - *15:02:46*

I read there's a way, in Flumine, to only run a trading strategy at particular times. However, I'm having trouble finding the functionality in the code base. Does anyone know where it is?

*Tags: Strategies*

---

**liam** - *15:04:27*

np, `strategy.process_market_book` only gets called if the check returns True

*Tags: Strategies*

---

**dan2002** - *16:30:01*

how do i select market based on market start date and country code? streaming_market_filter in Flumine dosn;t have market start date . Thanks

*Tags: General Technical*

---

**liam** - *16:34:32*

Uses the bflw streaming filter for country_code, you can't filter streaming data by start date so just ignore anything you don't want

*Tags: General Technical*

---

**Jono** - *19:23:36*

hey all is it possible to alter an orders handicap market instead of cancelling it and having to place the cancelled amount in a new order at the new desired handicap? Thanks for the help.

*Tags: General Technical*

---

**Theo Caplan** - *22:34:35*

Hi - new to this so apologies if I have stupid questions.



When I have a &lt;RunnerBook&gt; object, the `.ex` attribute doesn't seem to return anything, even though it looks like it should return an &lt;ExchangePrices&gt; object. Any idea why this might be?

*Tags: Getting Started*

---

**Theo Caplan** - *22:35:58*

Also this is just using bflw rather than flumine

*Tags: General Technical*

---

**Newbie99** - *23:24:57*

I believe [@U4H19D1D2](@U4H19D1D2) may be able to help you out with key activation if need be

*Tags: General Technical*

---

## 2020-10-20

**James Norman** - *07:18:18*

Thank you very much PeterLe, that guide was very helpful and I've managed to login with it. Much appreciated.

*Tags: General Technical*

---

**Peter** - *08:53:12*

Lambda layer for Betfairlightweight. Before trying to build my own, which i suspect will be difficult on a Mac without learning some docker, I wondered whether anybody here already has one working?

*Tags: General Technical*

---

**D** - *10:51:44*

I have used a layer for BetfairLightweight. I didn't use docker, I created it on an EC2 instance. Using this to install locally before creating the zip file for upload: pip3 install betfairlightweight -t ./lambda --upgrade

*Tags: Getting Started, Deployment*

---

**Peter** - *15:18:48*

First is the paths available for python layers on Lambda. Seems I need to root the package at python/lib/python3.x/site-packages/, so the layer gets bound to the version of python I'm using when I create the Lambda function and I'd need to recreate a new layer if I want to upgrade my version of python, which is a bit of a pain.

*Tags: General Technical*

---

**Peter** - *15:22:59*

Second issue is how to handle the certs and key. Ideally I'd pass them in as strings from the environment variables. But BFL  appears to allow me only to specify a folder or individual files. So I'm thinking maybe my Lambda function needs to be uploaded as a zip file containing the code and a certs folder, but that's extra overhead for each update and I'm not wild about deploying lots of copies of my certs. Alternatively I could try to make the certs folder a layer too.

*Tags: Deployment*

---

**D** - *16:10:24*

Hi Peter, I don't think the Python version / library location is really an issue. Once the layer is there you can just import bflw as you normally would. The trickiest thing for me was the certs part. I created a folder in the Lambda for the certs and used this line of code to reference it iirc: certs = r'task/certs'

*Tags: General Technical*

---

**Peter** - *16:21:04*

Many thanks [@URMM9463X](@URMM9463X) for the earlier pointer. I have a fully working version now much more quickly than I expected. So big hat tip to you. I have a number of start of day data collection scripts and strategies that run close to an event's start that I suspect will be more efficient run as lambdas than on a server that's mostly idle for the rest of the time. My concern over the python version, was about ongoing maintenance rather than importing within the lambda, which as you say is trivial, but upon reflection, I would expect to create new layers for upgrades to BFL way more frequently than I upgrade the underlying version of Python, so not really an issue.

*Tags: Deployment*

---

**D** - *16:25:13*

You're welcome [@U9JHLMZB4](@U9JHLMZB4). Sounds like your project is similar to something I did, whereby a step function schedules a lambda to run at x minutes before market_start_time. Must be the cheapest way to run BFLW on AWS!

*Tags: Deployment*

---

**dan2002** - *18:01:45*

i am just wondering if anyone has used flumine for live trading?

*Tags: Deployment, Strategies*

---

**Jonjonjon** - *18:12:13*

Good question. I am just using Flumine for backtesting at the moment.

*Tags: General Technical*

---

## 2020-10-21

**ricky** - *12:16:15*

In football, how to filter UEFA champions league?

i am using flumine,  is there any country_codes in streaming_market_filter for international match?

If i only filter in check_market_book with competition.id i got "trying to subscribe to 1706 markets whereas max allowed number was: 1000"

*Tags: General Technical*

---

**liam** - *12:25:07*

Or duplicate the strategy by country code 

*Tags: Strategies*

---

**ricky** - *12:35:00*

[@U4H19D1D2](@U4H19D1D2) Do you mean have duplicate strategy for champions league and manually enter market id as filter for each match?

*Tags: Strategies*

---

**mandelbot** - *14:27:37*

`trade = Trade(`

    `market_id="1.2345678",`

    `selection_id=123456,`

    `handicap=1.0,`

    `strategy=strategy`

`)`

*Tags: Strategies*

---

**mandelbot** - *14:28:07*

can someone tell me what the `handicap` and `strategy` parameters do in the above?

*Tags: Strategies*

---

**liam** - *14:29:03*

handicap is runner handicap (check out the betfair api docs as its required for orders)

*Tags: General Technical*

---

**liam** - *14:29:24*

strategies have trades and trades have orders, we pass the strategy to the trade as its then used for exposure/reset calculations

*Tags: Strategies*

---

**ricky** - *21:07:27*

Last hour i got warning message from flumine: "levelname": "WARNING", "message": "High latency between current time and MarketBook publish time".

Is it just me? It looks like 10-15 mins delay.

*Tags: Performance*

---

**ricky** - *21:17:44*

Apologise, I havent used the lastest version of flumine. Occasionally when inplay football began, my CPU loading was over 100%, and memory usage is 16.4%.

I use Ubuntu 20.04. Any suggestion what tool i can use for debugging the root cause of high CPU load? My initial thought was the filesystem (nautilus), becasue i keep recording data into a file, but i have not way to prove it becuase it happened occasionally.

*Tags: Errors Debugging, Performance*

---

**ricky** - *22:37:39*

I am running home servers via Vmware. VM allocated 4 processes and 8G of RAM.

The issue was the process handling data recording via flumine was very high (over 100%).

I will upgrade to latest version of flumine and give a try tomorrow. other possibility may be filesystem issue.

*Tags: Deployment*

---

## 2020-10-22

**liam** - *06:49:01*

There was a bug where the latency warnings were being shown incorrectly on snapped data but his has been fixed. However if you are just recording data you shouldn’t be getting high CPU, have you set the streaming_timeout? Should be None for data recording 

*Tags: Errors Debugging, Performance*

---

**ricky** - *11:18:31*

Thanks Liam, i just upgraded to the latest version. I will set streaming_timeout to None and give a try.

*Tags: General Technical*

---

**ricky** - *11:29:52*

Before i left streaming_timeout unset. The default should be None. I didnt config stream_class either,  because if i set stream_class=DataStream, check_market_book wont called, i left stream_class by default value.

*Tags: General Technical*

---

**ricky** - *11:51:30*

Yes i am recording data, i wrote class similar to pricerecorder.py ( i stored raw data like back,lay price with other additional data into a file).

If i selected the wrong stream_class, like marketdata to record data, it is likely to cause the CPU overload problem?

*Tags: General Technical*

---

**Seabass** - *23:46:33*

Hi, I'm still working on getting a time and sales file generated from the pro market data. You guys were right that when the quantity is not exactly double what was removed from the order book, it is generally because of cross-matching. I am a little confused about how to handle what seems to be currency rounding. Is there any documentation about when Betfair would send trd updates because of currency rounding? These strange updates seem to happen close to the hour change.

*Tags: General Technical*

---

## 2020-10-23

**Mo** - *09:37:35*

Looks like ggplot2 theme so either ggplot2 in R or seaborn in Python is my guess

*Tags: General Technical*

---

**Jonjonjon** - *09:42:36*

I'm wondering if it's worth updating BFLW to exclude those funny data points. Though given the simulated execution delay, and the tiny sizes of those false fills, it shouldn't have much impact on backtesting.

*Tags: General Technical*

---

**liam** - *10:51:52*

Do you mean flumine? Would never change bflw to adjust data that comes from the source

*Tags: General Technical*

---

**Seabass** - *19:51:43*

Thanks! I used Plotly and Python. The chart is for a soccer game.

*Tags: General Technical*

---

## 2020-10-24

**dan2002** - *21:40:20*

Hi, I have a question about flumine. in example.py in flumine, framework.run() will never stop? if i want trading to stop after race finished, how shall i do it? Thanks.

*Tags: Strategies*

---

**Lee** - *21:53:00*

you can still keep the process running, just filter out races you don’t want in a strategy

*Tags: Strategies*

---

## 2020-10-25

**Alessio** - *09:14:31*

actually, if you do finish() on a strategy, does it also shut down the stream as well? With international matches that you cannot just filter for i was thinking of just spawning strategies and destroying them (maybe with a custom event), but i am not sure it cleans up everything when you finish()

*Tags: Strategies*

---

**Alessio** - *09:17:37*

OK, so there's no way to 'remove' a strategy and clean its stream, atm ?

*Tags: Strategies*

---

**Alessio** - *09:18:21*

(i could launch an entire flumine framework for each of those markets, but looks a bit overkill lol)

*Tags: General Technical*

---

**Alessio** - *09:22:18*

select a bunch of markets (via event filter -&gt; market_filter, not a streaming filter)

*Tags: General Technical*

---

**Alessio** - *09:22:30*

run a strategy with a streaming_filter with the markets

*Tags: Strategies*

---

**Alessio** - *09:22:53*

when it's done, shutdown the single strategy and its stream

*Tags: Strategies*

---

**liam** - *09:22:57*

So the restrictive filter is your problem 

*Tags: General Technical*

---

## 2020-10-26

**liam** - *09:38:58*

[https://github.com/liampauling/betfair/blob/master/HISTORY.rst#291-2020-10-26|2.9.1](https://github.com/liampauling/betfair/blob/master/HISTORY.rst#291-2020-10-26|2.9.1) released, historic improvement ([@UBS7QANF3](@UBS7QANF3)) and I have moved all C/Rust libraries to a speed install

```pip install betfairlightweight[speed]```

*Tags: Getting Started, Performance*

---

**dan2002** - *11:25:41*

when I tried to output the back test Order list  of backtesr.py to cvs through logging control in flumine, I get different order history each time on release mode, and in debug mode order history is consistent. My question is that are we supposed to get the same order list each time we run backtesr.py? Anyone can share an example of  a csv logging control? Thanks.

*Tags: Errors Debugging*

---

**Lee** - *11:33:42*

i only use the logging control for running live. when backtesting i get the orders the same way as this example and the output is consistent. [https://github.com/liampauling/flumine/blob/master/examples/backtest.py#L34](https://github.com/liampauling/flumine/blob/master/examples/backtest.py#L34)

*Tags: Deployment*

---

**dan2002** - *11:43:43*

I wrote the orders (status, size matches etc) into a csv  after extending loggingcontrol, I got different order status, size matched etc each time I run backtesr.py on debug mode.

*Tags: Errors Debugging*

---

**liam** - *20:27:09*

What does the strategy look like? Are you saying you get completely different data written to the csv on each run?

*Tags: Strategies*

---

**dan2002** - *20:34:55*

the strategy is the LowestLayer strategy in backtest.py, i get slightly different result(same number of rows/orders, but slightly different in terms of fills)

*Tags: Strategies*

---

## 2020-10-27

**Jono** - *14:58:40*

is there a attribute to in the runner class that lets you know if its listed in the home or away slot for markets where that applies like football, rugby etc? If not not is there a convenient way of determining this? Im looking through basic historical handicap data and am trying to determine the selection in question is the home or away team, cheers

*Tags: General Technical*

---

**Seabass** - *16:40:57*

No, but I saw he had answered a similar question on Betfair's FAQ, I will send him a message. Thanks!

*Tags: General Technical*

---

**jhaa** - *16:55:43*

do you just do pip install betfairlightweight -U on the live system or is there anything that will break?

*Tags: Getting Started, Deployment*

---

**Seabass** - *17:00:23*

Got it, that it makes it really inefficient to process the data though. It would be really helpful if Betfair had a field indicating if it was just a currency update. I will give your suggestion a try though, thanks!

*Tags: General Technical*

---

**liam** - *17:07:25*

Originally used ujson as it is quicker than the standard library however it has become very unstable with the new maintainers. Moved to orjson as it was all the rage but that can be tricky to install and the latest release has issues with some versions of python. So it is now setup to use orjson if installed and fall back on the standard library 

*Tags: Getting Started*

---

**Jono** - *17:09:28*

hey sorry to keep at this but in the properties i can see on each runner there is no sort_priority option, so i cant just do runner.sort_priority                                These are all the attributes i can see `['selection_id', 'status', 'total_matched', 'adjustment_factor', 'handicap', 'last_price_traded', 'removal_date', 'sp', 'ex', 'orders', 'matches', 'matches_by_strategy', '__module__', '__doc__', '__init__', '__str__', '__repr__', '__dict__', '__weakref__', '__hash__', '__getattribute__', '__setattr__', '__delattr__', '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__', '__new__', '__reduce_ex__', '__reduce__', '__subclasshook__', '__init_subclass__', '__format__', '__sizeof__', '__dir__', '__class__']`

*Tags: Strategies*

---

## 2020-10-28

**river_shah** - *08:52:32*

```file_list = trading.historic.get_file_list(...)

requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='[http://historicdata.betfair.com|historicdata.betfair.com](http://historicdata.betfair.com|historicdata.betfair.com)', port=443): Read timed out. (read timeout=16)```



*Tags: Errors Debugging, Strategies*

---

**Jonjonjon** - *10:22:39*

What is the difference between E and M file types when downloading historical data from betfair?

*Tags: Data Quality*

---

**river_shah** - *11:00:16*

I downloaded a lot of files, now I keep getting this when I invoke `trading.historic.get_file_list`

```[!DOCTYPE html](!DOCTYPE html)



&lt;html&gt;

&lt;head&gt;

    &lt;meta name="viewport" content="width=device-width" /&gt;

    &lt;title&gt;ngErrorRedirect&lt;/title&gt;

&lt;/head&gt;

&lt;body&gt;

    &lt;div&gt; 

        Error

    &lt;/div&gt;

&lt;/body&gt;

&lt;/html&gt;```

*Tags: Errors Debugging, Strategies*

---

**river_shah** - *11:01:56*

is it possible to get a more detailed error message?

*Tags: Errors Debugging*

---

**V** - *16:38:32*

Hey guys, hopefully I can ask this here: what type of response latency can be expected for placing orders? In my case I see it in the couple hundred ms range. This is most likely a betfair thing as others have mentioned a similar response time in the betfair forums, but I couldn’t find any real info as to why that is.

*Tags: Performance*

---

**V** - *16:39:37*

Right, so that kind of latency is to be expected?

*Tags: Performance*

---

**AP** - *20:27:02*

Was Watchtower the Python package recommended for logging to AWS cloudwatch?

*Tags: Deployment*

---

**dan2002** - *21:28:25*

Hi I can not see any number been set explicitly in flumine BetfairStream for variable conflateMs when i run example.py, is the default value 50ms ?

*Tags: General Technical*

---

**AP** - *22:34:45*

Is there a way to add in a custom customer_strategy_ref to a new BaseStrategy class in Flumine?

*Tags: Strategies*

---

**dan2002** - *22:48:42*

another question about example.py,, it intended to cancel £1.51 of the £2 order, but i always see the whole £2 order get cancelled instead? Anyone know why? Thanks.

*Tags: General Technical*

---

## 2020-10-29

**liam** - *07:56:39*

[@U01BBMTK6R3](@U01BBMTK6R3) fairly sure you will get the INVALID_PROFIT_RATIO error if trying to cancel 1.51 of a 1.01 order, what do the logs say?

*Tags: Errors Debugging*

---

**liam** - *07:57:16*

Regarding conflate, the default is None but can be changed:

```strategy = ExampleStrategy(

    market_filter=streaming_market_filter(market_ids=["1.174788821"]),

    conflate_ms=500,

)```

*Tags: Strategies*

---

**liam** - *08:01:22*

[@UNQGKT0CR](@UNQGKT0CR) customerStrategyRef is used to differentiate multiple running instances of flumine (hostname), flumine will package via the OrderPackage multiple orders from different strategies together. Expectation is the user logs orders through the LoggingControl.

*Tags: Strategies*

---

**river_shah** - *08:03:50*

what is the correct way to login to betfair without needing username or password please? is there ssh only support? [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Non-Interactive+%28bot%29+login](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Non-Interactive+%28bot%29+login) have the certs etc generated, currently just pass username and password as plain text in my python script. would like to make this secure.

*Tags: General Technical*

---

**dan2002** - *19:43:04*

[@U4H19D1D2](@U4H19D1D2) I only run the example strategy on paper trading mode and backtesting mode, I don't see INVALID_PROFIT_RATIO, maybe when i go live it will show in the log. What puzzled me is that it seems flumine has changed the size reduction from 1,51 to 2 as i can see the whole £2 order has been cancelled. Do you know where flumine has make this adjustment?

*Tags: Deployment, Strategies*

---

**birchy** - *19:43:59*

If going down the S3 route, assuming you have `~/.aws/credentials` , you can mount it as a local network drive and use like a normal file system using `s3fs` v1.85+:

```$ mkdir ~/AWS_S3

$ s3fs bucketname ~/AWS_S3```

*Tags: Deployment*

---

**klozovin** - *21:45:36*

how does bflw handle when there's `Infinity` as a value for `totalAvailable` or a `size` in listMarketBook response? what does that even mean when Betfair sends it...

*Tags: General Technical*

---

## 2020-10-30

**AP** - *00:35:28*

Is there an example of how to deal with an order through LoggingControl?

*Tags: General Technical*

---

**liam** - *08:26:47*

You are correct, there was no handling for size reduction in the simulated execution, created a PR to fix it [https://github.com/liampauling/flumine/pull/305](https://github.com/liampauling/flumine/pull/305)

*Tags: Errors Debugging*

---

**liam** - *08:54:54*

There is [https://github.com/liampauling/flumine/pull/306|now](https://github.com/liampauling/flumine/pull/306|now) pending the minor change in baseflumine.py

*Tags: General Technical*

---

**dan2002** - *09:42:50*

But for living trading using flumine, can the size reduction be handled?

*Tags: Strategies*

---

**dan2002** - *09:43:56*

Ok. I will appreciate if you let me know once this issue with simulation is fixed. Thanks.

*Tags: Errors Debugging*

---

**jp** - *15:16:31*

[@U919M37DL](@U919M37DL) You can also get infinity size for virtual prices in some rare cases. Neil has confirmed that this is a bug in computation of virtual prices (and it did not sound as something they prioritized to fix)

*Tags: Errors Debugging*

---

## 2020-10-31

**birchy** - *12:56:39*

Yeah, they're not keen on "leaching". It's always safer to run at least one strategy while data mining.

*Tags: Strategies*

---

**Remi** - *12:57:59*

Was not leaching, always been betting. Had over 200 bets unmatched on 1 strategy and I think something like 150 of another. They didn’t even allow me to cancel those bets so those stayed open to be matched…

*Tags: Strategies*

---

**Goldpan** - *13:45:23*

Normally I don't post here but this is quite interesting (and scary) as I've had an active  BF account since BF started as a business.  I was at first thinking it was a sportsbook related thing, but they would only throttle your bets if winning to much. If you're not using a VPN outside of the UK as you say you're not and your not breaking any of T&amp;C knowingly then this is very strange. To go straight to account closure is quite unusual indeed because normally they would suspend an account pending an investigation. There is something very wrong here either with what you are claiming or what they perceive is happening so you really need to contact BF again and keep trying.  My personal interest is that I'm about to start using a bot for betting and noticing your high amount of betting activity I also don't want to go foul for something wrong unknowing. In that regard I hope you share if possible what the problem is when you find out.

*Tags: Strategies*

---

**Goldpan** - *14:23:07*

and ur not selling tips or services I suppose. I'm thinking it has something to do with the API, but really u need to contact them again. An article I found might help. [http://www.betfairprotrader.co.uk/2016/07/accounts-being-closed-at-betfair.html](http://www.betfairprotrader.co.uk/2016/07/accounts-being-closed-at-betfair.html)

*Tags: General Technical*

---

## 2020-11-01

**Remi** - *17:47:29*

And the answer to every single question I ask via the chat service is ‘your account has been closed’.

*Tags: General Technical*

---

## 2020-11-02

**Remi** - *16:30:58*

I don’t think any laws have changed. I was not doing matched betting but they did give me a bunch €50 free bets that I did use.

*Tags: Deployment, Strategies*

---

## 2020-11-03

**Michael** - *09:35:47*

Hi all,

Just starting out and a little confused by streaming, the example here: [https://liampauling.github.io/betfair/streaming/](https://liampauling.github.io/betfair/streaming/) works for me. However this seems to print the market only when the Market itself is changed, ie added, goes in play or is closed/finished. How then do you fetch the regular price changes (very often) that you would see in listMarketBook from the stream? Do I need to used Snap()? or do I need some form of separate cache that subscribes to events as briefly mentioned here [https://support.developer.betfair.com/hc/en-us/articles/360000402291-Market-Order-Stream-API-How-does-it-work-](https://support.developer.betfair.com/hc/en-us/articles/360000402291-Market-Order-Stream-API-How-does-it-work-) ?

*Tags: General Technical*

---

## 2020-11-05

**Newbie99** - *09:05:10*

This is quite worrying, how can you tell if you are using too much data and is it all streaming, or a combination of REST calls too? Also, it seems a bit over-zealous from Betfair, why not suspend an account and drop an e-mail explaining the problem?

*Tags: General Technical*

---

**Chris** - *09:07:18*

I was purely using the REST Api, no streaming

*Tags: General Technical*

---

**Jorge** - *10:47:28*

Is it a good practice to use the delayed AppKey when testing bots and only using the live AppKey when trading considerable amounts? Or do you guys just use the live AppKey all the time and there is no problem about that? I found that for pre-game strategies the delayed key does not have any delay on getting the odds/volume.

*Tags: Deployment, Strategies*

---

**Jono** - *15:03:29*

hey [@UBS7QANF3](@UBS7QANF3) ive noticed that in basic historical data market_book.runners contains the last traded price but the market_book.market_definition.runners contains the handicap values . The former doesnt have as many many entries so there isnt a one to one relation between the list of last traded prices and handicaps. I was wondering if there was a way to correctly index through the all the runners ensuring that the ltp and handicap match up correctly. I believe a way to get the lst from the market_definition runners would be a solution to my problem but i havent managed to find a way to do this. Any help is massively appreciated cheers

*Tags: Data Quality*

---

**Jono** - *16:56:31*

no problem, ill be sat here at the desk all evening :slightly_smiling_face:

*Tags: General Technical*

---

**Mo** - *17:39:31*

```import betfairlightweight

trading = betfairlightweight.APIClient(username='', password='', app_key='')

stream = trading.streaming.create_historical_generator_stream(file_path='/tmp/1.139219567', listener=betfairlightweight.StreamListener())

for mbs in stream.get_generator()():

    mb = mbs[0]

    for runner in mb.runners:

        print(mb.publish_time, runner.selection_id, runner.handicap, runner.last_price_traded)```

```2018-01-21 22:18:14.880000 8881 6.5 1.8

2018-01-25 14:54:03.321000 8881 6.5 1.8

2018-02-15 12:00:04.048000 8881 6.5 1.8

2018-02-15 12:00:22.621000 8881 6.5 1.8

2018-03-19 13:54:56.907000 8881 6.5 1.8

2018-03-19 13:54:56.907000 8875 -4.5 1.84

2018-03-19 14:12:12.995000 8881 6.5 1.8

2018-03-19 14:12:12.995000 8875 -4.5 1.84

2018-03-25 21:57:43.540000 8881 6.5 1.8

2018-03-25 21:57:43.540000 8875 -4.5 1.84

2018-03-25 21:57:43.540000 8875 -6.5 1.84

2018-03-25 22:01:39.401000 8881 6.5 1.8

2018-03-25 22:01:39.401000 8875 -4.5 1.84

2018-03-25 22:01:39.401000 8875 -6.5 1.84

2018-03-25 22:01:39.401000 8875 -5.5 1.77

2018-03-26 12:54:23.488000 8881 6.5 1.74

2018-03-26 12:54:23.488000 8875 -4.5 1.84

2018-03-26 12:54:23.488000 8875 -6.5 1.84

2018-03-26 12:54:23.488000 8875 -5.5 1.77

2018-03-28 15:20:20.765000 8881 6.5 1.74

2018-03-28 15:20:20.765000 8875 -4.5 1.84

2018-03-28 15:20:20.765000 8875 -6.5 1.84

2018-03-28 15:20:20.765000 8875 -5.5 1.77

2018-03-29 13:09:47.930000 8881 6.5 1.74

2018-03-29 13:09:47.930000 8875 -4.5 1.84

2018-03-29 13:09:47.930000 8875 -6.5 1.84

2018-03-29 13:09:47.930000 8875 -5.5 1.77

2018-03-29 13:09:47.930000 8875 -7.5 1.91

2018-03-29 23:15:06.301000 8881 6.5 1.74

2018-03-29 23:15:06.301000 8875 -4.5 1.84

2018-03-29 23:15:06.301000 8875 -6.5 1.84

2018-03-29 23:15:06.301000 8875 -5.5 1.77

2018-03-29 23:15:06.301000 8875 -7.5 1.97

2018-03-29 23:15:06.301000 8881 7.5 2.02

...```

*Tags: Strategies*

---

**Jonjonjon** - *20:53:41*

If I start running a market recorder to watch all WIN, PLACE and OTHER_PLACE Horse and Dog markets, is there anything I need to be aware of beforehand? I've read a few prior posts about limits, but am not sure what they refered to

*Tags: Data Quality*

---

**liam** - *20:55:16*

If you are streaming you should be fine, the recent closures have been because people have been making silly amounts of requests with little or no commission paid 

*Tags: General Technical*

---

## 2020-11-06

**Jonjonjon** - *12:13:19*

Suppose I have a Flumine Strategy with a single parameter.



I want to run that using 2 different values of parameter.



At a later date, I want to download my cleared orders, split them by the parameter value. And then do some analysis.



Would the recommended way to do that using this framework be:



• Add some `notes`, when creating the `Trade` objects.

• Use a custom `LoggingControl` to log the `notes` and `customer_order_ref`'s?

• Use the log to help me split up `cleared_orders`

?

*Tags: Strategies*

---

## 2020-11-07

**birchy** - *11:04:58*

In Flumine backtesting, is there a parameter that shows the total risk or liability for all matched bets on a market?

*Tags: General Technical*

---

**liam** - *12:52:24*

Currently only what is in the blotter class which restricts to strategy but this needs to be expanded 

*Tags: Strategies*

---

**Oliver Varney** - *18:53:13*

[@U4H19D1D2](@U4H19D1D2) is there a minimum roi your looking for with a bsp strategy, I assume you move the price once you start the strategy

*Tags: Strategies*

---

## 2020-11-09

**liam** - *08:39:53*

You can either log every order as it is created through `_process_order` or at the closure with `_process_cleared_orders_meta` that latter contains a list of all flumine `Order` objects for that particular market as per this [https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py|example](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py|example)

*Tags: General Technical*

---

**liam** - *10:46:26*

Reminder that this is happening tomorrow, not sure what the impact will be on order streaming if you are not running v2.10.0

*Tags: General Technical*

---

**dan2002** - *14:57:22*

In flumine, if i place an limitorder with persistance type as market on close. will it be possible for my order get matched soon after i place it, or i have to wait until the SP action to get matched there?

*Tags: General Technical*

---

**dan2002** - *19:17:57*

No, Today I placed an limit order of less than £10 exposure for market on close Persistence type , it didn’t get filled. [@U4H19D1D2](@U4H19D1D2) is it possible modify flumine backtest for this so it won’t get filled in backtest either? Thanks.

*Tags: General Technical*

---

**liam** - *19:23:50*

Sure, can you create an issue? It would be on this line [https://github.com/liampauling/flumine/blob/364988c715176ba5d4bc3b2478ca3b38a287f993/flumine/backtest/simulated.py#L204|https://github.com/liampauling/flumine/blob/364988c715176ba5d4bc3b2478ca3b38a287f993/flumine/backtest/simulated.py#L204](https://github.com/liampauling/flumine/blob/364988c715176ba5d4bc3b2478ca3b38a287f993/flumine/backtest/simulated.py#L204|https://github.com/liampauling/flumine/blob/364988c715176ba5d4bc3b2478ca3b38a287f993/flumine/backtest/simulated.py#L204)

*Tags: General Technical*

---

## 2020-11-10

**Oliver Varney** - *10:13:48*

morning, does anyone know if there is a filter that can be applied to streaming to only stream all weather races?

*Tags: General Technical*

---

**Newbie99** - *10:15:50*

There may be a better way, but I use a background worker (in flumine) that polls the racecard api and can get track, going etc. from that

*Tags: General Technical*

---

**Remi** - *14:29:00*

The way it happened was that I had an idea for a strategy and made a small trial using the rest api. It worked well and so I scaled it to most event types without making a proper implementation first. They didn’t notice for a while (and I didn’t realize I was making a retarded number of requests) and then they cut me.

*Tags: Strategies*

---

**Remi** - *14:29:48*

So now I am back! Going to do this streaming thing.

*Tags: General Technical*

---

**Remi** - *14:33:34*

I was looking at the streaming [https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py|example](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py|example).  Does the  `stream.subscribe_to_markets`  subscribe once to the current live markets that meet the specification, or does it continue to subscribe to new markets that also meet the specification as more markets come online?

*Tags: Deployment*

---

**liam** - *14:37:18*

the latter, you subscribe and then new markets are added once available with bflw doing the hard work

*Tags: General Technical*

---

**Remi** - *14:39:44*

Say I subscribe to all of soccer but I have a limit of 200. Does betfair or bflw choose which subset I subscribe to or do I just get errors?

*Tags: Errors Debugging*

---

**liam** - *14:40:09*

error on subscription, try it

*Tags: Errors Debugging*

---

**Remi** - *14:41:42*

you are capped at 200 without errors?

*Tags: Errors Debugging*

---

**Remi** - *14:43:13*

And if you want to use multiple streams but process them with the same strategy. Do you just give them the same queue to push to?

*Tags: Strategies*

---

**Chris** - *14:45:19*

In a similar situation where I was using bflw rather than flumine and had my account suspended, back up and running now and trying to get my head around flumine, to confirm if I am getting data with flumine betfair don't have an issue with that like they do with the REST API?

*Tags: General Technical*

---

**Remi** - *14:47:25*

Sorry for the many dumb questions. Coming at it more from a math background than from software engineering background.

*Tags: General Technical*

---

**klozovin** - *14:47:46*

could you say how many requests were you making? at least a ballpark figure... btw, glad you got that resolved!

*Tags: General Technical*

---

**Chris** - *14:48:49*

If it helps I was told this...



```Your account was closed due to your excessive amount of reading for your level of betting activity. You have made over 150 million data requests to the exchange which is not acceptable given your level of betting and this volume of reading is deemed to be for commercial usage. ```

That is probably over around a year to 18 months

*Tags: Strategies*

---

**liam** - *14:53:12*

Flumine uses streaming for markets and orders so you shouldn't have an issue, [@UU1URJ8L8](@UU1URJ8L8) have you asked for a subscription bump?

*Tags: General Technical*

---

**Miroslav** - *18:15:01*

Hi,



I have the following questions, as far as I am aware William Hill API generates system delay on placing single/multple bets:

• Does betfair exchange API has it?

• So have you ever try to negate/remove this delay?

*Tags: General Technical*

---

**liam** - *18:54:10*

Used to be a bug where the funds validation would occur after the delay, this meant you could place an order and if you didn’t want it to enter the matcher transfer all your funds to another wallet and the place request would error 

*Tags: Errors Debugging*

---

**Mo** - *19:07:42*

Was the fix just checking the funds before the delay?

*Tags: Errors Debugging*

---

**klozovin** - *20:41:30*

it would seem so to me, too... but the way [@U01A92RPAEP](@U01A92RPAEP) phrased his question seemed like there should be another way :slightly_smiling_face:

*Tags: General Technical*

---

## 2020-11-11

**thambie1** - *13:52:11*

Hey folks. Question for those who are running backtests in the cloud, how much is it costing you? For comparison my infra is costing me about $0.50 per 1 year of backtest over pro soccer historical data. Some days I do a ton of backtests, which can add up.  I'm curious how that compares to others.

*Tags: Data Quality*

---

**Mo** - *17:16:31*

`pantab` package is great for building these extremely quickly from `pandas` DataFrames

*Tags: Feature Engineering*

---

**Mo** - *17:18:22*

Then the file can be dropped into my Windows VM and opened in Tableau. Way way faster than connecting Tableau directly to a database

*Tags: General Technical*

---

**Unknown** - *22:05:56*

[@U017FQEPV1U](@U017FQEPV1U) Easiest way is to use Flumine MarketRecorder to gather data. Then create a strategy framework and use Flumine backtest. In theory you can change the framework to live without changing the strategy.

[https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py](https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py)

*Tags: Deployment, Strategies*

---

## 2020-11-12

**Jonjonjon** - *09:12:18*

Lots of flumine modules contain this at the top:



```logger = logging.getLogger(__name__)```

What is the best way to send log all of them to a single file?

*Tags: General Technical*

---

**Mo** - *09:15:05*

Add a handler to the `'flumine'` logger

*Tags: General Technical*

---

**liam** - *09:15:14*

Here is a good start, scroll down for example in how to include a library, also see [https://liampauling.github.io/flumine/advanced/#logging|here](https://liampauling.github.io/flumine/advanced/#logging|here) to setup the jsonlogger

*Tags: Getting Started*

---

**azevedo** - *10:54:00*

[!here](!here) hey guys, I’m getting certificate_verify_failed error all of a sudden when trying to connect to streaming. (it’s the same code that’s been running for a while and ran this morning; and the certificate shouldn’t expire until next year) 



has anyone had that before? what could that be due?

*Tags: Errors Debugging*

---

**Johnny** - *10:56:17*

main website had an ssl error too... but that seems to be fixed now

*Tags: Errors Debugging*

---

**Phil** - *10:56:31*

[!here](!here) betfair doing load testing, causing issues, should be fixed shortly

*Tags: Errors Debugging*

---

**liam** - *10:56:59*

load testing on production??

*Tags: Deployment*

---

**Mo** - *11:01:10*

Adding `verify=False` here: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/login.py#L48](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/login.py#L48) is a workaround

*Tags: General Technical*

---

**Misha** - *11:04:52*

Apparently it's fixed anyway

*Tags: Errors Debugging*

---

**Jorge** - *12:01:23*

Streaming kept working for me fine, but keep_alive NG call failed at 10:28, it succeded at 10:29 though :slightly_smiling_face:

*Tags: Errors Debugging, Deployment*

---

**Chris** - *15:09:38*

Probably a silly question but when streaming and hitting the limit of the number of markets (`SUBSCRIPTION_LIMIT_EXCEEDED`) - is that limit per stream that is opened, or across all streams?

*Tags: General Technical*

---

**Chris** - *15:20:25*

Thanks, and I guess the obvious question coming is, is there a limit to the number of streams that can then be opened?

*Tags: General Technical*

---

## 2020-11-13

**Julio** - *11:43:05*

i had the same issue as you. I asked them to increase the limit to 1000. Unfortunately during weekends there are more than 1000 markets... So I had issues running my strategy when there was the most markets.

I re-write to them and they lifted the restriction...

Maybe before granting it they look at your usage (they want you to bet enough to generate enough revenue) and do not just record markets for instance

*Tags: Strategies*

---

## 2020-11-15

**Alessio** - *17:31:12*

question for the footies.. do you have any way to compute the length of the halftime in a specific soccer match?

*Tags: General Technical*

---

**Alessio** - *18:22:33*

which is an annoying risk of screw up when training on historical data of course

*Tags: Data Quality*

---

**Ryan Clapham** - *18:42:57*

Evening, quick question is it possible to record the available lay price with the runners selection id? Thanks

*Tags: General Technical*

---

**Ryan Clapham** - *19:21:05*

Thank you for that [@UFTBRB3F1](@UFTBRB3F1) On 'available_to_back' it prints out &lt;betfairlightweight.resources.bettingresources.PriceSize object at 0x038D1148&gt;, &lt;betfairlightweight.resources.bettingresources.PriceSize object at 0x038D1118&gt; This teaches me for jumping into the deep end. Only been learning python a couple of months. Think we need to swap usernames lol.

*Tags: Strategies*

---

**Newbie99** - *19:33:52*

its also worth noting, when checking for prices to build in some error handling to account for null values (e.g. if the market is 1000 / or / 1.01)

*Tags: Errors Debugging*

---

**dan2002** - *20:37:43*

Hi is there a queue for SP price matching in flumine backtest and in live? Thanks.

*Tags: Deployment*

---

**Jonjonjon** - *23:08:22*

[@U4H19D1D2](@U4H19D1D2) Do you think it would be beneficial to add logging telling users why validation has failed in here?



[https://github.com/liampauling/flumine/blob/4dff28ab03969d8d17a9a031d8db5169df929794/flumine/strategy/strategy.py#L141-L158](https://github.com/liampauling/flumine/blob/4dff28ab03969d8d17a9a031d8db5169df929794/flumine/strategy/strategy.py#L141-L158)

*Tags: Errors Debugging, Strategies*

---

## 2020-11-16

**liam** - *06:12:22*

Yeah that would help although some users might get a lot of logs from it, could add a variable? log_validation or similar 

*Tags: General Technical*

---

**liam** - *06:13:16*

[@U01BBMTK6R3](@U01BBMTK6R3) flumine assumes 100% matching at SP however this is not the case when live as betfair do not guarantee it

*Tags: Deployment*

---

**Newbie99** - *13:38:26*

np, in the end I just stopped the AWS instance and re-started, not the most elegant way, but it worked!

*Tags: Deployment*

---

**liam** - *14:00:15*

Yeah, bflw will try and gracefully [https://github.com/liampauling/betfair/blob/1af4115d5f7493a0b0b4dfb674987830b556464f/betfairlightweight/streaming/betfairstream.py#L68|close](https://github.com/liampauling/betfair/blob/1af4115d5f7493a0b0b4dfb674987830b556464f/betfairlightweight/streaming/betfairstream.py#L68|close) but if the server has disconnected you are left waiting 

*Tags: Deployment*

---

**Chris** - *17:49:13*

I can't see anyway in `streaming_market_filter` to filter based on the events start time, is that because it's not possible, or I am looking in the wrong place? I notice there is `time_range` too but not sure how or if I can use that with streaming

*Tags: General Technical*

---

**birchy** - *18:39:30*

Regarding Flumine, should we try to avoid bet violations, or is it a EAFP function? i.e. I presume that `strategy.max_selection_exposure` is a safeguard that is EXPECTED to violate once the limits are reached?

*Tags: Strategies*

---

## 2020-11-17

**Mo** - *12:21:17*

betfairlightweight can handle the official historic data. Start here: [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**birchy** - *20:52:06*

Anyone know of a way to track approximate progress from python multiprocessing? This is specifically for Flumine backtesting but I have no idea how long the backtest will take. In single processes, it's easy enough to keep track of the market count vs total markets (file paths) and calculate % progress, but it's not so easy when multiprocessing.

*Tags: General Technical*

---

**Lee** - *20:55:25*

```with futures.ProcessPoolExecutor(max_workers=multiprocessing.cpu_count()) as pool:

    results = [

        pool.submit(backtest_market, markets=[market], strategy_cls=strategy_cls)

        for market in markets

    ]

    for _ in tqdm(futures.as_completed(results), total=len(results)):

        pass```



*Tags: Strategies*

---

**Vincent Mele** - *22:11:13*

Apologies if this is an obvious question, but is there a way to go from a json serialised MarketBook object to a new MarketBook object?

```#m_book is a valid MarketBook

j = m_book.json()



# Solution is:

copied_m_book = MarketBook(**json.loads(j))```

*Tags: General Technical*

---

**Vincent Mele** - *22:22:18*

No, I'm trying to serialize historic streaming data at checkpoints (like 30 seconds to start time) and then store that snapshot in Postgres (as jsonb) for feature building and examination in the future at a lower resolution than streaming ticks.

*Tags: Feature Engineering*

---

**Misha** - *22:25:26*

Just noting (with all the performance discussion), that in terms of performance, serialisation and deserialisation is very slow, and should be done as little as possible)

*Tags: Performance*

---

## 2020-11-18

**birchy** - *13:16:07*

In Flumine, if you have a strategy running and at some point in the future you decide you're finished with this market, is there a way to unsubscribe before market closure? This is currently for backtesting but will also be useful when live. More specifically, if code in `check_market_book()` identifies the market is no longer required, what's the best way to skip to the next market file? I know we can use `market_filter` in the framework setup, but sometimes the market changes after subscription, i.e. we've hit our exposure limit, too many runners withdrawn, etc.

*Tags: Getting Started, Deployment, Strategies*

---

**birchy** - *13:24:43*

Doesn't have to be that specific function, but any function within the strategy. I've just found `market.close_market()` :man-facepalming:

*Tags: Strategies*

---

**Oliver Varney** - *13:36:17*

does that remove the market across all strategies though? could be an issue down the line where one strategy wants the market and one is done. simply returning false from the check function could be an option

*Tags: Strategies*

---

**Lee** - *14:38:09*

But like [@ULDAVFDRP](@ULDAVFDRP) said, i wouldn't bother with closing the market early, i think it will cause more problems/bugs in the long run

*Tags: Errors Debugging*

---

**Lee** - *16:44:25*

I think it's better to have exactly the same logic in a strategy for backtesting/unittests/live

*Tags: Deployment, Strategies*

---

## 2020-11-19

**Mo** - *11:44:42*

Ostensibly to provide support for users of the betfairlightweight Python package but in reality to discuss all aspects of (automated) betting

*Tags: Strategies*

---

**Artur Gräfenstein** - *11:49:54*

ahh cool! Do you think js would be too slow for live betting on betfair? Im actually working on a project with js and want to add Betfair API. But don’t know if it would be better to switch to Python.

*Tags: Performance, Deployment, Strategies*

---

**D C** - *11:58:11*

I use nodejs for certain things. The async nature makes it a great choice IMO (I only place inplay bets) but because I am not an "expert" node coder I tend to use it for fairly simple tasks (price pulling and FoK bet placement off simple triggers) - nothing large scale or overly complex but that is down to my own limitations with the language (not a node expert by any stretch and my code is no doubt very "ugly" to purists - but it gets the job done efficiently and dev time is fast). Decent package support too - didn't take me long to find a good https client tool and decompression package at all. I personally don't use BFLW but if I were coming to this game as a newbie it would certainly be my first choice because of the feedback from other users here and the helpful nature of a lot of the experienced members.

*Tags: General Technical*

---

**Newbie99** - *12:09:41*

My intention was to use node, but then I found this Slack group and decided with the support on offer here it was far better for me to switch to Python, so I pretty quickly abandoned the node idea and just started using Python + BFLW (and more recently Flumine too)!

*Tags: General Technical*

---

**Artur Gräfenstein** - *12:19:27*

[@UUE6E1LA1](@UUE6E1LA1) Thank you! My problem is, im experienced in nodejs, but Python is new to me! So it would take a long time to learn.

*Tags: Getting Started*

---

**Artur Gräfenstein** - *12:22:45*

[@UFTBRB3F1](@UFTBRB3F1) do you use it with UI? Im Building a Electron Application for win and mac where you need to setup your strategie. So nodejs is really helpful but it musst be very fast as well!

*Tags: Getting Started*

---

**D C** - *12:52:37*

[@U01EYDA8345](@U01EYDA8345) I think it is natural to stick with what you know and to your strengths. I use C++ for me serious bot work but I would never recommend anyone learn C++ to write bots. I think node is a very natural choice for a betfair bot and if you are experienced already maybe stick with it. You talk about speed a great deal but is this so important? If you are doing seriously complex back end computation to determine bet selection then I can understand you being worried about node speed. Only you know the answer to that question though. I have heard people on here (Liam I think but don't shoot me if I have that wrong) who say that if you need something THAT fast, then Python is not the right choice. I believe something along the lines of "if milliseconds are important to you then Python is not the tool to use". Apologies if I have that wrong but I am sure I remember a discussion along those lines.

*Tags: Performance*

---

**Artur Gräfenstein** - *13:40:54*

Did anyone get problems with betfair because of to many bets placed or something like that. I need for example to place unmatched bets for about 10-20 seconds. When the bet is not matching after this time i need to update the price. And this maybe 100 times in a market during the live soccer match. I have payed the 299£ activation fee and dont want to get banned or something else.

*Tags: Deployment*

---

**birchy** - *13:42:00*

You can place 5000 bets an hour via betfair API

*Tags: General Technical*

---

**Michael** - *13:47:25*

It won't be a problem.

*Tags: General Technical*

---

**Michael** - *14:08:58*

There will be exceptions but it's an 'in general' thing.

*Tags: Errors Debugging*

---

**Artur Gräfenstein** - *15:08:07*

What are you guys working on or what are you doing with BFLW? If anyone want to share his secret.. :smile:

*Tags: General Technical*

---

**Artur Gräfenstein** - *16:13:09*

I test it before going live. I build 2 Ways for testing. 1. I have a database with all odds and match live data with timestamps. I record this data by myself 7/24. And when I have an idea for a strategie, I build a bot and test it against the Data in the Database. If erverything looks good, I start testing it the second way. Running it “live” but not placing the bet. The Bot only saves and shows me the odd on witch he would place the bet. After the match ended, the bot shows red or green for every simulated bet and show me a sum up if Im positive or not. Sometime I run these bots over many days in sumlation before starting to place money. Saves me a lot of money.

*Tags: Deployment*

---

**mandelbot** - *16:15:41*

Lots of people do that and there's built in backtesting functionality in flumine to do this. Though there's always merits to live testing as well.

*Tags: Deployment*

---

## 2020-11-20

**Ben** - *00:28:57*

Apologies if this has been asked already, but Slack is pretty bad for viewing historic questions: any pointers on parsing the historical data files to look for strategies? I've got a few years of the professional historic data for horse racing but am struggling to get it in a useful format to analyse.



I've got the stream generator working in bflw with no issues but I can't seem to figure my way through turning that output into data that I can analyse.

*Tags: Data Quality*

---

**Gary** - *01:43:28*

[@U01FSSHULHE](@U01FSSHULHE) think of a strategy, anything. Could be: bet on maidens whose prices moves in by 10% in the last two mins before the off.  Could be: back the fav .  Doesn't matter just code if. Then Test it by traking bet wins/losses.  Most likely, you won't find it works!  No sweat. Now tweak the strategy. Test again. Then tweak. Then learn. Then tweak. Then learn again.  Repeat.  Often.

*Tags: Strategies*

---

**Ben** - *02:10:23*

It's the thinking of a strategy that I was wanting to approach with a bit more structure. Coming up with a data driven hypothesis to test, for example if a bet over $x on the favourite is accepted, then the price of the second favourite will move by $y



Even look to load it into a neural network for time series analysis or some such, but I just can't seem to get my head around wrangling the data.

*Tags: Strategies*

---

**qwerty.nat** - *02:26:16*

one approach you could use is feed the raw data into bflw and modify it at X second intervals to dump each runners top Y levels of their order book (price and size ) then you can easily create an input for a machine learning system. this will nornalise the data somewhat to only information that is more relevant, then you can simply append data to this 'normalised' format with extra features you want to model on.

*Tags: Feature Engineering, Strategies*

---

**Mo** - *06:55:31*

[@U01FSSHULHE](@U01FSSHULHE) I would suggest that you start by transforming the historic data files into tabular format:



• One row per publish time, selection and price

• Up to you and factors such as your available storage whether you want to do filtering at this stage or not. For example, do you want to keep all prices and have a depth column or just generate the table for the best prices

• Other filtering considerations might include whether you want to only output pre-event data

Put this in a database along with:



• Market level data such as market start time, jump vs flat, distance, race course etc. etc.

• Not immediately a requirement but useful to have at some stage: horse level data

Now you can very easily join and filter to, for example, say you want the best prices on all horses 5 minutes before the scheduled off for all flat races alongside what the horse's eventual starting price was

*Tags: General Technical*

---

**Michael** - *07:22:53*

Technically I use a similar approach to what Mo suggests but rather than a database I just construct big CSVs and use Jupyter notebooks but then that's because I'm a bit basic. In terms of strategy development it depends on what you're interested in doing. I think it's easier for new players to do something market based rather than data driven, if that's your leaning I would suggest that you just start with something vanilla like lay everything at 1.5 or try to make a 1.2 (120%) book and simulate that - or better yet try it out. Obviously it won't work - if it seems like it does work then your data stet is too small or your methodology is flawed. Try to understand why it doesn't work both conceptually and in reality then try to work out the type of circumstances in which it might work or whether there's some sub-set of the bets that are profitable and work on that. Adapt accordingly, rinse and repeat. You can use any sport, if you're using horse racing avoid 'only for maidens' type filters, that's the wrong way to go.

*Tags: Strategies*

---

**Misha** - *09:24:59*

[@UPMUFSGCR](@UPMUFSGCR) - data driven is where you start out with the non-market based data (prior results, statistics, etc), come up with some way to assign probabilities, or something similar, and once you have a model, then test against pricing to see if you can make it profitable. Basically testing against the market is the last step, and shouldn't influence the model, lest you overfit

*Tags: Strategies*

---

**Misha** - *09:27:00*

I am working on tennis (with one other colleague), and our goal is to come up with a probability at any point in any match. If we get this better than the market, we win. Only in the last step do we take our probability and work out how to beat the market

*Tags: General Technical*

---

**Misha** - *09:28:38*

It's not common because it is very hard to get right. but rewards are huge. In essence you can have a model that can roll over year-in/year-out making huge sums

*Tags: Strategies*

---

**Misha** - *09:30:06*

My colleague has been going part-time for 12 months on tennis modelling, which we are shortly rolling out. But we have only just "scratched the surface" as yet. So a big time investment

*Tags: Strategies*

---

**Mo** - *09:36:59*

Sample size of one but I can tell you this is exactly how I got started:



1) Find a tennis model in the academic literature

2) Identify a way to improve the model

3) Implement to have probabilities independent of the market to identify value prices



End result was a highly successful strategy

*Tags: Strategies*

---

**Misha** - *09:40:28*

[@UPMUFSGCR](@UPMUFSGCR) - to make really big money you don't hedge. You trust your model, and some days are awful. My current points model looks good, but I can lose 1/4 of estimated monthly profits in one day, and go down at least 1/2 estimated monthly profits. I reckon over time I could probably have a losing month

*Tags: Strategies*

---

**D C** - *09:44:11*

If you want some idea of how to map data to probabilities [@UPMUFSGCR](@UPMUFSGCR) look into generalised linear models - particularly logistic regression for binary stuff like US sports (lack of a draw makes US sport very attractive). I started with MLB and logistic regression with data scraped off Yahoo sports. Scraping that data (and cleaning it) was by far the hardest (and most boring) component of it all.

*Tags: Strategies*

---

**Mo** - *09:44:23*

This was many years ago so I cannot remember all of the details but the model used the most basic easily available tennis data you can find. The value in the strategy was a sophisticated model, at least more sophisticated than the market makers. Testing was largely live testing against the market and growing stakes organically. 

*Tags: Deployment, Strategies*

---

**Misha** - *09:45:37*

[@UBS7QANF3](@UBS7QANF3) - have you been listening in to my conversations :joy: Our strategy to a tee. We are currently in the growing organically stage

*Tags: Strategies*

---

**Michael** - *10:06:07*

A good in-between example is using GPS for horse race betting. You have a data source but it's  of middling quality and a lot of other people have it too. So to make the most of it you need a data driven model grafted onto an understanding of the very particular market dynamics of in-running horse racing. In general market driven operations will be churning vast numbers of smaller bets and outright stats models will be more likely to take fewer, bigger bets. some people make lots of money in both ways but in both cases it's not many people.

*Tags: Strategies*

---

**Alessio** - *10:30:47*

Yeah you get basically into a bigger risk/rewards model

*Tags: Strategies*

---

**Alessio** - *10:31:43*

[@UBS7QANF3](@UBS7QANF3) out of curiosity, if you rolled out that model for tennis today, would it still have an edge? one thing it's not clear to me is how much the pricing of odds is getting more efficient year over year (say)

*Tags: Strategies*

---

**Alessio** - *10:34:49*

[@U01093Z1KF0](@U01093Z1KF0) in general, though, it's very different. exchange trading is a bit like a simplified financial market, with the huge advantage that most of the data is openly accessible to many players. So you go fish for less informed players and take value out of them. For predicting the outcome it's different it's a pure probability game and it's one where you can't really play unless you have a sizeable bank, because the variance will hit and it may criplle your long term +EV strategy

*Tags: Strategies*

---

**Alessio** - *10:38:11*

and I agree with Mo data is king tbh. Think about those xG models for soccer. If you had the data to build one for leagues that currently aren't covered, you would be rich soon, probably. Independent of where you apply it

*Tags: Strategies*

---

**Mo** - *11:06:41*

[@U01C12ZEADQ](@U01C12ZEADQ) 100% would not make money. The model worked for about 6 years before reaching the stage where it was not profitable 

*Tags: Strategies*

---

**brightcake** - *11:08:04*

[@U01C12ZEADQ](@U01C12ZEADQ) have you done much in the way of price prediction on Betfair? I'm trying to set it up as a classification problem at the moment and just wondering if there has been some success in this area.

*Tags: General Technical*

---

**Alessio** - *11:08:54*

not much, for now I only look at markets with extra data on the side to help understanding how dangerous it is to enter now

*Tags: General Technical*

---

**Alessio** - *11:10:20*

it is definitely a classif problem, i think the problem though it depends on the sport. for soccer, since it's a scarce-goals game, it's almost like a random event, so you need to use a lot of side data imho. other sports where you have more clear signal somebody is winning during the match, it's probably easier (that's my 2c)

*Tags: General Technical*

---

**Alessio** - *11:12:01*

also what you care is value, rather than accuracy, so you probably want to frame it slightly different problem. Odds per se are a prior (slightly altered bythe need of the bookie to make money). It's almost adversarial: you want to classify the situations where their model is underpredicting / overpredicting.

*Tags: Strategies*

---

**brightcake** - *11:20:02*

Yeah, i'm currently trying to set it up for soccer and it seems like external data would definitely be extremely helpful. But for now I wanted to try something with only using recorded data as a proof-of-concept before trying anything more advanced.

I haven't really thought about it from value perspective, definitely something for me to think about

*Tags: General Technical*

---

**Artur Gräfenstein** - *11:26:19*

Interesting disscussion in the thread. [@U016535QCJ2](@U016535QCJ2) said that it would be almost impossible not to overfit. Isint it the way you find a profitable strategie? You are overfiting it to see if under your defined conditions you will be profitable. You just need to make sure that every filter/definition has enough testcases by its own. If thats the case the overfitting is not a problem.

*Tags: General Technical*

---

**Alessio** - *11:33:09*

It's like training a model on bitcoin vs usd in the last month. You don't want to do that :slightly_smiling_face:

*Tags: Strategies*

---

**Alessio** - *11:35:32*

then comes the coding challenge to put protections when the strategy goes haywire because something has changed into the assumptions

*Tags: Strategies*

---

**Alessio** - *11:36:10*

yes, but an algorithm is only as good as the data you feed in. machine learning is garbage in, garbage out, independently from how much data you put

*Tags: General Technical*

---

**Oliver Varney** - *11:48:04*

is a measurement of performance

*Tags: Performance*

---

**Charlie 303** - *21:55:11*

Thanks [@U7E6NE1DM](@U7E6NE1DM) that's helpful to know.

*Tags: General Technical*

---

## 2020-11-21

**Charlie 303** - *00:42:47*

has anyone used betfairlightweight within an asyncio project?

*Tags: General Technical*

---

**Lee** - *13:15:54*

actually it seems like `place_requests` is total requests made since flumine was running. place_requests from previous hour + transaction_count from last hour

*Tags: General Technical*

---

**liam** - *20:52:45*

To reduce you cancel and to increase you replace, example in the example strategy 

*Tags: Strategies*

---

## 2020-11-23

**Unknown** - *08:38:34*

removed an integration from this channel: [https://betfairlightweight.slack.com/services/B01FH6AHWJ0|outgoing-webhook](https://betfairlightweight.slack.com/services/B01FH6AHWJ0|outgoing-webhook)

*Tags: General Technical*

---

**Jonjonjon** - *10:17:30*

Would it be easy to add `market_version` when placing orders with Flumine?

*Tags: General Technical*

---

**Mo** - *10:33:22*

[https://slack.com/intl/en-gb/help/articles/201658943-Export-your-workspace-data#export-data](https://slack.com/intl/en-gb/help/articles/201658943-Export-your-workspace-data#export-data)

*Tags: General Technical*

---

**liam** - *12:31:34*

Dave you using flumine?

*Tags: General Technical*

---

**Dave Simonds** - *12:35:50*

No, my own code that I’ve been running for about 3 years now, don’t worry, flumine is fine, this is my own doing

*Tags: General Technical*

---

**Dave Simonds** - *12:49:10*

Ouch!!!! Do you mind if I ask if that was a legit loss or a charge from BF? 



I only ask because I want to know if I’m likely to get this back (or at least reduced).



My worst unattended afternoon was -£300 but that was down to getting screwed by a non runner and a reduction factor turning me from green to red.... I don’t do that strategy any more 

*Tags: Strategies*

---

**Amanda** - *13:02:10*

A loss - started a new strategy, let It run and walked out the door. Very careful with anything new from then on.

*Tags: Strategies*

---

**liam** - *14:23:40*

Yeah controls are important, flumine would block this, thinking about adding a request counter within bflw itself that could be used to prevent this happening 

*Tags: General Technical*

---

**Dave Simonds** - *14:31:04*

I do have controls in place but this just slipped through the net, it’s actually an old strategy that I brought back this weekend.... it was originally written before the controls were.



We all make mistakes and I’m not embarrassed or beating myself up over it, I just hope I don’t end up forking out a grand because of it.



To be honest tho, I’ve not heard back from either Neil or customer services. I’m not counting on getting this back.



I should really take a look at flumine, not done so yet because I’m not a fan of python. I only joined the page to see how many other people were perusing bot-trading and it’s been quite eye opening.

*Tags: Strategies*

---

**dan2002** - *14:32:51*

Hi Liam, if i place an order, then I want to cancel it before place another one so betfair is not going to count exposure on both of my orders at the same time. How to i do it in flumine? what value of  order status shall i wait for?

*Tags: General Technical*

---

**liam** - *14:34:05*

[https://github.com/liampauling/flumine/blob/fdcf9961330f9807f18029ad7120999d0d62af93/flumine/order/order.py#L26|https://github.com/liampauling/flumine/blob/fdcf9961330f9807f18029ad7120999d0d62af93/flumine/order/order.py#L26](https://github.com/liampauling/flumine/blob/fdcf9961330f9807f18029ad7120999d0d62af93/flumine/order/order.py#L26|https://github.com/liampauling/flumine/blob/fdcf9961330f9807f18029ad7120999d0d62af93/flumine/order/order.py#L26)

*Tags: General Technical*

---

**liam** - *14:34:56*

You have a helper property called complete as well, check out the logic [https://github.com/liampauling/flumine/blob/fdcf9961330f9807f18029ad7120999d0d62af93/flumine/order/order.py#L159|https://github.com/liampauling/flumine/blob/fdcf9961330f9807f18029ad7120999d0d62af93/flumine/order/order.py#L159](https://github.com/liampauling/flumine/blob/fdcf9961330f9807f18029ad7120999d0d62af93/flumine/order/order.py#L159|https://github.com/liampauling/flumine/blob/fdcf9961330f9807f18029ad7120999d0d62af93/flumine/order/order.py#L159)

*Tags: General Technical*

---

**dan2002** - *14:38:01*

ok i will use the helper. Thanks.

*Tags: General Technical*

---

**Misha** - *21:19:36*

I saw a tweet from someone a day ago that suggests that it might be fixed

*Tags: Errors Debugging*

---

**Misha** - *21:19:58*

But the P&amp;L page has had problems for a long time, and I never rely on it

*Tags: General Technical*

---

**D C** - *21:58:57*

It is fixed now (at least for me). Happened last week (well that was when I noticed it. For me it got horse racing market totals all wrong but ONLY on the web page. Downloading excel dump for it and calculating totals gave correct results, but the web page itself refused to calculate the correct totals. And ONLY for horses. Totally bizarre behaviour, but it does work for me now and matches the API results. Would love to know what it didn't work because every market total was correct but it could not sum them correctly.

*Tags: Errors Debugging*

---

## 2020-11-24

**sartux** - *10:49:05*

Hi guys,

I created a very trivial script that takes the last 100 operations and writes them to a MySQL table, in order to create a statistic of the account.



```balance = trading.account.get_account_statement()

b = json.loads(balance.json())

gc = b['accountStatement']

cc = json_normalize(gc)```

until a few hours ago it worked correctly, now I get it as an error



```TypeError: __init__() missing 1 required positional argument: 'marketName'```



I investigated the exception and the only difference is this new record



                "legacyData": {

                    "avgPrice": 0.0,

                    "betSize": 0.0,

                    "betType": "B",

                    "betCategoryType": "E",

                    "eventId": 0,

                    "eventTypeId": 0,

                    "fullMarketName": "Credit",

                    "grossBetAmount": 0.0,

                    "marketType": "NOT_APPLICABLE",

                    "placedDate": "2020-11-24T09:26:32.000Z",

                    "selectionId": 0,

                    "startDate": "0001-01-01T00:00:00.000Z",

                    "transactionType": "ACCOUNT_CREDIT",

                    "transactionId": 0,

                    "winLose": "RESULT_FIX",

                    "avgPriceRaw": 0.0

                },



could it be that the method goes into exception for this reason?



maybe I'm wrong in the analysis of the problem, possibly how can I simply extract the last 100 operations?

*Tags: Errors Debugging, Strategies*

---

**sartux** - *12:59:57*

last question :slightly_smiling_face: what is the method for listing all open orders?

*Tags: General Technical*

---

**jgnz** - *13:09:38*

hi guys, i am jumping back in after a couple of years away. i am trying to create a basic strategy using streaming data, which is dependent on the runner name. I have a couple of questions about that;

• In a market catalogue, is a selection id unique for each runner name across all markets and events? ie a can slowly build up a reliable mapping table of selection ids and runner names.

• is it possible to make a streaming filter request that returns the market catalogue? or does it need to be a separate list_market_catalogue call(i assume yes)?

Given that my strategy needs the runner name and it is slow to make a request just for that, i am planning on building a table of selection ids and runner names in local database. checking if the runner's selection id is present, and only call list_market_catalogue if it is not found. Does that sound like a reasonable action if i need the runner name for my strategy?

*Tags: Performance, Strategies*

---

**jgnz** - *13:17:17*

ok so it will have less of an effect on the strategy than my inefficeint coding choices everywhere else :slightly_smiling_face:

*Tags: Strategies*

---

**Artur Gräfenstein** - *13:18:46*

From the point of view of a database design, this makes no sense because it means redundancies.

*Tags: General Technical*

---

**Artur Gräfenstein** - *13:25:22*

Yes it may be. In any case, this means that Betfair has solved this very poorly. I thought Betfair was the non plus ultra API.

*Tags: General Technical*

---

**liam** - *13:27:05*

Yes, example [https://github.com/liampauling/flumine/blob/master/examples/workers/inplayservice.py|here](https://github.com/liampauling/flumine/blob/master/examples/workers/inplayservice.py|here) 

*Tags: General Technical*

---

**Alessio** - *14:51:06*

Yes, you can use external databases that are a bit more clean

*Tags: General Technical*

---

**Artur Gräfenstein** - *15:22:06*

It doesn’t solve my problem. I already have a clean database. The problem is the linking to Betfair. Because Betfair doesn’t use unique ids my bot may miss some matches because its an unknown id.

*Tags: General Technical*

---

**birchy** - *15:28:55*

Currently testing my first live Flumine strategy. Have noticed that if I stop the bot and restart, bets are being repeated regardless of `max_trade_count` ,`max_live_trade_count` , `max_selection_exposure` , etc. Obviously the new instance handles its own bets but not any other bets that were previously placed. Same applies to bets that are manually cancelled - Flumine does not attempt to bet again until the instance is restarted. The latter is not a big issue, but obviously doubling exposure on restart is. What's the recommended procedure to avoid double betting when an instance is restarted? Is there a way to allow the new instance to take ownership of its previous bets and/or avoid re-betting based on the strategy_ref or similar?

*Tags: Deployment, Strategies*

---

**liam** - *15:50:53*

[@ULDAVFDRP](@ULDAVFDRP) I believe you do some clever stuff within the trade, flumine will pull in the active orders but not the trade count although we probably could? 

*Tags: General Technical*

---

**birchy** - *16:10:26*

Not really practical when dedugging/developing a new strategy. The transition from backtest to live has highlighted a couple of issues that I need to rectify, so I'm in a bit of a run-&gt;stop-&gt;edit-&gt;run loop at the moment. Is that not normal procedure when developing/testing?

*Tags: Deployment, Strategies*

---

**Oliver Varney** - *16:11:28*

in the add_market function in flumine I call list_current_orders. I Rebuild trades using the customer_order_ref for which I have altered to suit my requirements for now (likely to move to either logs or redis), then build the order from the current order object and add it to the blotter. I store trades in a "strategy market dictionary" (I think flumine has its own so probably not necessary for you). This should prevent double order / double trades

*Tags: Strategies*

---

**mandelbot** - *16:29:57*

Whats the best way to grab a price n seconds ago in flumine?

*Tags: General Technical*

---

**liam** - *16:41:55*

Up to the user to store within the strategy or keen to create some analytics middleware to bring in this data but nothing public yet

*Tags: Strategies*

---

**birchy** - *17:43:26*

This is preplay bot. It will obviously run 24/7 once debugged. I was just curious why it didn't pick up where it left off when restarted. I don't fully understand the internals of Flumine as it's a bit of a black box for end users. Just wondering if the cache could be pickled and reloaded?

*Tags: Errors Debugging*

---

**liam** - *19:01:04*

Yeah it’s a streaming limitation, need to inject the complete orders on startup, if someone creates an issue I am happy to pick this up as it shouldn’t be too difficult, the issue is any trade/order state which is then lost but it’s never going to be perfect 

*Tags: General Technical*

---

**liam** - *19:14:07*

Can we make the assumption that each complete order had a single trade, link to the correct strategy class and let the user override if required? 

*Tags: Strategies*

---

**Oliver Varney** - *19:22:10*

customer_order_ref = strategy_hash + internal_trade_id + bet_id

*Tags: Strategies*

---

**liam** - *19:29:55*

Sweet, what’s the design for this? When do you do it in the flumine startup?

*Tags: General Technical*

---

**birchy** - *19:33:46*

Oh nooooo, what have I started now? :man-facepalming::joy:

Surprised the stream doesn't auto reload the previous state, or at least have an option to do so. Bit of an oversight on Betfair's behalf.

Good to see that you've embraced this and have a potential solution already. Regarding my initial problem...what's the recommended way to cancel all unmatched/executable bets on exit/loss of heartbeat using Flumine?

*Tags: General Technical*

---

**Oliver Varney** - *19:33:55*

most likely to coupled as I extend the trade class and abstract alot away from flumine.

*Tags: General Technical*

---

**birchy** - *19:41:51*

Forgive my stupidity, but I've often wondered why we have Trades _and_ Orders? For polling, it's just a case of building a list of bets and firing them off via `placeOrders` but I have no idea how streaming handles this. I probably need to RTFM.

*Tags: General Technical*

---

**liam** - *19:46:31*

It’s a typical design you see in trading systems, strategies have trades and trades have orders, a strategy has multiple trades and a trade can have multiple orders. For example you might want 1k on a runner but split across multiple orders to reduce impact 

*Tags: Strategies*

---

**liam** - *19:47:51*

Flumine still uses placeOrders, streaming is just for market data (listMarketBook) and order data (listCurrentOrders)

*Tags: General Technical*

---

**liam** - *19:55:09*

Flumine will send all place requests through the use of packages if required / if it can but that is just to save on threads/CPU cycle 

*Tags: General Technical*

---

**A** - *22:55:38*

Hello all. Just getting started with Flumine and BFLW, really pleased to have stumbled on this project and community. Looking to see if I can swing trade the horses. First question (been bugging me for a while), what does NG stand for in API-NG?!

*Tags: Getting Started, Errors Debugging*

---

## 2020-11-25

**Oliver Varney** - *05:55:19*

Just out of interest but how are people splitting strategies out in terms of running it. How many individual strategies are they running on an instance of flumine?

*Tags: General Technical*

---

**Misha** - *07:58:26*

As I said, it's not a simple problem to solve. I have probably spent a week on development of my matching system over the past year

*Tags: General Technical*

---

**Mo** - *08:25:51*

I agree with [@U016535QCJ2](@U016535QCJ2), name matching is the way to go. You will need to employ a variety of techniques:



1. Normalisation - e.g. replace all dashes with spaces, make everything lower case, substitute tokens like "utd" with "united"

2. Exact matching

3. Fuzzy matching such as Levenshtein distance

4. Maintaining a database of manually curated mappings when it's simply too difficult to derive the mapping programmatically. For example, a female tennis player gets married and changes her surname. One data source has her maiden name, another has her married name. Football teams change their names as well, for example due to mergers

*Tags: General Technical*

---

**Artur Gräfenstein** - *08:30:24*

I’ve already looked for such a database where several team spellings are listed, but couldn’t find anything. Will make my own.

*Tags: General Technical*

---

**Lee** - *09:12:43*

I was thinking of some sort of measurement of the methods called every update. Guess the latency warning gives a pretty good idea if things are getting too slow.

*Tags: Performance*

---

**liam** - *09:14:08*

In my old framework I had some tracing built within the Event object that I could port across to give you creation time / age / latency / time spent being processed etc. which was then logged that might be handy

*Tags: Performance*

---

**Oliver Varney** - *09:15:59*

Id get value out of that personally, anything to help find my inefficient code

*Tags: General Technical*

---

**liam** - *09:20:34*

Comes at a performance cost though but can look to enable / disable by default 

*Tags: Performance*

---

**liam** - *09:21:39*

[https://docs.sentry.io/product/performance/distributed-tracing/|https://docs.sentry.io/product/performance/distributed-tracing/](https://docs.sentry.io/product/performance/distributed-tracing/|https://docs.sentry.io/product/performance/distributed-tracing/)

*Tags: Performance*

---

**liam** - *09:33:31*

I was involved in the alpha but haven’t used it since, we could do some really cool stuff with tracing within flumine for monitoring latency throughout 

*Tags: Performance*

---

**liam** - *09:33:59*

Sentry is great / helps me sleep 

*Tags: General Technical*

---

**Alessio** - *12:31:27*

also, you should be using the streaming API instead of polling for prices

*Tags: General Technical*

---

**A** - *12:47:46*

Blimey. Ok yeah I think I've connected to a market stream inherently through using Flumine. What approach would you recommend for storing the market catalogue? Is the some form of side worker? Stored in memory or a DB? Quite new to the concepts and how the system is put together (I'm an iOS dev by trade) - should be good once I get the basics

*Tags: Getting Started, Performance*

---

**birchy** - *12:48:46*

Looking for horse racing metadata in a Flumine strategy, but can't seem to locate it. Anyone able to point me in the right direction?

*Tags: Strategies*

---

**A** - *12:58:54*

I'm looking at the same here -&gt; [https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1606258538466600](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1606258538466600)

*Tags: General Technical*

---

**Lee** - *13:01:06*

Flumine makes the market catalogue call for you. You can access it at `market.market_catalogue`

*Tags: General Technical*

---

**Lee** - *13:06:29*

I use this [https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py#L185](https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py#L185)

*Tags: General Technical*

---

**birchy** - *13:39:08*

OK, it's related to:

```market_filter= bflw.filters.streaming_market_filter(

    event_type_ids=['7'],

#    country_codes=['GB'],

),```

Works fine when I use the GB filter.

*Tags: General Technical*

---

**liam** - *13:56:39*

makes the requests in 25 [https://github.com/liampauling/flumine/blob/c466dc6a076719f0fbb5886bfcded1c400f50207/flumine/worker.py#L117|chunks](https://github.com/liampauling/flumine/blob/c466dc6a076719f0fbb5886bfcded1c400f50207/flumine/worker.py#L117|chunks) so will depend on your ping + number of markets but the logs will tell you

*Tags: General Technical*

---

**ricky** - *14:02:53*

[@UUCD6P13J](@UUCD6P13J) What middleware you are reference? For backtesting, i am not sure if i can inject market_catalogue object into a strategy or only inject at the beginning of file?

*Tags: Strategies*

---

**ricky** - *14:10:54*

Did you inject market_catalogue object into your exist strategy object? I have been google a while, but still not sure if possible in python.

*Tags: Strategies*

---

**Artur Gräfenstein** - *14:11:23*

Yes the runner names would be really helpful on first message! It would take the load off their API as well.

*Tags: General Technical*

---

**Lee** - *14:14:38*

flumine has middleware functionality, i just do something like the following

```class MarketCatagloueMiddleware(Middleware):

    def __call__(self, market):

        with open(catalogue_path) as f:

            market.market_catalogue = MarketCatalogue(**json.loads(f.read()))



flumine.add_market_middleware(MarketCatalogueMiddleware())```



*Tags: General Technical*

---

**ricky** - *15:28:19*

MarketCatalogue(**json.loads(f.read())) return error, because MarketCatalogue is undefinition.

*Tags: Errors Debugging*

---

**Lee** - *15:32:07*

you need to import `from betfairlightweight.resources import MarketCatalogue`

*Tags: General Technical*

---

**Lee** - *15:32:57*

I wouldn't just copy and paste that code, add some error handling around reading the file/parsing to json etc

*Tags: Errors Debugging*

---

**A** - *17:54:14*

That's good to know. Thanks. Long shot... but is there anywhere with better documentation than Betfair's website? Or am I just going to have to get used to it/&gt;

*Tags: General Technical*

---

**Lee** - *17:56:09*

The betfairlightweight and flumine docs are pretty helpful too. Plus going through the examples and reading the code i found useful.

[https://liampauling.github.io/betfair/streaming/](https://liampauling.github.io/betfair/streaming/)

[https://liampauling.github.io/flumine/](https://liampauling.github.io/flumine/)

*Tags: General Technical*

---

**Newbie99** - *22:33:59*

if you're on a delayed key then it will look weird, tbh I found it pretty much impossible for streaming with the delayed key!

*Tags: General Technical*

---

**Newbie99** - *23:09:27*

If you're streaming you should be getting updates every x milliseconds with a live key (depending on the market, but definitely for UK racing...well during the day...perhaps not at this time)!

*Tags: Deployment*

---

## 2020-11-26

**Oliver Varney** - *12:38:53*

in flumine / maybe also bflw when a runner is removed, I am right in what I am seeing that market_book.runners removes it from its list but market_book.market_definition.runners does not ?

*Tags: General Technical*

---

**Oliver Varney** - *12:47:00*

accessed via market.market_book in flumine

*Tags: General Technical*

---

**A** - *12:49:49*

What editor is this? Is that a debugger?

*Tags: Errors Debugging*

---

**Oliver Varney** - *12:51:10*

yes the debugger

*Tags: Errors Debugging*

---

**Oliver Varney** - *13:04:50*

[@U4H19D1D2](@U4H19D1D2) would it be worth adding the latency attribute in _process_market_books to the market book, so that in check_market_book if for what every reason there is a temporary spike in latency we can ignore processing this market_book? or is this a bad idea / already available somewhere

*Tags: Performance*

---

## 2020-11-27

**Oliver Varney** - *08:04:40*

morning all, im looking for some advice when it comes to effectively batching up orders (place and cancels) within flumine. Now I have upped the number of trades + strategies I think I may be ineffectively sending updates to betfair (leading to latency from time to time, not confirmed yet but a hunch from looking at the logs ). I believe order packages are designed to combat this, by batching up from market orders from the market blotters (I am right in my understanding?). I am looking to better understand this process and how I could improve my code to prevent unnecessary extra messages to betfair. Does anyone have any advice / be able further explain the process and how it can be optimised ?

*Tags: Performance*

---

**liam** - *08:12:13*

So a market update comes in and processed through all strategies, after this an order package is created which like say can have multiple orders to reduce the number of requests.



The number of orders shouldn’t increase latency, are you monitoring CPU / ram? AWS? And have you tried profiling? I use cprofilev, quickly show you anything that is slow.

*Tags: Performance, Deployment*

---

**A** - *09:20:34*

Morning. Can anyone recommend a decent book for Python programming suitable for experienced programmers in other languages?

*Tags: General Technical*

---

**Alessio** - *09:21:57*

At the time, diveintopython3 was good for me

*Tags: General Technical*

---

**mandelbot** - *13:21:56*

lol, I want to grab the amount matched from my orders within a strategy :clown_face:

*Tags: Strategies*

---

**liam** - *13:24:17*

Just loop all the orders and sum? We don’t have any specific helper functions for this [https://github.com/liampauling/flumine/blob/c466dc6a076719f0fbb5886bfcded1c400f50207/flumine/markets/blotter.py#L26|https://github.com/liampauling/flumine/blob/c466dc6a076719f0fbb5886bfcded1c400f50207/flumine/markets/blotter.py#L26](https://github.com/liampauling/flumine/blob/c466dc6a076719f0fbb5886bfcded1c400f50207/flumine/markets/blotter.py#L26|https://github.com/liampauling/flumine/blob/c466dc6a076719f0fbb5886bfcded1c400f50207/flumine/markets/blotter.py#L26)

*Tags: General Technical*

---

**Christian Tox** - *13:44:31*

Hey everyone, quick question regarding streaming: Is it allowed to run more than one streamer to subscribe to more than 200 markets? Or how do you guys overcome this constraint (if possible)?

*Tags: General Technical*

---

**liam** - *16:20:25*

Should be another before or after which tells you why, you using an old version of flumine?

*Tags: General Technical*

---

## 2020-11-29

**Oliver Varney** - *10:17:03*

Morning all, is there a preferable function to create orders from in flumine, i.e process_market_book or process_orders or another place ?

*Tags: General Technical*

---

**liam** - *10:26:03*

Either is fine, [https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/markets/market.py#L52|market.place_order](https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/markets/market.py#L52|market.place_order) is the function, strategy.place_order is just a proxy for it. Only limitation is that a[https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/baseflumine.py#L153|process_market_orders](https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/baseflumine.py#L153|process_market_orders) is called as this is when the orders get batched up and processed 

*Tags: Strategies*

---

**liam** - *10:35:34*

[https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/streams/streams.py#L111|https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/streams/streams.py#L111](https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/streams/streams.py#L111|https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/streams/streams.py#L111)

*Tags: General Technical*

---

**liam** - *10:35:58*

[https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/streams/orderstream.py#L51|https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/streams/orderstream.py#L51](https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/streams/orderstream.py#L51|https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/flumine/streams/orderstream.py#L51)

*Tags: General Technical*

---

**birchy** - *22:46:05*

In Flumine logs, I see: `BackgroundWorker poll_account_balance starting` but how do I access available balance from within a strategy? Want to add a check in `check_market_book()` along the lines of `if balance &lt; 'N': return`

I don't have an infinite balance like some of you... :joy:

EDIT: Found it! `self.client.account_funds.available_to_bet_balance`

*Tags: Strategies*

---

## 2020-11-30

**Chris** - *10:47:29*

Beginners question, probably more Python related than bflw, as things stand I generally run scripts as they are to do what I want, and any processes that I need as "daemons" are running in loops, or via cron, now I've moved over to the streaming API I'd like to understand how I can have the streams running and then access the data from external other scripts (I currently have them running via a supervisord process which is a loop that connects to the stream and writes all of the market books to MySQL) I then query the MySQL database in my other Python scripts, I'd like to miss out that step if possible and just query the stream directly from my other python scripts, anyone have any pointers?

*Tags: Getting Started*

---

**Newbie99** - *10:54:38*

[@UFPEU7URG](@UFPEU7URG) Others are far more sophisticated than me, but one thing I'd say, is that for streaming I very quickly found a MySQL database impossibly slow to connect to for streaming purposes, so I would consider another option if you need to refer to external data sources (e.g. json, csv or Reddit as example).



In terms of accessing other scripts, with BFLW using multiple threads I think is probably the approach you need, but it may be worth considering Flumine as that has 'worker' threads built in by design which appear to be what you might need.

*Tags: Performance*

---

**liam** - *10:55:23*

Streaming is super lightweight so just create a new stream per process, I don’t agree with the design of having a single stream that all your process use (unless you are running at a large scale.. [@UBS7QANF3](@UBS7QANF3) I think you disagree with me)

*Tags: General Technical*

---

**liam** - *11:01:38*

Yeah I use a streaming timeout / snap (built into flumine) to give a constant update 

*Tags: General Technical*

---

**Lennart** - *11:46:14*

Hi everyone, Sorry to crash the party with what I'm sure is a pretty elementary question - I was surprised to see that the place_order()  function doesn't execute before the in-game delay has lapsed

*Tags: General Technical*

---

**Lennart** - *11:47:52*

I was working on a strategy that would continue looking at prices during that delay period. Before trying to get around with some sort of parallel computation set-up, I thought I asked here whether there is a way around that.

*Tags: Strategies*

---

**D C** - *12:05:35*

On the topic of Python and elementary questions, does Python have a decent and reliable asynchronous HTTPS client package/library/component?

*Tags: General Technical*

---

**Dave** - *13:54:51*

I.e. if I am broadcasting market X internally then I can write a simple gui client that visualises the exact same data that some strategy is currently using as input

*Tags: Strategies*

---

**Oliver Varney** - *14:10:37*

in terms of python web frameworks (and visualisation of raw data) does anyone have any favourites/ recommendations? Currently just using basic Flask to serve json, for debugging, but likely to look to improve this a little more in the future.

*Tags: Errors Debugging*

---

**liam** - *16:46:59*

[https://forum.developer.betfair.com/forum/betfair-premium-newsletter/29456-premium-newsletter?p=33644#post33644|November Premium Newsletter](https://forum.developer.betfair.com/forum/betfair-premium-newsletter/29456-premium-newsletter?p=33644#post33644|November Premium Newsletter)



Looks like they stole our idea..



```Premium Webinar

Planning is underway within the Premium team to bring you a webinar in early 2021 to share exclusive insight into:

• Exchange Liquidity

• Developing Markets

• New and existing API features

• In-play feed suppliers

• Historical data sources

Further communication with date and time to be finalised. Look out for this coming soon.```

*Tags: Data Quality, Feature Engineering*

---

**MacrcoPolo** - *18:01:34*

Re the convserations above, I use tornado to keep things async, single stream pushes data for consumption by multiple strats (I don't use flumine though - rolled my own)

*Tags: General Technical*

---

**Alessio** - *18:05:30*

interesting, how do you put something like flumine into an asyncio framework? you have an asyncfunc that polls from it ?

*Tags: General Technical*

---

**MacrcoPolo** - *18:07:13*

Async in python is pretty tricky I find though, not a lot of great documentation out there...

*Tags: General Technical*

---

**MacrcoPolo** - *18:08:46*

I think in general, you have to read up on general async concepts from computer science (futures, promises and such) to understand how/why it's done like that in python

*Tags: General Technical*

---

## 2020-12-01

**river_shah** - *08:07:39*

I really like the tone out of betfair lately. Outside of the occasional ban without warning, most of the steps they are taking are supportive of pro / tech savvy gamblers/ exchange eco system. They seem happy monetising via premium charge and let the best modelling / tech win.

*Tags: Strategies*

---

**Misha** - *10:05:22*

Fiddler is the best I have seen, for which there is an updated application, Fiddler Everywhere - [https://www.telerik.com/blogs/fiddler-everywhere-web-debugging-official-release](https://www.telerik.com/blogs/fiddler-everywhere-web-debugging-official-release)

*Tags: Errors Debugging*

---

**D C** - *10:21:12*

well yes but it is good to have the knowledge of how to access it if/when it does become available. Betfair football scores are so far behind reality I cannot see how anyone finds them useful

*Tags: General Technical*

---

**Misha** - *10:23:38*

Tennis is a 3 (or 5) second delay, and being a second faster isn't really going to help. But being 10 seconds behind can be a problem

*Tags: General Technical*

---

**D C** - *10:25:45*

As I see it, the market data tells the story overall but there are some things that are good to know - early red card in a football match for example. I really asked the question as there are a lot of these endpoints that appear to keep popping up and knowing how to analyse web traffic is only going to be more useful as time goes by.

*Tags: General Technical*

---

**D C** - *10:26:54*

I guess what is actually useful depends on every person and their strategy. Its still good to know how to get at it even if it does end up being not so useful. Teach a man to fish... and all that.

*Tags: Strategies*

---

**Artur Gräfenstein** - *10:44:48*

For example: “[https://stats.fn.sportradar.com/en/Europe:Berlin/gismo/config_tree_mini/41/0/1/](https://stats.fn.sportradar.com/en/Europe:Berlin/gismo/config_tree_mini/41/0/1/)” this will lead in an error. But placing a big customers name “ibcbet” from asia: “[https://stats.fn.sportradar.com/ibcbet/en/Europe:Berlin/gismo/config_tree_mini/41/0/1/](https://stats.fn.sportradar.com/ibcbet/en/Europe:Berlin/gismo/config_tree_mini/41/0/1/)” you will get the config three for soccer.

*Tags: Errors Debugging*

---

**Twatter** - *11:12:32*

[@U01EYDA8345](@U01EYDA8345) Thanks for sharing! I've been clicking through some pages and it looks like a great tool. Just looking at some of the fixture pages for football and they have odds posted (e.g. [https://s5.sir.sportradar.com/mi002/en/1/season/77179/fixtures](https://s5.sir.sportradar.com/mi002/en/1/season/77179/fixtures)) Don't suppose you know where these odds are from?

*Tags: Errors Debugging*

---

**Lennart** - *20:44:31*

Hey everyone, got another noob question -  Looks like I'm getting prices/odds through trading.betting.list_market_book() that are off from what I can see directly on Betfair's website. Generally it seems that these prices are better than those on the website. For instance, I might get 5.6 to back when I'm seeting 5.3 on the website. This is for in-game football matches, if that makes a difference.

Is this at all possible or should I be digging into/debugging my code?

*Tags: Errors Debugging, Strategies*

---

**Lennart** - *21:59:13*

I appreciate all the help btw, I'm still very new to this (despite having joined the slack a long time ago)

*Tags: Getting Started*

---

**Lennart** - *22:02:34*

That's good to know. No big debugging session for now - thanks a lot!

*Tags: Errors Debugging*

---

## 2020-12-02

**Oliver Varney** - *10:53:20*

In flumine, do fully cancelled orders that have no match at all remain in the market blotter ?

*Tags: General Technical*

---

**Thomas JAMET** - *11:25:32*

Hello everyone, I am new to sport betting, looking into tennis events. The first step for me is to gather historical information on tennis matches to feed into a model. My objective is to gather 20 years of data. I am scraping a few web-sites but I am getting blocked by Captchas. Did anyone resolve this scenario?

I am currently considering developing a Chrome extension to help facilitate the scraping while giving human access to the Captcha page. Any suggestion welcome. Thanks!

*Tags: Getting Started, Strategies*

---

**Thomas JAMET** - *11:50:16*

I am surprised you don't :grin:

I think that players tend to graduate from ITF to ATP Challenger (and on to other tournaments) and I hope that tracking their performance early on can give some insight in events offered on BetFair. Does it make sense?

*Tags: Performance*

---

**Thomas JAMET** - *12:11:25*

Is it a purely a trading strategy or do you take a side in the outcome?

*Tags: Strategies*

---

**jhaa** - *12:18:26*

Is there some sort of in-play streaming? I can only find [http://trading.in|trading.in](http://trading.in|trading.in)__play__service

*Tags: Strategies*

---

**Mo** - *12:18:49*

No streaming, not even for the official scores endpoint (due to licensing reasons)

*Tags: General Technical*

---

**Mo** - *12:22:48*

• GPS data is provided by TPD ([http://www.totalperformancedata.com/](http://www.totalperformancedata.com/))

• Betfair have a number of licenses they give out for free (speak to your account manager)

*Tags: Performance*

---

**Mo** - *12:47:29*

I guess you get match fixing at all levels

*Tags: Errors Debugging*

---

**Misha** - *12:48:40*

But that's less of a problem for a fully automated system than minor injuries during the match

*Tags: General Technical*

---

**Chris** - *16:26:13*

I am very low turnover and they had no problem increasing me to 1k

*Tags: General Technical*

---

**Chris** - *16:39:37*

Maybe a #beginners channel with a pinned post would be good, could be used for the more simple questions too

*Tags: Getting Started*

---

**Artur Gräfenstein** - *19:25:10*

I would suggest someone (who has the rights [@U4H19D1D2](@U4H19D1D2)) create a #beginners channel. And every time a question is asked that fits the rubric, I or someone who speaks native English can copy and pin it. So the initial effort is very low and it is filled up over time. Beginners can also ask their questions there and anyone who is annoyed by the beginner questions simply does not join the channel.

*Tags: Getting Started*

---

**liam** - *19:46:54*

Happy to add another channel but it’s a bit of a hassle as I have to invite everyone. Don’t think anyone minds answering the beginners questions assuming it’s not shown in the examples or docs 

*Tags: Getting Started*

---

**Mo** - *19:48:00*

My suggestion is start the documentation repo and put an FAQ page in it 

*Tags: General Technical*

---

**Mo** - *19:51:49*

I’m so regular I don’t need the documentation 

*Tags: General Technical*

---

## 2020-12-03

**jgnz** - *12:48:43*

hi guys, i am using flumine and i have got a basic strategy working well enough i can run it using the API client or back testing client. most of the behavior i have extended is from process_market_book. but my strategy actually requires that positions are closed after a certain amount of time(not yet implemented).



just wondering if someone could point me in the right direction for how to call this timed behavior?  an easy way is just to call a method at the very start of process_market_book, but it means that bets might be open for longer than i actually want. do you think i should look at BackgroundWorker and adding custom events to the handler_queue?

*Tags: Strategies*

---

**liam** - *12:50:59*

If I understand correctly it would be similar to this 2s [https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/examples/strategies/lowestlayer.py#L55|fillkill](https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/examples/strategies/lowestlayer.py#L55|fillkill) as process_orders will be called every 0.25s whilst you have live orders, however you can do something similar in process_market_book by looping the orders `market.blotter`

*Tags: Deployment*

---

**Chris** - *12:51:46*

With the streaming API when collecting the marketbook info with the API-NG you have the option to "virtualise" which shows the back/lay prices of the opposite side of the books as the price like the website does, if I want to do this when using using streaming do I need to calculate these prices myself by using both sides of the market, or is there some other way I am not aware of?

*Tags: General Technical*

---

**Chris** - *12:54:38*

[@UBS7QANF3](@UBS7QANF3) thanks, I was using the API explorer and seeing it was passing "virtualise": "true", in a bad habit of comparing the API-NG and the Streaming API when I should just be reading the docs, my bad

*Tags: General Technical*

---

## 2020-12-04

**Ben** - *05:10:25*

Hopefully a simple question, but is there a way to access trade history for a runner in runner books? I can get a listing of all available to back and available to lay offerings, but is there somewhere in the cache for each runner where I can find trade prices and volumes?



I know the stream sends that information, but can't seem to find out how to access if via the bflw classes

*Tags: General Technical*

---

**Jorge** - *09:50:13*

Hi guys, is there any problem with running bots from 2 different Servers at the same time? I am running them locally for testing and in a production server, wondering if Betfair would like that or not...

*Tags: Deployment*

---

**Charlie 303** - *21:12:37*

(not sure if this is an appropriate question) can you give me an example of what kind of opportunities [@U4H19D1D2](@U4H19D1D2)? does this imply that 'riskless' arbs do still occur when bf's xm algo doesn't properly balance the book? For some reason I cant shake the idea that *most of the opportunities one might see in the non-virtual book are going to be squashed in reality by XM...but if you're saying some creep through due to XM not being accurate, how would you know that will be the case?

*Tags: General Technical*

---

**Charlie 303** - *21:52:53*

So I have a coarse streaming filter which amounts to about 800 markets and I need runner metadata for each of these markets.  My first thought was to start with list_market_catalogue (using same coarse filter and RUNNER_METADATA projection) to build the index, then start the stream and apply updates matching the event ids.  However, I get the APING 'too much data' error on list_market_catalogue.  Applying a max_results=200 to the list_market_catalogue call seems to work, but it doesnt appear there's any pagination feature so I'm wondering how to get the remaining ~600 markets?

*Tags: Errors Debugging, Feature Engineering*

---

**Lee** - *21:56:06*

Here’s an example of how flumine does it [https://github.com/liampauling/flumine/blob/master/flumine/worker.py#L120|https://github.com/liampauling/flumine/blob/master/flumine/worker.py#L120](https://github.com/liampauling/flumine/blob/master/flumine/worker.py#L120|https://github.com/liampauling/flumine/blob/master/flumine/worker.py#L120)



*Tags: General Technical*

---

**Charlie 303** - *22:18:31*

Does flumine do this by default (e.g. if I simply follow the readme example strategy and change the marketFilter, will all the runner meta data be available when process_market_book is called)?

*Tags: Strategies*

---

**Charlie 303** - *23:54:44*

thanks! been focused on the mechanics of bflw i forgot to rtfm on flumine :wink:

*Tags: General Technical*

---

## 2020-12-05

**Charlie 303** - *01:31:35*

Follow-up - I need to run some code every time a new market is added. the docs mention an _add_live_market() function is "called when new Market received through streams".  Is that the right spot? are there any examples of implenenting that? (still brushing off the dust on my python skills, not sure how to override the Flumine class)

*Tags: Deployment*

---

**Charlie 303** - *02:33:42*

Figured that this is where Middleware comes into play, but could only find one example at [https://github.com/liampauling/flumine/blob/master/flumine/markets/middleware.py](https://github.com/liampauling/flumine/blob/master/flumine/markets/middleware.py)....

*Tags: General Technical*

---

**Charlie 303** - *02:48:33*

In case it helps others, the following is a very basic way to add middleware:

```class Middleware:

    def __init__(self, market):

        pass

   

    def __call__(market) -&gt; None:

        pass

    

    def add_market(market) -&gt; None:

        print('new market added')

        pass



    def remove_market(market) -&gt; None:

        pass



#...etc...

framework = Flumine(client=client)

framework.add_market_middleware(Middleware)```

*Tags: General Technical*

---

**liam** - *06:58:43*

I will add that to the docs, best practice:



```from flumine.markets.middleware import Middleware



class CustomMiddleware(Middleware):

    def __call__(self, market) -&gt; None:

        pass  # called on each MarketBook update



    def add_market(self, market) -&gt; None:

        print("market {0} added".format(market.market_id))



    def remove_market(self, market) -&gt; None:

        print("market {0} removed".format(market.market_id))```

*Tags: General Technical*

---

**Oliver Varney** - *17:29:07*

might be a silly question (havent really looked into the class that much), but generally what is the purpose of the middleware

*Tags: General Technical*

---

**Oliver Varney** - *17:31:45*

flumine is very flexible, it seems like quite a few places could easily do most stuff

*Tags: General Technical*

---

**liam** - *17:35:45*

Yeah because it shouldn’t be strategy specific 

*Tags: Strategies*

---

## 2020-12-06

**Will Morrison** - *07:45:04*

It seems like my market_book objects never contain data for runner.last_price_traded, or any other price-related stuff like total_available, total_matched, etc. Is this a problem that anyone else has seen before?

*Tags: General Technical*

---

**Will Morrison** - *08:15:58*

market_filter = streaming_market_filter(

    market_ids=market_id_list

)

market_data_filter = streaming_market_data_filter(

    fields=["EX_MARKET_DEF", "EX_BEST_OFFERS"], ladder_levels=5

)

*Tags: General Technical*

---

**Will Morrison** - *08:17:29*

Thanks Mo! I was just searching that page as you sent it. That should fix me right up!

*Tags: Errors Debugging*

---

## 2020-12-07

**Cagdas Yetkin** - *11:23:01*

We have put together some leagues and we will be testing a few things on the go :slightly_smiling_face: Plus, we started writing a blog regarding our 30-day-betting challenge. We are already on day-9. It is going to be fun. Our friendly project is here: [http://betalertpro.com/](http://betalertpro.com/) no login required... There is a feature using the betfairlightweight but it is not active yet.



it is only in-play... So the matches will be visible today evening only.

*Tags: Feature Engineering, Strategies*

---

**D C** - *13:39:55*

I have had a quick look but can't find the answer. Might seem a stupid question but for the purposes of bet limits per hour numbers, does a fill or kill type order count as a single order if it is NOT matched or is there a hidden or implied cancellation order actioned?

*Tags: General Technical*

---

**Mo** - *13:40:34*

It’s a good question and I can’t confidently answer but I’m pretty sure it counts as a single transaction

*Tags: General Technical*

---

**D C** - *15:46:51*

You can supply a minimum fill amount to a fill or kill.Other than  that you can't place a bet but demand that only part of it gets matched.... Unless I misunderstand the question

*Tags: General Technical*

---

## 2020-12-08

**Robert** - *16:06:23*

Although the market catalogues don’t have versions in their response according to betfair documentation. Seems a bit broken since you can can’t connect them easily then to the market books which does have versions. 

*Tags: General Technical*

---

**river_shah** - *16:59:49*

writing a unittest and trying to mock a book. this gives type errors by the `get_price` function. could you please guide me on how to mock this correctly:

*Tags: Errors Debugging*

---

**river_shah** - *17:00:07*

```from flumine.utils import get_price

from betfairlightweight.resources.bettingresources import PriceSize

back_price = PriceSize(**{"price": 2.00, "size": 1024})

mock_book.ex.available_to_back = [back_price]

mock_back_price = get_price(mock_book.ex.available_to_back, 0)```



*Tags: Strategies*

---

**liam** - *17:07:11*

Because we [https://github.com/liampauling/flumine/commit/64ae76f0cb5571a040ce9904febf35df733931c4|patch](https://github.com/liampauling/flumine/commit/64ae76f0cb5571a040ce9904febf35df733931c4|patch) it in flumine 

*Tags: General Technical*

---

**liam** - *18:26:04*

To other wrapper maintainers / those not using bflw did you know that the rc streaming update may not include all runners and the marketDefinition.runners needs to be used as well to build a correct marketBook? 

*Tags: General Technical*

---

**Oliver Varney** - *18:52:22*

strange,  some of the decisions on the streaming side initialisation seem a little backwards to me. They seem to prefer making the user do two separate requests (i.e. not sending matched positions) to get to a current state

*Tags: General Technical*

---

**thambie1** - *19:37:52*

I like using it to just see what bets I have open, but doesn't seem to be working some of the time after I launched a more active strategy.

*Tags: Strategies*

---

## 2020-12-09

**Aaron Smith** - *10:14:41*

Hey guys! Maybe its obvious in the code, but i couldnt figure it out right now :smile: :

Suppose i place an order in process_market_book (in a BaseStrategy instance), but then i continue processing stuff to maybe place a 2nd order. Is the first order delayed by the calculations coming afterwards in process_market_book or is it sent immediatly to betfair and then process_market_book continues?

*Tags: Strategies*

---

**Oliver Varney** - *10:28:29*

[https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/baseflumine.py#L113](https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/baseflumine.py#L113)

*Tags: General Technical*

---

**mandelbot** - *13:48:44*

Have to say, the more I delve into bflw and flumine the more I enjoy the process. I am learning python through using it and can't think of a better way to learn. Thanks to Liam and the rest of the developers and sorry for all the past and future newbie questions :smile:

*Tags: General Technical*

---

**Robert** - *14:20:30*

I know how to get whether a horse is a reserve runner or not using the market catalogue.  I was wondering if there is a way to check if there is a reserve runner still (and not declared runner or non runner yet) but with no care about which horse that is.  And ideally not requiring continuous updating of the market catalogue.  Perhaps using numberOfRunners and numberOfActiveRunners?

*Tags: General Technical*

---

**Robert** - *14:27:30*

"If false, runners may be added to the market" yeah had seen this but wasnt sure about the cases this might cover

thanks lots for the help -- I will look into this -- solves our problem nicely if it works

*Tags: General Technical*

---

## 2020-12-10

**Jonjonjon** - *11:44:46*

As a baseline, how long is the delay? Can it cause problems?

*Tags: General Technical*

---

**liam** - *11:45:56*

problems as in betfair updating slower than betdaq/others? yes, check out the graphs after a removal

*Tags: Performance*

---

**Oliver Varney** - *12:13:59*

ive probably asked this before, but think ill recheck due to the market_book.runners issue ( requiring market catalogue also to rebuild runners), but is it a safe assumption to assume that runners remain in the same position in the list, even after stream reconnects (which flumine handles). Off memory the cache in lightweight adds them and the order remains, but just want to double check

*Tags: Performance*

---

**river_shah** - *16:53:45*

about to buy some historic horse racing data from betfair. are some months better to buy than others to back test models on please? i.e more liquid and important markets or some other criterion leading to better model generalizations.

*Tags: Strategies*

---

**Michael** - *16:57:45*

Be aware that Summer flat racing data won't help you much with NH racing in the winter.

*Tags: General Technical*

---

**river_shah** - *17:04:33*

Why would horse racing change drastically please? For example I can understand that for football having crowds makes a huge difference (due to home/ away impact), how does crowd size interact with horse performance?

*Tags: Performance*

---

## 2020-12-11

**Charlie 303** - *08:14:04*

does flumine handle stream connection retries automatically? e.g does it already incorporate the goodies in the bflw prod example: [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

*Tags: General Technical*

---

**liam** - *08:15:01*

[https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/streams/marketstream.py#L13|yep](https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/streams/marketstream.py#L13|yep)

*Tags: General Technical*

---

## 2020-12-12

**Alessio** - *12:09:29*

[@U4H19D1D2](@U4H19D1D2) hey mate, when you ran Lambdas over the market files, how do you usually ship BFLW? you use a containerized image? or a zippie?

*Tags: General Technical*

---

**Alessio** - *12:30:07*

so something like a conda environment, install bflw (and dependencies) there, zip and declare as a layer to be used in the lambda?

*Tags: Getting Started*

---

**Alessio** - *12:40:33*

something along these lines: [https://www.serverless.com/blog/serverless-python-packaging](https://www.serverless.com/blog/serverless-python-packaging)  but no docker because you don't use compiled stuff like numpy?

*Tags: Deployment*

---

**Alessio** - *12:47:57*

ah no, this seems even simpler: [https://docs.aws.amazon.com/pinpoint/latest/developerguide/tutorials-importing-data-create-python-package.html](https://docs.aws.amazon.com/pinpoint/latest/developerguide/tutorials-importing-data-create-python-package.html) without having to use PIP and docker

*Tags: Deployment*

---

**AP** - *17:22:52*

[https://www.serverless.com/plugins/serverless-python-requirements|https://www.serverless.com/plugins/serverless-python-requirements](https://www.serverless.com/plugins/serverless-python-requirements|https://www.serverless.com/plugins/serverless-python-requirements)

*Tags: Deployment*

---

**Alessio** - *18:10:58*

in general if you don't need c++-based stuff like numpy you can just pack them in a zip and use them as a layer, for a lot of other things like numpy there are pre-built layers you can just add.. in the end I was happy to skip docker :stuck_out_tongue:

*Tags: General Technical*

---

**Lennart** - *19:40:48*

Hello everyone - another basic question: what markets provide decent volatility whilst working without delay (which I guess is all pre-play?) ? So far I'm seeing some volatility in the last few minutes leading up to horse races

*Tags: General Technical*

---

**Lennart** - *19:42:50*

I realise that this might be a super open-ended question but do you know what drives pr-event volatility in horse racing?

*Tags: General Technical*

---

## 2020-12-13

**Misha** - *02:26:15*

Beyond that, two questions immediately come to mind. 1) Is this an app that people would want? 2) What do you get out of it?

*Tags: General Technical*

---

**Misha** - *02:52:30*

If you plan to make money from it I suspect you will have a problem with the IP of providing prices from other sites. The "dodgy" ones may not care, but the legit ones, like Sportsbet, certainly won't be amused

*Tags: General Technical*

---

**bb** - *08:29:54*

The aggregation sites are unlikely to show odds from a bookmaker they dont have a commercial agreement in place with as it would be sending customers out of the  acquisition funnel that underpins their business model

*Tags: Strategies*

---

**Matthieu Labour** - *14:34:09*

Hi all. Question on Backtesting. When backtesting using flumine and betfair, how does one get event updates during in-play such as goals scored, points won, game stats such as first service being lost in tennis. It would be great to learn what folks have been exploring to join betfair historical data with in-play event updates. Thank you!

*Tags: Data Quality*

---

**Matthieu Labour** - *14:46:48*

Do you run the in-play service endpoint continuously such that you build a historical database overtime?

*Tags: Data Quality*

---

**thambie1** - *14:47:30*

[@UUE6E1LA1](@UUE6E1LA1) Hmm, good to know. I'm currently have very tight liability controls such that I can't lose much on any given soccer event. But I was not aware this was a failure scenario. Question though, what is VAR?

*Tags: General Technical*

---

**thambie1** - *14:48:50*

Huh?!?! I was under the impression that betfair doesn't provide any services for in-play soccer scores, etc. Can you give me a link to the documentation?

*Tags: General Technical*

---

**Matthieu Labour** - *14:48:52*

Can flumine be the middleware and extended such that one can add an additional stream of messages?

*Tags: General Technical*

---

**liam** - *14:51:07*

You can do what you want, middleware is called on each marketBook update [https://liampauling.github.io/flumine/markets/#middleware|https://liampauling.github.io/flumine/markets/#middleware](https://liampauling.github.io/flumine/markets/#middleware|https://liampauling.github.io/flumine/markets/#middleware)

*Tags: General Technical*

---

**Unknown** - *14:52:42*

There is also an official scores API which currently just has scores for tennis: [https://betfairlightweight.slack.com/files/UBS7QANF3/FU7E3UTDF/scores_api_tennis.pdf](https://betfairlightweight.slack.com/files/UBS7QANF3/FU7E3UTDF/scores_api_tennis.pdf)

*Tags: General Technical*

---

**liam** - *14:52:47*

For example Lee made an example which updates the Market object with the catalogue when backtesting [https://github.com/liampauling/flumine/blob/master/examples/middlewares/marketcatalogue.py|https://github.com/liampauling/flumine/blob/master/examples/middlewares/marketcatalogue.py](https://github.com/liampauling/flumine/blob/master/examples/middlewares/marketcatalogue.py|https://github.com/liampauling/flumine/blob/master/examples/middlewares/marketcatalogue.py) 



Although [@UUCD6P13J](@UUCD6P13J) you should optimise this to only be called on the add_market function as it’s currently being called on every update 

*Tags: General Technical*

---

**thambie1** - *14:58:34*

Ahh, that must be why I missed it. I was aware of some inplay stuff for other sports but was pretty confident there was nothing official for soccer. But cool to see that bflw got the unofficial stuff working!  I'm not using bflw which is why I never came across it, but gonna have to read up on that section of the code :slightly_smiling_face:

*Tags: General Technical*

---

## 2020-12-15

**nthypes** - *04:29:24*

Folks, any way to get historical data with a discount or cheaper price?

*Tags: Data Quality*

---

**Jonjonjon** - *12:17:51*

I'm unsure at the moment. Just want to see if it is feasible to base a strategy on trading the final price before a race starts. But I don't want to take bsp.

*Tags: Strategies*

---

**river_shah** - *12:25:05*

Ok, the bz2 file gets read by the flumine engine /bflw stream as is?

*Tags: General Technical*

---

**Mo** - *12:28:47*

```    ...

    stream = trading.streaming.create_historical_generator_stream(

        file_path=path,

        listener=listener

    )



    with patch("builtins.open", smart_open.open):

        g = stream.get_generator()

        for market_books in g():

            ...```

*Tags: Strategies*

---

**river_shah** - *12:42:35*

[@U4H19D1D2](@U4H19D1D2) how may I use the bz2 file directly with a flumine backtest please? for example this will fail:

*Tags: General Technical*

---

**river_shah** - *12:42:45*

```from flumine import BaseStrategy, FlumineBacktest, clients





class ExampleStrategy(BaseStrategy):

    def start(self):

        # subscribe to streams

        print("starting strategy 'ExampleStrategy'")





    def check_market_book(self, market, market_book):

        # process_market_book only executed if this returns True

        if market_book.status != "CLOSED":

            return True





    def process_market_book(self, market, market_book):

        # process marketBook object

        print(market_book.status)





client = clients.BacktestClient()

framework = FlumineBacktest(client=client)



strategy = ExampleStrategy(

    market_filter={"markets": ["./1.166912450.bz2"]}

)

framework.add_strategy(strategy)



framework.run()```



*Tags: Strategies*

---

**river_shah** - *12:52:40*

I am sorry but I am not entirely clear on how and where I do so in flumine

*Tags: General Technical*

---

**river_shah** - *12:55:25*

wow…thats it. and here I was looking deep through the innards of flumine

*Tags: General Technical*

---

**river_shah** - *13:02:02*

```from unittest.mock import patch



import smart_open

from flumine import BaseStrategy, FlumineBacktest, clients





class ExampleStrategy(BaseStrategy):

    def start(self):

        # subscribe to streams

        print("starting strategy 'ExampleStrategy'")





    def check_market_book(self, market, market_book):

        # process_market_book only executed if this returns True

        if market_book.status != "CLOSED":

            return True





    def process_market_book(self, market, market_book):

        # process marketBook object

        print(market_book.status)





client = clients.BacktestClient()

framework = FlumineBacktest(client=client)



strategy = ExampleStrategy(

    market_filter={"markets": ["./1.166912450.bz2"]}

)



framework.add_strategy(strategy)



with patch("builtins.open", smart_open.open):

    framework.run()```



*Tags: Strategies*

---

**Alessio** - *19:07:54*

Youu can wrap the HistoricalDataStream with a 'GzipHistoricalDataStream' if you don't want to patch it.  But that's because i'm not a native pythonist and i didn't know you could patch builtins :stuck_out_tongue:

*Tags: General Technical*

---

**river_shah** - *19:17:28*

We can benchmark using the dummy strategy by the way. The patching method that [@UBS7QANF3](@UBS7QANF3) suggested is very easy. Using bz2, in some simple backtests I see runtime increase 3-6% but space saving is 8-10X. Well worth the speed vs space saving for my use case.

*Tags: Performance, Strategies*

---

**river_shah** - *19:34:02*

very cool. do you know if the file stays in host memory once opened with smart_open? i.e if I want to read the same file multiple times and want to avoid excess latency of s3

*Tags: Performance*

---

**Mo** - *19:34:35*

Great question but I don’t know 

*Tags: General Technical*

---

**Alessio** - *19:41:19*

is your problem mainly prefetching during backtest? because there you just read stuff once

*Tags: General Technical*

---

## 2020-12-16

**liam** - *07:07:55*

Python?

*Tags: General Technical*

---

**Oliver Varney** - *18:00:51*

id recommend doing a little bit of reading of the general chat, then coming up with some specific questions, rather then a broad question if your looking for advice. Maybe try and explain your situation (where are you in your journey) + goals and what you are trying to learn.

*Tags: General Technical*

---

## 2020-12-17

**Oliver Varney** - *14:55:20*

sounds like your at the start of the journey. Id say no, you dont need to know anything about horses at the start, I personally didnt. Having said this it might be easier to start with a sport you understand / have a passion for. In terms of betting/ trading strategies, this is something you will have to discover for yourself, as people wont be giving away their strategies for free. If you have specific questions about how markets operate, what is value/value betting, Betfair API, code examples for betfairlightweight and flumine then there are many great people in this chat.

*Tags: Strategies*

---

**Bradley** - *16:33:05*

Now, I haven't programmed with Python before but have knowledge of programmin languages and electronics so I could understand and perhaps implement something some day. I just need basic stuff. 



If you can provide, I apreciate it.

*Tags: General Technical*

---

**Oliver Varney** - *16:42:23*

Start with the examples on the github for flumine and betfairlightweight. Maybe watch some basic python programming youtube tutorials, then read through the flumine / betfairlightweight code. Some advice, you will have to be more proactive if your looking to get anywhere with this. It kind of seems you dont have a game plan and are asking for someone just to give you the  answer for how to make money.

*Tags: General Technical*

---

**Jonjonjon** - *20:12:43*

I just started with a spray and pray strategy. Plotted lots of charts, and observed where my good and bad fills came from. I think [@UGV299K6H](@UGV299K6H) had some great tips for novices a week or two ago.

*Tags: Strategies*

---

## 2020-12-19

**Will Morrison** - *00:40:08*

I'm thinking about moving my code to a server, but I'm located in Thailand. Do you guys know if Betfair will let me run stuff from a server in the UK, or do I have to stay in my country of residence? I asked their help, but they seem to be totally swamped right now.

*Tags: Deployment*

---

**Misha** - *08:01:48*

Still is. They go from a standard geolocation database that might be weeks out of date

*Tags: General Technical*

---

**Jonjonjon** - *09:13:18*

I had a similar problem running from a godaddy server. And the idiot from godaddy told me I should trust him, and that it wasn't a problem...

*Tags: Deployment*

---

**Artur Gräfenstein** - *09:46:37*

This is my problem because I often stay in Germany and always have to use a proxy. It should not be tied to the current location, but to the home of the account owner. Betfair checks that in a pretty detailed way. I had to send in quite a few documents.

*Tags: General Technical*

---

**Alessio** - *10:33:12*

Never trust OVH .. 0 support, 0 help. Also, the 'geolocated IPs' they promise you.. well.. maxmind really doesn't care..the server is still in france, hence they keep it located in france :)

*Tags: Deployment*

---

**Jorge** - *10:50:50*

Hi guys, I'm trying to get the matched orders using an order stream. In github docs: "'price point' matched backs and matched lays are stored in the order cache in matched_lays / matched_backs." How can I access those? Do I need to look into the CurrentOrders.streaming_update?

*Tags: General Technical*

---

**Jorge** - *15:42:14*

Aha, so right now I need to cache the CurrentOrders.streaming_update['orc']['mb'] first time I call `output_queue.get()`. And then add the upcoming streaming_updates to it, so I get the full matched backs in a market for a given selection. Would this include all orders with status == 'EXECUTION_COMPLETE'?

*Tags: Getting Started*

---

**Jorge** - *15:44:35*

I'm happy to help with the refactor. It'd be very useful if the CurrentOrders class had a matched_backs_dict and matched_lays_dict as attributes!

*Tags: General Technical*

---

## 2020-12-23

**birchy** - *13:55:07*

Quick question regarding Flumine...to get the matched bets liabilities for each runner, do we need to calculate in process_orders() or are they already calculated but I've not found the parameter yet?

*Tags: General Technical*

---

**liam** - *14:06:12*

I think there are a few helper functions in the blotter 

*Tags: General Technical*

---

**Newbie99** - *14:06:42*

[https://github.com/liampauling/flumine/blob/master/flumine/markets/blotter.py](https://github.com/liampauling/flumine/blob/master/flumine/markets/blotter.py)



Selection exposure might be what you're after?

*Tags: General Technical*

---

**Crofty** - *15:30:41*

New guy here.



Manually traded for 4 years and realise would be sensible to explore automation properly.



My first task is to learn python.

*Tags: General Technical*

---

**birchy** - *16:08:36*

That's with at least one fully matched order according to `process_orders()`  in my strategy.

*Tags: Strategies*

---

## 2020-12-24

**Sam Asin** - *05:49:18*

Hey guys, I'm new to the betfair API and having some fun messing with it.

*Tags: Getting Started*

---

**Jonjonjon** - *20:24:42*

Happy Christmas everyone. BFLW has put in a solid performance for me this year. Thanks for sharing [@U4H19D1D2](@U4H19D1D2).

*Tags: Performance*

---

## 2020-12-25

**bogdan** - *17:30:35*

thoroughbreds_event_filter = betfairlightweight.filters.market_filter(

    event_type_ids=[7],

    market_countries=['NZ'],

    market_start_time={

        'to': (datetime.datetime.utcnow() + datetime.timedelta(days=1)).strftime("%Y-%m-%dT%TZ")

    }

)

*Tags: General Technical*

---

**bogdan** - *19:02:30*

TypeError: market_filter() got an unexpected keyword argument 'country_codes'

*Tags: Errors Debugging*

---

**Ruben** - *19:09:24*

`country_codes`  is for streaming_market_filter()

*Tags: General Technical*

---

**bogdan** - *19:33:18*

I wonder if it is an error from betfair..

*Tags: Errors Debugging*

---

## 2020-12-26

**Mo** - *11:09:01*

I have found having a database table containing meta data for S3 objects to be useful so you can use that for searching rather than making S3 API requests

*Tags: General Technical*

---

**Alessio** - *11:12:06*

Also, if you keep the structure they use for historical data you can just sync subdirectories of it, instead of the whole bucket

*Tags: Data Quality*

---

**Dave** - *15:10:09*

I'll check rsync, might be better than the standard sync tool from aws CLI thanks! Although the overhead from mounting it might not be worth it for the way I use S3. Also I must be doing something funky with cloudwatch, my cloudwatch costs are 97% of my total AWS bill. In absolute it's a negligible amount, the breakdown is odd though :sweat_smile:

*Tags: Deployment*

---

**Ruben** - *17:36:37*

This is probably a dumb question, but here it goes anyway: can you get matched with your own orders? does betfair allow that? If not, how can it be avoided?

*Tags: General Technical*

---

**Mo** - *17:42:50*

Not really sure how to answer how to avoid it... you should know what you have sitting in the market

*Tags: General Technical*

---

**Oliver Varney** - *17:45:51*

thats not really out of the question for me, two models can disagree

*Tags: Strategies*

---

**Mo** - *17:48:20*

It will depend on the context I am sure. If you are manipulating the market in the way [@UUE6E1LA1](@UUE6E1LA1) describes or you are using it for premium charge avoidance as I have been party to then they will shut it down quickly. If you have the situation you’re describing with a rare model disagreement then they probably don’t care or it’s less visible

*Tags: Strategies*

---

**Oliver Varney** - *17:50:12*

wouldnt be intentional in my case, the model is the brains in the outfit :joy:

*Tags: Strategies*

---

**Oliver Varney** - *17:52:28*

kinda a similar topic but do you guys measure how one model impacts another ?

*Tags: Strategies*

---

**nthypes** - *18:02:10*

Folks, anyone with good statistics background that I can DM? I have a question abouta  probability model.

*Tags: Strategies*

---

**Dave** - *18:25:12*

[@U01A64T6DJQ](@U01A64T6DJQ) might be useful for many if you just post it as an open question

*Tags: General Technical*

---

## 2020-12-27

**Rory** - *14:40:14*

quick question ...



I currently have separate processes for market streaming, decision making, order placement (and lots of other stuff) e.g.



Stream market updates-&gt;topic-&gt;Decision Maker-&gt;topic-&gt;Order Placement-&gt;Betfair



I use Bflw for streaming, order placement etc and this all works very well. However, I'd like to integrate with flumine. I'm thinking something like this based on the poll_market_catalogue pattern ...



Flumine Strategies-&gt;process_market_book-&gt;topic-&gt;Decision Maker-&gt;topic-&gt;Flumine Background Workers-&gt;???-&gt;Strategy-&gt;place_order



I've set up a quick test and this works fine. The custom BackgroundWorkers poll for messages. My question is, what's the best way place a trade, based on the message received by the BackgroundWorker and have that Trade available in Strategy.process_orders?

*Tags: Strategies*

---

**Oliver Varney** - *14:52:04*

FYI flumine handles all the streaming - both market and order

*Tags: General Technical*

---

**Oliver Varney** - *14:55:41*

For me I use a flumine process for everything apart from model prediction which I run in a separate process and use redis streams to push to and read from. If you not doing anything super complicated Flumine can handle quite a few strategies in one process.

*Tags: Strategies*

---

**Rory** - *14:58:36*

Part of my motivation of moving to flumine is to remove all the custom stuff I've written, which flumine handles now. I like your suggestion of having process_market_book place each trade as it becomes available from the decision maker.

*Tags: General Technical*

---

**Rory** - *15:15:14*

I still have the question of the best (flumine) way to get that data from the BackgroundWorker to the Strategy. In the simple case, I suppose the strategy could just have a queue of trades that gets pushed to.

*Tags: Strategies*

---

**Oliver Varney** - *15:18:40*

[https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/markets/market.py#L27](https://github.com/liampauling/flumine/blob/ab25ef0fcfd42cff461145c0f83fc43f8a9a0ca3/flumine/markets/market.py#L27)

*Tags: General Technical*

---

**Oliver Varney** - *15:18:57*

I think people use the context fields for user data. In the worker you can pass flumine in as a param, then find your market in flumine.markets and populate the context dict

*Tags: General Technical*

---

**liam** - *15:30:20*

Yeah context and a custom worker / event with a callback, check out the inplayservice [https://github.com/liampauling/flumine/blob/master/examples/workers/inplayservice.py|example](https://github.com/liampauling/flumine/blob/master/examples/workers/inplayservice.py|example) 

*Tags: General Technical*

---

**PeterLe** - *15:45:03*

Just a quick question please to the statisticians amongst you..When analysing a set of results and faced with a choice of either using a T-Test or Monte Carlo, would you have a preference? Thanks in advance

*Tags: General Technical*

---

**nthypes** - *19:50:01*

Question: Imagine that i'm using Bayes Theorem to infer what the probability of a Horse Win in a 7 horses race given that the horse in question is the most traded horse based on volume. I imagine that what I would like to infer is the _P(WIN | MOST TRADED)._



I can get this probability using frequency data analysis. Is Bayesian approach here useless, since I can get this info from looking at the average win from most traded horses historical data?

*Tags: Data Quality*

---

**nthypes** - *19:52:59*

Or what I really get looking the historical data is _P(MT | WIN)_?

*Tags: Data Quality*

---

## 2020-12-28

**Misha** - *11:07:05*

Certainly when I was last doing Australian racing (2017), Betfair last price matched was better than the Tote or any combination of agency fixed prices (I did do that analysis over 5 years of data)

*Tags: Errors Debugging*

---

**nthypes** - *12:41:08*

I agree with you. So the frequentist approach is kind of useless right? The number return from this calculation looking at the historical data, it's a generalization, so kinda of useless when looking at specific race, right?

*Tags: Data Quality*

---

**Mo** - *13:09:18*

It sounds like you want to use the indicator whether a horse is the most traded horse as a feature in a model and then add more features

*Tags: Feature Engineering, Strategies*

---

**nthypes** - *13:10:38*

Yes, makes sense. Bayes help us dealing with uncertainty as you said. Since we know that the horse is the most traded, we have a fact here. So makes no sense using Bayes.

*Tags: General Technical*

---

**Mo** - *13:21:57*

That’s not my point. The use of conditional probability to account for uncertainty in which horse will be the most traded is still an option (note that this is just a question of conditional probability and nothing to do with Bayes theorem) because you do not which horse will be the most traded at the time of betting

*Tags: Strategies*

---

**Mo** - *13:22:46*

My point is that building a model (classifier) by weighting different features has nothing to do with Bayes

*Tags: Feature Engineering, Strategies*

---

**Oliver Varney** - *13:37:07*

I found it very useful at the start, still nice and quick to use these days. I have all the historical pro data now so I can get more specific pricing stuff from that when building models

*Tags: Strategies*

---

**nthypes** - *20:31:07*

What about _*Hierarchical Bayesian Modeling*_

*Tags: Strategies*

---

## 2020-12-29

**Ruben** - *18:32:35*

Hi everyone, quick Flumine question: to increase/reduce the size of an order, should I create/cancel orders to obtain the desired size, or is there a specific method that does that? I have seen `update_order()`  but that seems to only update the `persistence_type` of the order, thanks a lot

*Tags: General Technical*

---

**Oliver Varney** - *18:35:06*

[https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/order/order.py#L303](https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/order/order.py#L303)

*Tags: General Technical*

---

**Tianyao Wu** - *23:30:28*

Hi guys, is it true that basic historical data has a lot of missing soccer matches?

*Tags: Data Quality*

---

## 2020-12-30

**Artur Gräfenstein** - *16:13:28*

I’m trying to develop my first model. I wanted to do it next year, but you can never start early enough. :smile:

*Tags: Strategies*

---

**Artur Gräfenstein** - *17:38:58*

Does anyone have any idea where to get the exact live time for a soccer event? Many providers are a little imprecise. The time on the Betfair website is about 1-2 minutes delayed. I need the time quite accurately for my model.

*Tags: Deployment, Strategies*

---

**Artur Gräfenstein** - *17:52:41*

Hmm, I don’t know how accurate that is. Or how to check that well. Streams are also mostly delayed and can’t compare it properly.

*Tags: General Technical*

---

**Alessio** - *17:53:01*

Is your problem aligning streams?

*Tags: General Technical*

---

## 2020-12-31

**Misha** - *00:12:07*

Request from the API (from the request/response API, not the streaming API)

*Tags: General Technical*

---

**river_shah** - *12:46:05*

Currently I am using cloud service providers for back testing however when it comes to live trading, just launching scripts off my London based laptop (have high bandwidth reliable residential broadband). Could I please be guided on what the best way to run a prod model may be from a colo perspective? Speed is not so important to me but if I can get a few milliseconds for free running from AWS Dublin or some other location, won’t say no to it. Think this topic was discussed earlier but search history has run out. Thanks

*Tags: Performance, Deployment, Strategies*

---

**nthypes** - *13:04:50*

[https://betfairlightweight.slack.com/archives/C4H05KKMY/p1609411414466400](https://betfairlightweight.slack.com/archives/C4H05KKMY/p1609411414466400)

*Tags: General Technical*

---

**Graham** - *14:51:55*

please feel free to tell me to f-off but would someone have a historical dataset of intraday horse racing BF price changes that I could tap into please?

*Tags: Data Quality*

---

**liam** - *15:16:41*

nothing generic but just finished a [https://github.com/liampauling/betfair/pull/372|PR](https://github.com/liampauling/betfair/pull/372|PR) to add matches to the streaming output which would be the easiest way

*Tags: General Technical*

---

**nthypes** - *15:42:56*

Anybody handles with PostgreSQL @ scale?



I'm storing 100k lines per day. It's viable in long run ?

*Tags: General Technical*

---

**Oliver Varney** - *15:56:34*

so no need to manually load them back when you start / restart a stream (how / does this effect flumine)?

*Tags: General Technical*

---

**Mo** - *15:58:22*

Yeah I wouldn’t put that in a database 

*Tags: General Technical*

---

**liam** - *15:59:59*

Like many on here I keep all data in raw streaming form and then process when backtesting / modelling into ram using redis or just massive text files 

*Tags: Strategies*

---

**liam** - *16:53:22*

High level is you build cache of the alt data (pt: data) and then do a lookup based on the marketBook pt and inject it into the context for your strategy to use (just like live) 

*Tags: Deployment, Strategies*

---

**Graham** - *17:44:20*

I've got the following code for unpacking the historic price history (Basic) and exporting to csv. I'm really hitting a blank trying to pair the "id" with horse "name" if someone wanted to give me a nudge.

```import pandas as pd

import json

from bz2 import BZ2File



file_name = "30095040.bz2"



with BZ2File(file_name, "r") as f:

    lines = f.readlines()



price_changes = []



for line in lines:

    content = json.loads(line)

    market_changes = content.get("mc", [])

    for market_change in market_changes:

        market_id = market_change.get("id")

        market_definition = market_change.get("marketDefinition")



        runner_changes = market_change.get("rc")

        if runner_changes:

            for runner_change in runner_changes:

                print(runner_change)

                price_changes.append([content.get("pt"), runner_change.get("id"), runner_change.get("ltp")])



df = pd.DataFrame(price_changes, columns=["timestamp", "selection_id", "price"])

df.to_csv("prices.csv", index=False)```

*Tags: Feature Engineering*

---

**Oliver Varney** - *17:45:58*

what about using betfairlightweight / flumine (there is historical stream functionality)?

*Tags: General Technical*

---

**Oliver Varney** - *18:11:31*

betfairlightweight can load in files, build and update a cache and stream them (it will save you alot of headache). You do realise the file messages are deltas? I think you can put dummy values in for login details (if im not mistaken)

*Tags: General Technical*

---

**Oliver Varney** - *18:12:51*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**Graham** - *18:23:21*

I'm apparently far too stupid to follow a well documented git repo.

```UnicodeDecodeError: 'charmap' codec can't decode byte 0x8d in position 13: character maps to &lt;undefined&gt;```



*Tags: Errors Debugging*

---

**liam** - *19:13:06*

Debug it 

*Tags: Errors Debugging*

---

**Oliver Varney** - *23:59:37*

Its a question that is irrelevant especially if your starting out. profit is profit. Will the answer to that question change a non profitable strategy into a profitable one?

*Tags: Strategies*

---

## 2021-01-01

**nthypes** - *03:50:09*

It helps to understand market micro structure

*Tags: General Technical*

---

**Mo** - *08:47:30*

I think it's not very insightful to have an overall figure per race but if you can identify the breakdown per horse I think it can be the basis of a profitable strategy

*Tags: Strategies*

---

**Oliver Varney** - *09:08:42*

I did chat to someone whos entire strategy was to copy my bets, but I was a little easier to identify then the typical person when betting with that strategy

*Tags: Strategies*

---

**Oliver Varney** - *09:14:28*

so in that sense [@U01A64T6DJQ](@U01A64T6DJQ) knowing who money is whos can be profitable, but if my model says its value, I dont care who im matching against. For me if you start questioning the model you get into sticky situations (blind gambling)

*Tags: Strategies*

---

**Graham** - *10:51:41*

I remember in the earlier days it was ridiculous what they were doing. Just copying prices from BF - to the point you could put an order up in BF and Smarkets simultaneously in an illiquid market and it would match on Smarkets no problem

*Tags: General Technical*

---

**Dave** - *11:02:20*

I think [@U01A64T6DJQ](@U01A64T6DJQ)’s question about participant breakdown definitely has some basis. E.g. on some financial exchanges it is even possible to restrict matching against a set of cptys who you may believe is particularly sharp or have a post-trade profile that is quite poor/toxic. If you trade on shorter horizons you could benefit from not matching against such cptys. Anyways not that Betfair offer such functionality anyway, just a side point

*Tags: General Technical*

---

**Graham** - *11:29:09*

It's not quite the intention of this slack group, but if anyone's interested in collaborating on a Horse Racing pricing model I'd welcome the help.

*Tags: Strategies*

---

**Graham** - *11:51:50*

the whole process really. I've trained plenty of models with varying degrees of success. But, it's a rather isolating experience and I don't have the network with sufficient domain knowledge &amp; coding to just throw ideas around.

*Tags: Strategies*

---

**Graham** - *11:53:31*

^ all in python fyi

*Tags: General Technical*

---

**Misha** - *14:21:22*

The reasoning is that you use a model that prices the entire field. You back/lay anywhere you think you have a significant advantage, and it stands to reason that if you think a 1.50 runner should be at 1.30, a back. most of the rest of the field will be lays

*Tags: Strategies*

---

**D C** - *14:29:59*

[@U016535QCJ2](@U016535QCJ2) I am surprised you don't look at horses more given this inside knowledge of how these big clients work. I mean you know that these orders are coming in and you know roughly when they will come and how they are filtered into the market. How are you not able to use this to your advantage? After all, as you say, YOU wrote the software so this must be a HUGE edge knowing that these price moves are coming? I realise without model parameters you won't know WHAT is going to be backed, but knowing how the orders are pushed through must mean you have sufficient knowledge to take advantage of this?

*Tags: Strategies*

---

**Misha** - *14:32:44*

[@UUE6E1LA1](@UUE6E1LA1) - that's a good point. And with enough time and effort I could "follow" in what these big players do (I probably know enough how to detect it). I could live on about 2% of what my client made on UK racing. But using what I have learnt I reckon eventually I might be able to get close in just in-play tennis, so I'm playing the long game :wink:

*Tags: Deployment*

---

**Misha** - *14:42:28*

Started with bet sizes today, that if the modelling is good, will now pay my way. If it works I can increase that 10-fold relatively easily. I'm chasing a bigger pay-day because I think we can get there

*Tags: Strategies*

---

**Matthieu Labour** - *22:24:50*

Hello! I am looking for some guidance in Flumine. How to receive a timeout in a strategy. Let’s say I want a strategy to get a notification some time before a market starts. Would you recommend having a background worker send the notification? Ideally the solution should work for both live and backtesting.

*Tags: Deployment, Strategies*

---

## 2021-01-02

**Charlie 303** - *06:17:39*

Trying to collate BSP csv data (from [https://promo.betfair.com/betfairsp/prices](https://promo.betfair.com/betfairsp/prices)) and bf historical for the same market -- BSP data gives only the 'event_id', so I figured I could get the corresponding historical data by using the eventId filter, but neither trading.historic.get_file_list(... event_id) in betfairlightweight, nor the crappy download form at [http://historicdata.betfair.com/$/mydata|historicdata.betfair.com/$/mydata](http://historicdata.betfair.com/$/mydata|historicdata.betfair.com/$/mydata) works. Has anyone used eventId filter successfully or otherwise have any hints to get the corresponding historical market id(s) associated with a target event from a BSP file? I suppose I could just download all markets on a given day and grep through the entire collection for the eventId

*Tags: Data Quality, Strategies*

---

**liam** - *07:53:23*

You can ensure by setting `strategy.streaming_timeout` to a value such as 5, this will result in the cache being snapped every 5s and a market book event being sent through regardless of an update 

*Tags: Strategies*

---

**Matthieu Labour** - *08:05:45*

Thank you. I will try. This will work for live but not for backtesting, correct? The strategy code will be different depending on backtesting or live, correct?

*Tags: Deployment, Strategies*

---

**Newbie99** - *08:26:27*

(note though you seem to have to prefix a '1' on that historical data 'event_id' for it to equal a standard 'market_id', so in MySQL, something like this:



```a.market_id = concat('1.',b.event_id)```

*Tags: Data Quality, Errors Debugging*

---

**Mo** - *08:41:15*

You should drop the `1.` prefix and store the market IDs as integers in your database (IMHO)

*Tags: Errors Debugging*

---

**Mo** - *09:07:40*

Still easily solved by having an exchange enum column

*Tags: General Technical*

---

**Misha** - *09:15:02*

Negligible in terms of performance - makes no difference at all

*Tags: Performance*

---

**Misha** - *09:34:02*

Basically I know what takes time to execute in my code, and converting string lookups to integer lookups wouldn't save me 0,01% of the processing time. Good design is how you gain performance. I have a pipeline threading framework so even if running hundreds of threads there is no thread contention. Immutable objects that can be shared across thread with no copying is the biggest performance gain I have made in the last couple of years

*Tags: Performance*

---

**Michael** - *09:56:03*

Some of [@U0128E7BEHW](@U0128E7BEHW)'s comments from yesterday made me think about my default programming habits and whether they were right. For example I tend to do try/except in place of an 'if' conditional where I expect the condition to almost always be met, e.g. try X/Y except ZeroDivisionError instead of X/Y if y &gt; 0. Is this a bad habit or a good one? I've obviously picked it up somewhere but is it well founded?

*Tags: Errors Debugging*

---

**Michael** - *10:04:35*

I do it a lot for building dictionaries too - like try dictionary[key] except KeyError (then insert the key). Any comments?

*Tags: Errors Debugging*

---

**liam** - *10:04:51*

[https://stackoverflow.com/questions/2522005/cost-of-exception-handlers-in-python](https://stackoverflow.com/questions/2522005/cost-of-exception-handlers-in-python)

*Tags: Errors Debugging*

---

**D C** - *10:04:52*

Exception handling in some languages is quite expensive but I don't know much about how Python operates. It's a good question though and that specific example likely has different answers for different languages (C for example has no exception handling mechanism)

*Tags: Errors Debugging*

---

**D** - *10:11:58*

I understood that the try/except approach was 'pythonic' and recommended?

*Tags: General Technical*

---

**Michael** - *10:12:24*

Thanks that's really helpful.

*Tags: General Technical*

---

**liam** - *10:16:23*

it is pythonic but ugly code tends to be quicker, its a balancing act between readability and speed. Backtesting opens up the performance issues Dave mentions because you start to make 100's of millions of function calls so saving 0.001s on each execution suddenly becomes seconds or even minutes

*Tags: Performance*

---

**liam** - *11:30:47*

On the subject of performance just been [https://github.com/liampauling/betfair/pull/374/commits/52e439d3b6ee6bd3ca585bd7bd4cacf1ac40168e|refactoring](https://github.com/liampauling/betfair/pull/374/commits/52e439d3b6ee6bd3ca585bd7bd4cacf1ac40168e|refactoring) the marketCache in the bflw streaming code, seen huge reduction in function calls and thus runtime by only serialising the runner data when there has been an update. This will also fix the issue you raise [@ULDAVFDRP](@ULDAVFDRP) with removed runners sometimes not being present in the book



[@UBS7QANF3](@UBS7QANF3) hoping to look at your PR on refactoring the Available class as well soon (sorry been distracted recently)

*Tags: Errors Debugging, Performance*

---

**liam** - *11:36:11*

I need to have another look but @[https://github.com/synapticarbors|synapticarbors](https://github.com/synapticarbors|synapticarbors) example isn't representative of the data you get from the streaming API as you get very different speed comparisons due to the way python caches certain calls + the o notation this problem has

*Tags: Performance*

---

**Mo** - *11:39:15*

Even ignoring his example, when I retested it myself the performance was pretty much the same as the pure Python implementation

*Tags: Performance*

---

**Mo** - *12:25:52*

```mberk@mberk-desktop:/tmp $ /usr/bin/python3.8 -m venv ./venv-baseline                                                                                                                                       

mberk@mberk-desktop:/tmp $ ./venv-baseline/bin/pip install [git+git://github.com/liampauling/betfair](git+git://github.com/liampauling/betfair) tqdm

Collecting [git+git://github.com/liampauling/betfair](git+git://github.com/liampauling/betfair)                                                                                                                                                         

  Cloning [git://github.com/liampauling/betfair](git://github.com/liampauling/betfair) to ./pip-req-build-iboy4gb7                                                                                                                                  

  Running command git clone -q [git://github.com/liampauling/betfair](git://github.com/liampauling/betfair) /tmp/pip-req-build-iboy4gb7                                                                                                             

Collecting tqdm                                                                                                                                                                                             

  Using cached tqdm-4.55.0-py2.py3-none-any.whl (68 kB)                                                                                                                                                     

Collecting requests&lt;2.26.0                                                                                                                                                                                  

  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)                                                                                                                                                 

Collecting urllib3&lt;1.27,&gt;=1.21.1                                                                                                                                                                            

  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)                                                                                                                                                 

Collecting certifi&gt;=2017.4.17                                                                                                                                                                               

  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)                                                                                                                                              

Collecting chardet&lt;5,&gt;=3.0.2                                                                                                                                                                                

  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)

Collecting idna&lt;3,&gt;=2.5    

  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)

Using legacy setup.py install for betfairlightweight, since package 'wheel' is not installed.

Installing collected packages: tqdm, urllib3, certifi, chardet, idna, requests, betfairlightweight

    Running setup.py install for betfairlightweight ... done

Successfully installed betfairlightweight-2.11.1 certifi-2020.12.5 chardet-4.0.0 idna-2.10 requests-2.25.1 tqdm-4.55.0 urllib3-1.26.2

mberk@mberk-desktop:/tmp $ /usr/bin/python3.8 -m venv ./venv-c                                         

mberk@mberk-desktop:/tmp $ ./venv-c/bin/pip install [git+git://github.com/mberk/betfair@cache-c-extension](git+git://github.com/mberk/betfair@cache-c-extension) tqdm

Collecting [git+git://github.com/mberk/betfair@cache-c-extension](git+git://github.com/mberk/betfair@cache-c-extension)

  Cloning [git://github.com/mberk/betfair](git://github.com/mberk/betfair) (to revision cache-c-extension) to ./pip-req-build-kfss8qxh

  Running command git clone -q [git://github.com/mberk/betfair](git://github.com/mberk/betfair) /tmp/pip-req-build-kfss8qxh

  Running command git checkout -b cache-c-extension --track origin/cache-c-extension

  Switched to a new branch 'cache-c-extension'

  Branch 'cache-c-extension' set up to track remote branch 'cache-c-extension' from 'origin'.

Collecting tqdm

  Using cached tqdm-4.55.0-py2.py3-none-any.whl (68 kB)

Collecting requests&lt;2.26.0

  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)

Collecting certifi&gt;=2017.4.17

  Using cached certifi-2020.12.5-py2.py3-none-any.whl (147 kB)

Collecting urllib3&lt;1.27,&gt;=1.21.1

  Using cached urllib3-1.26.2-py2.py3-none-any.whl (136 kB)

Collecting chardet&lt;5,&gt;=3.0.2

  Using cached chardet-4.0.0-py2.py3-none-any.whl (178 kB)

Collecting idna&lt;3,&gt;=2.5

  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)

Using legacy setup.py install for betfairlightweight, since package 'wheel' is not installed.

Installing collected packages: tqdm, certifi, urllib3, chardet, idna, requests, betfairlightweight

    Running setup.py install for betfairlightweight ... done

Successfully installed betfairlightweight-2.11.1 certifi-2020.12.5 chardet-4.0.0 idna-2.10 requests-2.25.1 tqdm-4.55.0 urllib3-1.26.2

mberk@mberk-desktop:/tmp $ ./venv-baseline/bin/python benchmark.py                                     

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5791.84it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5790.26it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5777.81it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5800.95it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5790.11it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5800.39it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5774.89it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5786.89it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5787.31it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5771.91it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:37&lt;00:00,  3.79s/it]

37.88023018836975

mberk@mberk-desktop:/tmp $ ./venv-c/bin/python benchmark.py                                            

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5804.39it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5812.31it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5797.65it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5781.36it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5792.19it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5803.93it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5759.62it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5762.72it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5749.46it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 5721.92it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:37&lt;00:00,  3.79s/it]

37.93798065185547```

*Tags: Getting Started*

---

**Mo** - *12:26:21*

Where benchmark.py is:



```import time



import betfairlightweight

from tqdm import tqdm

from tqdm import trange

from betfairlightweight import StreamListener



trading = betfairlightweight.APIClient("username", "password", "app_key")

listener = StreamListener(max_latency=None)

stream = trading.streaming.create_historical_generator_stream(

            file_path="/tmp/1.172557162",

                listener=listener,

                )

gen = stream.get_generator()

# Warmup

number_of_market_books = sum(1 for market_books in gen())

start_time = time.time()

for _ in trange(10):

    for market_books in tqdm(gen(), total=number_of_market_books):

        pass

print(time.time() - start_time)```

*Tags: Performance, Strategies*

---

**Mo** - *12:33:08*

This is 373:



```mberk@mberk-desktop:/tmp $ ./venv-373/bin/python benchmark.py 

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6116.54it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6117.33it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6093.97it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6100.20it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6107.77it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6096.63it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6119.97it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6113.64it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6103.55it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:03&lt;00:00, 6109.20it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:35&lt;00:00,  3.59s/it]

35.89195704460144```



*Tags: General Technical*

---

**Unknown** - *14:58:08*

Considering a non-PC scenario.



Only average fee of 6% over profits. Looks like the break-even is BSP*+0.22*



Maybe my calculation is wrong?



I simulated a back bet on excel using a fixed stake on every runner selection on 2018-2020 races.

*Tags: Errors Debugging*

---

**Alessio** - *17:04:04*

AWS folks who run their production strategy there, which instance kind do you use? Is something in the free tier powerful enough (esp. in terms of RAM) ?

*Tags: Deployment, Strategies*

---

**liam** - *17:07:34*

Free tier is micro? What’s your setup? Flumine? How many markets?

*Tags: Getting Started*

---

**Alessio** - *17:08:51*

There's no set up yet :wink: Free tier is T3.Micro . It's going to be flumine.. to start it will try to enter ~ 10% of the markets.. so uhm.. 5-6 at the same time?

*Tags: General Technical*

---

**Unknown** - *17:12:07*

So for context here is one instance of flumine on a micro, all racing win markets (but 2 streams) with about 10 strategies 

*Tags: General Technical*

---

**Matthieu Labour** - *18:23:23*

Following up. Do u think this approach would work as well? [https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1609433260374700](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1609433260374700) with a file with timestamps clock events that one wants to inject. And for InPlay, I would have a background worker generates CustomClockEvent? Thank you!

*Tags: General Technical*

---

**nthypes** - *21:52:57*

Thie betfair data has a lot of flaws. What the change os BSP prices @ 1.01?

*Tags: Deployment*

---

**Charlie 303** - *22:00:58*

Would it be safe to say that BSP on horse win markets will pretty much always beat racebook win odds?  Have a win ticket strategy at the racebook that just breaks even, so I would intuit that if I simply mirrored the strategy on exchange, backing my selections at BSP would make it profitable....but im wondering if that logic is flawed / if i'm overlooking the dynamics of bsp in practice

*Tags: Strategies*

---

**Charlie 303** - *22:47:21*

Thanks fair enough.  In my nascent phase I'm figuring if im paying commission it means im mkaing profitable transactions = good problem to have :wink:

*Tags: General Technical*

---

## 2021-01-03

**B3tfairDev** - *01:08:59*

hello, I've got a question about betfairlightweight. I wanted to implement a Timer function that calls some betfairlightweight functions but apparently Timer and threading functions in python 3 are not supported by betfairlightweight. What is the appropriate way to call a function with a timer or a function in the future in python 3?

*Tags: General Technical*

---

**liam** - *10:45:25*

[@UBS7QANF3](@UBS7QANF3) when you get a second can you try issue373 again on your machine? With a small change required on the listener I am getting the following (same file)



```listener = StreamListener(max_latency=None, lightweight=True, debug=False, update_clk=False)```

```# master: 6.879s  (100%|██████████| 10/10 [00:06&lt;00:00,  1.46it/s])

# 373: 3.640s  (100%|██████████| 10/10 [00:03&lt;00:00,  2.75it/s])```

This is even without c/rust libraries :sunglasses:

*Tags: Errors Debugging, Performance*

---

**Lee** - *10:46:10*

wow, that's a massive performance increase

*Tags: Performance*

---

**Mo** - *10:51:19*

```(venv) mberk@mberk-desktop:/tmp $ ./venv-373/bin/python benchmark.py 

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 45216.03it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 45572.29it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 44281.82it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 45045.24it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 45123.00it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 45002.55it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 44920.42it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 44952.91it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 44624.22it/s]

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21919/21919 [00:00&lt;00:00, 44506.68it/s]

100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:04&lt;00:00,  2.05it/s]

4.88512659072876```

*Tags: General Technical*

---

**steve** - *11:57:02*

hi guys new to using live odds. im trying to find the near_price and far_price bsp, but keep getting 'None'. Can get the back and lay prices ok. i'm using the following code. anyone know what im doing wrong?



price_filter = bflw.filters.price_projection(

    price_data=['SP_AVAILABLE','SP_TRADED','EX_BEST_OFFERS','EX_ALL_OFFERS','EX_TRADED']

    )

market_books = trading.betting.list_market_book(

    market_ids=[1.177514203],

    price_projection=price_filter

    )

for runner in market_books[0].runners:

    print(runner.sp.near_price)

    print(runner.sp.far_price)

*Tags: Getting Started, Deployment, Strategies*

---

**steve** - *12:27:16*

i keep getting



APIError: SportsAPING/v1.0/listMarketBook



is there another bit of code i might be missing? works ok without the SP_PROJECTED

*Tags: Errors Debugging*

---

**steve** - *12:28:05*

this?



APIError: SportsAPING/v1.0/listMarketBook

Params: {'marketIds': ['1.177514198'], 'priceProjection': {'priceData': ['SP_AVAILABLE', 'SP_TRADED', 'EX_BEST_OFFERS', 'EX_ALL_OFFERS', 'EX_TRADED', 'SP_PROJECTED'], 'exBestOffersOverrides': {}, 'virtualise': True, 'rolloverStakes': False}}

Exception: None

Error: {'code': -32602, 'message': 'DSC-0018'}

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32602, 'message': 'DSC-0018'}, 'id': 1}

*Tags: Errors Debugging*

---

**Mo** - *12:29:00*

Sorry I misread your code, I thought you were using streaming

*Tags: General Technical*

---

**steve** - *12:30:22*

just using this



market_books = trading.betting.list_market_book(

    market_ids=['1.177514198'],

    price_projection= bflw.filters.price_projection(price_data=['SP_AVAILABLE','SP_TRADED','EX_BEST_OFFERS','EX_ALL_OFFERS','EX_TRADED','SP_PROJECTED'])

    )

for runner in market_books[0].runners:

    print(runner.sp.near_price, runner.sp.far_price)

*Tags: Strategies*

---

**Mo** - *12:30:47*

SP_PROJECTED is streaming specific so it's not surprising it doesn't work

*Tags: General Technical*

---

**Mo** - *12:31:43*

If you use streaming in conjunction with SP_PROJECTED you will get the near and far BSPs but I don't know how you can get them using the REST API, if it's even possible

*Tags: General Technical*

---

**steve** - *12:32:35*

ok thanks mate. ill have a look into streaming then

*Tags: General Technical*

---

**Dave** - *14:04:06*

Liking the optimizations [@U4H19D1D2](@U4H19D1D2) , glad to see less attribute lookups!

*Tags: Performance, Strategies*

---

**Matthieu Labour** - *15:03:10*

Hi, looking for guidance. When backtesting and filtering on market_type, would you recommend the following

```    market_filter = {

        "market_types": [

            "MATCH_ODDS",

        ],

        "markets": [market],

    }



    strategy = MyStrategy(

        market_filter=market_filter,

        max_order_exposure=1000,

        ...

    )```

or

```    strategy = MyStrategy(

        market_filter={"markets": [market]},

        max_order_exposure=1000,

        max_selection_exposure=1000,

    )



class NyStrategy(BaseStrategy):

        def check_market_book(self, market, market_book):

            if market.market_type not in ['MATCH_ODDS']:

                return False```

Thank you!

*Tags: Strategies*

---

**liam** - *15:13:37*

Processing a market is expensive so I would filter before you get to that point, ie know the markets you want or open them up and take a peak at the marketDefinition on the first line before even getting to flumine 

*Tags: General Technical*

---

**Matthieu Labour** - *15:57:48*

Thank you. I am a bit confused on how to build `market_filter` when creating a strategy. For example, I can create the dictionary with `market_types` and `markets` keys. However, I wanted to use a factory method such as `def streaming_market_filter`  but it does not have `markets` . Will the following work?

```   market_filter = {

        "market_types": [

            "MATCH_ODDS",

        ],

        "markets": [market],

    }```

*Tags: Strategies*

---

**liam** - *16:03:29*

When live trading market filter is a `streaming_market_filter` but when backtesting you instead pass the `markets` var so in your example above "market_types" is ignored when backtesting

*Tags: Deployment, Strategies*

---

**liam** - *16:22:31*

yep and then pass to flumine, you could of course filter using the `check_market_book` function but that would result in you processing the entire file which is a waste of cpu

*Tags: General Technical*

---

## 2021-01-04

**river_shah** - *08:54:57*

After a restart, how do I ensure that strategy exposures for each selection take into account all filled orders please? Currently the exposures get reset to 0.

*Tags: Strategies*

---

**Newbie99** - *10:22:28*

If its just a lost connection flumine should retain the info automatically, however if its a manual stop or lengthily cut out (i.e. where it can't immediately reconnect) then you would need to make a REST API call to get the info (list current orders probably from what you're saying is what you need).

*Tags: General Technical*

---

**Oliver Varney** - *11:54:16*

dodginess remains within flumine._add_market (for now, havent had time to move it)_

*Tags: General Technical*

---

**Oliver Varney** - *12:04:16*

pseudo code is roughly, call list_current_orders, create the trade (for which the orders will sit on), from the CurrentOrders, create the flumine orders and append to the trade, add the orders into the market.blotter, add the trade into whereever you store it for later access (most people use the context dicts)

*Tags: General Technical*

---

**Chris C** - *12:18:15*

Maybe a bit dodgy, but simple. At restart I poll current_orders and do this:

framework.handler_queue.put(flumine.events.events.CurrentOrdersEvent([current_orders]))

*Tags: General Technical*

---

**Oliver Varney** - *12:24:18*

I think [@U01A8EKD545](@U01A8EKD545) example will be the easiest by the looks, depends if you need to rebuild the trade or not. The trade class does a fair bit of functionality for me (but I think im very much the exception to how I use this class)

*Tags: Errors Debugging*

---

**Alessio** - *14:18:12*

there's a bunch of util functions in flumine to get the nearest valid price exactly for this :slightly_smiling_face:

*Tags: General Technical*

---

**bogdan** - *21:21:41*

Hello,I want to get the horse markets from New Zealand in the next day (I fount the symbol for it is NZ), but even though I specify this, sometimes I get markets from AUS, USA. Do you know why? Below is my filter:





thoroughbreds_event_filter = betfairlightweight.filters.market_filter(

    event_type_ids=[7],

    market_countries=['NZ'],

    market_start_time={

        'to': (datetime.datetime.utcnow() + datetime.timedelta(days=1)).strftime("%Y-%m-%dT%TZ")

    }

)



For example using this filter I got this market: [https://www.betfair.com/exchange/plus/horse-racing/market/1.177574798](https://www.betfair.com/exchange/plus/horse-racing/market/1.177574798), which is an AUS race

*Tags: General Technical*

---

**Charlie 303** - *22:30:27*

if you're using a list_ (NG api) function, you use market_filter.  if you're trying to use the filter for streaming, you need to use streaming_market_filter

*Tags: General Technical*

---

## 2021-01-05

**bogdan** - *08:52:00*

I think if double quotes were important, I would get an error.

*Tags: Errors Debugging*

---

**bogdan** - *08:53:06*

market_filter is ok because I don't use streaming

*Tags: General Technical*

---

**liam** - *08:58:01*

There is a helper function for this in filters 

*Tags: General Technical*

---

**liam** - *09:35:52*

is this flumine?

*Tags: General Technical*

---

**liam** - *09:37:33*

magic [https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/streams/streams.py#L46|here](https://github.com/liampauling/flumine/blob/707a81990d1c51d64e964c781c5e700fe58646de/flumine/streams/streams.py#L46|here)

*Tags: General Technical*

---

**Lee** - *09:40:16*

You'll be able to check at startup from the logs as it has the same stream id (2000 in this example)

```2021-01-04T12:01:44.916563534Z app[prod.1]: {"asctime": "2021-01-04 12:01:44,916", "levelname": "INFO", "message": "Creating new &lt;class 'flumine.streams.marketstream.MarketStream'&gt; (2000) for strategy StrategyOne"}

2021-01-04T12:01:44.917101796Z app[prod.1]: {"asctime": "2021-01-04 12:01:44,916", "levelname": "INFO", "message": "Adding strategy StrategyOne"}

2021-01-04T12:01:44.917474590Z app[prod.1]: {"asctime": "2021-01-04 12:01:44,917", "levelname": "INFO", "message": "Using &lt;class 'flumine.streams.marketstream.MarketStream'&gt; (2000) for strategy StrategyOne"}

2021-01-04T12:01:44.917878853Z app[prod.1]: {"asctime": "2021-01-04 12:01:44,917", "levelname": "INFO", "message": "Adding strategy StrategyOne"}

2021-01-04T12:01:44.918276110Z app[prod.1]: {"asctime": "2021-01-04 12:01:44,918", "levelname": "INFO", "message": "Using &lt;class 'flumine.streams.marketstream.MarketStream'&gt; (2000) for strategy StrategyTwo"}```

*Tags: Strategies*

---

## 2021-01-06

**liam** - *16:26:40*

This is going to be so painful for me, api / strategy setup / database 

*Tags: Getting Started, Strategies*

---

**liam** - *17:18:26*

My problem is I don’t know where the problems are going to be till it changes 

*Tags: General Technical*

---

## 2021-01-07

**birchy** - *11:28:45*

Question regarding analysis of settled bets...

What setups do you have to handle this? Or more specifically, what bet data do you save? I've always been a bit slack-arsed with post-race analysis: if the P&amp;L is &gt;£0, I leave the bot alone (or maybe make some minor tweaks if I can see some _obvious_ improvements) and if P&amp;L &lt;£0, I simply change some trigger values and try again. There's no real analysis behind it other than what I _think_ the triggers should be. I'm designing something to improve this and want to keep things simple, so was thinking of creating a CSV per strategy with the following data in it:



1. triggers that activated the bet being placed

2. market conditions when triggers were satisfied

3. the actual bet info (price, stake, etc)

4. result/P&amp;L for the bet

Obviously the above is potentially a LOT of data per bet, so I'm wondering what you would consider to be essential? Or maybe there is a better approach?

*Tags: Getting Started, Strategies*

---

**Misha** - *11:36:03*

Start with every field from the listClearedOrders API call (there are quite a lot of fields). Store the results in a database and you have the first steps to analysis

*Tags: General Technical*

---

**Misha** - *11:38:44*

CREATE TABLE [dbo].[Bf_Bet](

	[TransactionId] [bigint] NOT NULL,

	[EventTypeId] [bigint] NOT NULL,

	[EventTypeName] [varchar](100) NOT NULL,

	[EventId] [bigint] NOT NULL,

	[EventName] [varchar](100) NOT NULL,

	[MarketId] [varchar](100) NOT NULL,

	[MarketName] [varchar](100) NOT NULL,

	[SelectionId] [bigint] NOT NULL,

	[AsianSelectionId] [bigint] NOT NULL,

	[SelectionName] [varchar](100) NOT NULL,

	[AsianSelectionHandicap] [decimal](18, 2) NOT NULL,

	[BetId] [bigint] NOT NULL,

	[MarketSubType] [varchar](100) NULL,

	[UtcStartTime] [datetime] NOT NULL,

	[NumberOfWinners] [int] NULL,

	[BetCategoryType] [varchar](100) NOT NULL,

	[BetOutcome] [varchar](100) NULL,

	[BetPersistenceType] [varchar](100) NOT NULL,

	[BetStatus] [varchar](100) NOT NULL,

	[BetType] [varchar](100) NOT NULL,

	[CancelledSize] [decimal](18, 2) NULL,

	[GrossProfit] [decimal](18, 2) NULL,

	[IsPriceReduced] [bit] NOT NULL,

	[MatchedPrice] [decimal](18, 2) NULL,

	[OrderRef] [varchar](100) NULL,

	[RequestedPrice] [decimal](18, 2) NOT NULL,

	[SettledSize] [decimal](18, 2) NULL,

	[StrategyRef] [varchar](100) NULL,

	[UtcMatchedTime] [datetime] NULL,

	[UtcPlacedTime] [datetime] NOT NULL,

	[UtcSettledTime] [datetime] NULL,

	[LastUpdated] [datetime2](0) NOT NULL,

 CONSTRAINT [PK_Bf_Bet] PRIMARY KEY CLUSTERED

(

	[TransactionId] ASC

)WITH (PAD_INDEX = OFF, STATISTICS_NORECOMPUTE = OFF, IGNORE_DUP_KEY = OFF, ALLOW_ROW_LOCKS = ON, ALLOW_PAGE_LOCKS = ON, FILLFACTOR = 80) ON [PRIMARY]

) ON [PRIMARY]

GO

*Tags: Strategies*

---

**birchy** - *11:41:12*

Yeah, am going to implement `listClearedOrders` in bflw, probably polling once a day or something like that. Never used databases, it always looks like another language to learn.

*Tags: General Technical*

---

**birchy** - *11:45:45*

TBH, I've never seen the advantage of a database vs reading CSVs? Both need custom python code and for a simpleton like me, reading a CSV doesn't require anything beyond Python, whereas a database ALSO requires database knowledge.

*Tags: General Technical*

---

**D C** - *11:48:42*

File IO can be slow though. I store a large amount of market data as text files and extracting anything useful takes a lot longer than would be the case if it were all in a database. Depends on what you need really and how much data you are processing but DB definitely has advantages.

*Tags: Performance*

---

**Misha** - *11:53:40*

I have every bet I have attempted in a database, including cancelled bets (about 700,000 in the last 3 months), Queries take less than a second, no matter how I group the bets and summarise them

*Tags: General Technical*

---

**Misha** - *11:55:05*

I do a fairly quick poll for settled bets - every 15 seconds to check and write them to a database. I usually have the data available close to 30 seconds after a tennis market is closed

*Tags: General Technical*

---

**D C** - *12:16:15*

Anyone else getting stream 503 errors right now???

*Tags: Errors Debugging*

---

**Lee** - *12:17:10*

yeah, got a few errors from the api calls

*Tags: Errors Debugging*

---

**Misha** - *12:17:22*

Getting a few API errors at the moment

*Tags: Errors Debugging*

---

**ash** - *12:23:03*

I think i errored out on a accountfunds call a few minuets ago..  but the I have done that call again since and stream seems to be working for me right now?

*Tags: Errors Debugging*

---

**Misha** - *12:23:51*

Errors come and go. I never worry unless I get a heap of them over an extended period of time

*Tags: Errors Debugging*

---

**liam** - *13:45:37*

You a bflw user?

*Tags: General Technical*

---

**Taking Value** - *13:50:12*

Good question. I don't know is the honest truth. I was strapped for time when I set up the listeners and have limited coding experience so just messed about with the files to make sure all market updates that were streamed were recorded to txt files. Got GB's of data in my S3 buckets, finally have had time to come back and process it over Christmas. Its great, its got all the data I need but it takes too long for me to process the files. Thinking of going back and only recording market updates every 5 secs.

*Tags: General Technical*

---

**liam** - *13:52:35*

Full fat then, you can patch priceSize like flumine does, this gives 3-4x speed improvement and you can wait for v2.12.0 which will hopefully give the same again once I have finished it. Aim is for sub 1s per file, do you know how many seconds each file is currently taking? Are you processing using multiple cores / processes?

*Tags: Performance*

---

**Taking Value** - *13:57:35*

I provisioned an EC2 spot instance with no less than 8gb of ram and 2cpus. Currently it takes me 20mins to scrub a file using the python code I wrote (which seems far too long for me). I am processing each file and adding the desired data to a MySQL Amazon RDS database rather than query the files themselves. I don't know what flumine is beyond seeing its name mentioned in here, on my to do list.

*Tags: Deployment*

---

**Taking Value** - *13:58:55*

Infact I say 20mins but that's how long it takes on my laptop, the EC2 environment is deffinately running slower than that but I haven't had time to investigate why yet. Spent most of yesterday re-doing my code to see if that could speed it up but it hasn't.

*Tags: Performance, Deployment*

---

**Taking Value** - *14:00:03*

I suspect the problem is related to me only coding for just over a year. All otehr problems have always been with my code up till now.

*Tags: General Technical*

---

**Mo** - *14:01:24*

You're not inserting into the database one row at a time are you?

*Tags: General Technical*

---

**Taking Value** - *14:05:40*

[@UBS7QANF3](@UBS7QANF3) I am inserting one row at a time and thought this would be a problem so looked into other ways to do it. However when I stopped the insert function the code still took a long time to run so I concluded that it wasn't the rate limiting step.

*Tags: General Technical*

---

**Taking Value** - *14:06:47*

Pretty certain the way I process the data is the biggest problem followed by the 1 row at a time.

*Tags: General Technical*

---

**Taking Value** - *14:15:49*

Ok cool, I will start there and let you know how it goes. Thanks for your help all.

*Tags: General Technical*

---

**Robert** - *14:42:34*

happy new year.  The betfair historical data goes back to ~2016  for uk horse racing.  I was wondering if anyone knows where can get some (at least partial) historical betfair data which goes back for 2014 and 2015?

*Tags: Data Quality*

---

## 2021-01-08

**river_shah** - *17:27:17*

In backtest mode, how do I receive the `market_book.streaming_update` before the runner books are updated please? I tried changing the stream class to `DataStream` but does not seem to work. ideally my strategy would get called first at `process_raw_data` / flumine then processes and then the standard `process_market_book` callback.

*Tags: Strategies*

---

**river_shah** - *17:39:12*

ok, I can put the appropriate handling in my strategy. thank you

*Tags: Strategies*

---

## 2021-01-09

**thambie1** - *17:44:59*

[@ULDAVFDRP](@ULDAVFDRP) From reading the Betfair documentation, that seems to be the case

*Tags: General Technical*

---

## 2021-01-10

**river_shah** - *14:59:28*

question for horse traders please: which months should I censor over the past 12 months as not being representative of typical market behaviour / liable to break backtests due to covid

*Tags: General Technical*

---

**Michael** - *17:28:28*

Although all the covid era months seem very non-typical to me I wouldn't discard any of the data, especially if that's all the data you have. The betting has been unusual in that there have been more opportunities to bet and it's been really easy to get bets matched but the signals that are ultimately used for triggers are just the same as ever and that's usually what you're looking for when you back test. Just keep in mind that your back-tested returns are likely to be on the high side, so if your strategy looks a bit marginal tread carefully. From where I sit the betting now is just as good as at any time and I'm continuing to see superb returns relative to time of year, race type and so on. Who knows how long it will last but for the time being I guess we all just enjoy it.

*Tags: Strategies*

---

**Dilla** - *20:36:31*

:wave: Perhaps someone knows how to open bet365 livescore bypassing the main site?

In my country bet365 is blocked, and VPN does not accept this site or only for a short time.

I could only find such options. But they are very slow. They differ in the speed of updating the livescore, which is on the main site. And also there does not show every point in tennis, only games.

[https://livescore.sportradar.com/bet365/en/tennis](https://livescore.sportradar.com/bet365/en/tennis)

[http://ls.betradar.com/ls/livescore/?/betfair/en/page](http://ls.betradar.com/ls/livescore/?/betfair/en/page)

*Tags: Performance, Deployment*

---

## 2021-01-11

**liam** - *08:14:20*

Just released a beta of [https://github.com/liampauling/betfair/pull/371|2.12.0b0](https://github.com/liampauling/betfair/pull/371|2.12.0b0) which contains the streaming refactor, using [@UBS7QANF3](@UBS7QANF3) benchmark looking at the following improvements:



```# lightweight (2.3x improvement)

#  master: 6.860s (100%|██████████| 10/10 [00:06&lt;00:00,  1.46it/s])

#  2.12.0: 2.998s (100%|██████████| 10/10 [00:02&lt;00:00,  3.34it/s])



# Non lightweight (3.4x improvement)

#  master: 36.005s (100%|██████████| 10/10 [00:35&lt;00:00,  3.60s/it])

#  2.12.0: 10.625s (100%|██████████| 10/10 [00:10&lt;00:00,  1.06s/it])



# Non lightweight with flumine patching/listener/stream (2.8x improvement)

#  master: 13.896s (100%|██████████| 10/10 [00:13&lt;00:00,  1.39s/it])

#  2.12.0: 5.005s (100%|██████████| 10/10 [00:04&lt;00:00,  2.00it/s])```

A quick [https://github.com/liampauling/flumine/pull/361|test](https://github.com/liampauling/flumine/pull/361|test) on flumine and this equates to around a 2.3x speed improvement which is massive and equates to flumine running through a market faster than the current lightweight implementation! CPU has also halved when running live which is a nice bonus.



Going to stay in beta until I can test this properly in production but I have carried out some regression tests on various markets throughout (hasn't just been optimised for this one market) However feel free to test when backtesting and let me know if there any bugs.

*Tags: Errors Debugging, Performance, Deployment*

---

**river_shah** - *10:28:24*

login to betfair fails for me when using docker (no problems using local host / am providing the certs inside docker through mount). has anyone using docker seen this please?

*Tags: General Technical*

---

**river_shah** - *10:28:28*

```        except requests.ConnectionError as e:

&gt;           raise APIError(None, exception=e)

E           betfairlightweight.exceptions.APIError: None

E           Params: None

E           Exception: HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by SSLError(SSLError(398, '[SSL: CA_MD_TOO_WEAK] ca md too weak (_ssl.c:3911)')))```



*Tags: Errors Debugging*

---

**river_shah** - *10:29:06*

my base image is `FROM python:3.7`

*Tags: General Technical*

---

**Dilla** - *10:55:55*

I mean the website. I don't know how to code yet:)



Enetpulse used to be always worse at Grand Slam tournaments

*Tags: General Technical*

---

**river_shah** - *11:04:51*

problem fixed. the issue was that docker container `openssl.cnf` was missing this section:

```[ ssl_client ]

basicConstraints = CA:FALSE

nsCertType = client

keyUsage = digitalSignature, keyEncipherment

extendedKeyUsage = clientAuth```

appending it fixes docker login issue

*Tags: Errors Debugging*

---

**Carsten** - *19:32:05*

Hi everyone Im new in here. Just started out exploring the betfair api. Im not an experienced programmer either, but do know my basic python, and a bit of api as well. Being from Denmark there is some special things that need to be done. Trying to follow the guide to Non-interactive, but having trouble with the certifications.  Failing for me here: *Create a certificate signing request (CSR..  Also trying one of the scripts from the forum, that should be good to go, but i dont get a ssoid in my response.. Hope someone is able to help and guide me in the right direction.* 

*Tags: General Technical*

---

## 2021-01-12

**Carsten** - *08:38:12*

[@U4H19D1D2](@U4H19D1D2) Just tried without certificate. Using betfairlightweight i get a 200 SUCCES in response.  In script it then looks for this: session_token = re.findall(

    "ssoid=(.*?);", resp.headers["Set-Cookie"]

My response in headers - set-cookie is:

wsid=********-54b0-11eb-b2c0-********; Domain=.[http://betfair.com|betfair.com](http://betfair.com|betfair.com); Path=/, vid=********-54b0-11eb-b2c0-*********; Domain=.[http://betfair.com|betfair.com](http://betfair.com|betfair.com); Expires=Wed, 12-Jan-2022 08:29:16 GMT; Path=/

Just changed some of the ID to ***.. Dont know if they are personal

*Tags: General Technical*

---

**Carsten** - *08:51:26*

when running the code as is in example i get this :

 trading.set_session_token(session_token[0])

IndexError: list index out of range

*Tags: Errors Debugging, Strategies*

---

**Carsten** - *09:11:29*

betfairlightweight.exceptions.AppKeyError:

*Tags: Errors Debugging*

---

**Carsten** - *09:16:34*

sorry if its noob questions. :disappointed:  Should i have a automated beting program access on the website ?

*Tags: General Technical*

---

**liam** - *09:20:47*

do you know how to use dev tools in your browser?

*Tags: General Technical*

---

**Carsten** - *09:46:15*

really appreciate all the help liam. :pray:  on top of all this, im also learning a ton. :slightly_smiling_face:

*Tags: General Technical*

---

**Carsten** - *10:33:41*

i got a errand shortly, and will be away for a couple of hours. again thanks for all the help. Hope we can continue later on.  hope i can provide you all the info you need to help me. I was using chrome browser would it make a difference using other browser ?  Also if you think it would be easier if you saw the output your self, we could setup a remote view of it. If its easier and speed up the process so i dont keep you active in here for weeks :slightly_smiling_face:

*Tags: Getting Started, Performance*

---

**Matthieu Labour** - *10:40:54*

Hello,

I am looking for guidance when using Flumine. I want to manually force/enter an order using the API. What would you recommend?

I see a few options:

Option1: I create a separate program that uses BetfairLightWeight only. No Flumine involved.

Option2: I create a strategy in Flumine that triggers an order when receiving a CustomEvent.

Option3:  I run flumine and share the BetfairClient as shown below:

```trading = betfairlaightweight.APIClient("username")

client = clients.BetfairClient(trading)

framework1 = Flumine(client=client)

framework2 = GatewayToSendOrder(client=client)```

Others?

Thank you for your help!

*Tags: Strategies*

---

**Matthieu Labour** - *12:48:02*

Haha. Ok. Let’s say I have a prediction engine that runs outside of flumine and wants to trigger an order through flumine. How would you recommend the recommendation engine to send messages to Flumine?

*Tags: General Technical*

---

**Matthieu Labour** - *15:32:05*

Would that take place within Flumine? The idea is to prepare an order (in CSV or via API like Oliver &amp; liam suggested) but, then, where does the reading of the CSV is taking place? In a Strategy? A background worker?

*Tags: Deployment, Strategies*

---

**river_shah** - *15:37:04*

My suggestion would be that your prediction / recommendation engine project should have a few more modules that interact with flumine. So you still launch a flumine strategy when it comes time to trade, first that process instantiates your prediction engine and then it instantiates relevant flumine pieces. please take a look the example strats. Unless the recommendation engine is super heavy, splitting up recommendations from the trading is not a good idea because you’ll add a ton of code complexity (csv, redis, zmq etc) and backtesting will be that much harder

*Tags: Strategies*

---

## 2021-01-13

**jgnz** - *01:06:14*

When using market recorder in flumine, what is the best way to keep the zipped files? I only just realised flumine is deleting them after 3600 seconds, so all my recorded markets are gone.



Should I setup a crontab to move zipped the files out before flumine deletes them? Or is there a background worker that does this?

*Tags: Getting Started, Data Quality*

---

**liam** - *06:11:52*

Just update the MarkettRecorder strategy so it does what you want, it’s just an example (delete code is in clean_up)

*Tags: Strategies*

---

## 2021-01-14

**birchy** - *08:34:08*

[@U4H19D1D2](@U4H19D1D2) used to do that when I was using my own none-streaming library. My bots would get the mc as json/dict at init, then only update on a version change. The mc is not really mission critical, it's only useful for initial market filtering.

*Tags: General Technical*

---

**Unknown** - *08:50:34*

Is this in betfairlightweight Streaming or flumine?

*Tags: General Technical*

---

**liam** - *08:50:37*

Flumine 

*Tags: General Technical*

---

## 2021-01-15

**Jono** - *09:41:55*

so what are the values that are returned when i access these attributes in the code? Since this doesnt cause errors are they place holder numbers

*Tags: Errors Debugging*

---

**Jono** - *10:03:23*

so did i, apologies for not sending in a code snippet to clarify exactly whta is happening. Im clearing my directory and downloading the BASIC data again to try and rule out the chance its iterating over advanced and pro data i have stored. Will shoot another message in a few hours if im still having probs with it. Like you and the documentation suggests there really shouldnt be values for the atb and atl info

*Tags: Data Quality*

---

**JC** - *12:04:48*

If you have recorded data and are wanting to generate some CSVs, is there a way to conflate the market books so that you can have a row every 0.25 secs for example? Can see conflate_ms for live streaming but not sure if there is a way for historical. Cheers

*Tags: Deployment*

---

**Mo** - *12:11:58*

pandas resampling can help

*Tags: Feature Engineering*

---

**JC** - *12:37:28*

Ok so there’s not a way to handle it in flumine? 

*Tags: General Technical*

---

**JC** - *13:27:20*

Yeah was looking to group all the updates that occur within an interval, and checking there wasn’t a flumine way. No worries will work something out 

*Tags: General Technical*

---

**Alessio** - *14:22:29*

Yeah i run a strategy that writes into a file every X "virtual" milliseconds.

*Tags: Strategies*

---

**Aaron Smith** - *17:50:57*

hey guys, quick question: whats the market_type for the AvB market in horse racing? cant seem to find it somehow

*Tags: General Technical*

---

## 2021-01-16

**jgnz** - *07:36:25*

ok thanks liam. i've just added a crontab for now as i dont want to restart the market recorder that i have running. still a bit gutted i lost the data from the India vs Australia match, but its my own fault for copying and pasting too much.

*Tags: Data Quality*

---

## 2021-01-17

**Taking Value** - *21:03:19*

Additional quick question - Is there an API out there that can be used to get data such as prize money and class for a horse race. I was getting it via the betfair racing post API but the script I was using decided to switch its self off a few months ago and I didn't realise(_ok it *may* have been poorly designed and I *might* be at fault_) . Corrected the error but I have a 3 month hole in my data I need to fill.

*Tags: Errors Debugging*

---

## 2021-01-18

**azevedo** - *10:06:34*

Hey guys! What’s the best way to deal with voided bets inplay? (e.g. when Betfair fails to suspend on a red card in football and then voids some bets while still inplay) 

Does something come through in streaming?

*Tags: General Technical*

---

**river_shah** - *10:18:39*

how to I request betfair to disconnect all of my api connections please? seems like two connections are stuck in a bad state

*Tags: General Technical*

---

**Matthieu Labour** - *17:50:00*

Hi, I am looking for guidance in flumine again.

`MarketBook` has a list of `RunnerBook`

`MarketBook` has `MarketDefinition`. `MarketDefinition` has a list of `MarketDefinitionRunner`

`MarketDefinitionRunner` has `name` but `RunnerBook` does not.

Hence when one wants to retrieve a `RunnerBook` by `name`, one has to first retrieve the runner in the list of `MarketDefinitionRunner` then use the `selection_id` (should we use the handicap too?) to retrieve the `RunnerBook`. Is it correct? Thank you!

*Tags: General Technical*

---

## 2021-01-19

**river_shah** - *08:57:21*

does flumine support async order_package handling? is doing this sufficient to get async behaviour but still keep all risk and blotter objects in good state?

```order_package.async_ = True

self.handler_queue.put(order_package)```



*Tags: General Technical*

---

**river_shah** - *10:18:08*

moving this to DM but want this to return immediately.

```elif event.EVENT_TYPE == EventType.ORDER_PACKAGE:

    self._process_order_package(event)```

strategy does not care if bet was placed (as long as eventually the blotter reflects true matched and executable bets)

*Tags: Strategies*

---

**liam** - *10:22:40*

you will still have the request latency blocking in the main thread

*Tags: Performance*

---

## 2021-01-20

**Mo** - *12:18:09*

You'll get a BET_TAKEN_OR_LAPSED error for that cancellation

*Tags: Errors Debugging*

---

**D C** - *12:39:05*

Yes I get that quite a bit. It's not a problem as I was assuming that any cancellation call would count. But that said if I cancel a batch of 50 and they have all matched that counts as 50 towards the total if I understand correctly. I will have to adjust my code to not count if bet cancelled successfully and to count for each failure within a batch response. I suppose a partial match still counts as a successful cancellation?

*Tags: General Technical*

---

**river_shah** - *16:51:24*

I see that each instance of flumine uses up two connections, one for `MarketStream` and another for `OrderStream`. Is this expected behaviour or am I logging in wrong? Can we not use the same connection for both market stream and order stream or does that entail some performance issues?

*Tags: Performance*

---

**river_shah** - *17:20:35*

Sorry Liam, I am being dense here. Could you please expand on what this means? `MarketStream` uses one socket and `OrderStream` another and one instance of flumine will then take two connections?

*Tags: General Technical*

---

## 2021-01-21

**Misha** - *00:21:50*

Why don't you get market and order data through the same stream? Personally I split the streams, for lower latency, but initially I had both from the same stream. Now I split my markets over multiple streams and have an additional stream for orders

*Tags: Performance*

---

**Misha** - *06:45:52*

Not sure what the question is - can you expand?

*Tags: General Technical*

---

**liam** - *06:46:52*

Sharing a socket means you have to handle all the edge cases and more latency, having a separate socket per connection removes all of this

*Tags: Performance*

---

**Misha** - *06:48:59*

So yes, if you are worried about order latency, then you can use two connections/sockets

*Tags: Performance*

---

**Misha** - *06:50:53*

The question was about using 2 connections per market set (if I got that right), so you can be subscribing to 5 sets of markets

*Tags: General Technical*

---

**Misha** - *06:53:05*

To run two market strategies (different markets) do you need two instances of flumine?

*Tags: General Technical*

---

**Misha** - *06:56:35*

OK, so you could do it with one connection, but risk adding latency to the orders (coming through the same connection as markets). But chosen not to (a fair enough design decision - I do too)

*Tags: Performance*

---

**Mo** - *16:46:24*

bflw doesn't use the application key for that endpoint does it?

*Tags: General Technical*

---

## 2021-01-22

**thambie1** - *17:06:46*

Thanks jptrader! Gonna have to start making listMarketCatalogue requests then. Follow up question, is making 100 simultaneous rest api requests considered bad behavior?

*Tags: General Technical*

---

## 2021-01-23

**Mo** - *13:23:11*

You’re not using flumine?

*Tags: General Technical*

---

**thambie1** - *13:23:46*

Wrote my own before I found out about bflw/flumine

*Tags: General Technical*

---

**thambie1** - *13:41:28*

Curious though, what in the above message tipped you off to me not using flumine?

*Tags: General Technical*

---

**Mo** - *13:41:53*

I was just asking because I believe flumine takes care of getting the market catalogues for you

*Tags: General Technical*

---

**thambie1** - *13:42:09*

Ah, time to read some flumine code :slightly_smiling_face:

*Tags: General Technical*

---

## 2021-01-24

**liam** - *13:58:34*

You can get that from bflw as well in metadata 

*Tags: General Technical*

---

**Taking Value** - *16:46:37*

Can I just check what the different intended functions is of bflw vs flumine. Broadly speaking is it that bflw is for data collection and that flumine is for placing orders and logging those orders?

*Tags: General Technical*

---

**river_shah** - *16:50:23*

bflw handles all the critical features for market and order streams and all other helper functions to communicate with betfair servers for market catalogues, runner meta data. flumine, builds on top of this core to provide a feature rich strategy design, backtesting and productionizing framework.

*Tags: Feature Engineering, Deployment, Strategies*

---

**river_shah** - *16:52:29*

cheers to [@U4H19D1D2](@U4H19D1D2) and all other contributors. before flumine, writing a robust strat from scratch was months. now it is a week (if you already have the business logic ideas worked out that is)

*Tags: General Technical*

---

**birchy** - *19:23:27*

I'm not really qualified enough to be specific, but as I understand it, bflw was the original implementation of the streaming API and users then had to consume the data &amp; JSON dicts themselves for handling strategies, bet placement, etc. That also means you have to do your own error handling, connection handling, etc. I believe Flumine has been developed on top of bflw in order to address those issues and create a much simpler interface for end users, particularly where strategy development and backtesting is concerned.

*Tags: Errors Debugging, Strategies*

---

## 2021-01-25

**Unknown** - *08:25:28*

bflw [https://github.com/liampauling/betfair/blob/master/HISTORY.rst#2120-2021-01-25|2.12.0](https://github.com/liampauling/betfair/blob/master/HISTORY.rst#2120-2021-01-25|2.12.0) now released, recommend testing first as I have refactored the streaming logic, everything is cached and reused if possible so there is the potential for side effects if you do not treat the data from streaming as read only (as you should). I and a few others have been using it with no issues and have backtested a couple thousand markets and everything is as expected.



As mentioned a few weeks back it is now rapid when backtesting (quicker than my naive golang backtester) also looking at about a 40-50% reduction in CPU when using live.

*Tags: Deployment*

---

**Jonjonjon** - *08:39:02*

Is it compatible with the latest flumine?

*Tags: General Technical*

---

**liam** - *09:23:34*

now released [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1160-2021-01-25|1.16.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1160-2021-01-25|1.16.0) (test first)

*Tags: General Technical*

---

## 2021-01-27

**Peter C** - *08:21:11*

Hi All - new here from more general value betting. Currently backtesting my first strategy and reading about the law of large numbers. Wondering if anyone could offer me their thoughts on what they would consider adequate backtesting? Currently testing with 400 cricket matches - c. 1500 matched bets, but this feels like not enough to overcome any variance.

*Tags: Strategies*

---

**Mo** - *08:31:24*

Depends exactly what markets you are betting on but most likely those 1500 are not independent so your effective sample size is the number of matches: 400. A suitable sample size will depend on the nature of the strategy (e.g. is it betting on favourites or longshots) but I agree, you probably want 10 or 100 times what you have in order to conclusively say whether you have something profitable. And that may be impossible given frequency (or lack thereof) of matches

*Tags: Strategies*

---

**Peter C** - *08:48:40*

Cheers for your thoughts. Please could you expand on suitable sample sizes based on strategy? I hadn't really thought about the difference but my initial thought was that with only two runners the variance in betting on favourites/longshots would take the same amount of time to come out given the pairing of the odds, but that's just a guess really.

*Tags: Strategies*

---

**Misha** - *08:54:24*

Lots of "trading". Have been watching it for the last couple of years. Franchise T20 is the target. If you have a really good model, you could make a lot of money. Data available is good. Matches are constrained. And if you look hard you might be able to get ball-by-ball for pretty much every franchise T20 match ever played. Would take some effort, but think what you could do with that!

*Tags: Strategies*

---

**Misha** - *09:48:46*

My first "foray" into sports stats was building a cricket stats database back in 2004. I had a database of every bit of data you could get from Test and ODI scoreboards, going back to the first Test in 1876 and the first ODI

*Tags: General Technical*

---

**Misha** - *09:56:53*

It was for a client, so when I moved on, the client kept the database

*Tags: General Technical*

---

**birchy** - *13:44:39*

Anybody here read [https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew-ebook/dp/B07SPWLYQJ|THIS](https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew-ebook/dp/B07SPWLYQJ|THIS) book and/or the second edition? Just wondering if it's any good for noobs?

*Tags: Strategies*

---

**liam** - *14:01:16*

```MARKET INFORMATION



For further information please see Rules &amp; Regs.



Who will finish 1st or 2nd in this race? NON RUNNERS DO NOT CHANGE THE PLACE TERMS. Should the number of runners be equal to or less than the number of places available as set out above in these rules all bets will be void. Betfair Non-Runner Rule applies. This market will turn IN PLAY at the off with unmatched bets (with the exception of bets for which the "keep" option has been selected) cancelled once the Betfair SP reconciliation process has been completed. Betting will be suspended at the end of the race. This market will initially be settled on a First Past the Post basis. However we will re-settle all bets should the official result at the time of the "weigh-in" announcement differ from any initial settlement. BETS ARE PLACED ON A NAMED HORSE. Dead Heat rules apply.```

*Tags: Errors Debugging, Strategies*

---

## 2021-01-28

**Jono** - *08:30:59*

Morning everyone. Im about to deploy an odds scraper and i would like to have an instance of the scraper running for each of the main markets of a given sport. Does logging into the api in separate instances cause problems? For example if i am recording the match_odds market and then start up another program that will login and begin recording the handicap market will one of the sessions terminate or expire etc?

*Tags: Deployment*

---

**Jono** - *09:58:12*

anything in the documentation that states exactly what you can and cant get away with in terms of retrieving the odds or pinging the api?

*Tags: General Technical*

---

**Peter C** - *10:00:13*

I've been reading and understand the concept of closing line value when betting pre off - but I'm not sure (and can't find anything) about the validity of this method for comparing value to in play betting. If a punter, for example, waited for a runner to come in x% and then backed them (assume this is a -eV bet), and then compared to the clv - are you going back and self selecting winners (now more likely to win because they've come in) and therefore skewing the value calc, or would you say that the punter who waited for them to come in first wasn't taking anywhere near full value, even though he makes a decision based on information that wasn't available at the time the clv price was taken?



I guess my question is is the clv valid if you're now running with information that wasn't available at the time the clv was taken.

*Tags: Strategies*

---

**liam** - *10:21:17*

I have been pushing them for more clarification but they only give [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Application+Keys#ApplicationKeys-Delay&amp;LiveApplicationKeysOverviewPBA|this](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Application+Keys#ApplicationKeys-Delay&amp;LiveApplicationKeysOverviewPBA|this), you should only by using your live key when placing orders, the problem is the delayed key is useless..

*Tags: Deployment*

---

**thambie1** - *10:23:09*

One thing I've gathered is that they're more concerned with rest API usage than streaming API. So focusing your scraping on streaming is likely safer.

*Tags: General Technical*

---

**Michael** - *10:43:15*

They won't let you scrape for long even streaming. Honestly it's not such a bad thing - you learn so much more from betting than you can from simulating. Just pick a sport, make up any crappy strategy and bet a few quid. That's almost always how it's done.

*Tags: Strategies*

---

**Alessio** - *13:22:35*

also, frequent scoring, the scarcity of goals in soccer is an interesting problem

*Tags: General Technical*

---

**Robert** - *15:18:48*

looking at the betfair historical data I downloaded from the betfair website + using the streaming api for parsing this -- I notice that total_matched in market book is missing most of the time -- is this expected?

*Tags: Data Quality*

---

**liam** - *15:27:42*

how are you parsing this? bflw?

*Tags: General Technical*

---

**liam** - *15:59:04*

```trading = betfairlightweight.APIClient("username", "password")



# create listener

listener = StreamListener(max_latency=None)



# create historical stream (update file_path to your file location)

stream = trading.streaming.create_historical_generator_stream(

    file_path="/tmp/BASIC-1.132153978",

    listener=listener,

)



# create generator

gen = stream.get_generator()



data = []

for market_books in gen():

    for market_book in market_books:

        data.append(market_book)```

*Tags: Performance, Strategies*

---

**Robert** - *16:51:46*

thanks for the help

*Tags: General Technical*

---

**river_shah** - *17:47:44*

```from flumine.strategy.strategy import DEFAULT_MARKET_DATA_FILTER```



*Tags: Strategies*

---

**river_shah** - *17:48:46*

can you please step through debug and check if you get sensible numbers here: `runner_book.ex.traded_volume: RunnerBook`

*Tags: Errors Debugging*

---

**Robert** - *18:43:38*

for the old data I have runner total_matched which is too small compared to the market total_matched (but dont see this problem with recent data)

*Tags: General Technical*

---

## 2021-01-29

**Misha** - *08:57:42*

I'm surprised that Mo bets on more matches than me - there basically isn't anything left apart from the ITF matches with no liquidity. I even bet on early ITF, which most people think is crazy (possible match fixing)

*Tags: Errors Debugging*

---

**Misha** - *09:02:53*

I got an exact total from my database of full results, and also from parsing all steam data from 2015

*Tags: General Technical*

---

**Mo** - *09:13:32*

There are definitely cases where the starting price spread is wide but it's worth betting in play; I understand why it creates problems for [@U016535QCJ2](@U016535QCJ2)'s approach but definitely worth finding another starting price you can use in my opinion

*Tags: Strategies*

---

**Misha** - *09:17:21*

I think I have a way to do it, but need to confirm (over a few months) the current model

*Tags: Strategies*

---

**Misha** - *10:46:25*

I thought this might be how my model works, but mostly I just hammer one side point after point. Can look ugly when there is a final set tiebreaker :wink:

*Tags: Strategies*

---

**Oliver Varney** - *12:22:58*

probably been asked before but i cant find it in the search, but in flumine on backtesting is there a function / attribute for profit?

*Tags: General Technical*

---

**birchy** - *14:31:54*

Can anyone confirm if [mailto:info@betfair.com|info@betfair.com](mailto:info@betfair.com|info@betfair.com) is still valid? Sent them an email but didn't get the usual automated response. This was for a follow-up to a CS question via their live chat/*mess*aging facility 5 days ago, on which they have not acted yet (I requested a lifetime P&amp;L report). Did speak to Neil a few days ago but it's not his department, although he did "escalate" it, but still no response or confirmation...

*Tags: Deployment*

---

**Alessio** - *19:02:58*

(the error was on my side to start, but still..)

*Tags: Errors Debugging*

---

**Dave** - *20:03:25*

Lack of delay for passive should help!

*Tags: General Technical*

---

## 2021-01-31

**Mo** - *08:21:07*

Let's also not overlook the importance of the extra information like knowing when points start (good for pulling bets) and when a player calls for a medical timeout (extremely important to be aware of for a fully automated strategy)

*Tags: Strategies*

---

## 2021-02-01

**Michael** - *10:37:08*

It depends how you're answering the question - would it increase my betting? Well yeah, because I'd have to increase my betting - do I want it? Probably not.

*Tags: Strategies*

---

**birchy** - *17:21:58*

That would be a decent tool for the bflw/flumine examples

*Tags: General Technical*

---

**Sam Asin** - *23:47:09*

i tried to make a little betfair strategy and managed to trade like $2k-$3k with myself

*Tags: Strategies*

---

## 2021-02-04

**Sam Asin** - *00:30:00*

unfortunately for now that's the only way I'm manually doing trades, sometimes I have a bflw code that does stuff but sometimes i just wanna make hand decisions.

*Tags: General Technical*

---

**Mo** - *07:00:21*

Yeah I had problems with the US president market but using average odds made them go away

*Tags: General Technical*

---

**Newbie99** - *09:02:16*

[@U016535QCJ2](@U016535QCJ2) yes good point, I had forgotten about that, also switching away from Chrome helped (but that might just be something on my local machine), other browsers seem to work better (e.g. Brave, even Edge)!

*Tags: General Technical*

---

**Mo** - *13:46:17*

```def market_book_to_data_frame(market_book: Union[MarketBook, dict]) -&gt; pd.DataFrame:

    if type(market_book) is MarketBook:

        market_book = market_book._data



    return pd.DataFrame(

        {

            'market_id': market_book['marketId'],

            'inplay': market_book['inplay'],

            'selection_id': runner['selectionId'],

            'side': side,

            'depth': depth,

            'price': price_size['price'],

            'size': price_size['size'],

            **({'publish_time': market_book['publishTime']} if 'publishTime' in market_book else {})

        }

        for runner in market_book['runners']

        for side in ['Back', 'Lay']

        for depth, price_size in enumerate(runner.get('ex', {}).get(f'availableTo{side}', []))

    )





def file_to_data_frame(path_to_betfair_price_file: str) -&gt; pd.DataFrame:

    trading = betfairlightweight.APIClient(username='', password='', app_key='')

    stream = trading.streaming.create_historical_generator_stream(

        directory=path_to_betfair_price_file,

        listener=StreamListener(max_latency=None, lightweight=True)

    )



    with patch("builtins.open", smart_open.open):

        g = stream.get_generator()

        return pd.concat(market_book_to_data_frame(mbs[0]) for mbs in g())```

*Tags: Feature Engineering, Performance, Strategies*

---

## 2021-02-05

**Paul McMahon** - *01:09:41*

Hi all, new kid question - I’ve been looking at a variety of different options, and I think i’ve settled on getting my own app key. I understand the cost is 200 quid - My query is how much “testing” and “development” against the delayed interface needs to be done before them handing over the real-time key?

*Tags: General Technical*

---

**Robert** - *08:28:15*

Hi, anyone know if it is possible to purchase subset of betfair historical data? If just wanted runner book total_matched from 2018 to 2020 say and was not interested in purchasing other columns?

*Tags: Data Quality*

---

## 2021-02-06

**Misha** - *05:15:05*

I can't speak for horse racing betting, and I'm only really talking about models based on stats (not market models), but absolutely nothing is better than understanding the sport

*Tags: Strategies*

---

**Unknown** - *08:09:39*

• US centric but an excellent introduction to understanding betting markets and how to think about value: [https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller-ebook/dp/B07RG6H8C2](https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller-ebook/dp/B07RG6H8C2)

• Seminal paper for modelling football (soccer): [https://betfairlightweight.slack.com/files/UBS7QANF3/F010NTJJK88/dixoncoles.pdf?origin_team=T4G9NBD2M&amp;origin_channel=C4HL6EZTQ](https://betfairlightweight.slack.com/files/UBS7QANF3/F010NTJJK88/dixoncoles.pdf?origin_team=T4G9NBD2M&amp;origin_channel=C4HL6EZTQ)

• A follow up on in-running football: [https://betfairlightweight.slack.com/files/UBS7QANF3/F010A4MK9MY/dixon1998.pdf?origin_team=T4G9NBD2M&amp;origin_channel=C4HL6EZTQ](https://betfairlightweight.slack.com/files/UBS7QANF3/F010A4MK9MY/dixon1998.pdf?origin_team=T4G9NBD2M&amp;origin_channel=C4HL6EZTQ)

• A classic horse racing paper: [https://www.gwern.net/docs/statistics/decision/1994-benter.pdf](https://www.gwern.net/docs/statistics/decision/1994-benter.pdf)

• An excellent starting point for tennis modelling: [http://www.strategicgames.com.au/PhD.pdf](http://www.strategicgames.com.au/PhD.pdf)

• Outstanding book on cognitive biases: [https://www.amazon.co.uk/Thinking-Fast-Slow-Daniel-Kahneman-ebook/dp/B005MJFA2W](https://www.amazon.co.uk/Thinking-Fast-Slow-Daniel-Kahneman-ebook/dp/B005MJFA2W)

• Excellent introduction to probabilistic forecasting in general: [https://www.amazon.co.uk/Signal-Noise-Art-Science-Prediction-ebook/dp/B0097JYVAU](https://www.amazon.co.uk/Signal-Noise-Art-Science-Prediction-ebook/dp/B0097JYVAU)

• Inspiration for how far you can go: [https://www.amazon.co.uk/Man-All-Markets-Beating-Street-ebook/dp/B07ZWJFYW5](https://www.amazon.co.uk/Man-All-Markets-Beating-Street-ebook/dp/B07ZWJFYW5)

If you can be more specific about what sport(s) I can recommend more papers

*Tags: Performance, Strategies*

---

**Misha** - *09:19:42*

Notwithstanding [@UBS7QANF3](@UBS7QANF3)'s excellent reading list, I have found that doing tennis we have used nothing from the literature out there (and we have read widely). One of our models uses linear regression, so using a general modelling technique, but other than that we have completely ignored any of the tennis ideas out there and decided to just tackle the problem solely by using our knowledge of tennis (my partner has played tennis at a high level for many decades). I realise that this certainly isn't an approach for everyone, but I wouldn't get too worried about needing to read too widely. Nothing beats getting some data and betting with low amounts to get a feel for things

*Tags: Strategies*

---

**Mo** - *09:56:15*

I agree with [@U016535QCJ2](@U016535QCJ2) both that the IID assumption is flat out wrong and that most academic papers and the particular one he’s referring to use laughably small sample sizes. However, all models are approximations of reality and I’ve never used a tennis one without the IID assumption. Hasn’t stopped my strategies from being wildly successful by anyone’s measure. Of course you don’t NEED to use academic papers to develop strategies but it’s hard to see how it can be in any way negative to read them provided you do so critically 

*Tags: Strategies*

---

**PeterLe** - *10:11:23*

Just out of interest [@UBS7QANF3](@UBS7QANF3) have you/do you play tennis, and do you have detailed knowledge of the game in the manner [@U016535QCJ2](@U016535QCJ2) describes above? Or do you solely employ a modelling and stats based approach? For my own part; whilst I have traded (gambled :grinning:) hundreds of thousands of horse racing markets, whilst I have an appreciation for the sport in general, I still know very little about it overall.

*Tags: Strategies*

---

**Misha** - *10:56:28*

Shows that there are many ways to approach the same problem :wink:

*Tags: General Technical*

---

## 2021-02-08

**Aaron Smith** - *13:57:22*

how do i go about running 2 instances of flumine?

*Tags: General Technical*

---

**liam** - *14:04:40*

available by default when running under a different [https://github.com/liampauling/flumine/blob/0b0bb9d8b7a2ca94c05996284692bccaede679c1/flumine/config.py#L8|host](https://github.com/liampauling/flumine/blob/0b0bb9d8b7a2ca94c05996284692bccaede679c1/flumine/config.py#L8|host)

*Tags: General Technical*

---

**Aaron Smith** - *14:28:19*

so for 2 diffrent instances on the same ec2 instance wouldnt work (without mass warnings) without changing flumine-code i guess?

*Tags: Deployment*

---

**liam** - *14:28:56*

use docker and it will be fine, but yeah you would have to

```flumine.config.hostname = "myuniqueuuid"```

*Tags: General Technical*

---

**liam** - *14:48:36*

Yeah a mixture of CPU and separating anything that involves a lot of computation on the process as it risks adding some latency to your other strategies as they are processed one by one. You also need to be aware that any orders are sent in the same request per update (read into that what you will)

*Tags: Performance*

---

**Jon K** - *15:32:19*

Hi all, Just got back into programming strategies using the Betfair API after a 10 year absence :slightly_smiling_face:

Quick question, is there a simple way of adding / subtracting tick values via the API ?

I remember I used to have a function that I could send any number at and say round up/down so e.g. if I sent it 5.07 it would send back 5.1 (as that is an accepted tick value for Betfair).

I don't really want to re-invent the wheel again if this functionality now exists via the API, any ideas ?

*Tags: General Technical*

---

**Mo** - *15:40:26*

[https://github.com/liampauling/flumine/blob/0b0bb9d8b7a2ca94c05996284692bccaede679c1/flumine/utils.py#L103-L113](https://github.com/liampauling/flumine/blob/0b0bb9d8b7a2ca94c05996284692bccaede679c1/flumine/utils.py#L103-L113)

*Tags: General Technical*

---

## 2021-02-09

**thambie1** - *16:55:28*

May want to hire some help soon, plus I'm a US citizen which complicates things.

*Tags: General Technical*

---

## 2021-02-10

**Michael** - *14:48:58*

You can just hire your help 'retail' [@U01DCR5PXDY](@U01DCR5PXDY) - on occasions when I've hired specialists that's what I've done - it's just a simple free lance contract like getting someone to tile your bathroom.

*Tags: General Technical*

---

**thambie1** - *15:00:36*

I'm thinking of hiring people on a more ongoing basis  where you really have to be careful of how you classify them (freelancer vs employee) or you risk breaking laws/regulations. I think hiring a freelancer under an umbrella company would be fine, but I want to get advice to make sure.

*Tags: Deployment*

---

**Michael** - *15:18:30*

Yes I understand that and I've absolutely no notion of how you tackle the US taxation thing. If I were in our shoes I think I'd be a bit concerned about potentially making an entirely legitimate activity open to question by adding layers of complexity. A lot of freelancers have their own limited companies and employ themselves, tax issues aside I'd prefer that to owning the company and hiring the person. Ideally you don't want to own a company that doesn't function like a normal company (customer is also beneficial owner) if you can avoid it.

*Tags: General Technical*

---

## 2021-02-11

**Jonjonjon** - *14:00:08*

I guess that's my next pre-event strategy sorted.

*Tags: Strategies*

---

## 2021-02-13

**Ryan** - *04:51:05*

Are you coding in C#? Most of the discussion here seems to be around python.

*Tags: General Technical*

---

**Misha** - *05:13:07*

Yes. On using the Python libraries I don't comment, But there is a lot of other general discussion here that is worthwhile, and when you work solo or in a team of two, it's good to see other ideas

*Tags: General Technical*

---

**Stefan** - *08:44:25*

[@U01NDEXURBK](@U01NDEXURBK) most of developers here use different programming languages. My bots are programmed in C# and F# so .net programming languages, but I use R and python to integrate ML to my F# betfair bots.

*Tags: General Technical*

---

**Newbie99** - *21:03:58*

[https://github.com/liampauling/flumine/blob/master/flumine/utils.py](https://github.com/liampauling/flumine/blob/master/flumine/utils.py)

*Tags: General Technical*

---

## 2021-02-14

**liam** - *13:56:39*

streaming

*Tags: General Technical*

---

**liam** - *13:57:27*

[https://liampauling.github.io/betfair/streaming/](https://liampauling.github.io/betfair/streaming/)

*Tags: General Technical*

---

**klozovin** - *17:04:21*

when streaming, what does _bflw_ do if there's no response from Betfair for longer than `heartbeatMs` ?

*Tags: General Technical*

---

**tobsve** - *19:53:39*

Hi guys! I assume this question has been asked before, but I just can't get a grip on the [https://github.com/liampauling/betfair.git|locale](https://github.com/liampauling/betfair.git|locale) as I am from sweden, i need to change go API request to .se but I just can't understand what variable I should pass with [https://github.com/liampauling/betfair.git|locale](https://github.com/liampauling/betfair.git|locale) Tried with SE, .SE, sweden, swe etc

*Tags: General Technical*

---

**tobsve** - *20:06:59*

aha, okey. then I was lucky then as I could find the answer of my question in the log :slightly_smiling_face:

*Tags: General Technical*

---

## 2021-02-15

**Sam Asin** - *03:06:54*

no worries if i'm asking too many dumb questions haha

*Tags: General Technical*

---

**Sam Asin** - *03:09:32*

gotcha, my question about those mechanics was specific to tennis since its the one you know, just trying to be sure i understand it correctly. (bids and offers go away while the ball is in play, being hit back and forth, and then after a point is won, before the next begin, bids and offers come in)

*Tags: General Technical*

---

**Sam Asin** - *03:14:49*

Oh man, I probably need to think about this a bit more just to get to the point where I can follow your explanations well. I may ask ya some more questions later, thanks for trying to explain it to me Misha!

*Tags: General Technical*

---

**Misha** - *03:30:39*

Everyone does their own "strategy"

*Tags: Strategies*

---

## 2021-02-16

**Jono** - *08:09:05*

Morning everyone im trying to access deposit and withdrawal info from various bf accounts. I was wondering the best way to do this. Ideally id like to pull through a list of the recent account transaction information excluding money placed on bets. What is the best way for me to do this? Iterating to test out different end points is taking about an hour an attempt due to things out of my control so was wondering if there is an already established best method. Any help is greatly appreciated Thanks

*Tags: General Technical*

---

**Mo** - *08:19:59*

If you're using bflw then make sure to use lightweight mode

*Tags: General Technical*

---

**Jon K** - *12:43:53*

Quick question, I'm just using the following filter to get the best prices:

price_filter = betfairlightweight.filters.price_projection(

    price_data=['EX_BEST_OFFERS']

)



However, if I look on the site itself, they're not the best prices.

I take it, this is Betfairs Cross Matching offering the prices on the site ?

*Tags: General Technical*

---

**Robert** - *12:59:44*

Im looking at the runner_matched in the historic data.  I asked betfair about why sum runner matched =/= total matched and they asked me to provide example.  Could someone explain to me or point me to documentation to how the data we download from betfair historic works and how you calculate the total_matched and runner_matched from this data.

*Tags: General Technical*

---

**Robert** - *13:00:22*

I can see from the streaming that they are not adding up but need to make sense of where this fails.  If its their raw data need to extract an example.

*Tags: General Technical*

---

**liam** - *13:01:52*

[https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/cache.py#L224|market](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/cache.py#L224|market)

*Tags: General Technical*

---

**liam** - *13:02:10*

[https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/cache.py#L233|runner](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/cache.py#L233|runner)

*Tags: General Technical*

---

**Unknown** - *13:24:28*

I got an error when I added  EX_BEST_OFFERS_DISP, I think that's just for streaming ?

I found an extra option called virtualise though for the Price Filter, doesn't seem to make much difference though.

I'll do some more digging though, thanks for your help.

*Tags: Errors Debugging*

---

**Mo** - *13:25:23*

And start using streaming

*Tags: General Technical*

---

**Angelos** - *17:17:14*

hey, does anyone know if we got anything that would help in terms of figuring out which runner points at what kind of bet?



ie. i’m working towards creating an enum with all the possible bets and implementing something like this:



```get_odds(market_catalogue, MATCH_ODDS, HOME_WIN, BACK) -&gt; List[PriceSize]```

wonder if the library already has something like this

*Tags: General Technical*

---

**Jonjonjon** - *21:28:21*

Has anyone tried using this with flumine backtesting?



[https://medium.com/distributed-computing-with-ray/how-to-scale-python-multiprocessing-to-a-cluster-with-one-line-of-code-d19f242f60ff](https://medium.com/distributed-computing-with-ray/how-to-scale-python-multiprocessing-to-a-cluster-with-one-line-of-code-d19f242f60ff)

*Tags: General Technical*

---

## 2021-02-17

**liam** - *07:58:50*

This isn’t something we would add to bflw as it is just a wrapper to the API but sounds like something handy for flumine 

*Tags: General Technical*

---

**Alessio** - *08:22:56*

eexactly. now I tried Ray ~ a few months ago, but the main problem is that if you get an exception you are not guaranteed to understand where it happened

*Tags: Errors Debugging*

---

**Angelos** - *08:26:14*

oh wow. flumine is where i am trying to get at! great work!

*Tags: General Technical*

---

**Jono** - *10:10:16*

Hi there. Ive been messing around with pulling through account statements using bflw but am occasionally running into problem an INVALID_APP_KEY error. Do you know if the delayed key is unable to access this endpoint/resource for some reason or should this not affect account retrieval?

*Tags: Errors Debugging*

---

**Jono** - *10:17:12*

ah right no problem, also do you ever find yourself manually constructing the the request you send to bf eg:



`{"jsonrpc": "2.0", "method": "SportsAPING/v1.0/listClearedOrders", "params": {"betStatus":"SETTLED", "groupBy":"BET", "betIds": ['bet_id']}, "id": 1}`

*Tags: General Technical*

---

**Mo** - *10:19:19*

Never, why can't you just use bflw?

*Tags: General Technical*

---

**Jono** - *10:20:57*

was wondering why in your initial replies to my question about account retrieval you said "If you're using bflw..." implying i might be accessing the api some other way

*Tags: General Technical*

---

**Mo** - *10:21:48*

Ah right, well some people on here are not Python developers and don't use bflw but I would still expect them to use a library rather than constructing the requests by hand

*Tags: General Technical*

---

**Jono** - *10:23:01*

ok no problem, thanks for the help

*Tags: General Technical*

---

**Robert** - *10:55:02*

For anyone interested in why the historical data for runner total_matched is not correct whereas the live API is, here is my response from betfair:

```The total cumulative traded volume per runner (at all prices) isn't included in the PRO data set, unlike in the live Stream API.



For a single price point, 'tv' is cumulative, so each "trd" update contains the price and cumulative traded volume traded so far at that price.  The final sum of the 'tv' at each price point for each selection will be equal to the market traded volume.



"op":"mcm","clk":"9702272675","pt":1563905629308,"mc":[{"id":"1.160659053","rc":[{"trd":[[4.1,11.11]],"ltp":4.1,"tv":11.11,"id":9938916},{"atb":[[4.1,9.93]],"id":9938916}],"con":true,"img":false}]}

{"op":"mcm","clk":"9702305064","pt":1563905813250,"mc":[{"id":"1.160659053","rc":[{"trd":[[4.1,31.31]],"ltp":4.1,"tv":31.31,"id":9938916},{"atb":[[4.1,0]],"id":9938916},{"atl":[[4.1,4.9]],"id":9938916}],"con":true,"img":false}]}

{"op":"mcm","clk":"9703398965","pt":1563917405197,"mc":[{"id":"1.160659053","rc":[{"trd":[[4.1,37.64]],"ltp":4.1,"tv":37.64,"id":9938916},{"atl":[[4.1,1.74]],"id":9938916},{"spn":6.6,"spf":1.0,"id":11110173}],"con":true,"img":false}]}

The runner tv ("tv") represents the cumulative traded amount at the last price traded (ltp) or cumulative amount of all 'trd' amounts in the update if multiple prices are included.```

*Tags: Data Quality, Deployment*

---

**Jon K** - *14:28:35*

Does anyone know any 'total idiots guide' to streaming using Python based on this page ?

[https://github.com/liampauling/betfair](https://github.com/liampauling/betfair)

*Tags: General Technical*

---

**Mo** - *14:29:08*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**liam** - *14:29:23*

[https://liampauling.github.io/betfair/streaming/|docs](https://liampauling.github.io/betfair/streaming/|docs) not good enough?

*Tags: General Technical*

---

## 2021-02-18

**KG** - *06:38:35*

hey guys. has anyone used bflw to read in the PRO historical data sets from their TAR form? I'm following the [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py|sample code](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py|sample code) and getting the following error, which I assume is due to the TAR format?



```UnicodeDecodeError: 'charmap' codec can't decode byte 0x90 in position 569: character maps to &lt;undefined&gt;```

*Tags: Data Quality, Errors Debugging*

---

**KG** - *07:09:20*

thanks [@UBS7QANF3](@UBS7QANF3) - I amended with the patch, still getting the same error though - any chance you can cast your eyes over my code and see if you can catch my mistake? I'm not a python native, but trying to get the library to work:



```import logging



from unittest.mock import patch

import smart_open



import betfairlightweight

from betfairlightweight import StreamListener



# setup logging

logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



# create trading instance (don't need username/password)

trading = betfairlightweight.APIClient("username", "password")



# create listener

listener = StreamListener(max_latency=None)



# create historical stream (update file_path to your file location)

stream = trading.streaming.create_historical_generator_stream(

    file_path="../_data/2021_01_JanRacingPro.tar",

    listener=listener,

)



with patch("builtins.open", smart_open.open):   

    gen = stream.get_generator()



    for market_books in gen():

        for market_book in market_books:

            print(market_book)```

*Tags: Getting Started, Errors Debugging, Performance, Strategies*

---

**Angelos** - *08:22:07*

hi there, i really like the strategy builder in [https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py|flumine](https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py|flumine) and i’m giving a go now!



i’m wondering if there’s a way to configure it to *not use* the streaming API and run the framework on a fixed interval instead?



alternatively, is there a way to configure the stream to trigger the fetch every minute?

*Tags: Errors Debugging, Strategies*

---

**Mo** - *08:26:55*

Use the conflate_ms parameter to set the frequency of streaming updates 

*Tags: General Technical*

---

**liam** - *08:54:39*

yeah use conflate and you can set the `streaming_timeout` to 60 so that you can an update every 60s [https://liampauling.github.io/flumine/advanced/#parameters|regardless](https://liampauling.github.io/flumine/advanced/#parameters|regardless)



```strategy = ExampleStrategy(

    market_filter=streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB"],

        market_types=["WIN"],

    ),

    conflate=5000,  # 5s

    streaming_timeout=60,  # 60s (should probably change this to match conflate)

) ```

*Tags: Strategies*

---

**Alessio** - *10:31:29*

Opta, Sportradar and Statsbomb are the only 3 i know of that do live data delivery (wyscout and instat don't)

*Tags: Deployment*

---

**Alessio** - *10:36:48*

[https://insidersport.com/2021/02/09/genius-sports-launches-lawsuit-against-sportradar/|I think they had a dispute with Sportradar even ... they cannot even ](https://insidersport.com/2021/02/09/genius-sports-launches-lawsuit-against-sportradar/|I think they had a dispute with Sportradar even ... they cannot even )

*Tags: Deployment*

---

**Alessio** - *10:45:48*

no, everybody has their own, and that's why you need to make sure your live data and historical data are from the same provider :stuck_out_tongue:

*Tags: Data Quality, Deployment*

---

**John A** - *10:49:02*

What’s the delay on in football data, that’s the problem. Is it just someone watching the game on sky?

*Tags: General Technical*

---

**river_shah** - *10:53:33*

Thanks for the input [@U0128E7BEHW](@U0128E7BEHW) While the public endpoint is good, I am hesitant to use it. Complexity in terms of data capture, one has to build the data archive over time and it feels that one is always one step away from evoking betfair’s ire. Sometimes throwing money at the problem is a good enough solution (so betgenius can enjoy their monopolistic rent collection).

*Tags: General Technical*

---

## 2021-02-19

**KG** - *06:24:26*

hey guys, I've been playing with using bwlw to parse the historical data sets. am I right in my understanding that, although the historic data files include runner and market name, because the stream API doesn't include the names bflw doesn't support pulling the names out of the historic files? very happy to be corrected if my understanding is wrong here, otherwise keen to hear if anyone has a work around or if there are plans to support it in the package in the future?

*Tags: Data Quality*

---

**liam** - *06:32:43*

And same with [https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/resources/streamingresources.py#L32|runner](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/resources/streamingresources.py#L32|runner) name 

*Tags: General Technical*

---

**captainonionhead** - *08:04:13*

Morning!  I guess I'm coming to the betfair game a little late but better late than never :slightly_smiling_face:  I'm having trouble keeping up with markets using the delayed key though and have a slightly noob question.

I've seen it suggested that I can get my key for the live feed activated as long as I'll be placing a "reasonable" number of bets?  What sort of magnitude is considered "reasonable"?  Until I have more experience, I don't want to be risking huge sums and I have no innate sense of what people consider to be large or small bets and volumes?

Many thanks!  - Jon

*Tags: Deployment*

---

**captainonionhead** - *08:09:38*

Thanks Mo - I guess I'd expected that there wasn't a fixed boundary for this but was wondering if it would be problematic if I was to be making minimum-sized bets until I'd got a better feel for how everything moves or if the volume of bets I made was relatively small whilst I'm developing and testing.

*Tags: Errors Debugging*

---

**Mo** - *08:10:50*

All we can definitely say is that zero bets would be problematic

*Tags: General Technical*

---

**Angelos** - *08:18:16*

hey, i’ve started using flumine a few days ago. is it possible to unsubscribe from an event_id within my strategy? for example, if the match gets passed the 65' mark i would like to stop the stream for that particular event

*Tags: Strategies*

---

**Oliver Varney** - *08:20:42*

[@U019Z6V6DHS](@U019Z6V6DHS) can you not just have logic within check_market_book function to return False if your strategy doesnt want to process the market_book

*Tags: Strategies*

---

**Oliver Varney** - *08:23:05*

I think unsubbing will not be worth it if you run multiple strategies on one instance, for example one strategy might only need the first 65 mins the second strategy the last 25-30 mins. You start to cap yourself unnecessarily, making things messy down the line when you add more and more strategies.

*Tags: Strategies*

---

**Aaron Smith** - *08:45:34*

[@U01NJ85MP7F](@U01NJ85MP7F) basic / advanced historical data is pretty much useless and pro data is costly. I d rather subscribe to a market and put the money in bets instead of buying the pro-data with it. Having a strategy running will probs give you a better feel for the market and even if your "strategy" sucks, as long as your placing bets on a runner with low price spread, you wont lose a lot. Even if you place low stake bets at random, you ll win some and lose some and it would take some time until you actually reach what you would ve paid for the pro data set

*Tags: Data Quality, Strategies*

---

**Michael** - *14:06:10*

I didn't try too hard with finding the error.

*Tags: Errors Debugging*

---

**Oliver Varney** - *15:32:21*

pre off stuff I had in mind. It will be the highest weighted coefficient in the model most likely, and at the start of betting I think its a fairly safe assumption to say that there will be volume available at initial stake sizes, especially closer to the off

*Tags: Strategies*

---

**liam** - *16:16:31*

mentioned because recently scraping / python has been used, not sure on the profitability of some of the guys on there but probably worth a skim

*Tags: General Technical*

---

**liam** - *16:32:03*

not sure I agree on this, you have a 'super-good' model which ideally tells you where the value is, why when it doesn't detect value would you find a random forecast and use it? Seems very random to me

*Tags: Strategies*

---

**Mick** - *16:35:01*

In general if you have multiple imperfect forecasters of something then employing a weighted sum of their estimates will yield greater accuracy than simply choosing the best of them. You can tune the weights from historical data.

*Tags: Data Quality*

---

**William Martin** - *19:10:09*

thanks! very new to automated stuff, used trade football markets in college a good few years ago, with a lot of success but got to time consuming with no automation and when I finished college I didnt have the free time I did before!



I'm looking to build a prototype of an app idea I have so not 100% sure if I am in the right place but eager to learn what people are doing with the betfair API and the bot trading aspect is really interesting

*Tags: Getting Started, Strategies*

---

**Unknown** - *19:19:40*

I don't think using a worse model as a tie breaker is a good idea :scream:

*Tags: Strategies*

---

**Oliver Varney** - *19:25:26*

[@U01N8P7UFPB](@U01N8P7UFPB) flumine is a great place to start I think (thanks to [@U4H19D1D2](@U4H19D1D2) ). it provides both live trading and backtesting functionality and creating strategies is easy. Id play around with it look at the code and ask questions in here. As far as I know theres a mix of sports in here but mostly horses, tennis and football (soccer)

*Tags: Deployment, Strategies*

---

**William Martin** - *19:50:23*

noob question but I am trying to pull in premier league matches but for the life of me I can't find the ID for the Premier League, is there a place in the betfair documentation that has a list of the different league and market IDs?

*Tags: General Technical*

---

**Oliver Varney** - *19:52:19*

have you looked at the API documentation: [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listEvents](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listEvents)

*Tags: General Technical*

---

**Oliver Varney** - *19:54:26*

then have a look at the betting endpoint in the betfairlightweight repo

*Tags: Strategies*

---

**Oliver Varney** - *19:55:01*

[https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/endpoints/betting.py#L10](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/endpoints/betting.py#L10)

*Tags: Strategies*

---

## 2021-02-20

**Mick** - *09:27:53*

I wish I'd never mentioned the tie breaking idea... but do you dispute my more general point, namely "if you have multiple imperfect forecasters of something then employing a weighted sum of their estimates will yield greater accuracy than simply choosing the best of them. You can tune the weights from historical data."

*Tags: Data Quality*

---

**Dave** - *09:48:52*

Ensembles are a very popular modelling technique yes.

*Tags: Strategies*

---

**liam** - *18:16:53*

Not populated when streaming 

*Tags: General Technical*

---

**Richard** - *20:45:49*

Hi everyone, long time lurker first time poster. How would I get a rolling 2 minute window of total volume traded for each runner every 30 seconds on the streaming data?

*Tags: Getting Started*

---

## 2021-02-21

**river_shah** - *12:42:22*

sometimes flumine fails to connect (not enough connections available for example). how / where do I set retry timeout please?

*Tags: General Technical*

---

**Peter C** - *13:03:54*

To add on to the list discussion from above, am I right in thinking that if you were adding items to a list and streaming more than one market at once, for example pushing the most recent price from each runner to a list, that each runner[i] from each market would end up in the same list? If so, how do you get round this?

*Tags: General Technical*

---

**Peter C** - *13:09:03*

Thanks for the pointer. I ran into this problem for the first time today so good timing!

*Tags: Getting Started*

---

**river_shah** - *14:21:31*

what is the graceful way to tell the framework to shutdown please? I have a handle to the framework in the strategy. Calling this causes errors

```self.framework._process_end_flumine()

self.framework.__exit__()

CRITICAL Unknown error API logout FAIL: INPUT_VALIDATION_ERROR```



*Tags: Errors Debugging, Strategies*

---

**liam** - *14:28:13*

TerminationEvent [https://github.com/liampauling/flumine/blob/master/examples/workers/terminate.py|https://github.com/liampauling/flumine/blob/master/examples/workers/terminate.py](https://github.com/liampauling/flumine/blob/master/examples/workers/terminate.py|https://github.com/liampauling/flumine/blob/master/examples/workers/terminate.py)

*Tags: General Technical*

---

**Pierino S** - *20:46:52*

Hi how's everyone ? I just downloaded the betfairlightweight and got it working in Python, its all very cool.

*Tags: General Technical*

---

**Pierino S** - *20:49:54*

Am wondering if its possible to get it feeding directly into Google Sheets ? Am not at all versed in Python and am confident with Google Sheets. I'm really expert in Excel, but am warming to Google Sheets with all its collaborative and web-based functionality which do out-smart Excel.

*Tags: General Technical*

---

**birchy** - *20:54:47*

Sounds like hard work to me. Be easier to learn python in the long run, particularly given that you'll need to learn it to be able to develop strategies in bflw/Flumine anyway.

*Tags: General Technical*

---

**Pierino S** - *21:06:18*

ok did not see this Flumine strategies so will load and see how i get on with that one. thks.

*Tags: General Technical*

---

**Beeblebrox** - *21:51:15*

There is a Python library for reading and writing to Google sheets if you really want to go down that route: [https://www.twilio.com/blog/2017/02/an-easy-way-to-read-and-write-to-a-google-spreadsheet-in-python.html](https://www.twilio.com/blog/2017/02/an-easy-way-to-read-and-write-to-a-google-spreadsheet-in-python.html)

*Tags: General Technical*

---

## 2021-02-22

**Michael** - *08:59:10*

[@U016TGY3676](@U016TGY3676) is right except that it's probably easier in the short run too. I'd suggest you learn a bit of python and read up on Pandas. You can do a lot of analysis really easily with Pandas in Jupyter notebook and there's tonnes of tutorials available.

*Tags: Feature Engineering*

---

**Oliver Varney** - *10:49:07*

Morning chaps, just looking into improving exception handling / logging /performance monitoring etc, and I know [@U4H19D1D2](@U4H19D1D2) and other use sentry. Other then their site is there any good resources people would recommend?

*Tags: Errors Debugging, Performance*

---

**Oliver Varney** - *10:53:25*

do you use the performance monitoring stuff

*Tags: Performance*

---

**liam** - *10:53:54*

i then use the slack plugin so I get notifications on errors, I have done but it was in beta and started slowing things down for me so i left it but it was very cool

*Tags: Errors Debugging, Performance*

---

**Oliver Varney** - *10:56:06*

are you logging betfair trade errors responses and stuff along these lines?

*Tags: Errors Debugging*

---

**liam** - *10:56:55*

i set level to error



```    if sentry:

        # sentry

        sentry_logging = LoggingIntegration(

            level=level,  # Capture info and above as breadcrumbs

            event_level=event_level,  # Send errors as events

        )

        sentry_sdk.init(

            dsn="",

            integrations=[sentry_logging],

            release="flumine-prod@{0}".format(version),

            environment=environment,

        )

        with sentry_sdk.configure_scope() as scope:

            scope.user = {"id": instance_id}

            scope.set_tag("application", instance)

            scope.set_tag("flumine", flumine.__version__)```

*Tags: Errors Debugging*

---

**Oliver Varney** - *10:59:24*

cool cheers :raised_hands: and are you also sending stuff up from try and except blocks or simply still using logging and it is captured by sentry via logging ? apologies if these are basic silly questions, just trying to get a sense of how to use it best

*Tags: General Technical*

---

**Unknown** - *11:01:43*

see [https://github.com/liampauling/flumine/blob/0b0bb9d8b7a2ca94c05996284692bccaede679c1/flumine/execution/betfairexecution.py#L205|here](https://github.com/liampauling/flumine/blob/0b0bb9d8b7a2ca94c05996284692bccaede679c1/flumine/execution/betfairexecution.py#L205|here) to how you can get the full traceback, ie. flumine will give it to you by default on all errors, just don't set level to info or warning as it will just kill your CPU (learnt that the hard way!)

*Tags: Errors Debugging*

---

**river_shah** - *11:54:27*

also, how do I access total `transaction_count` from the framework please? I see the following code, but unsure how to access `self.framework.?transaction_count`:



```class MaxOrderCount(BaseControl):



    """

    Counts and limits orders based on max

    order count.

    Only prevents placeOrders if limit is

    reached.

    """



    NAME = "MAX_ORDER_COUNT"



    def __init__(self, flumine, client: BaseClient):

        super(MaxOrderCount, self).__init__(flumine)

        self.client = client

        self.total = 0

        self.place_requests = 0

        self.cancel_requests = 0

        self.update_requests = 0

        self.replace_requests = 0

        self._next_hour = None

        self.transaction_count = 0```



*Tags: General Technical*

---

**liam** - *14:49:07*

Just released flumine [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1170-2021-02-22|v1.17.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1170-2021-02-22|v1.17.0), contains some potential breaking changes (controls now per order and `strategy.*_order` to be depreciated in 1.18.0) so recommend reading the changelog and testing before upgrading however this upgrade has a lot of improvements including user configurable order placement:



```market.place_order(order)  # as per strategy.place_order



with market.transaction() as t:

    market.place_order(order)  # executed immediately in separate transaction

    t.place_order(order)  # executed on transaction __exit__



with market.transaction() as t:

    t.place_order(order)

    ..

    t.execute()  # above order executed

    ..

    t.cancel_order(order)

    t.place_order(order)  # both executed on transaction __exit__```

*Tags: Strategies*

---

**Mo** - *16:43:48*

[https://www.betfair.com/aboutUs/Betfair.Charges/#TranCharges2](https://www.betfair.com/aboutUs/Betfair.Charges/#TranCharges2)



&gt; *What is a failed transaction?*

&gt; An action that leads to an API error, resulting in a failed bet placement, failed cancellation or other failed transaction. Full descriptions of each failed transaction type can be found in the link below:

&gt; [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Enums#BettingEnums-ExecutionReportErrorCode](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Enums#BettingEnums-ExecutionReportErrorCode)

*Tags: Errors Debugging, Strategies*

---

**river_shah** - *17:55:03*

[@U4H19D1D2](@U4H19D1D2) having just freshly paid £200 in txn charges, would it be possible to expose `transaction_count`  to the framework / strategy. need to back off doing stupid actions on edge cases that were not seen and/or are hard to replicate in backtests

*Tags: Strategies*

---

**Lee** - *17:56:26*

would `self.client.chargeable_transaction_count` help?

*Tags: General Technical*

---

**river_shah** - *18:03:13*

My global account loss limit was set too low so a lay bet kept meeting validation inside flumine but betfair would reject, the algo was unaware of bf rejection and tried to insert the bet again and again. to top it off I was on a coffee walk and did not see the flumine warning logs about too many transactions

*Tags: General Technical*

---

**river_shah** - *18:03:33*

`"status": "FAILURE", "errorCode": "LOSS_LIMIT_EXCEEDED"`

*Tags: Errors Debugging*

---

**birchy** - *20:25:42*

Has anyone ever had any problems with scraping data from the spurtingwife website? This is more about being IP banned or throttled rather than actual code issues. I've always been courteous and used a `sleep(1)` but am wondering if that's actually necessary?

*Tags: General Technical*

---

## 2021-02-23

**Oliver Varney** - *06:50:47*

in terms of flumine [@U4H19D1D2](@U4H19D1D2) what is the expected behaviour, for the streams to shutdown?

*Tags: General Technical*

---

**liam** - *09:19:26*

fix inbound [https://github.com/liampauling/flumine/pull/376](https://github.com/liampauling/flumine/pull/376)

*Tags: Errors Debugging*

---

**river_shah** - *09:45:01*

do you have any example how I can grab a pandas df or csv out of this please?

*Tags: Feature Engineering*

---

**mandelbot** - *11:25:36*

are `max_order_exposure` and `max_selection_exposure` judged per strategy or across all strategies?

*Tags: Strategies*

---

**liam** - *11:36:09*

the former is per order and latter per strategy, but you can create your own control to do the latter if you require

*Tags: Strategies*

---

**Oliver Varney** - *11:40:46*

just trying to scope out how to efficiently use logs / sentry better, at INFO level there are many request that it could get lost in

*Tags: General Technical*

---

**liam** - *11:41:25*

invalid bet size would be picked up at the control level which would be an error

*Tags: Errors Debugging*

---

## 2021-02-24

**Jorge** - *08:23:22*

Hi, I'm using Order Streaming in bflw and I'd like to implement a hearbeat that tells my bots if the Streaming is working. How do you guys parse the HEARTBEAT messages?

*Tags: General Technical*

---

**liam** - *08:25:06*

create your own listener or patch so that you can update the `stream.on_heartbeat` [https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/stream.py#L46|func](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/stream.py#L46|func)

*Tags: General Technical*

---

**liam** - *08:26:21*

but when failing the stream will be under a [https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/listener.py#L208|503](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/listener.py#L208|503) `listener.status` or connection will not be possible

*Tags: General Technical*

---

**Oliver Varney** - *08:36:11*

nar flumine save my ass now

*Tags: General Technical*

---

**Oliver Varney** - *08:36:45*

orders are cached by flumine meaning I wont keep resending

*Tags: General Technical*

---

**Oliver Varney** - *08:38:34*

might be a code rework [@U0155J92A7Q](@U0155J92A7Q) but id recommend looking at flumine

*Tags: General Technical*

---

**Jorge** - *08:44:04*

Yep I should invest some time in learning how flumine works, thanks for the tip:smile:

*Tags: General Technical*

---

**river_shah** - *09:51:52*

How do I get the output from the accountstatememt all transactions using bflw or flumine please? Sorry struggling to figure out how to do this. Basically the output that lives here in the web gui: [https://myaccount.betfair.com/summary/accountstatement](https://myaccount.betfair.com/summary/accountstatement)

*Tags: Deployment*

---

**Ian** - *11:18:04*

Hi all, does anyone know if there is a resource that can provide market meta data for historical market ids? Specifically racing - looking to add certain points such as distance, race code, class and if possible going to my flumine collected streams. The BF basic historical data does not seem to contain this.

*Tags: Data Quality*

---

**liam** - *11:28:32*

FYI flumine [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1171-2021-02-24|1.17.1](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1171-2021-02-24|1.17.1) released with transaction control fix and `blotter.strategy_selection_orders` function added

```{

    "asctime": "2021-02-24 11:00:54,794",

    "levelname": "INFO",

    "message": "Execution new hour",

    "filename": "clientcontrols.py",

    "funcName": "_check_hour",

    "module": "clientcontrols",

    "process": 6,

    "threadName": "MainThread",

    "current_transaction_count_total": 387,

    "current_transaction_count": 357,

    "current_failed_transaction_count": 30,

    "total_transaction_count": 831,

    "total_failed_transaction_count": 61,

    "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7fd48db7adc0&gt;"

}```

*Tags: Errors Debugging, Strategies*

---

**river_shah** - *13:08:29*

`AttributeError: 'Flumine' object has no attribute 'config'` where do I need to change  the hostname so that two instances of the framework running on the same host don’t trample on each other’s orders please?

*Tags: Errors Debugging*

---

**liam** - *13:08:59*

use containers :wink: / flumine != Flumine

*Tags: General Technical*

---

**liam** - *13:09:16*

```from flumine import config```

*Tags: General Technical*

---

**river_shah** - *13:26:32*

simple. this works. no more warnings one host and container.

```hostname = "{}_{}".format(socket.gethostname(), instance_id)

flumine.config.hostname = hostname```



*Tags: General Technical*

---

**river_shah** - *13:28:19*

instance_id is just my internal model reference id so I can do live A/ B tests. this remains persistent through time

*Tags: Deployment, Strategies*

---

**Pierino S** - *21:50:39*

Yes, I'd not seen that video, but he has a similar one on his Twitter, with lots of big greens. I have seen him refer a lot to scalping, so do think that's a big part of his philosophy. I've never seen what bank he has, I think you could figure it out, as he definitely shows individual market big greens, so with the Betfair historical data, you could guess-timate what he was up to.........Are there any other Top Drawer traders out there ?

*Tags: Data Quality*

---

## 2021-02-25

**Mo** - *05:37:57*

Personally I would like to help but it’s not worth it to me to jeopardise my relationship with Betfair by redistributing data I don’t have the rights to

*Tags: General Technical*

---

**Misha** - *07:01:22*

What is a fast feed? Given that anyone can get the streaming data with the same delay

*Tags: General Technical*

---

**Aaron Smith** - *09:44:03*

His profits may be legit, but thats about it. If you sell your ideas for 2.5k, thats only really worth it when you expect those ppl and however more they may spread the ideas will not make more than 2.5k with it, otherwise it would hurt his buisness more than it helped him. He is basically running a course where he doesnt want you to actually suceed, i m certain he is not giving away his true edge in any of those courses. Even if you need an income to stay in the UK, any buisness makes more sense than that. Just open the 10millionth bar in the UK and be done with it. Less time investment (as you can delegate it) and more profitable (under the assumption you are actually giving away your edge)

*Tags: General Technical*

---

**William Martin** - *16:34:23*

Now if there was a course on getting started with the betfair API I would buy it! struggling to wrap my head around it as I'm new to programming and all examples are horse racing whereas I am interested in football

*Tags: Getting Started*

---

**William Martin** - *17:03:06*

I am looking to get the live data of outright markets for different football competitions, premier league, champions league, Euros etc.



Its not for a trading bot but for a prototype of a new interface I designed for a gambling product

*Tags: Deployment, Strategies*

---

**Oliver Varney** - *17:18:47*

[@U01C12ZEADQ](@U01C12ZEADQ) maybe able to help?

*Tags: General Technical*

---

**Alessio** - *18:03:24*

actually, can't help there. i am only aware of places for per-match rather than per competition ...

*Tags: General Technical*

---

**William Martin** - *18:14:15*

[@U01C12ZEADQ](@U01C12ZEADQ) no problem, I was initially looking at match data but decided to scale back to just outright markets as they are more static

*Tags: General Technical*

---

**Alessio** - *18:32:20*

Oh i thought you needed from outside betfair API, sorry i completely misunderstood the question.. lol

*Tags: General Technical*

---

**William Martin** - *18:33:07*

no problem, I could have explained better lol

*Tags: General Technical*

---

## 2021-02-26

**Unknown** - *07:52:49*

Market total matched delay

Not using delay key but seeing (in a slow moving market this time in the AM) that the total matched seems to be behind - using betfairlightweight==2.12.0

Anything obvious to check?

*Tags: Performance*

---

**Misha** - *08:26:06*

This isn't a new problem

*Tags: General Technical*

---

**Unknown** - *08:47:40*

Hi [@U4H19D1D2](@U4H19D1D2), just getting back to this.. I overrode the `stream.on_data` [https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/listener.py#L111|func](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/streaming/listener.py#L111|func) so that I store

`data.get("statusCode")` and `self.status`. This is enough for me, as this 2 variables are updated with every heartbeat, right?

*Tags: General Technical*

---

**liam** - *08:53:13*

yep, have a look at the `on_data` func, it updates these values / error handles before it gets down to the operation type on line 131

*Tags: Errors Debugging*

---

**Jorge** - *09:31:31*

Okay perfect, if  `data.get("statusCode") != "FAILURE"` it means that the Streaming is working fine, right?

*Tags: General Technical*

---

**Oliver Varney** - *19:21:54*

I guess the issue is that they probably only want help on one off bets as you say rather than giving you the option to pick and choose horses in each race across the whole festival

*Tags: General Technical*

---

**Michael** - *19:24:53*

Yeah it mentioned multiples which are often insane value. It's a question of how much they're taking out of the middle though.

*Tags: General Technical*

---

**James T** - *19:28:48*

I suppose it’s not really their business model. More of a sportsbook thing. 

*Tags: Strategies*

---

**thambie1** - *22:03:06*

Do you guys find Betfair properly honors the stream subscription heartbeatMs field, which sets the minimum period during which you expect a message from the stream? Tried setting the field, and am not getting regular messages. After some debugging, I think the issue is on Betfair's end.

*Tags: Errors Debugging*

---

## 2021-02-27

**Oliver Varney** - *13:33:17*

This is my understanding, hopefully someone will correct me if im wrong (Ive never digged deep into it as its a useless number most of the time on the site), but say you backed £1000 at 4.0 and the horse moved in to 2.0 you would need to lay £2000 I believe. If in the ladder there was only £2 @ 2.0 then, £1998 @1.01 it would basically say your position is worthless as it would calculate down to that, it may however take into account cross matching, not sure on this. Betangel watchlist view is what you want in terms of how to get a rough idea of the EV of you current positions

*Tags: General Technical*

---

## 2021-02-28

**Dave** - *09:43:31*

[https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/resources/bettingresources.py#L358](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/resources/bettingresources.py#L358)

*Tags: Strategies*

---

## 2021-03-01

**birchy** - *10:00:38*

[@U4H19D1D2](@U4H19D1D2) in Flumine strategy, are the orders in `def process_orders(self, market, orders)` per strategy or per market? i.e. do we need to filter the orders to avoid picking up bets placed by other strategies?

*Tags: Strategies*

---

**liam** - *10:03:45*

per market / strategy [https://github.com/liampauling/flumine/blob/bbd145b7f18a63790fa844e9a0cdc9bab3991542/flumine/baseflumine.py#L215](https://github.com/liampauling/flumine/blob/bbd145b7f18a63790fa844e9a0cdc9bab3991542/flumine/baseflumine.py#L215)

*Tags: Strategies*

---

**Lee** - *19:47:06*

I hd a large difference in commission again this week from the api and pc page.

```api profit == pc page

api profit - api commission == profit and loss page

api commission != pc commission```

I'm not sure how the first two sums above can match but commission is not even close. Question for BDP?

*Tags: General Technical*

---

**Beeblebrox** - *20:21:22*

Hi, I've only just started using Flumine and have a couple of questions which I hope you can answer to clarify my understanding.  I've had a look through the docs and the code, but would like to make sure I'm correct in my thinking.



I'm adding a strategy that uses the last traded prices x, y and z seconds before the race start.



My first question is, is there a best practice for how/where to store the prices of each runner at the times I'm interested in?  At the moment I'm using the `market.context dict` when `process_market_book()` gets called.  Is that the right thing to be using?



Secondly is there a way to make sure `process_market_book()` get called at x, y and z seconds before the race start?



And lastly, because I'm only interested in the market x seconds before the race starts, is it best to filter out calls to `process_market_book()` by making `check_market_book()` only return True when it's close to the time that I'm interested in, or should I filter it out when initialising the strategy class by setting `"listener_kwargs": {"seconds_to_start": 1234}`? Or does it not really matter?

*Tags: Strategies*

---

**Lee** - *20:46:05*

Hi,

`market.context` is fine to use, I use it too



I think you can put a streaming timeout on the strategy when instantiating to make it trigger? Not sure what the default is.



I’d do both, `listener_kwargs` just filters out to speed up backtesting but you’ll need to do it in `check_market_book` for live trading / testing logic

*Tags: Performance, Deployment, Strategies*

---

**Beeblebrox** - *20:55:33*

OK, thanks, that's helpful.



Looking at the code and docs `streaming_timeout` is set to None by default.  It says it calls `snap()` every x seconds if set.  I presume `snap()` just means it sends the most up to date data from the stream?  There's also a `conflate_ms` option when instantiating - how does that differ from `streaming_timeout`?

*Tags: General Technical*

---

**Lee** - *21:08:41*

conflate is the frequency of streaming updates from betfair

*Tags: General Technical*

---

## 2021-03-02

**klozovin** - *19:59:58*

historical data has market and selection names (in the "mcm" message) and the live streaming data does not? right? there's no way to get the names from the stream as well...

*Tags: Data Quality, Deployment*

---

## 2021-03-03

**captainonionhead** - *08:40:08*

Morning, newbie question I'm afraid and apologies if this is covered somewhere in the docs, I couldn't find anything specific to this...  I'm trying to implement a simple scalping algorithm (as an exercise to get going with flumine) and am confused by the life-cycle of a Trade and the Orders within it, partly because there seems to be some conflation of the terms "trade" and "order" even though these are different objects within the framework.

I had assumed that a "live" trade was one that had unmatched orders, but (at least during backtest) it appears to be live until the market is closed?

Am I expected to manually call `complete_trade()` once the orders within it have been either matched or cancelled?

*Tags: Deployment*

---

**liam** - *08:46:58*

Logic is [https://github.com/liampauling/flumine/blob/9a6ad8847c7e86129215d057888ba2b7b89ba0ac/flumine/order/trade.py#L80|here](https://github.com/liampauling/flumine/blob/9a6ad8847c7e86129215d057888ba2b7b89ba0ac/flumine/order/trade.py#L80|here), tbh it’s not perfect and there are a few edge cases around it and I know a few have written their own trade classes instead 

*Tags: General Technical*

---

**captainonionhead** - *09:01:17*

:thumbsup: thanks - I've been looking through the code for this.  I guess my question boils down to: is the expected usage of this something like:



• Create Trade()

• Create Orders() from Trade

• Use Market.transaction() to place orders

• Call Trade.complete_trade() &lt;- i.e. call this explicitly to indicate no further orders to be created on this Trade object?

&lt;elsewhere&gt;

• Use the Trade.complete property to check whether the trade has actually completed

*Tags: General Technical*

---

**liam** - *09:14:15*

The trade will complete as whenever an order status is updated it checks, via this [https://github.com/liampauling/flumine/blob/9a6ad8847c7e86129215d057888ba2b7b89ba0ac/flumine/order/order.py#L84|func](https://github.com/liampauling/flumine/blob/9a6ad8847c7e86129215d057888ba2b7b89ba0ac/flumine/order/order.py#L84|func) so the user shouldn't have to do it manually as such

*Tags: General Technical*

---

**captainonionhead** - *09:29:12*

Hmm, thanks, that fits with what I was expecting but I must be doing something wrong as I'm only ever getting one Trade of orders placed (because I check runner_context.live_trade_count in process_market_book() as per the example but it doesn't reset back to zero when the orders have matched).

More debugging to do - thanks for your help!

*Tags: Errors Debugging, Deployment*

---

**Aaron Smith** - *16:17:40*

seen the first line for the same reason as jon, 2nd line i cant find in either flumine nor bflw (unless its new in v1.17 or higher)

*Tags: General Technical*

---

**liam** - *16:19:02*

likely to be due to the above issue with market settlement, first line is saying an order has come through the stream but isn't present locally, second line is saying the market for said order isn't available locally (probably because it has closed and been removed from flumine)

*Tags: General Technical*

---

**Jonjonjon** - *16:19:23*

Is there a betfair problem now? I've had nothing settled since 15:07

*Tags: General Technical*

---

## 2021-03-04

**Jono** - *11:31:32*

Thanks, that clears this problem up.

*Tags: General Technical*

---

## 2021-03-05

**liam** - *15:21:00*

you will get an error through the stream when trying to subscribe

*Tags: Errors Debugging*

---

**liam** - *16:21:38*

There is for the main resources as you can do object.json() [https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/resources/baseresource.py#L18|https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/resources/baseresource.py#L18](https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/resources/baseresource.py#L18|https://github.com/liampauling/betfair/blob/f370d111d2e7adf1ca4221fc8cc4b028e4b2b2f8/betfairlightweight/resources/baseresource.py#L18)

*Tags: General Technical*

---

## 2021-03-06

**V** - *11:07:10*

Yeah, this is how I’ve done it historically. But my stuff is relatively latency sensitive so want to reduce internal latency on the trading path as much as possible.



[@U4H19D1D2](@U4H19D1D2) this is probably a stupid question, but then again this is Betfair we’re talking about so can never be too sure: they only care if you’re consuming data and trading at the account level right? So if you’re only consuming with one client, but trading (and also consuming) with another, that’s fine?

*Tags: Performance, Strategies*

---

**Beeblebrox** - *14:01:00*

I'm using Flumine to paper trade a strategy and in the logs I can see it placing an order and it getting matched, but I can't see if it ever reports the outcome of the order once the market closes.



Is this something I have to add to the Strategy myself? Should I be using `process_closed_market()` and the `market.blotter` list of orders to output this?

*Tags: Strategies*

---

**Beeblebrox** - *20:16:15*

One query I have with the LoggingControls is how do you know what the `event.event` property is in each of the class methods?  Looking at the examples I can see that for `_process_cleared_orders(self, event)` `event.event` is a list of orders and for `_process_end_flumine(self, event)` it's the `framework`.



Having looked through the code it's not immediately clear to me how you'd work this out for the other methods.  Can anyone point me in the right direction?

*Tags: General Technical*

---

## 2021-03-08

**Unknown** - *10:05:00*

Hi, psychoff posted his profit on Brighton-Leicester and his P&amp;L for that section of the day in response to a question, see it attached. Is there a way to get the trades for the market, it might be possible to reverse-engineer his trade on this market and hence his mode of thinking ?

*Tags: General Technical*

---

**Pierino S** - *13:02:11*

ok [@UGV299K6H](@UGV299K6H), well the flumine and the Python stuff is not easy for me to follow, as I'm not a coder or along those lines, so I can't post usefully about that, much as I'd like to. So most of the discussions do go over my head. I am interested in football and i can kind of get in-play trading on that one. I do understand green pictures and this guy is out there, he does post green stuff, so hence my interest.

*Tags: Strategies*

---

**Michael** - *13:11:39*

I'm not a suspicious person, so I look forward to seeing your questions on other matters addressed.

*Tags: General Technical*

---

**Michael** - *13:13:20*

For example, you could ask about Flumine, BFLW, Python and automated betting - which is what we do here.

*Tags: Strategies*

---

**Aaron Smith** - *13:17:05*

There are certainly diffrent ways to be successful in sports trading and many of them dont requiere you to write a single line of code. However, if that is what you are trying to achieve, this slack group will probably of little help, as its about bflw and flumine.

*Tags: Strategies*

---

**Michael** - *13:20:05*

After all the same question could  have been raised without any reference to a particular individual and without a picture of a P&amp;L page.

*Tags: General Technical*

---

**Mo** - *13:20:44*

I 100% agree with you that this psychoff guy has had more than enough attention here although I think the original question and discussion was a useful contribution. I think the question about reverse engineering bets is also a legitimate one and is actually a very interesting topic

*Tags: General Technical*

---

**Michael** - *13:21:23*

Yes I agree that the substantive question is interesting.

*Tags: General Technical*

---

**Aaron Smith** - *13:22:19*

I have to agree with michael, the promotion idea came to my mind aswell. In the end the post was only a showcase of psychoffs success and a question of if you could reverse engineer his trades. Its pretty obvious that by only using his pnl, thats pretty close to impossible, so i m not sure if he truely wanted a serious answer to that question. However there is some way that immedialty comes to mind if you want to know his way of thinking - his courses.

*Tags: General Technical*

---

**Mo** - *14:21:21*

[https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723/](https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723/)

[https://www.amazon.co.uk/Trading-Bases-Fortune-Betting-Baseball/dp/0451415175](https://www.amazon.co.uk/Trading-Bases-Fortune-Betting-Baseball/dp/0451415175)

[https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew/dp/1079013458](https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew/dp/1079013458)

[https://www.amazon.co.uk/Precision-Statistical-Mathematical-Methods-Racing/dp/1432768522](https://www.amazon.co.uk/Precision-Statistical-Mathematical-Methods-Racing/dp/1432768522)

[https://www.amazon.co.uk/Efficiency-Racetrack-Scientific-Financial-Economics/dp/981320351X](https://www.amazon.co.uk/Efficiency-Racetrack-Scientific-Financial-Economics/dp/981320351X)

[https://www.amazon.co.uk/Kelly-Capital-Growth-Investment-Criterion/dp/9814383139](https://www.amazon.co.uk/Kelly-Capital-Growth-Investment-Criterion/dp/9814383139)

[https://www.amazon.co.uk/Man-All-Markets-Beating-Street/dp/1786071975](https://www.amazon.co.uk/Man-All-Markets-Beating-Street/dp/1786071975)

*Tags: Strategies*

---

**Mo** - *14:52:28*

I think it's a very good book to recommend to newbies as it is quality technical content, even if we're at the level where we would prefer something along the lines of "Statistical Sports Models in Python", and would be very valuable to someone who has no idea where to begin calculating their own fair price

*Tags: Strategies*

---

**Pierino S** - *18:22:09*

How do I access historical data on Flumine ?

*Tags: Data Quality*

---

**Dave** - *18:31:46*

You'll most likely want to record it yourself ([https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py))

*Tags: General Technical*

---

**Dave** - *18:32:26*

alternatively, some of the usually-paid historical data is free (specifically the data from april/march 2020 i think) if you want to get a feel for it.

*Tags: Data Quality*

---

**Alessio** - *18:37:16*

I think he wants his trades.. try looking around here: [https://betfair-datascientists.github.io/api/apiPythontutorial/#get-cleared-orders](https://betfair-datascientists.github.io/api/apiPythontutorial/#get-cleared-orders)

*Tags: General Technical*

---

**Pierino S** - *19:19:48*

I did download some historical data, think I got December 2020 and unzipped it so i can see its in json format. Can I put that through flumine ?

*Tags: Data Quality*

---

**river_shah** - *19:21:21*

[@U01NU8PTC5Q](@U01NU8PTC5Q) recommend that you please follow the examples here: [https://github.com/liampauling/flumine](https://github.com/liampauling/flumine) and [https://github.com/liampauling/betfair](https://github.com/liampauling/betfair) the people on this slack channel are more than helpful, but best to self serve as much as possible especially for basics. also recommend first having a primer on python and coding (udemy and other many online resources), that way the members of this channel will be better able to help you concretely.

*Tags: General Technical*

---

**Pierino S** - *19:55:12*

hi [@U01B8031PM1](@U01B8031PM1) yes did take a look at that, like i said, its cool for coders, but it went right over my head. in the area where i'm an expert, i definitely help people out, nothwithstanding how silly their questions seem. no such thing as a silly question, the school i went to.

*Tags: General Technical*

---

**D C** - *20:03:29*

[@U01NU8PTC5Q](@U01NU8PTC5Q) the problem with helping people with programming is that often it is only possible to help with specific questions. Like if you post some code and explain what doesn't work, then someone can probably spot quite quickly what the issue is. It is really hard to give general advice though. It is not a lack of willingness to help, it is just a very hard task turning someone with no coding experience into a competent programmer via a forum. With programming you have to really get stuck in to learn how it works. The more you do that the more you learn and the more precise questions you can ask. [@U01B8031PM1](@U01B8031PM1) advice above is definitely the way to start. It is daunting task to learn to program from scratch but there are so many resources available. It is very frustrating to start off but it really does become very rewarding very quickly when you start to make progress. The effort really is worth it!

*Tags: General Technical*

---

**Michael** - *20:07:31*

I'd suggest you get a 'python for beginners' type book - like literally a paper one and just work your way through it. You don't need to be much of a coder to work with this stuff - take it from me I'm shit. Half the book will probably get you there.

*Tags: Getting Started*

---

**Dave** - *20:59:48*

[@U01NU8PTC5Q](@U01NU8PTC5Q) may I ask how you intend to consume this historical data? Do you want to, say, convert it to a CSV and then browse it in Excel?

*Tags: Data Quality*

---

**Beeblebrox** - *21:29:21*

Are Betfair OK with us polling the inplay service to get the current score?  I've got it polling it every 30 seconds for each market I'm subscribed to for one particular strategy.  Is that acceptable or is that taking the mickey?

*Tags: Strategies*

---

**Beeblebrox** - *21:37:21*

I presume other people use that endpoint with no problems?

*Tags: General Technical*

---

## 2021-03-09

**Jonjonjon** - *08:25:07*

Nice. Please let us know when it's on Amazon, or if you need help with proof reading!

*Tags: General Technical*

---

**Dave** - *09:23:49*

Not suitable for any relatively high frequency strategy though.

*Tags: Strategies*

---

**D C** - *09:47:29*

You could forgive a dodgy inplayfeed if you knew the market management was sound at the Betfair end. I suppose overall it works but they make some shocking errors with disallowed goals etc since VAR came to the party.

*Tags: Errors Debugging*

---

**D C** - *09:52:16*

Why not? I mean prematurely closing a market that subsequently has to be reversed due to a disallowed goal in a top league game has to be seen as a bad error. Suspend the market for sure but do it completely until the decision becomes irreversible. I don't know how it is actually done internally so maybe VAR is just not compatible with Betfair market settlement right now. Even more reason to provide us with unmanaged markets as a side market. We get it with NFL - someone "scores" market goes crazy, play gets reviewed and reversed - all without a market suspension. I just don't get why football is treated in such a different manner. Fine keep the managed markets but at least give us an option with unmanaged ones.

*Tags: Errors Debugging*

---

**river_shah** - *10:25:16*

Not to mention any cross market hedging if that’s what the strategy does

*Tags: Strategies*

---

**Jonjonjon** - *13:29:11*

By statistical arbitrage on Horse Racing, are you thinking of betting above/below an estimate of the fair value? Or some sort of back-the-win / lay-the-place strategy?

*Tags: Strategies*

---

**MacrcoPolo** - *13:35:50*

Although in fairness I suspect it's based on Benter's arb model

*Tags: Strategies*

---

**MacrcoPolo** - *13:37:37*

Haha exactly. In fairness, the harville model in it's raw state is garbage

*Tags: Strategies*

---

**Mo** - *13:40:26*

But I think it's a much better starting point for newbies than building a ML model or technical indicator which seem to be the typical starting points

*Tags: Strategies*

---

**Jonjonjon** - *13:44:44*

I've just done technical analysis so far. Using any of those models above just seems to make my head hurt, and I've never found anything worth trying.

*Tags: Strategies*

---

**Jonjonjon** - *13:52:22*

The problems with these sorts of books, is that the authors can only really give some starting points for further research. Or perhaps some interesting analysis techniques. But it's against their interests to give away a full working method.

*Tags: General Technical*

---

**Mo** - *13:57:06*

Of course, but really you want to teach a man to fish. There are some really fundamental things that newbies overlook. For example, use the market to tell you the true price wherever possible (EV calculations, model parameter estimation, statistical arbitrage)

*Tags: Strategies*

---

**MacrcoPolo** - *14:41:40*

None of it is state-of-the-art anymore, and he glosses over some key details (notably parameter estimation - which is non-trivial in his model) but it takes you through a rigorous approach to sports betting

*Tags: Strategies*

---

**Jonjonjon** - *14:43:49*

How big a project, would it be to get the data required to do some backtesting with Benter's model?

*Tags: Strategies*

---

**Mo** - *14:44:13*

Honestly, if you want ideas for models or strategies, I think academic papers are better than books

*Tags: Strategies*

---

**MacrcoPolo** - *14:46:42*

Getting the data isn't too tough. But for one thing, certain aspects of his model are non-trivial and you need to know a lot about statistical inference to fit it correctly and verify you don't have a pile of turd at the end

*Tags: Strategies*

---

**MacrcoPolo** - *14:47:41*

In addition, HK was perfect for his model. There is only 2 tracks, the weather is relatively consistent, the pool of horses is closed each season and the number of runners is similar each race, as is the length of race

*Tags: Strategies*

---

**Mo** - *14:49:28*

Welcome to algorithmic strategy development in sports betting!

*Tags: Strategies*

---

## 2021-03-10

**Dave** - *08:06:47*

Question about BSP: The BSP docs constantly talks about involving "exchange" bets in the reconciliation. I thought BSP reconciliation only Involved those that have specifically marked to participate in BSP (i.e take sp/market on close/limit on close). What are they referring to when they mention "exchange bets" here?

*Tags: General Technical*

---

**user34** - *17:13:57*

I'm sorry if the answer to this question is already in the chat or examples, but I am looking to migrate my betfairlightweight strategy to flumine for hopefully better reliability and logging and I am having a hard time figuring out how to do this. My comfort zone for coding is very much defining functions that process specific inputs so this could be something in Python I am missing. My current set-up has separate processes that i) identify appropriate event ids in the next day, ii) identify the relevant market ids from those event ids and manage streaming subscriptions, and iii) receives data from the streams at fixed intervals and decides whether to place bets or cancel existing bets.

*Tags: Errors Debugging, Strategies*

---

**user34** - *17:15:03*

This seems like a natural architecture to have, but when I look at the flumine examples I can't see how to replicate this; the ExampleStrategy class has function definitions, but doesn't seem to do anything.

*Tags: Strategies*

---

**user34** - *17:42:33*

In this case it would be BaseStrategy.

*Tags: Strategies*

---

**Aaron Smith** - *17:44:54*

what functions are being called eventually comes down to what event you are getting. The main run thread is the flumine class. From there on you can follow down each path to see what is happening

*Tags: General Technical*

---

**Aaron Smith** - *17:47:37*

otherwise you can also "trust" that everything in BaseStrategy is being called when it intuitively makes sense. process_market_book is where you will want to put your decisionmaking and order placing.

*Tags: Strategies*

---

## 2021-03-11

**Steve Roach** - *09:28:56*

Can anyone point me to some documentation that will give me a quick-and-dirty start to getting a live feed. Over time, I will be going through the documentation and refining my knowledge, but I'd like a preview if that's possible.

*Tags: Deployment*

---

**Mo** - *09:30:08*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py|https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py|https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**Mo** - *13:20:11*

A man for all markets and trading bases are more autobiographical so I think those are fine on kindle. Precision, efficiency of race track betting markets and the Kelly criterion book are all text book-y so better to have hard copies if you like to make notes/highlight and easier to refer back to that way. The logic of sports betting is somewhere between the two. Statistical sports models in excel is a very practical book so nice to be able to read on a computer while also running excel 

*Tags: Strategies*

---

**river_shah** - *18:25:05*

working from a dodgy internet connection at the moment on my dev machine. having trouble finding the parameter that controls this: `WARNING High latency between current time and MarketBook publish time` where can I set this high enough please to reduce occurrences of this log?

*Tags: Performance*

---

**Lee** - *18:26:14*

```latency = time.time() - (market_book.publish_time_epoch / 1e3)

if latency &gt; 2:```

*Tags: Performance*

---

## 2021-03-12

**Stefan** - *13:42:14*

Yes, you are right. First I thought races under 1m are started from stalls, but noticed 2m races started from stalls as well. So it is maybe question of racecourse, or race type?

*Tags: General Technical*

---

## 2021-03-15

**mandelbot** - *01:45:50*

betfair's new business model :joy:

*Tags: Strategies*

---

**Steve Roach** - *09:10:18*

Can anyone tell me what is the difference between BFLW and Flumine?

*Tags: General Technical*

---

**Mo** - *09:10:52*

BFLW is a lower level implementation of the Betfair API in Python. Flumine sits on top of this and aims to be a fully featured trading platform

*Tags: Feature Engineering, Strategies*

---

**Steve Roach** - *09:35:49*

Hi Guys,



I would like to build a database of historic horse racing data with which to analyse at my leisure. I have run the bflw example; examplehistoricdata.py and it is returning some data. I notice right away that is is going to my account on Betfair's Exchange Historical Data and first lists all of the files that I have so far purchased. Right now there are 2 off the Basic plan, and 1 off the Advanced. I see no reason that, given the right amount of effort, I could pull the data into the form that I want. However, at the moment, I'm trying to keep costs down, so I'm just getting the free stuff.



However, I also have access to Betfair's Data Portal (beta) page. Amongst others, there is a set of files "Horse Racing - Files - Month_by_Month_Pro". These are fairly large (some up to 10Gb) and they look like a zipped up set of market files, each covering a month, dated from May 2015 to Feb 2021.



A few questions about this...



• Can bflw access the Data Portal files? Is this even a useful thing to do?

• Can these files be usefully used to create a history?

Thanks in advance.

*Tags: Data Quality*

---

**Misha** - *09:50:00*

Basically Betfair Australia is run as a highly customer-focused organisation, so if you are nice and helpful, they can help you out with data access and other stuff

*Tags: General Technical*

---

**Misha** - *09:50:55*

Betfair Australia is NOT run like Betfair UK, thankfully. They can be very helpful

*Tags: General Technical*

---

**Misha** - *09:51:26*

Streaming data

*Tags: General Technical*

---

**Steve Roach** - *10:02:32*

So, [@U4H19D1D2](@U4H19D1D2) does bflw pump the data straight through from the files or do you do much processing on the way through?

*Tags: General Technical*

---

**liam** - *11:14:41*

the historical endpoint just allows you to view/list/download the files, the streaming endpoint then allows you to process the data as the files themselves just contain recorded streaming data which is images + deltas so you need to do some processing (bflw is heavily optimised for just this :slightly_smiling_face:)

*Tags: General Technical*

---

**Unknown** - *16:08:04*

Guys (gals), following recent chats about Monte Carlo, bet analysis, etc, I threw together a Python project to quickly analyse a BetHistory.csv, which can be obtained from [https://myaccount.betfair.com/activity/bettinghistory](https://myaccount.betfair.com/activity/bettinghistory) when logged in to the betfair website. For those of us that are not statisticians or maths professors, I think it's a useful beginners tool. Would appreciate any feedback, improvements, etc.

Advance warning: I've used a tkinter gui. It's not the prettiest thing in the world, but it gets the job done and is already part of the Python standard library.

*Tags: Getting Started, Strategies*

---

**Sam Asin** - *20:08:11*

good question, I have no idea really

*Tags: General Technical*

---

## 2021-03-16

**Outthebox sports trading** - *07:07:32*

whats the best strategy to make some profits on the betfair exchange?

*Tags: Strategies*

---

**Mo** - *07:38:28*

I find the best strategy is to place bets that have positive expected value

*Tags: Strategies*

---

**Oliver Varney** - *07:42:29*

to be constructive [@U01PUHXHEKC](@U01PUHXHEKC), best to phrase the question in a more specific way so we can give some constructive feedback and starting points. The initial question wont get answers as people wont give your their strategy on a plate

*Tags: Strategies*

---

**Mo** - *07:57:00*

```kelly_fraction = true_probability - (1 - true_probability) / (odds - 1)

if kelly_fraction &gt; 0:

    stake = 0.5 * kelly_fraction * bankroll_size

    place_bet(stake, odds)```

You’re literally guaranteed to make money this way!

*Tags: Strategies*

---

**Misha** - *08:13:15*

A related question: how many on this workspace generate a true price for a sporting event which they use to bet? I know [@UBS7QANF3](@UBS7QANF3) does, and I do as well, but interested to know how many others do as well? (this is more related to stats-based modelling rather than betting based on price movements)

*Tags: Strategies*

---

**Outthebox sports trading** - *08:17:35*

whats the best strategy for Cheltenham today??

*Tags: Strategies*

---

**Jonjonjon** - *08:35:56*

Another question...



Do you produce a point forecast for the true price, a range of likely values, or something else?

*Tags: General Technical*

---

**Unknown** - *08:50:12*

looking to get involved, dutching a good strategy?

*Tags: Strategies*

---

**Oliver Varney** - *08:53:00*

Im not too sure what your trying to get out of the questions. People in this slack group work for months to years to come up with strategies. It appears maybe you just want a little bit of fun around chelts which is all good so just enjoying the racing and bet responsibly, its very likely that your bets will be random and therefore the outcome random.

*Tags: General Technical*

---

**Oliver Varney** - *08:55:27*

your questions are not specific. who knows if dutching is a good strategy. What does your empirical backtesting show? Is it +EV?

*Tags: Strategies*

---

**user34** - *09:23:20*

I produce an estimated distribution of possible prices before betting. It seems to help catch both situations with vague data and where the model might be missing something.

*Tags: Strategies*

---

**D C** - *09:40:26*

This is what I used to do with logistic regression models - use the SE given to get a rough CI around the point estimates.

*Tags: Strategies*

---

**Mo** - *09:41:13*

One way you can take advantage of this would be if you are trying to fit a model to the implied probabilities, don’t try to find the parameters that give the closest probabilities to the point estimates but the ones where all probabilities fit within the intervals

*Tags: Strategies*

---

**Misha** - *09:48:21*

The current model uses no ML or sophisticated maths. Models to follow will use linear regression and similar techniques

*Tags: Strategies*

---

**Misha** - *09:49:04*

I see this model as the "funding" for more sophisticated modelling to follow

*Tags: Strategies*

---

**Misha** - *09:51:12*

BTW, the more sophisticated modelling is being done by someone else (not my area of expertise)

*Tags: Strategies*

---

**Alessio** - *09:53:44*

If I understand correctly what [@UBS7QANF3](@UBS7QANF3) alludes to is more akin to Bayesian inference rather than standard inference, where either analytically or by simulation, you have an understanding of how certain is your estimate of that probability. The more certain you are, the more you can trust your Kelly, for example (scaling the 0.5 or understanding the odds are 'almost unreal'). Which makes a lot of sense to me. Did I understand correctly?

*Tags: Strategies*

---

**D C** - *09:55:44*

Bayesian you would get a distribution rather than the frequentist point estimates. But other stuff that generates a point estimate and a SE is "usually" going to be normally distributed in the CI sense. I am not speaking for [@UBS7QANF3](@UBS7QANF3) here this is just based off my own (outdated) days of modelling and basics of the premise of Bayesian versus Frequentist statistics.

*Tags: Strategies*

---

**D C** - *10:00:49*

SE - standard error. Like a standard deviation you can apply around a point estimate. From memory most maximum likelihood estimates thrown out by a linear model are shown to be normally distributed.

*Tags: Errors Debugging, Performance, Strategies*

---

**D C** - *10:02:34*

I recall from logistic regression with logit link that because of the simple formula for logit there is a similar formula to map upper and lower confidence intervals for your modelled probability. This is what I used to use to give myself a "cushion" around the estimates. I am certain my model worked mainly because the bookies prices were so shite back in the mid to late 2000's rather than any modelling skill on my part. After all, it only used public data scraped from yahoo sports. Typing this makes me realise how long ago this was.....

*Tags: Strategies*

---

**Unknown** - *11:50:27*

Stupid question perhaps but it sounds rather counter-intuitive to fit a model against implied probabilities derived from the market with the aim to beat the market?

*Tags: Strategies*

---

**Mo** - *11:52:03*

Fit an efficient market to find value in an inefficient market. Two examples:



1. Statistical arbitrage e.g win versus place

2. Use starting prices to fit a model to identify value in running

*Tags: Strategies*

---

**Mo** - *11:53:16*

I think it's definitely something you should take a closer look at. Use the market to tell you the true price whenever you can. In a lot of cases it's going to be way better than any model you can build

*Tags: Strategies*

---

**Misha** - *11:57:06*

2. above is definitely part of my model

*Tags: Strategies*

---

**Mo** - *12:13:29*

Same thing can be said about assessing realised returns from a strategy versus calculating expected value using market implied probabilities

*Tags: Strategies*

---

**Beeblebrox** - *12:18:33*

This is what I do. Mainly because I'm not smart enough to build my own models, so I just piggy back on the wisdom of the crowd.

*Tags: Strategies*

---

**Lee** - *12:54:46*

I guess my question is more around if you have a 10 runner race, at some point during the race at least 9 are going to appear back value against a starting price but they're not. Once you're in play i'm not really understanding how SP is relevant.

*Tags: General Technical*

---

**liam** - *12:56:48*

I have never made it work (using SP for modelling purposes inplay)

*Tags: Strategies*

---

**Mo** - *12:57:44*

Example: fitting your model to BSP gives you the distribution of speeds of the horses. You use these distributions in running combined with each horse's distance to the finish line to calculate an in running probability. Then you compare that to the current prices to find value. You're not comparing the current prices to the BSP to find value

*Tags: Performance, Strategies*

---

## 2021-03-17

**Oliver Varney** - *19:47:43*

there was a thread recently where i asked a similar question, might be able to find via the search

*Tags: General Technical*

---

## 2021-03-18

**rd123** - *16:06:12*

are you implying that there will always be a tennis expertise layer that the market price takes into account, which can't be replicated by the models? Or that the best model is already out there, and it is impossible to beat? (second option seems very doubtful to me)

*Tags: Strategies*

---

**Sam Asin** - *18:01:12*

i looked at logit models with indicators for teams, which seems fine

*Tags: Strategies*

---

**Sam Asin** - *18:02:46*

is trueskill not very popular out of curiosity? any sense of that? the only r package i could find looked pretty shit, and the python one i saw didn't look mindblowing either

*Tags: General Technical*

---

**Mo** - *18:04:14*

Details of TrueSkill are too opaque. Easy to update the ratings but almost impossible to work out how to make predictions from them

*Tags: General Technical*

---

**Sam Asin** - *18:04:17*

thats the main problem with elo it seems, main reason why trueskill sounded better

*Tags: General Technical*

---

**Mo** - *18:07:13*

You shouldn’t be so dismissive of Elo. Yeah, it has some obvious shortcomings but all models are approximations of reality. Ease of implementation is an important consideration. I think the 80/20 rule applies here

*Tags: Strategies*

---

**Mo** - *18:13:32*

Something else to consider is stacking or ensembling these different methods and also different versions of the same method, for example, Elo models with different values of K

*Tags: Strategies*

---

**Atho55** - *18:15:12*

[@U01J98RDHRN](@U01J98RDHRN) I have a spreadsheet that has Home Win odds v Actual result that I can post if it helps. Lo tech approach

*Tags: General Technical*

---

**Sam Asin** - *18:24:22*

[@UBS7QANF3](@UBS7QANF3) For sure for sure. That's a good point. Stacking different models is probably a great way to have something a bit more intricate than the general field or whatever.

*Tags: Strategies*

---

## 2021-03-19

**river_shah** - *16:28:27*

How do I signal a backtest framework to terminate gracefully please? This is giving:

```flumine.handler_queue.put(TerminationEvent(flumine))

AttributeError: 'list' object has no attribute 'put'```

*Tags: Errors Debugging*

---

**Lee** - *18:50:14*

off the top of my head you might be able todo something in a worker like following. no idea if this works and probably not great practice to access private variables but might help point in the right direction

```for stream in flumine.streams:

   if stream._running```

*Tags: General Technical*

---

**Lee** - *18:51:44*

i've not done this as there's not much i can do to start it again apart from restarting the whole flumine process, also the retry has always fixed things quicker than i could

*Tags: Errors Debugging*

---

## 2021-03-20

**Peter C** - *16:25:48*

Is it possible to get flumine to submit bets at BSP? I'm trying to automate cashing out whilst I test my strategy

*Tags: Strategies*

---

**Ruben** - *17:12:02*

probably a stupid question, but why do you care about the individual prices at which your order is matched, as long as the VWAP is above the price you specified? the fact that it is FoK guarantees that everything gets matched, or nothing gets matched

*Tags: General Technical*

---

**Oliver Varney** - *17:20:18*

Is one advantage of that betfair implementation is that if the min fill size is not met the market never sees the order, I don't know if this is true but a question I've always had?

*Tags: General Technical*

---

**Aaron Smith** - *17:28:02*

One more question: My respone from betfair that my order has been played immediatly shows up as an EventType.CURRENT_ORDERS i guess? so i can just process my live order in process_current_orders [@UBS7QANF3](@UBS7QANF3) ?

*Tags: Deployment*

---

**Mo** - *17:28:34*

I can’t answer any flumine questions :wink:

*Tags: General Technical*

---

**Oliver Varney** - *17:29:21*

In horses in the last five I'd say it will make little difference apart from maybe super long odds stuff, even though I'm guessing your models pricing will not know the difference between the price you go for and the price one tick down. Having said this I still think it's a good idea to do your own in the long run.

*Tags: Strategies*

---

**D C** - *17:51:00*

That is a different question isn't it.

*Tags: General Technical*

---

## 2021-03-21

**Misha** - *03:23:57*

The only problem you may have is in very low liquidity markets, where you are asking for more volume than is really on offer (excluding the default "catch-all" offers that are often left there to match "mistakes"). But I would never bet into these markets as it just isn't worth the effort (and this comes from someone who bets on ITF matches)

*Tags: General Technical*

---

**Misha** - *10:07:04*

Fill or kill is a better execution strategy over a manual fire and cancel

*Tags: Strategies*

---

**Unknown** - *10:11:22*

I have the data to show it doesn't matter (would be less than a rounding error). I don't waste time duplicating functionality that would make no measurable difference to profitability

*Tags: Errors Debugging*

---

**Misha** - *10:16:36*

It scaled to $200M/year turnover for pre-off UK horse racing, but that wasn't my money (used fill or kill for the main strategy for that as well)

*Tags: Strategies*

---

**Misha** - *10:21:33*

For my system I am a long way from a "finished" product, so we will go through a lot of changes over the next 1-2 years. This strategy works for what I am doing at the moment, but at some stage we will be performing far more complex betting which will entail leaving offers there for certain amounts of time

*Tags: Strategies*

---

**Misha** - *10:23:05*

Priority always is, and has been, 1) get a decent working model; 2) get a decent working execution strategy; 3) enhance everything now that we are "self-funded"

*Tags: Strategies*

---

**D C** - *11:27:28*

So [@U016535QCJ2](@U016535QCJ2) as you use FoK and are placing huge numbers of bets maybe you can answer my question that I have not yet asked Betfair. As most people don't like FoK as implemented by Betfair seems nobody else can have as good a stab at the answer as you do. For a failed FoK bet, are you charged for a SINGLE  transaction or does it act as 2 (place bet/ cancel bet) ?

*Tags: Errors Debugging*

---

**Pierino S** - *23:31:10*

Hi, there were some mental results in the football these last couple of days....e.g. Bologna beating Juventus, West Ham blew a 3 goal lead against Arsenal. Does bflw take inputs from score type feeds, like flashscore or along those lines &amp; place bets according to the user's logic ?

*Tags: General Technical*

---

## 2021-03-22

**Dave** - *09:25:59*

I suppose the real question is how much are you willing to pay for an atomic place+cancel. For most people, presumably that figure is 0.

*Tags: General Technical*

---

**Misha** - *09:34:39*

It would be a very small fraction of a %. I would estimate less than 0.01% POT. Given that POT% is all over the shop, it wouldn't even measure as a rounding error

*Tags: Errors Debugging*

---

**Rich** - *10:19:33*

Is there a way to find which bz2 file a flumine backtesting strategy is acting on? for logging purposes? (i.e. i'm creating a DB of markets by name and storing which file the various markets are stored in?)

*Tags: Strategies*

---

**liam** - *10:22:28*

You have the python logs or you can use the `loggingcontrol` class

*Tags: General Technical*

---

**liam** - *10:30:17*

Each new strategy, market, trade, order, cleared market etc all result in a call to the [https://liampauling.github.io/flumine/advanced/#logging-controls|control](https://liampauling.github.io/flumine/advanced/#logging-controls|control)

*Tags: Strategies*

---

**liam** - *10:30:26*

The code should give a good overview of whats what [https://github.com/liampauling/flumine/blob/master/flumine/controls/loggingcontrols.py](https://github.com/liampauling/flumine/blob/master/flumine/controls/loggingcontrols.py)

*Tags: General Technical*

---

**Rich** - *11:09:41*

{"asctime": "2021-03-22 10:57:58,544", "levelname": "INFO", "message": "Market closed", "market_id": "1.123344868", "client": {"id": "e553f03d", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x0000020E6F6BEC10&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 818, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;

*Tags: Strategies*

---

**liam** - *11:26:22*

I see, and that is a problem because? 

*Tags: General Technical*

---

**Peter** - *11:40:05*

@*Pierino Schiavone BFLW can be used to access the undocumented Betfair scores API only, but I suspect the consensus here would be that that service isn't very accurate and you'd be better off running a separate process to ingest this sort of data from a more reliable source, caching it locally (e.g. with Redis or similar) and then reading it as needed for your BFLW/Flumine strategies.*

*Tags: General Technical*

---

**Pierino S** - *13:17:44*

ok cheers [@U9JHLMZB4](@U9JHLMZB4) for that, i have heard the Betfair scores are not that accurate, so good to have that confirmed here. Good tip about the Redis option, I see its tailor-made for open source database caching. I had paid someone to code something a long time ago and it was really cracking, just then something broke and it became unusable !! Just giving all this stuff a fresh look.

*Tags: General Technical*

---

**Ian** - *20:07:38*

Hi all, does anyone have a good tutorial or reference site on plotting json data using plotly. I'm looking at plotting the data from flumine - managed to process the JSONL into a single JSON object and looking to start simple by plotting the TV of each market along the time, using buckets of time, then the RC. I'm assuming that selecting the relevant elements of the JSON directly will be more performant than creating a simple DF with the require data. Any pointers appreciated.

TIA

*Tags: General Technical*

---

## 2021-03-24

**Peter C** - *12:12:25*

If I run a backtest of aus racing markets in flumine, does it take account of the extra fees for those markets or do I need to factor them in myself?

*Tags: General Technical*

---

**vorace** - *18:22:40*

hey guys, just want to confirm i didn't miss anything obvious regarding the order stream..



from the docs, with order stream subscribed, betfair will send order change message which contains 1+ order changes, but from what i can tell, there is no timestamp associated to the order changes itself.



so the question now is "how does one keep track of the match time(s) of one's orders?"



- happy path is obviously when the subscription doesn't go down (be it client or server issue) and you could use the publish time of the order change message itself to best represent the time of individual order changes..

- but in the not so happy path, if the subscription is disrupted because of client or server issues, and you then resubscribe with a clk/initialClk combo, the publish time of the order change message is no longer a good approximation of the time of the individual order changes

*Tags: Deployment*

---

## 2021-03-25

**JC** - *19:34:01*

Hi everyone, question for those of you who are involved in tennis markets. I've recently got access to OnCourt's database, and was wondering if anyone had managed to integrate this into their automated system i.e. have data updating from OnCourt and feeding into models etc. Seems like a bit of a nightmare as it's .mdb, and i'm currently running a virtualbox so that i can use Microsoft Access to use it. Maybe people only use it for historic data and getting historic scores data and scrape/rely on other sources for new data and updating models?

*Tags: Strategies*

---

**Mo** - *19:38:36*

I haven’t yet worked out how to update the database without running the application but I’m certain it’s possible

*Tags: General Technical*

---

**JC** - *19:39:16*

As an aside, was wondering if anyone has managed to create a model that can beat the market's pre-play accuracy of ~70%. I'm aware most of the opportunity is available in-play, but seems like an interesting challenge

*Tags: Strategies*

---

**JC** - *19:47:57*

I've managed to reverse engineer the historic scores as previously mentioned (with help from someone in this slack), but yeah could also work for updates

*Tags: General Technical*

---

**Beeblebrox** - *20:20:18*

I've got a back testing logging control that records orders when `_process_cleared_orders_meta()` is called (using the example in the repository) and that all works fine.  I'm now trying to write a logging control to record orders from a live strategy, but I'm not sure how to replicate what I've done for the back testing control because  `event.event` in `_process_cleared_orders_meta()` is a list of `Order` objects, but for `_process_cleared_orders()` it's a `ClearedOrders` object.



I want to be able to write `order.trade.market_notes` and `order.trade.nodes_str` for each order, which is easy for the back test control, but for the live control how do I access them from the `ClearedOrder`?



I thought instead I could write the orders when `_process_closed_market()` is called and then loop over the orders in the market blotter, but I also want to write the profit for the order and that doesn't exist as a property for real orders (you can get it when back testing from `order.simulated.profit`).



There must be a way to do it, but I'm probably missing something obvious.  Any clues as to how I can do it? Thanks

*Tags: Deployment, Strategies*

---

**Misha** - *22:30:47*

Re OnCourt: Only running the application will update the database. The code to update and rules are in the application

*Tags: General Technical*

---

**Misha** - *22:34:16*

I have everything fully automated but I probably spent a couple of months of development to get there (to have it run pretty much perfectly). I copy the full Access database into an SQL Server database, run a bunch of stored procedures on that database, and then run a lot of code performing various steps (processing tournaments, players, matches, seedings, etc) that updates my main database

*Tags: Deployment*

---

**JC** - *23:49:18*

Do you have the whole updating process automated as well from launching the OnCourt application to populating your database with parsed new information?

*Tags: General Technical*

---

**JC** - *23:50:52*

Also yes I foresee matching betfair's names and identifiers with my own to be a an upcoming problem

*Tags: General Technical*

---

## 2021-03-26

**Misha** - *00:20:50*

The odds and points are stored as text files on his server. The data is way too big for an Access database (and too complicated for maintenance)

*Tags: Deployment*

---

**Misha** - *00:21:09*

I have all the points in a database (around 45 million)

*Tags: General Technical*

---

**Misha** - *00:23:16*

As well as using 3 databases, a lot of views and stored procedures, the parsing code for all the data comes to about 6000 lines of C# code

*Tags: General Technical*

---

**Misha** - *02:06:54*

[@U0160E9HS2G](@U0160E9HS2G) - regarding name matching: the parsed OnCourt data/database IS our tennis stats database, so all matches get matched back to an OnCourt tournament and OnCourt players. This gives a tournament id and players ids for current matches that allow for historical player and tournament data to be used for current matches

*Tags: General Technical*

---

**Isaac Martinat** - *08:17:00*

For tennis database, do you know about this open source one: [https://github.com/JeffSackmann/tennis_atp|https://github.com/JeffSackmann/tennis_atp](https://github.com/JeffSackmann/tennis_atp|https://github.com/JeffSackmann/tennis_atp)? I was going to work on it this week, but if OnCourt is better, I will buy their license.

*Tags: General Technical*

---

**liam** - *09:00:28*

[https://github.com/liampauling/flumine/blob/62b40ad11988ecab039b3b822bc70779aed64bca/flumine/markets/blotter.py#L68](https://github.com/liampauling/flumine/blob/62b40ad11988ecab039b3b822bc70779aed64bca/flumine/markets/blotter.py#L68)

*Tags: General Technical*

---

**Misha** - *10:54:07*

[@U0160E9HS2G](@U0160E9HS2G) - OnCourt is a historical database, not a live one

*Tags: Data Quality, Deployment*

---

**Misha** - *14:31:59*

I use C# and the Microsoft.ACE.OLEDB provider to copy the whole Access database into an SQL Server database each time for data manipulation (takes a couple of minutes)

*Tags: Deployment*

---

## 2021-03-28

**Sam Asin** - *00:39:31*

[@UBS7QANF3](@UBS7QANF3) et all, if other people mess much with glicko/elo, I just tried to set up a very basic glicko model on some win/loss data. I haven't bothered to try to check it for betting specifically yet, but I started by just bucketing my data into 50-60% favorite, 60-70%, etc. according to the model, and seeing how often the favorite won in each bucket.

*Tags: Strategies*

---

**Sam Asin** - *00:44:14*

So I'm curious what sort of results you'd expect out of a glicko model?

*Tags: Strategies*

---

**Sam Asin** - *00:50:27*

I can't decide if this is good enough that I should be really suspicious I'm doing something wrong, I looked for an error and haven't found one yet. Or is that a pretty expected result from basic elo/glicko

*Tags: Errors Debugging*

---

**Mo** - *07:30:05*

This is just calibration which looks good for most models but says nothing about their accuracy or competitiveness versus the market. I don’t think it looks remotely “too good”. In fact it seems quite poor, 50% - 60% only winning 42% of the time? Having said that, it’s probably a sample size issue

*Tags: Strategies*

---

**Ruben** - *11:48:22*

Good morning everyone, question about Flumine: Say my bot crashes and I re-start it. On start up, is it possible to "poll" existing orders previously placed to avoid placing them again? I imagine this would somehow be prepopulating the blotter on start up? Any advice against doing this?

*Tags: General Technical*

---

**Sam Asin** - *17:23:33*

but yeah, thanks, that answers my question for now! I can stop looking for problems in my code and move onto checking vs. markets and maybe coming up with another model to stack etc. etc.

*Tags: Strategies*

---

**Mo** - *17:38:22*

To be clear, I'm rooting for you and am interested to see how the model evolves

*Tags: Strategies*

---

**Sam Asin** - *17:57:39*

roger thanks sir! Will get going. Also for my "to be clear," per me asking about this the other day, this sort of ELO stuff is nothing I'm experienced with at all, giving it a try to help a friend who wants to bet on this crap with just his "instincts." Wanna give him a puncher's chance. So yeah, don't judge me too hard for my glicko attempts. My main schtick involves a lot of somehow squeezing a little money out of really crappy models, so maybe that's why I have low expectations :slightly_smiling_face:. Okay time to work.

*Tags: Strategies*

---

**Sam Asin** - *17:57:43*

thanks for all the help!

*Tags: General Technical*

---

## 2021-03-29

**Rich** - *11:19:45*

Is there any built-in functions in flumine to give current cash-out (green up) profit/loss?

*Tags: General Technical*

---

**Oliver Varney** - *12:15:03*

I dont think so, I think the decision was not to add, as it would lead to people all requiring custom solutions . [https://github.com/liampauling/flumine/issues/267](https://github.com/liampauling/flumine/issues/267)

*Tags: General Technical*

---

**Rob (NZ)** - *12:38:07*

Hey all. Hope everyone is well.. I'm just wondering if someone could describe the best way to approach getting the following data...   I'm using python and betfairlightweight and trying to get a table that would have one row per race for nz and aussie gallops that are coming up . On that row something that described how long till the start of the race and then which runner is the favourite and what its projectes bsp or last traded price is... ideally this would be refreshing as the odds and races come and go...    any thoughts on a approach

*Tags: General Technical*

---

**Rob (NZ)** - *12:57:03*

Previously I got the markets using the code on betfair aussie examples but that was awhile ago so just not sure if there is better approaches now... I've spent the last year working on my model and now need to feed it live info rather than just my back testing which I've been doing manually

*Tags: Deployment, Strategies*

---

**Mo** - *12:58:34*

I'm not sure if you're referring to the betfairlightweight examples but all of the data you need is trivially available from streaming and the market catalogues so this is a good place to start: [https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**Rob (NZ)** - *13:03:34*

Awesome thanks I'll check that out.... I was using [https://betfair-datascientists.github.io/api/apiPythontutorial/|https://betfair-datascientists.github.io/api/apiPythontutorial/](https://betfair-datascientists.github.io/api/apiPythontutorial/|https://betfair-datascientists.github.io/api/apiPythontutorial/)

*Tags: General Technical*

---

**Mo** - *13:05:32*

Right, so the key thing that seems to be missing from that is the use of streaming which the example I linked to covers

*Tags: General Technical*

---

**river_shah** - *18:02:04*

do you have an example on how to poll the betgenius feed instead please? currently I am using this:

```import logging



from flumine.events.events import CustomEvent



logger = logging.getLogger(__name__)





def poll_in_play_service(context: dict, flumine, event_id: str) -&gt; None:

    trading = flumine.client.betting_client



    response = trading.in_play_service.get_scores(event_ids=[event_id])

    if response is None:

        logger.warning(

            "poll_in_play_service",

            extra={"response": response, "event_id": event_id},

        )

    for score in response:

        flumine.handler_queue.put(CustomEvent(score, callback))





def callback(flumine, event):

    # update market context

    score = event.event

    for market in flumine.markets:

        if market.event_id == str(score.event_id):

            logger.debug(

                "Updated market {0} with event {1} scores data".format(

                    market.market_id, market.event_id

                )

            )

            market.context["score"] = score```



*Tags: Errors Debugging, Strategies*

---

## 2021-03-30

**river_shah** - *06:49:53*

Does flumine support muxing data across market files to backtest multiple markets simultaneously? Use case is racing win vs place or football over unders vs each other 

*Tags: General Technical*

---

**Mo** - *09:27:16*

If you do it from AWS they can complain to AWS and have your instance terminated

*Tags: Deployment*

---

**liam** - *12:45:51*

I think 9.7.6 answers your question, you don’t lose out 



_For example, if the Premium Charge rate applicable to your betting increases from 20% to 40%, we will assume that you have always paid Premium Charges at 40%._ 

*Tags: Strategies*

---

## 2021-03-31

**charlie_v** - *10:06:58*

The fun of the betfair api, would have  presumed after such a long period of time they would have got it stable.

*Tags: General Technical*

---

**charlie_v** - *10:11:07*

I like him, because I made a lot of money from smarkets over the year,helped me buy a house so....

*Tags: General Technical*

---

**charlie_v** - *10:45:03*

so if betfair api comes back , anyone going to bother today? it will fall over again today won't it

*Tags: General Technical*

---

**charlie_v** - *11:09:36*

Right who has a hammer spare, where are the betfair servers, I'm going to hit them with a very large hammer to see if that helps and type a few kill -9 instructions .

*Tags: Deployment*

---

**Mo** - *13:12:39*

The CEO recognises that the exchange is their USP but whether he can successfully act on that is another question

*Tags: General Technical*

---

**James T** - *13:31:54*

I think it’s now recognised that although an exchange is effectively a better offering for casual punters because of the lower margins, the problem is that it costs more to acquire those customers (I think mainly because of the increased complexity of using an exchange). This means a traditional sportsbook is always going to be more profitable on a per customer basis. It turns out that the average punter isn’t actually that price sensitive and the simplicity of a sportsbook basically wins. I wonder if that’s ever going to change?

*Tags: General Technical*

---

**Alex A** - *15:54:57*

I have some mates who like a random gamble, and if you ask them how they’re doing it’s always “Roughly even” and then you show them how to access Net Deposits on whatever bookie they’re using and they see that they’re down hundreds.

*Tags: General Technical*

---

**Alessio** - *18:36:54*

i'd put 5 euros on 'database down' (unreplicated hot-cold RDBMS)

*Tags: General Technical*

---

**Ruben** - *19:46:53*

[@U01C12ZEADQ](@U01C12ZEADQ) I tried to look around for odds streaming services that included [http://betfair.com|betfair.com](http://betfair.com|betfair.com) exchange but the only things I found are crazy expensive services (starting at a few thousand/month) for syndicates and such

*Tags: General Technical*

---

**Sam Asin** - *23:42:09*

If it's a friend who isn't interested in betting anyways, does it put them in a bind? Genuine question, I just don't know.

*Tags: Strategies*

---

## 2021-04-01

**Misha** - *00:09:21*

Anyone successful in Australia with fixed odds agencies uses "bowler" accounts (friends, colleagues, etc). It's the only way to be able to bet with all but a small number of agencies. One person I know has been through about 50 accounts with one agency

*Tags: Errors Debugging*

---

**charlie_v** - *09:08:52*

What odds will people give me for betfair api being stable today...

*Tags: General Technical*

---

**Oliver Varney** - *10:00:19*

exchange isnt looking great still listmarketcatalogue errors ....

*Tags: Errors Debugging*

---

**Alessio** - *11:58:27*

(Also with a system with streaming queues, no matter the tech, good luck with finding where things broke and which state the system is in)

*Tags: General Technical*

---

**river_shah** - *18:33:33*

My suspicion is that for latency sensitive situations is it possible that my account or bets are being routed through a slow component

*Tags: Performance*

---

**PeterLe** - *18:45:50*

By the way, I noticed a post by Oliver earlier about cutting back on some strategies as the exchange was a bit flaky...obviously depends on the strategy but some of my biggest wins have been on days such as these (safety in mind takes precedence of course)

*Tags: Deployment, Strategies*

---

**Oliver Varney** - *18:47:49*

[@UQL0QDEKA](@UQL0QDEKA) its just that I use AI models, when features are empty / havent been seen im unsure whether the outcome will be great or not

*Tags: Feature Engineering, Strategies*

---

## 2021-04-02

**river_shah** - *13:30:41*

Is this the recommended setting for placing orders in play please?

```flumine.config.async_place_orders = True```



*Tags: General Technical*

---

**liam** - *13:51:56*

Potential race conditions that I haven’t had enough sleep to debug 

*Tags: Errors Debugging*

---

## 2021-04-05

**Peter C** - *09:20:06*

Hi All, I have a question about analysing bets placed by my strategy I'd appreciate a sense check on. My strategy places only lay bets, so I say that value is

```value = bsp - price taken```

I sum 'value' for all the bets my strategy places over a period as a quick and easy yes/no for is value being taken - if the total value is positive I say that I have taken value, and if it's negative I say I haven't. I would appreciate anybody's thoughts on whether this is an acceptable way to roughly determine whether my strategy takes value before further assessment of the bets. Cheers

*Tags: Strategies*

---

**RicHep** - *09:33:32*

amateur question here, once I start streaming the raw data from betfair, is there an easy way to get it into a dataframe including the usual fields you'd expect, market_book.market_id,

        market_book.inplay,

        market_book.status,

        market_book.total_matched,

        runner.selection_id,

        runner.status,

        runner.last_price_traded,

        runner.total_matched,

        runner.ex.available_to_back[0].price,

        runner.ex.available_to_back[0].size,using flumine?

*Tags: Feature Engineering*

---

**Peter C** - *09:34:43*

As an aside to this question - what is the worst losing streak you've been on? I've had a frustrating weekend even though I (think) I've beaten BSP both days. Obviously only 2 days but it'd be interesting to hear about other people's losing runs whilst I try and work out whether I am genuinely taking value or not.

*Tags: General Technical*

---

## 2021-04-06

**thambie1** - *16:15:46*

The default is 10 simultaneous streaming connections for the streaming API

*Tags: General Technical*

---

**ian mcneill** - *19:36:53*

Hi [@UBS7QANF3](@UBS7QANF3) - thanks for this - I'm trying to plug this into the multiline JSON that flumine creates. I've desearalized the JSON into the marketbook object - it should be a case of adjusting the keys to match what I need in the DF or is there a more efficient method of creating a single DF (my thoughts are this would need to be 3d with PT as one axis and the runners as another?

*Tags: General Technical*

---

## 2021-04-07

**liam** - *08:02:02*

flumine [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1180-2021-04-07|v1.18.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1180-2021-04-07|v1.18.0) now released, a few depreciations as warned in v1.17, cleared order now available (thanks [@U01MPC0GUK1](@U01MPC0GUK1)), a pretty major backtesting bug and now the ability to process event data as per live, big thanks to [@U01B8031PM1](@U01B8031PM1) on sponsoring this development and allowing it to be open source :smile:



Small change required to allow event processing:

```strategy = ExampleStrategy(

    market_filter={"markets": [..], "event_processing": True},

)```

The `Market` object has a helper method for accessing other linked event markets (live and simulated):

```place_market = market.event["PLACE"]```

This allows you for example to backtest win/place in racing or all football markets as per live, this also only seems to add around 5% in processing time. I don't think there is anything else out there on the market which allows backtesting like this :sunglasses:

*Tags: Errors Debugging, Deployment, Strategies*

---

**Newbie99** - *12:03:18*

[@U4H19D1D2](@U4H19D1D2), I was just trying to play around with that new linked events feature, but am being a bit dense, how do I format for live streaming?



Lets say this is the streaming filter, its not a dict, so where does the event processing key go?



```market_filter=streaming_market_filter(

                event_type_ids=[7'],

                country_codes=['GB'],

                market_types=['WIN','PLACE', 'EACH_WAY']

            )```



*Tags: Feature Engineering, Deployment*

---

**Taking Value** - *22:14:20*

Is there a method that can be called on market_book.streaming_update to segregate each race in the update into its own json/txt file or is this something I would have to code up myself. I am asking for logging purposes. Currently recording all market updates to one file and segregating them into individual races later, its in efficient. Want to record from the stream to a file unique to each race.

*Tags: General Technical*

---

## 2021-04-08

**liam** - *06:07:26*

I am confused, each runner? or each race? Are you using flumine?

*Tags: General Technical*

---

**Rich** - *12:23:49*

Has anyone seen an issue with Flumine where it won't correctly match bets on basic (per minute) data but works fine on per-second data?

*Tags: General Technical*

---

**William Martin** - *16:38:50*

So I've abandoned trying to get data via the api to build the mvp of the new betting product I want to create.



Im trying to get the historical data for soccer for April but when I try and unzip its saying there are errors with the files. do I need something special to unzip a .tar?

*Tags: Data Quality, Errors Debugging, Strategies*

---

**Phil Anderson** - *16:41:01*

Hi. New here. Just trying to get my head round it all.

Is there a way to only get tomorrow's data without inputting the specific date?

Just playing around with some code I found in examples.

```market_start_time={"to": "{0}T23:45:00Z".format(datetime.date.today())}```

This seems to get everything up until 23:45 today

```market_start_time={"from": "2021-04-09T00:00:00Z", "to": "2021-04-09T23:45:00Z"}```

And this will get everything for tomorrow.

But is there to do the same but without having to change the date every day?

Also what is the .format part of the code for? If I put specific date, it seems to work with or without it, but in the first example if I take it out I get an error

*Tags: Errors Debugging*

---

**Chris** - *16:44:06*

[@U01U34X8LHE](@U01U34X8LHE) You can create a variable which gets the current time and adds 1 day to it, example here; [https://www.geeksforgeeks.org/python-datetime-timedelta-function/](https://www.geeksforgeeks.org/python-datetime-timedelta-function/)

*Tags: General Technical*

---

**William Martin** - *18:29:08*

thanks Peter, thats a great help, I managed to get the data I wanted as a result.



Is there a cheat sheet anywhere with team IDs for example?

*Tags: General Technical*

---

**Sam Asin** - *21:45:48*

Hmm weird little half-baked modeling question that maybe I just need to think through a little more, but figured I'd see if someone wants to bounce ideas for a second or just already knows the answer.

*Tags: Strategies*

---

## 2021-04-09

**Taking Value** - *18:42:23*

Sorry each race not each runner. I have edited it accordingly.



Using betfairlightwait. When I set it up a few months ago I just set it record all updates from all races intto one text file each day. I have been segregating the data into a txt file for each race afterwards. Finally got the time to get back to it now and want to set it to record one file per race. Uncertain how to do it. Do I need one stream per race ?

*Tags: General Technical*

---

**liam** - *19:57:25*

No, tbh just use flumine and/or read the source code on how it does it with the market recorder strategy, reasonably straight forward 

*Tags: Data Quality, Strategies*

---

**Taking Value** - *20:04:00*

Ohh I have misunderstood. I thought Flumine was for backtesting and bflw was for recording. Been reading the flumine docs. Just found the market recorder strategy on the Flumine GitHub. I will read it over, thanks very much.



Did you build the BF lightweight library first then Flumine later? Just wondering - Seems Flumine does everything BFL does and then some.

*Tags: Data Quality, Strategies*

---

**liam** - *20:04:46*

Yep, bflw is a wrapper and flumine is a fully functional trading framework that uses bflw 

*Tags: Strategies*

---

## 2021-04-11

**John** - *09:27:54*

Hi I have been following [@U4H19D1D2](@U4H19D1D2)’s great library/page ([https://betfair-datascientists.github.io/historicData/jsonToCsvTutorial/](https://betfair-datascientists.github.io/historicData/jsonToCsvTutorial/)) parsing the advance data to a summary table. As the next step I am very curious to see how to parse the pro/advance historical data to time series. Tried google it and search here but can't find anything. Just in case I miss something, please could somebody give me a pointer? Many thanks!

*Tags: Data Quality*

---

**Taking Value** - *14:19:10*

Has anyone had any issues with flumine trying to delete the the same file twice during clean up when using it to record data? Got the following in the logs. Not certain where the issue is. It was working brilliantly until this occurred.

```{"asctime": "2021-04-10 13:35:39,550", "levelname": "INFO", "message": "Removing: /home/ubuntu/Recorder-test/race_records/33f72766/1.181722823, age: 3784.94s"}

{"asctime": "2021-04-10 13:35:39,551", "levelname": "INFO", "message": "Market removed", "market_id": "1.181722823"}

{"asctime": "2021-04-10 13:41:57,120", "levelname": "INFO", "message": "Removing: /home/ubuntu/Recorder-test/race_records/33f72766/1.181722823, age: 4162.5s"}

FileNotFoundError: [Errno 2] No such file or directory: '/home/ubuntu/Recorder-test/race_records/33f72766/1.181722823'```

*Tags: Errors Debugging*

---

**Jorge** - *15:17:35*

Yes, I get those error logs around 1-5 times per day. I don't know what's the issue. I guess you are using `force_update=True` and `remove_file=True`?

*Tags: Errors Debugging*

---

**Taking Value** - *15:42:39*

`Remove File = True` `Force Update =  False` . Wasn't entirely certain what Force up update is for.  `update after initial closure` is its description in the MarketRecorder class but I am uncertain what it was updating so left it as default False in the context KWARG that is fed to the class.  I have a work around of sorts but every time I have needed a work around with bflw or Flumine to date its just because I didn't get what I was doing with it. Trying to prevent months of using a work around that will likely cause me issues int the long run by getting the solution now.

*Tags: General Technical*

---

**azevedo** - *17:04:18*

[@U01L8D326AK](@U01L8D326AK) you can specify “EX_ALL_OFFERS” when setting up streaming 

*Tags: General Technical*

---

**Taking Value** - *19:36:02*

Can I just check I understand the correct way to use flumine for building a strategy. If I want to build a strategy then I should build a child class of the BaseStrategy class with  unique logic in it that is relevant to my strategy? Sorry if this is a basic question.

*Tags: Strategies*

---

**Taking Value** - *19:42:55*

Yea, thanks for those examples. As I am still relatively new to python and programming this has been a great way to understand the power of python and specifically the way to utilise classes properly.

*Tags: Getting Started*

---

## 2021-04-12

**liam** - *09:32:18*

[@U019HMPCQT0](@U019HMPCQT0) [@U0155J92A7Q](@U0155J92A7Q) there was a bug, I hadn't noticed as I delete all files on cleanup, small fix in [https://github.com/liampauling/flumine/blob/66089f81a8d5cd7f74ffe2d864745e823cb6bf51/examples/strategies/marketrecorder.py#L110|1.18.2](https://github.com/liampauling/flumine/blob/66089f81a8d5cd7f74ffe2d864745e823cb6bf51/examples/strategies/marketrecorder.py#L110|1.18.2) that checks the txt file exists. The reason it checks for the gz first is that we know a market has closed if that is the case, we can't rely on checking the txt file as it might be a very slow market.

*Tags: Errors Debugging, Performance*

---

**Taking Value** - *10:43:19*

Cheers Liam. If I pull the latest version of the relevant py file from the repository is there a way to direct it to add to the races files in the same sub directory on my ec2 environment rather than have it setup a new sub directory for the new stream?



My thought process is that this way it will simply add to the race files that have already been created and contain data rather than create a new sub-directory for the new stream and record the latest data to new files that I then have to merge with the old ones.

*Tags: Getting Started, Deployment*

---

**liam** - *11:47:50*

However bflw can handle this without an issue when backtesting and the data betfair sell also has this problem but obviously isn't as clean

*Tags: General Technical*

---

**Jorge** - *17:08:34*

Hi [@U4H19D1D2](@U4H19D1D2),  for this case, shouldn't he substitute `self.recorder_id = 'his_local_directory'` [https://github.com/liampauling/flumine/blob/66089f81a8d5cd7f74ffe2d864745e823cb6bf51/examples/strategies/marketrecorder.py#L33|here](https://github.com/liampauling/flumine/blob/66089f81a8d5cd7f74ffe2d864745e823cb6bf51/examples/strategies/marketrecorder.py#L33|here) ?

*Tags: General Technical*

---

## 2021-04-13

**John** - *08:40:47*

Hi finally got flumine working after asking betfair help to turn on my streaming. Was hoping to do paper trading or backtesting, but from the ExampleStrategy I don't see a way to record my bets and calculate P&amp;L. I wonder if I am missing something, or I need to write my own class/functions? Please could anyone give me a pointer? Many thanks.

*Tags: Strategies*

---

**liam** - *08:50:33*

Logging control [https://liampauling.github.io/flumine/advanced/#logging-controls](https://liampauling.github.io/flumine/advanced/#logging-controls)

*Tags: General Technical*

---

**liam** - *09:11:21*

np, few examples here as well [https://github.com/liampauling/flumine/tree/master/examples/controls](https://github.com/liampauling/flumine/tree/master/examples/controls)

*Tags: General Technical*

---

**Jono** - *12:44:35*

Hey everyone. i was wondering if along side the score and timeline objects giving insight to the current state of a football/rugby match there was a way of determining the team currently in possession of the ball? I understand some games are limited by what the api actually has available to return (ie small games not having retrievable scores) but in  general is there a parameter or class which indicates the team in possession? cheers for any help

*Tags: General Technical*

---

**Jono** - *13:36:25*

hi there im trying to compile a short list of all the retrievable details of a game from the api and the docs of github have helped clarify pretty much everything i was wondering. Opta seem to do all live updates like you said but i was curious if the team lists are available through the betfairlightweight package?

*Tags: Deployment*

---

**Jono** - *13:42:44*

ok no problem thanks for clarifying its been a great help :slightly_smiling_face:

*Tags: General Technical*

---

**river_shah** - *18:20:41*

cross market near-arbs or arbs can be seen as special case of choice markets. they give you option to agress on the side you like without incurring bid / ask spread. this can have beneficial impact on your strategy, both from trade entry and or exit. not a strategy in its own right but an execution optimization large scale strats should do

*Tags: Performance, Strategies*

---

## 2021-04-14

**KG** - *01:32:57*

in case you guys are interested, in Aus next week we're running a workshop using bflw to back test [https://www.betfair.com.au/hub/horse-racing-tips/|ratings from the Hub](https://www.betfair.com.au/hub/horse-racing-tips/|ratings from the Hub) against historic pricing data - might be up your alley. you're welcome to join live or I can post the YouTube link afterwards if you're keen :ok_hand: [https://www.eventbrite.com.au/e/betfairs-education-workshop-back-testing-hub-ratings-with-tom-bishop-tickets-148531815659](https://www.eventbrite.com.au/e/betfairs-education-workshop-back-testing-hub-ratings-with-tom-bishop-tickets-148531815659)

*Tags: Deployment*

---

**Chris C** - *11:06:38*

Hey, I’ve got a question on the simulated latencies for the backtest. I can see for the backtest the default place latency is 120ms. How’d you come up with this number? Surely the order arrives at betfair faster than 120ms. Does it rest at betfair that long before it gets executed?

*Tags: Performance*

---

**liam** - *11:11:53*

My median round trip is around 130-140ms depending on which way the wind is blowing, exchange cycles are 50ms, so in reality it should be around 80ms ish however:



a) You can change this by overriding in execution

b) Backtesting is never going to be 100% so this latency is trying to be worst case and if you are finding big differences in matching/profit based on this latency I recommend you stop and go live

*Tags: Performance, Deployment*

---

**Chris C** - *11:14:57*

Great, that makes sense. Next question: How do you even measure rtt?

(date_time_placed - date_time_created) ?

*Tags: General Technical*

---

**liam** - *11:15:55*

bflw has a variable called `elapsed_time` see it being logged here [https://github.com/liampauling/flumine/blob/66089f81a8d5cd7f74ffe2d864745e823cb6bf51/flumine/execution/betfairexecution.py#L275](https://github.com/liampauling/flumine/blob/66089f81a8d5cd7f74ffe2d864745e823cb6bf51/flumine/execution/betfairexecution.py#L275)

*Tags: General Technical*

---

**RicHep** - *11:39:08*

Liam, do you have your server set up near Betfairs to get that latency or is that just from the UK?

*Tags: Performance, Deployment*

---

**liam** - *11:43:14*

I use aws eu-west-1, quick enough for me :wink:

*Tags: Deployment*

---

**liam** - *11:46:43*

[https://betfairlightweight.slack.com/archives/C4H05KKMY/p1616952574408400](https://betfairlightweight.slack.com/archives/C4H05KKMY/p1616952574408400)

*Tags: General Technical*

---

## 2021-04-18

**Peter** - *06:38:26*

Welcome [@U01UNG783B5](@U01UNG783B5) I'm also a Laravel developer (since version 3) and the developer and maintainer of this [https://github.com/petercoles/Betfair-Exchange/|PHP Betfair Exchange API package](https://github.com/petercoles/Betfair-Exchange/|PHP Betfair Exchange API package). If I could offer some advice from that perspective. If you don't already know it, learn Python. You can trade with PHP scripts, but Python is better-suited, hugely better tooled for this activity (especially Betfairlightweight and Flumine), and a fairly straightforward jump from PHP.

*Tags: General Technical*

---

**Razvan Gheorghe** - *06:41:26*

I also think when you are dealing with a big amounts of data and you have so much requests python will be a better alternative for this use case.

*Tags: General Technical*

---

**Razvan Gheorghe** - *06:42:18*

I'm a bit sucked right now, because I have no previous experience with python, just some django.

*Tags: General Technical*

---

**Razvan Gheorghe** - *06:43:06*

And I didn't catch yet how to start the platform on my localhost, ubuntu20.10.

*Tags: General Technical*

---

**Peter** - *06:50:50*

My sports trading project is made up of two main parts: individual scripts with a single purpose, such as scrape data from specific site (initiated periodically by cron) or stream horse racing markets with Flumine and react when I see value (runs continuously) and analysis which is done in jupyter notebooks and which generates the graphic stuff. That wouldn't necessarily help if you're wanting to display stuff on web page, which coming from your background I used to do, but if you're the only person looking at it Jupyter notebooks are much quicker and easier to work with. One of the things I'm liking working with Python is escaping the clutches of a framework (Flumine excepted).

*Tags: Strategies*

---

**Razvan Gheorghe** - *06:57:02*

Yes, I will have to probably display data in a website, is more reliable for me than only use on desktop aplication. And probably to use flumine as a backend  for interacting with betfair api.

*Tags: General Technical*

---

**Razvan Gheorghe** - *06:58:44*

What aproach would you take for this? Open API connection from a cloud instance of flumine and get thse data in the web aplication to can create reports, keeping logs, etc?

*Tags: General Technical*

---

**Peter** - *07:11:07*

Back when I was displaying visualisations on web pages, I built an app API to deliver the data I needed in json, which was easy to  feed into one of the javascript charting frameworks. I think it was ChartJS, though there were other options and I'm sure they've moved on since then. I can't talk to web applications in R. Although I did learn some R a while back, BFLW and Flumine persuaded me to focus on Python. Not sure what your concerns with displaying data on a desktop would be, but accepting them, if you're the only person looking at it, you might want to look at Google colab, which is Jupyter Notebooks in the cloud and for free. You can import data and logs from your cloud server for analysis.

*Tags: Deployment*

---

**Razvan Gheorghe** - *07:17:16*

File "&lt;stdin&gt;", line 1, in &lt;module&gt;

NameError: name 'framework' is not defined

*Tags: Errors Debugging*

---

**Razvan Gheorghe** - *07:17:49*

I got this error when I try to run framework.run()

*Tags: Errors Debugging*

---

**Peter** - *07:24:48*

That suggests you haven't instantiated the framework before trying to run it. Are you basing your script on one of the example scripts that comes with Flumine?

*Tags: General Technical*

---

**Peter** - *07:27:41*

As long as you have a recent version on Python and the Flumine dependancies installed (which installation of Flumine should take care of for you) there shouldn't be any special steps needed to run on Ubuntu.

*Tags: Getting Started*

---

**Peter** - *07:30:23*

I would recommend getting one of the example strategy scripts such as "lowestlayer" working first and then adapting that to meet your specific needs.

*Tags: Strategies*

---

**MacrcoPolo** - *13:42:37*

Almost certainly a path error

*Tags: Errors Debugging*

---

**Taking Value** - *17:53:52*

Could I just take the forums opinion on three things from those of you who are active in the horse racing markets.

1. Do you think if I wanted to buy 2 years worth of pro horse racing data from Betfair it would warrant a discount if I asked for one? 

2. Could you find edge with 2 years worth of data. Are there enough data points to form statistically significant conclusions?

3. Has the disruption to the markets due to COVID caused a lot of models/strategies to fail in the past year? I can imagine there were probably additional opportunities too but I am more interested in how badly historical models were affected as I may or may not purchase data from the last year.

*Tags: Strategies*

---

**Michael** - *18:27:55*

I suppose it might depend on what kind of strategy you're thinking of - but I don't see why you couldn't just ask yourself how long you're willing to wait to be in profit then multiply that by three or four. If an edge isn't visible in that then you're not going to be willing to take the variability anyway.

*Tags: Strategies*

---

**Jonjonjon** - *20:09:00*

My main strategy got started with just 1 month of self-recorded data.

*Tags: Strategies*

---

**Dave** - *22:11:19*

nevermind the fact that the data you can record with flumine is a lot cleaner and has the advantage of being exactly the same data you will be trading on :slightly_smiling_face:

*Tags: Strategies*

---

## 2021-04-19

**Taking Value** - *08:39:51*

Is the pro data from BF not the same calibre as the live data recorded from the stream?



What are the difference? I had assumed pro would be of the same level of granularity.

*Tags: Deployment*

---

**Razvan Gheorghe** - *16:51:38*

I think I got more close to start the script under my local. Nice journey in python :)).

Can you please help me find the file where I have to place credentials for the betfair api?

*Tags: General Technical*

---

**Peter** - *17:11:21*

That's a bit of an "it depends" type question. It depends on your wider infrastructure and how you want to manage your environment variables (as that's where you'll probably want to put these credentials). Some people put them in their .bashrc or .zshrc files so that they are loaded when a user account logs in.

*Tags: General Technical*

---

**nthypes** - *18:20:28*

one question: price action in other runners does have a direct impact in each runner ? This influence is high, medium or irrelevant?

*Tags: General Technical*

---

## 2021-04-20

**klozovin** - *17:06:19*

is it possible to only start streaming markets once they turn in-play? looking at MarketFilter options it doesn't seem to be the case...

*Tags: General Technical*

---

**azevedo** - *22:00:44*

I’ve just started playing with the market recorder.py example in flumine, looks awesome! 

does it store the account’s Order stream updates somewhere along with market data updates? 

and what about market_catalogues?

*Tags: Data Quality*

---

## 2021-04-21

**Greg** - *06:47:51*

Stoked..long time lurker..first time poster noob (forgive the dumb questions if they come). Logged in via API and certs after teaching myself some python for the last 3 months:sunglasses: .Didn't think I could do it, let alone with 50 year old chemo brain  (fell asleep at my desk a lot..like an 80 year old man :laughing:) but life is full of surprises. Not sure I will get anywhere but it has been a hoot-like having a new superpower, Thank you Liam for writing this.You are a * and I am grateful. It has given me something to challenge meself with while the family is out (and I'm not sat in an undignified fashion on the toilet) Oral chemo starts tomorrow so thought I better get me thanks in before more turpentine melts my body again :grin: All the best for green across the board folks.:money_mouth_face:

*Tags: Getting Started*

---

**Rich** - *17:09:51*

Hi, how are people storing / keeping track of 'state' in their strategy classes in Flumine?  Do you extend runner_context, use class variables? or something else?  It seems markets share the class instance, so if I put a boolean flag at class level to say "initial bet placed" it only works once (understandably)

*Tags: Strategies*

---

**liam** - *17:10:31*

you have `strategy.context` and `market.context`

*Tags: Strategies*

---

## 2021-04-22

**Unknown** - *03:42:04*

FYI the recording of this session on using bflw to backtest ratings is now available on YouTube: [https://www.youtube.com/watch?v=0UfPdeghuN8&amp;t=1s](https://www.youtube.com/watch?v=0UfPdeghuN8&amp;t=1s)

*Tags: General Technical*

---

**KG** - *03:42:42*

there's also a new tutorial up that goes with the above, walking through the back testing process using bflw: [https://betfair-datascientists.github.io/historicData/backtestingRatingsTutorial/](https://betfair-datascientists.github.io/historicData/backtestingRatingsTutorial/)

*Tags: General Technical*

---

**Jorge** - *11:34:44*

This tutorial is very complete! What is the advantage of backtesting this way vs flumine?

*Tags: General Technical*

---

**liam** - *11:55:09*

Much quicker / easier to manipulate etc. whilst building a model. However imo the next step would be to move the model to flumine (or your own trading framework) so that you can better test implementation/ matching etc.



Quite a few follow the pattern of separating model building/generation and trading framework integration.

*Tags: Strategies*

---

**Oliver Varney** - *11:58:04*

model building stage in my opinion especially on the AI side should be done outside, then validate prices + volumes and matches with flumine.

*Tags: Strategies*

---

**Oliver Varney** - *12:00:03*

realtime prediction depending on the size of the model and latency requirements can either block or personally I stick into into a redis stream queue, picked up by a prediction service, then pushed back

*Tags: Performance, Strategies*

---

**D C** - *12:06:57*

[http://www.totalperformancedata.com/news-all/2021/4/22/tpd-buys-bet-mover-from-level-software](http://www.totalperformancedata.com/news-all/2021/4/22/tpd-buys-bet-mover-from-level-software)

*Tags: Performance*

---

**Jorge** - *12:13:18*

Aha, makes total sense to build the model outside of flumine, thanks guys!

*Tags: Strategies*

---

**Jorge** - *12:14:51*

The tutorial is really good, they use the "PRO" historical data but that can be replaced by the recorded streaming files from flumine

*Tags: Data Quality*

---

**D C** - *13:18:19*

@Dave given the average Joe uses Betfair live streaming that can be significantly delayed it could be a help. Problem is with the accuracy though if you ask me. Sometimes the GPS is not accurate at all. Probably need to be using video AND this to try and manually trade

*Tags: Deployment*

---

**liam** - *13:29:04*

a disclaimer is certainly needed, but [@UGV299K6H](@UGV299K6H) summed it up recently by saying how if you can make money without TPD then including it someway will certainly help

*Tags: General Technical*

---

**Dave** - *17:17:07*

Flumine q: Will there be a big performance difference if I instantiate a strategy twice with different params and add them to a flumine instance Vs adding one strategy that takes both sets of params and applies them based on the market? The former seems cleaner but not sure if you lose some performance by having multiple strategy objects

*Tags: Performance, Strategies*

---

**liam** - *17:18:04*

There will be but will depend on what the strategy does, assuming it’s simple it will be negligible I would just profile it

*Tags: Strategies*

---

## 2021-04-23

**KG** - *00:45:24*

yeah seconding [@U4H19D1D2](@U4H19D1D2)’s point, also it's a lower barrier of entry to use bflw instead of Flumine, particularly for less dev-centric users, which is the primary target here.



and [@U0155J92A7Q](@U0155J92A7Q) is right, should be able to replace with Advanced data set or recorded Streaming files with the same result :ok_hand:

*Tags: General Technical*

---

## 2021-04-26

**Phil Anderson** - *14:51:24*

Yes, thats exactly what I'm trying to do. I tried taking out the bet target type, but then get an error msg when I run it: 'errorCode': 'INVALID_INPUT_DATA', 'errorDetails': 'One or more inputs to the operation were invalid'



Forgive me if it's something simple, but I'm new to this and I'm still trying to get to grips with it.

*Tags: Getting Started, Errors Debugging*

---

**Phil Anderson** - *15:13:22*

It's Ok I figured it out - wouldnt work with bet_target_size. Just had to change it to size. Thanks for your help :+1:

*Tags: General Technical*

---

## 2021-04-27

**John A** - *00:27:47*

Quick question; does the betfair racing data purchase contain placed markets?

*Tags: General Technical*

---

## 2021-04-28

**Newbie99** - *17:51:30*

If I want to access all orders coming through the order stream (i.e. that includes those not placed via Flumine so they won't necessarily form part of a strategy), where should I be looking?



I have setup a background worker and then was heading down this path:



```def get_live_orders(context: dict, flumine) -&gt; None:



    for stream in flumine.streams._streams:

        # Find the order stream and do whatever```

But is there a preferred way to go about this (i.e. something more obvious I've overlooked)?

*Tags: Getting Started, Deployment, Strategies*

---

**Lee** - *18:04:35*

the orders are stored in the blotter on a per market basis (correct me if i'm wrong) so you could try something like this

```for market in flumine.markets.markets:

    for order in market.blotter:

        print(order.bet_id)```

*Tags: General Technical*

---

**Lee** - *18:05:12*

but accessing the orders not placed by flumine might be a problem

*Tags: General Technical*

---

**Lee** - *18:09:08*

you might need to go the raw stream route but similar to the market recorder but changing up the stream from MarketStream to OrderStream

*Tags: Data Quality*

---

**azevedo** - *18:13:01*

Is there a way to then replay your Order Stream (i.e. Current Orders from order stream) in a similar way to Historical market data using the trading.streaming.create_historical_generator_stream?

*Tags: Strategies*

---

**Newbie99** - *18:14:23*

I presume all orders are snapped somewhere and the ones that aren't placed in Flumine just discarded tbh, I just haven't found exactly where! I thought I might be heading in the right diretcion, but perhaps not!

*Tags: General Technical*

---

**Lee** - *18:14:39*

[https://github.com/liampauling/flumine/blob/66089f81a8d5cd7f74ffe2d864745e823cb6bf51/flumine/order/process.py#L99](https://github.com/liampauling/flumine/blob/66089f81a8d5cd7f74ffe2d864745e823cb6bf51/flumine/order/process.py#L99)

*Tags: General Technical*

---

## 2021-04-29

**Unknown** - *09:06:13*

[@UFTBRB3F1](@UFTBRB3F1) we filter only flumine orders through the [https://github.com/liampauling/flumine/blob/dfbad123dcdce6ee7573d23b53a5772dee9d7f38/flumine/streams/orderstream.py#L22|hostname](https://github.com/liampauling/flumine/blob/dfbad123dcdce6ee7573d23b53a5772dee9d7f38/flumine/streams/orderstream.py#L22|hostname), you can remove this filter but you will get warnings as it's likely the strategy wont be available to try and import/create locally

*Tags: Strategies*

---

**Taking Value** - *09:11:35*

If I want to run Flumine over a large amount of data (say a couple of 100 gigs) stored in an S3 bucket is there a more efficient way to do it that copying the files to an EC2 environment? Can they be streamed from S3. Or is this process just a very time consuming process.

*Tags: Deployment*

---

**Mo** - *09:14:21*

You can stream from S3 but unless you're streaming to an EC2 instance you should watch out for fees for transferring data out of AWS

*Tags: Deployment*

---

**liam** - *09:16:19*

It will be quicker from EC2 vs local, I do all my backtesting locally with a local repo/copy of my s3 data, i.e. it attempts to use the file locally if not it downloads. This suits my use case where I tend to use the same files/months over and over

*Tags: Deployment*

---

**Newbie99** - *11:09:05*

hmmm, this is going to get complicated isn't it, as then I need to capture them somehow as obviously the don't get added to the blotter.



As a plan b, I did think of starting up a separate BFLW order stream in a background worker and then just capture any orders not in flumine.markets, but (even assuming that approach works) then I'm using up a valuable stream (although logically when I reach the point where I need all the streams, hopefully all the non-flumine stuff will be fully on flumine so I guess that probably doesn't matter too much)!

*Tags: General Technical*

---

**MacrcoPolo** - *17:09:26*

Not sure if there's an equivalent for AWS but I use GCS Fuse on google and keep data in a bucket, produce outputs locally then post back to the bucket when I'm done

*Tags: Deployment*

---

## 2021-04-30

**Taking Value** - *08:37:33*

Cheers, I have two years worth of horse racing pro data. I was going to build models with one and backtest on the other. I had assumed everyone was just doing backtesting via their EC2 environment. Perhaps I will just download the data locally and see if my laptop can cope with modelling it.

*Tags: Deployment, Strategies*

---

**Taking Value** - *08:45:05*

Yea I have noticed these charges, I thought that I would get the same fees for shuttling files between S3 and EC2 though and wondered if streaming was cheaper to shuttling the file over.



 It also seems most people use S3 as backup storage but have a lot of data stored locally. I had assumed that as everybody uses S3 for storage they would be using EC2 for modelling but actually it seems most model locally so I will do the same.

*Tags: Deployment, Strategies*

---

**Mo** - *08:51:12*

There is no difference between streaming and copying the file. Either way you are transferring it, it’s just a question of whether you are saving a local copy. But there is no transfer cost between S3 and EC2

*Tags: Deployment*

---

**Oliver Varney** - *14:50:14*

[https://github.com/liampauling/flumine/blob/dfbad123dcdce6ee7573d23b53a5772dee9d7f38/flumine/markets/middleware.py#L75](https://github.com/liampauling/flumine/blob/dfbad123dcdce6ee7573d23b53a5772dee9d7f38/flumine/markets/middleware.py#L75)

*Tags: General Technical*

---

**Oliver Varney** - *14:57:23*

what are you trying to do ? is this for backtesting or something? if its not part of the standard workflow of flumine, such as you have captured a price from one hour ago and you want to calculate a price delta, you will need to store a list/ use flumines list which get updated when a removal comes through then if in this example you are comparing prices across time you will need to adjust any old captured prices to reflect any removals that have occurred inbetween the time periods.

*Tags: General Technical*

---

**mandelbot** - *14:58:37*

Indeed I think I can play around with the middleware to adjust my list as well. What do you mean by use flumine's list?

*Tags: General Technical*

---

**Jorge** - *16:06:03*

Hey guys! Can anyone recommend any data provider for European Soccer? I'm looking for historical point in time live data of Scores

*Tags: Deployment*

---

## 2021-05-01

**Jorge** - *12:39:39*

Is there any function in flumine/bflw that I can use to convert a streaming recorded file to a time series of "best available to back" and "best available to lay"?

*Tags: General Technical*

---

**liam** - *12:39:48*

Like the example? [https://github.com/liampauling/flumine/blob/master/examples/strategies/pricerecorder.py|https://github.com/liampauling/flumine/blob/master/examples/strategies/pricerecorder.py](https://github.com/liampauling/flumine/blob/master/examples/strategies/pricerecorder.py|https://github.com/liampauling/flumine/blob/master/examples/strategies/pricerecorder.py)

*Tags: General Technical*

---

**liam** - *12:43:07*

[@UBS7QANF3](@UBS7QANF3) shared something in pandas recently 

*Tags: Feature Engineering*

---

**Jorge** - *12:45:31*

My issue is how to convert a recorded file into MarketBook objects

*Tags: General Technical*

---

**liam** - *12:47:54*

Flumine does it out the box or use bflw low level gen as [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py|per](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py|per) 

*Tags: General Technical*

---

## 2021-05-03

**Steve** - *02:49:52*

Just picking up flumine. It's bloody awesome, thanks Liam and co. Quick questions. Firstly does anyone have some code for adding in the heartbeat API under the strategies start function?

*Tags: General Technical*

---

**onthepunt** - *05:41:35*

What is the best way to not display horses in the 'runners_df' dataframe whose 'status' = 'REMOVED' using the code from [212] and [213] on [https://betfair-datascientists.github.io/api/apiPythontutorial/](https://betfair-datascientists.github.io/api/apiPythontutorial/)?

*Tags: Feature Engineering*

---

**Peter** - *06:37:30*

Don't think you need it. Running a Flumine strategy keeps your connections open, and even restarts them if they fail.

*Tags: Strategies*

---

**liam** - *07:41:06*

Not using the API, you can with the flumine framework as it has backtesting and paper trading built in 

*Tags: Strategies*

---

## 2021-05-04

**Jonjonjon** - *19:41:51*

If I am backtesting a strategy that utilizes limit orders with the market on close persistence type, what is the easiest way to check if it was filled at BSP or not?

*Tags: Strategies*

---

## 2021-05-05

**Jono** - *10:26:12*

has anyone had any success using bflw with the aiohttp package? i have to perform a post request to a database after each event iteration performed and the wait time for the response is slowing the strategy down quite a bit. There isnt anything in the post response that is needed for the next iteration so i was thinking about implementing  some async logic along with aiohttp in order to perform other tasks without holding up the next run waiting for the response. I noticed though there is quite a bit suggesting that regular python requests will hold up an async event loop defeating the purpose of creating the event loop to save time in the fist place. Will this be the case with bflw and if so is ther a way to make my post request to db and not wait for the response before carrying on with the next event in the strategy?

Cheers!

*Tags: Performance, Strategies*

---

**Oliver Varney** - *10:27:02*

why do you need to make a request to a database if you dont mind me asking?

*Tags: General Technical*

---

**Jono** - *10:31:19*

There is an app that uses the database to display current orders and changes to them across multiple betfair accounts and this is away of keeping track of them all. It also has a few other book keeping purposes like linking bets to corresponding events,strategies, and platforms

*Tags: General Technical*

---

**Mo** - *10:34:04*

I run the same strategy across multiple accounts

*Tags: Strategies, Multi Client*

---

**Jono** - *10:35:16*

the database is never going to be available to me locally so wouldnt i still have to make a post request surely? in which case im still held up waiting for the post response before being able to carry on - goin the traditional python request route anyway

*Tags: General Technical*

---

**Oliver Varney** - *10:38:53*

redis does allow for remote connections/hosts but you will still have network latency(better then http request library from my testing). can you not run it in a background thread, continue with the event loop but have some kind of status check that wont fire new orders till confirmation has returned?

*Tags: Performance*

---

**Jono** - *10:40:30*

network latency specifically is the problem here, the database is hosted on a cloud server which regardless of how im keeping track of the bets will always be where i have to send updates to. I dont need the strategy to use any info from this db endpoint i just need to send it updates. the resp time is holding things up. If threading is what sounds best for this issue i will definitely look into it.

*Tags: Performance, Deployment, Strategies*

---

**Oliver Varney** - *10:41:32*

[@UBS7QANF3](@UBS7QANF3) if you dont mind me asking, does that mean you sub accounts with the same strategy fight for the same volume and some accounts will get a greater slice of the pie?

*Tags: Strategies*

---

**Mo** - *10:53:11*

[@U0154JA98TH](@U0154JA98TH) I think you could probably still benefit from a rearchitecture. I doubt you want the component that's placing bets to be accessing the database directly. For example, you could use a separate component accessing the Betfair order stream to update the orders n the database

*Tags: General Technical*

---

**Jono** - *10:53:13*

thanks guys. so certainly latency in the post made at the end of strategy is causing unnecessary time to be wasted. i suppose this kind of goes beyond bflw but basically i want to make this post request and forget about it.

*Tags: Performance, Strategies*

---

**Jono** - *10:54:28*

yes [@UBS7QANF3](@UBS7QANF3) that seems like a solution but one of the reasons i was reluctant to go down that path was trying to work out what bet is associated with each strategy.

*Tags: Strategies*

---

**Oliver Varney** - *10:55:14*

customerorderref and customerstrategyref I think they are

*Tags: Strategies*

---

**Mo** - *10:58:26*

Both references are very useful. Something outside the box you can do is use the stake size to tag orders, for example, encode stakes of £X.01 as one strategy, £Y.02 as another etc

*Tags: Strategies*

---

**liam** - *10:58:57*

Just create a new thread with a queue that handles all slow / db requests, flumine does this via logging control

*Tags: Performance*

---

**Jono** - *11:02:33*

Thats looks just like the field i'd need to do as you suggested [@UBS7QANF3](@UBS7QANF3) and set up an independant script keeping track of all relevant orders to the strat :ok_hand:

similarly [@U4H19D1D2](@U4H19D1D2) [@ULDAVFDRP](@ULDAVFDRP) i'll try out generating a thread to handle the slow requests in the event i run into problems attempting to separate the post to the db from my current strategy logic. Thank you very much everyone, ill shoot back with anything else i get held up with along the way, the helps been class as usual

*Tags: Performance, Strategies*

---

**liam** - *11:07:19*

Unless you are async best practise is to remove any blocking io/network requests from your strategies main thread, this will prevent latency or network errors causing issues

*Tags: Errors Debugging, Performance*

---

**Jono** - *14:59:14*

So far its looking as if introducing a separate thread to handle the latency filled request solves this issue ive been struggling to get around. I was wondering if the trading object can be used in multiple threads in order to speed up run times further? For instance if i wanted to run a strategy over 10 events currently listed on the exchange would it be possible to initiate a thread per event, pass each thread the trading instance, and then place/adjust orders without causing problems?

*Tags: Performance, Strategies*

---

**Oliver Varney** - *14:59:41*

are you using flumine ?

*Tags: General Technical*

---

**Oliver Varney** - *14:59:54*

or betfairlightweight?

*Tags: General Technical*

---

**Jono** - *15:00:09*

just bflw not flumine

*Tags: General Technical*

---

**Oliver Varney** - *15:00:30*

Id 100% advise looking at flumine, [@U4H19D1D2](@U4H19D1D2) has provided a gem

*Tags: General Technical*

---

**Jono** - *15:03:32*

would flumine allow for the afformentioned post to a database after each "iteration" or would this cause problems?

*Tags: General Technical*

---

**Oliver Varney** - *15:07:31*

if an iteration is a market book update then you will never be able to keep up whatever approach, if its once you made a trade and your waiting for that to update your database should be okay assuming you not looking to run high frequency strategies

*Tags: General Technical*

---

**Oliver Varney** - *15:14:31*

The update frequency can be controlled, but if for example your requirement is to raise a trade off the back of an market book update, send it to an api somewhere update the database and get a response, you can just choose to skip updates till you have that response. In this situation you wont block the main thread and backlog market books

*Tags: General Technical*

---

## 2021-05-06

**Sam Asin** - *00:08:28*

I am pretty sure there are detailed discussions in here somewhere about commissions and I'll do a search and find them before actually asking a question, but yeah, for the less sophisticated/newer people like me, be careful abotu this stuff, it is sometimes absurdly high. Probably quickly becomes worth the work to figure out how you should minimize your commissions, even if you don't make that much money.

*Tags: General Technical*

---

## 2021-05-07

**liam** - *12:09:10*

Anyone got experience in optimising their database (MySQL) when approaching 100m+ of rows? Indexes aren't cutting it anymore so looking at partitions however my issue is that I always have to join 3 tables to get the data I want market (1m+), trade (100m+) and order (100m+), or I might create another table/materialised view / UNF data warehouse style that sits on top

*Tags: General Technical*

---

**Rich** - *12:32:59*

Is most of it historical? could you archive into another table, and only keep x days/months in the current tables? (or [https://www.digitalocean.com/community/tutorials/understanding-database-sharding](https://www.digitalocean.com/community/tutorials/understanding-database-sharding))

*Tags: General Technical*

---

**Dave** - *12:36:44*

Generally the problem is most easily solved by chucking more money at it.

*Tags: General Technical*

---

**liam** - *12:37:17*

its all live order/trade data so constantly updating and my problem is only going to get worse, using Aurora MySQL

*Tags: Deployment*

---

**Dave** - *12:37:48*

Enabling row compression also can help provided your bottleneck is not in CPU but rather holding the index in RAM.

*Tags: General Technical*

---

**Rich** - *13:02:05*

That happens when the database engine tries to order the entire table, then you only actually want a few rows from it.

*Tags: General Technical*

---

**John** - *22:23:23*

Hi I was trying to do paper trading for a strategy with a range of parameters values (e.g. context{"a": 0.1}, 0.2, and 0.3) in flumine. I was using only one script to run all three systems by adding 3 strategy objects to the flumine framework. Since they were using the same strategy class, when writing the logs at the logging control (using the flumine example),  I was not able to differentiate these three systems. I wonder if there is a way for me to pass those parameter values (a=0.1, 0.2 and 0.3) from the BaseStrategy class to the LoggingControl class so I can differentiate these 3 systems? Any pointer will be highly appreciated!

*Tags: Strategies*

---

**Lee** - *22:49:04*

[https://github.com/liampauling/flumine/blob/master/flumine/order/trade.py#L31|https://github.com/liampauling/flumine/blob/master/flumine/order/trade.py#L31](https://github.com/liampauling/flumine/blob/master/flumine/order/trade.py#L31|https://github.com/liampauling/flumine/blob/master/flumine/order/trade.py#L31)

*Tags: General Technical*

---

**Alex** - *23:14:01*

Hi everyone, I have a couple of questions and was hoping someone could help me or point me in the right direction. My goal is to simply have an interactive, live representation of the orderbooks for the (live) matches in different sports markets (to begin with, only MATCH ODDS). I am using the bflw library for that as follows: I subscribe to different sports markets (tennis, football, ...) as a whole in my ‚streaming class‘, as there is a limit of how many streams are allowed to be running at a time. This class has an attribute called marketbooks, which is a dictionary (market_ids as keys) that is updated with the most recent streaming updates – therefore, older updates are simply overwritten – in an infinite while loop. I pass this dynamic marketbooks attribute to another class, which is basically dynamically starting processes that take in the marketbooks and filter it for the specific market_id assigned to the thread, thus only processing the orderbook for this specific market_id. In this sub-class I then log the changes in the orderbook in yet another while loop and potentially feed the data into a tkinter gui. While this is no problem with a single market_id and I get live data in my console, I am struggling with a concurrent implementation using asyncio that would start all processes concurrently, so that there is no delay (I tried it with threading first, but ran into issues).

My questions are: Does this implementation make sense? Is it efficient or am I overcomplicating things? How can I easily see – in my setup or in the optimal setup – if a game has ended? How would the asyncio logic look like so that I don’t wait on any of the infinite while loops to finish?

 If this is not appropriate to discuss here, I would appreciate if someone willing to help could send me a private message. Thanks!

*Tags: Getting Started, Deployment*

---

**Newbie99** - *23:17:19*

[https://github.com/liampauling/flumine](https://github.com/liampauling/flumine)

*Tags: General Technical*

---

## 2021-05-08

**Peter** - *08:32:49*

@Alex. the broad approach makes sense, but as [@UFTBRB3F1](@UFTBRB3F1) says, Flumine already does all the heavy lifting for you. Wrapping your head around it and refactoring your approach into the framework isn't trivial and takes some time, but once there, you'll wonder why you ever tried to do it all yourself, Knowing when a game has ended isn't trivial from the market_book unless you just need to know that it's over, with no time constraint, in which case a market.status of CLOSED would do it. If you need something more timely there is an undocumented scores API that BFLW can access for you.

*Tags: General Technical*

---

**liam** - *11:42:44*

You get the cleared order object [https://github.com/liampauling/flumine/blob/4924616d3764ab5ea861a21a72054dc93d812d61/flumine/order/order.py#L78|https://github.com/liampauling/flumine/blob/4924616d3764ab5ea861a21a72054dc93d812d61/flumine/order/order.py#L78](https://github.com/liampauling/flumine/blob/4924616d3764ab5ea861a21a72054dc93d812d61/flumine/order/order.py#L78|https://github.com/liampauling/flumine/blob/4924616d3764ab5ea861a21a72054dc93d812d61/flumine/order/order.py#L78)

*Tags: General Technical*

---

**Mo** - *16:17:03*

In January, Betfair ran a survey which included a question on preferred potential API features. One of these was a bet queue indicator: [https://www.surveymonkey.co.uk/r/YW3RFGJ](https://www.surveymonkey.co.uk/r/YW3RFGJ)

*Tags: Feature Engineering*

---

**Sam Asin** - *16:50:55*

[@U012XF5CNPN](@U012XF5CNPN) Yeah I get that logic, but there are some confusing situations especially when I'm dealing with the site and whatever potential latency there is. I.e. batb is 6, I put  $20 offer at 6.2, and the next update I get says there is $180 available at 6.2. I then put a $20 offer at 6.4, and next update there is $150 at 6.4 as well. Is someone getting in front of me? Behind me? It's hard to say.

*Tags: Performance*

---

**Sam Asin** - *17:01:03*

My background isn't really HFT, I just helped my friend with some stats etc. for his job a bit, and was looking at it as he was doing it, but yeah he could see it. I don't know if that's true for all exchanges, just the one time I was looking at one!

*Tags: General Technical*

---

**D C** - *19:18:34*

It doesn't answer your question but I am sure I read somewhere they were pulling this down to 12 hours

*Tags: General Technical*

---

## 2021-05-09

**Sam Asin** - *00:30:39*

so back with random commission crap, does this make sense to anyone? I actually decided to contact the help desk about it this time:

*Tags: General Technical*

---

**Dave** - *16:53:37*

&gt;  And i should probably just focus more on the other markets.

If you have a decent edge in To Score then there's no reason not to focus on it! It seems pretty illiquid so easy to be top of book while maintaining large margin over your model price

*Tags: Strategies*

---

## 2021-05-10

**Alex A** - *22:15:58*

Bit late, but a lot of exchanges either offer a price depth feed like Betfair, an “Order by order” feed where you can see exactly where you are in the queue and the breakdown of all orders ahead or behind you, or both, one typically with better latency than the other.

*Tags: Performance*

---

## 2021-05-11

**Alessio** - *08:12:52*

Since we were chatting about it the other day, re: databases, [https://www.narrator.ai/blog/using-postgresql-as-a-data-warehouse/](https://www.narrator.ai/blog/using-postgresql-as-a-data-warehouse/) contains a bunch of tricks for O(100M) tables

*Tags: General Technical*

---

## 2021-05-13

**Greg** - *01:08:50*

managed to place a bet using Flumine-bit of a win:grin: My limited brain can't see how to place a bet at best price -1 tick for example though.Looking through examples but if someone can point me at a simple command that would be great.Thanks.Enjoying the process

*Tags: General Technical*

---

**Oliver Varney** - *06:57:36*

Do you just mean the second index in the list of available to back /lay? might also be worth looking in the flumine utils and using get_nearest_price

*Tags: General Technical*

---

**santibat** - *15:46:08*

HI, is there a Github repo that has already worked out how to download all the data from the api? (all sports, all dates)

*Tags: General Technical*

---

**Peter** - *15:53:56*

Like [@U016535QCJ2](@U016535QCJ2) said. There is a marketrecorder example in the Flumine repo that sows how to collect streamed data but you still need to make selections as to sports, countries and markets and Betfair will limit the number of markets you can have in a stream so that would be another reason why you couldn't have them all.

*Tags: General Technical*

---

## 2021-05-14

**Jono** - *14:03:25*

Im currently getting to grips with the customerRef and strategyRef fields. After an order has been successfully placed whats the best way to display all current orders along with their associated references? i can see they arent in the standard current order response in the docs. Thanks

*Tags: Strategies*

---

**Jono** - *14:08:07*

sorry to clarify: just iterate through all current bets printing bet_id, size_remaining etc and customerRef/strategyRef

*Tags: Strategies*

---

**Newbie99** - *14:11:11*

[@U0154JA98TH](@U0154JA98TH) are you streaming or using the REST API (if the former you can get them from the order stream and if the latter you need to make an API call using list_current_orders (or cleared if settled)?

*Tags: General Technical*

---

**D C** - *14:14:47*

customerStrategyRef applies to ALL bets supplied to a single call of placeOrders whereas customerOrderRef is applied PER bet.

*Tags: Strategies*

---

**D C** - *14:15:09*

I don't think placeOrders actually returns the strategy ref in the response

*Tags: Strategies*

---

**Jono** - *14:16:34*

ok maybe ive been using the wrong field - and its the rest api at the moment. I want to keep note of what strategy part each bet corresponds to. i want to do that by appending a string to the order saying basically the strategy id and bet part something like    "id:123 bet_part:69 ". this will allow a seperate script to run independently making a note of the orders without having to bother doing it in the actual strategy bot. is there any way to retriving  all current orders along with their references - so not filtering for them actually retrieve all strings. what is the best field to use in order to do something like this

*Tags: Strategies*

---

**D C** - *14:18:45*

So for me with each distinct strategy I will generate a unique strategy string which forms part of the customer order ref for each bet placed under that strat - like your example. I then parse that customer order ref to identify which strategy the order belongs to.

*Tags: Strategies*

---

**D C** - *14:19:55*

You can supply a strategy string to filter listCurrentOrders so get all bets under that strategy.

*Tags: Strategies*

---

**liam** - *14:19:57*

```this will allow a seperate script to run independently making a note of the orders without having to bother doing it in the actual strategy bot```

why the added complication? This isn't very scalable

*Tags: Strategies*

---

**Jono** - *14:24:25*

for the purposes of filling in an orders to database that an app then uses to display bets, strategy performance etc. Sending the payloads to db directly during the stragey was slowing runtime quite so moving things to this other independent script seemed like a prefereable avenue

*Tags: Performance, Strategies*

---

**Jono** - *14:32:09*

it is an ever growing table currently so no doubt you are right. I tried out the threading solution you suggested about a week ago and saw a significant improvement in speed. Eventually tho it seemed to cause a performance dip when climbing to a high number of threads as the backlog of payloads gradually climbed

*Tags: Performance*

---

**liam** - *14:33:05*

you only need a single thread dedicated to recording to your db, should be zero performance impact

*Tags: Performance*

---

**Jono** - *14:43:01*

ok currently i have a things set up so that at the end of each strategy iteration a new thread is generated which sends a payload to the db and is then killed. I thought this would be the best thing to do in the likely scenario where the strategy has completed the next iteration before the previous thread is finished receiving a response from the db api for the previously sent payload. In the event i keep them in a queue to be dealt with in a single thread this will still result in rapidly growing queue of payloads that i imagine will cause problems down the line.

*Tags: Strategies*

---

**MacrcoPolo** - *14:48:02*

I might change my username to IHateDatabases...

*Tags: General Technical*

---

**Jono** - *14:51:04*

ok payload might be the inccorect term its just a json of basically the current order information, bet_parts, and strategy ids. - These are easy to keep track of inside the actual strategy script during run time but independently deducing which strategy and bet part an order is associated with is proving a difficult task

*Tags: Strategies*

---

**Jono** - *14:54:24*

so thats the problem. Threading has significantly improved run time, now i want increase it further by having independant record keeping of the bets outside the strategy script entirely.

*Tags: Strategies*

---

**Dave** - *14:56:04*

Orders can be modified unexpectedly days after settlement tbf so a database isn't too bad a choice given this mutability. But I'd agree that for data at this scale, flat files are way better generally.

*Tags: General Technical*

---

**Jono** - *15:05:31*

well it does have its inefficiencies for sure. I feel having an independant script take regular snapshots of the listed orders will help improve things - among other things itll also just be useful to know if i can append unique strings to say 5 orders and then retrieve those orders and associated strings. Looping back to can you pull through a beftair accounts current orders along side the references they were orignally placed with?

*Tags: General Technical*

---

**Misha** - *15:06:58*

The power of a database is the ability to do complex queries on millions of records almost instantaneously, and a well-optimised good database will easily cope with 100s of millions of records and more

*Tags: General Technical*

---

**Misha** - *15:08:01*

I store in my database all historical tennis points - around 50 million

*Tags: General Technical*

---

**MacrcoPolo** - *15:08:26*

Yeah, still that fits happily in memory and will load in a few secs into pandas

*Tags: Feature Engineering, Performance*

---

**Jono** - *15:21:51*

honestly tho do you think my concern of the back catalogue of json objects is valid due to the limited rate that the db can process them? I witnessed a slow down and assumed it was due to many existing threads hindering performance after multiple iterations. if i instead changed things to just have a single thread with a queue of the json objects it would solve the many threads issue but not the timing issues resulting in dozens (potentially hundreds or thousands) of payloads waiting to be sent and processed.

*Tags: Performance*

---

**liam** - *15:27:05*

Yeah almost certainly, run a threading.enumerate to find out how many. My question is why so many updates 

*Tags: General Technical*

---

**Paul** - *23:28:11*

Most people probably don’t want to stand a lot of infra up, so basically just write as fast as possible to anything that works, and have other stuff use that as input to give you info you want to read. If you care about latency: do it all in memory.

*Tags: Performance*

---

## 2021-05-15

**Greg** - *06:04:15*

Hi all. ..some more noob questions : what's the best way of ensuring you only have 1 matched bet on a runner? I have tried runner.total_matched == 0 and runner_context.live_trade_count == 0 but neither seem to work i.e.: I get multiple bets on each runner.Thanks

*Tags: Deployment*

---

**liam** - *06:07:40*

Set [https://github.com/liampauling/flumine/blob/8c69f49f28c145c9f4ca4d0130067e4277975c4e/flumine/strategy/strategy.py#L51|this](https://github.com/liampauling/flumine/blob/8c69f49f28c145c9f4ca4d0130067e4277975c4e/flumine/strategy/strategy.py#L51|this) to one when initialising the strategy 

*Tags: Strategies*

---

**Greg** - *06:21:56*

yes thanks Oliver. Probably but I am still fumbling around in the dark and trying simple things first.  :grin:Related : I couldn't figure out how to target a payout using a limit order . I am using something similar to this order_type=LimitOrder(price=2, size=5) Payout would be more useful but tried a variations but has me stumped

*Tags: Deployment*

---

**Oliver Varney** - *06:35:24*

for me I use a trade level exposure (derived from strategy +predictions strength etc), wtih the trade containing the logic to fire additional orders

*Tags: Strategies*

---

## 2021-05-16

**birchy** - *15:03:45*

Can anyone comment on how Flumine paper trading compares to backtesting? I'm just wondering whether to go from backtesting straight to live or run paper trading for a short while first?

*Tags: Deployment, Strategies*

---

**liam** - *15:31:30*

The same as it uses the same simulation logic, only difference will be the use of live data, can be a good final integration test 

*Tags: Deployment*

---

**Peter** - *16:35:02*

+1 for final integration. I find it useful as the markets arrive as they will for live trading, i.e. in a giant heap rather than neatly ordered in sequence. This can be especially significant when acting on signals from another market as backtesting's event_processing groups related markets nicely, but live data doesn't always exhibit the same helpful structure, especially when you can't rely on event_id as your group index.

*Tags: Deployment, Strategies*

---

## 2021-05-17

**Sam Asin** - *02:10:35*

kinda embarassing question to ask since I've been a "data" person for so long, but when you're comparing different models against each other to predict some sort of numeric value, do people look at other stuff other than the correlation between the predicted number and the actual number?

*Tags: Strategies*

---

**Sam Asin** - *02:11:40*

i.e. if i have two different models that try to predict the number of crosses a footballer will hit in a game, and then they predict stuff out of sample, and i can take mod1_prediction mod_2prediction and the actual crosses, anything fun I should do other than correlate them to the crosses?

*Tags: Strategies*

---

**Sam Asin** - *02:12:48*

Right now i'm having fun with some models where I'm gonna do my best to linear regression and see how good I can predict various stats, and then see about applying the ML stuff I never messed with. Curious what the "criteria" should be, if that makes sense.

*Tags: Strategies*

---

**Greg** - *06:05:45*

been sitting here for hours and can't for the life of me figure out 1. how to lay at the back price (or x ticks above it). I tried reading into the 'available_to_back' results obtained for each runner but have failed. The utils.py 'get_nereast_price' function is above my pay grade. Someone got a manual? :slightly_smiling_face: Might have to pay someone at this rate. Can anyone tell me how I lay on the back side as a start please : e.g. side="LAY", order_type=LimitOrder(price=20, size=5) thanks

*Tags: Errors Debugging*

---

**Oliver Varney** - *06:16:52*

[@U012PA3M7ST](@U012PA3M7ST) it may be worth firing up betangel (if you have it ) in ladder mode so you can visualise what the data from bflw / flumine provides. The available_to_back and available_to_lay should be sorted so index 0 will be the first price available to back at or lay at. get_nearest_price may look complicated in terms of implementation to you but the result should be fairly simple and should act as expected. It may be worth wring a few test case and see if your results match your expectation from your input.

*Tags: General Technical*

---

**Greg** - *06:28:02*

Thanks [@ULDAVFDRP](@ULDAVFDRP) . I have done BA automation for years (bots are running for weeks as we speak) and can see what is delivered in the amount_available_to_back in terms of price and size(when I print the output) but for some reason I can't extract it so instead of price=20 I should have something like price=ex.available_to_back[0].price (in my head) but the ex has an error so I have something wrong somewhere. learning python as one goes is very hard for the 50 yr old brain

*Tags: Errors Debugging, Deployment*

---

**Aaron Smith** - *06:29:40*

[@U012PA3M7ST](@U012PA3M7ST)

placing a lay order (copied from flumine example):

```trade = Trade(

                    market_id=market_book.market_id,

                    selection_id=runner.selection_id,

                    handicap=runner.handicap,

                    strategy=self,

                )

                order = trade.create_order(

                    side="LAY", order_type=LimitOrder(price=1.01, size=2.00)

                )

                market.place_order(order)```

Here price is just set to 1.01 as an example. If you want to place at the current available to back price, you can retrieve it beforehand and fill it in.

```best_atb = runner_book.ex.available_to_back[0]['price']```

and now you could put this in as `price = best_atb` .

`runner_book.ex.available_to_back`  is a list, with the first element being the best price and then going through the ladder of all available prices (or limited by how many you are requesting). So if you wanted to lay at the second best back_price, its  `runner_book.ex.available_to_back[1]['price']`  etc. This is not necesarrily the next best tick, as theoretically the next best available back price could jump over one tick as nobody placed any bet there, but that is no problem. The `get_nearest_price`  function does exacltly what it says, but may not actually be the perfect fit for what you are looking for. Instead you should maybe check `price_ticks_away` , which is also in utils.py

*Tags: Strategies*

---

**liam** - *07:03:41*

Note that it’s .price in bflw and [“price”] in flumine as we patch the resource so things are faster 

*Tags: General Technical*

---

**AndyL** - *21:27:57*

Just downloaded bflw, setup certs, ran exampleone.py, worked easy peasy :-)

*Tags: Getting Started*

---

## 2021-05-18

**Oliver Varney** - *08:29:50*

hes trying to get his hong kong model ready for happy valley tomorrow :stuck_out_tongue_winking_eye:

*Tags: Strategies*

---

## 2021-05-19

**Jono** - *11:09:10*

i know its a broad question but just curious if anyone benefits from the erratic motions in the market generally surrounding a suspension

*Tags: General Technical*

---

**AndyL** - *11:41:06*

Quick question, i set the marketrecorder running yesterday and it terminated itself at 00:04 last night, no reboot of the vps 

Is there some flumine timeout or anything that might have caused it to terminate?

Ive set it running again .. will see if it happens again tonight

*Tags: General Technical*

---

**azevedo** - *11:45:57*

another qn on market recorder. if you set force_update to True, what’s the expected behaviour vs it being False?

*Tags: Data Quality*

---

**Dave** - *11:46:26*

two things that have caused my flumine to die: oom kills and segfault due to dodgy JSON libraries. Latter might be fixed now so maybe it was the former. Otherwise my flumine instance runs for months with no issues.

*Tags: Errors Debugging*

---

**Dave** - *23:05:08*

This might be a stupid question by why not compute PNL yourself from ClearedOrder objects? Yes there are quirks in PNL computation such as the one you've seen with To Score markets, but should be fairly straightforward otherwise?

*Tags: General Technical*

---

**Sam Asin** - *23:06:26*

definitely not a stupid question

*Tags: General Technical*

---

**John** - *23:22:26*

Hi all, I was using flumine for AU horse racings, and was trying to differentiate thorough bred and pace/trot races (by checking if "pace" or "trot" appears in the race name). When backtesting with the PRO data, you can see race name (e.g. 'Redc (AUS) 1st Apr') from _market_book.market_definition.name_, but when in live/stream API, this name is a None. Guess I am missing something. Please could anyone point me to the right place in flumine to check this race name? Thanks!

*Tags: Deployment*

---

**Lee** - *23:24:28*

Add EX_MARKET_DEF to the streaming filter

*Tags: General Technical*

---

## 2021-05-20

**Unknown** - *00:05:56*

Thanks [@UUCD6P13J](@UUCD6P13J). I am using the default value for _market_data_filter_ in the BaseStrategy class, so EX_MARKET_DEF is added. I added it anyway when init my strategy class, but the the name is still a None.  Do I miss anything?

*Tags: Strategies*

---

**liam** - *05:54:13*

[@U01TXCYTCSF](@U01TXCYTCSF) that is historical data only [https://github.com/liampauling/betfair/blob/ee4488ddb4f4bee775450f7497c08ea0fb470e9b/betfairlightweight/resources/streamingresources.py#L171|https://github.com/liampauling/betfair/blob/ee4488ddb4f4bee775450f7497c08ea0fb470e9b/betfairlightweight/resources/streamingresources.py#L171](https://github.com/liampauling/betfair/blob/ee4488ddb4f4bee775450f7497c08ea0fb470e9b/betfairlightweight/resources/streamingresources.py#L171|https://github.com/liampauling/betfair/blob/ee4488ddb4f4bee775450f7497c08ea0fb470e9b/betfairlightweight/resources/streamingresources.py#L171)

*Tags: Data Quality*

---

**Peter** - *06:21:13*

nohup and &amp; has created streams for me that have have run for over a year untouched for me. I'd almost like a problem as I'd like to switch to Docker, but if they ain't broke I don't want to fix them.

*Tags: Errors Debugging*

---

**John** - *09:15:24*

Thanks [@U4H19D1D2](@U4H19D1D2) for pointing this out. I would like to differentiate between pace and thorough bred races. I wonder what's the best way/place to do this in flumine please could you/anyone give me a pointer? Thanks!

*Tags: General Technical*

---

**liam** - *09:16:04*

store the catalogue, the s3 recorder does this by default [https://github.com/liampauling/flumine/blob/a58380b2775f4741bb5dd6d737ee717fe8abfaf2/examples/strategies/marketrecorder.py#L214](https://github.com/liampauling/flumine/blob/a58380b2775f4741bb5dd6d737ee717fe8abfaf2/examples/strategies/marketrecorder.py#L214)

*Tags: General Technical*

---

**John** - *09:20:02*

No problem at all! I appreciate you were trying to help really.

*Tags: General Technical*

---

**liam** - *09:22:04*

You can then use [@UUCD6P13J](@UUCD6P13J) [https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py|middleware](https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py|middleware) to load it back in for backtesting

*Tags: General Technical*

---

## 2021-05-21

**AndyL** - *17:37:30*

If i am running marketrecorder for GB and AU horse racing, what's the general opinion on running 1) one python process and flumine client with 2 strategies 

2) run 2 python processes, presume flumine client can allow that?

3) run all in a single strategy 

Or other?

*Tags: Strategies*

---

**Lee** - *17:56:32*

Are you asking about adding the MarketRecorder strategy twice, once for each country?

*Tags: Strategies*

---

**AndyL** - *18:02:57*

Yes, or running in separate python processes

*Tags: General Technical*

---

**Lee** - *18:05:39*

Ask Betfair support to increase your number of markets per a connection and have the one strategy recording all countries for horse racing WIN market type.

*Tags: Strategies*

---

**AndyL** - *18:08:28*

So as it stands i would need multiple python instances?

*Tags: General Technical*

---

**Lee** - *18:12:26*

You could get away with multiple streams (a strategy per country) if you didn't raise the limit, but AU has a lot of markets so you hit the limit quite quickly

*Tags: Strategies*

---

**ian mcneill** - *19:33:28*

Evening all - I've been using bflw/flumine for around a year recording data and analysing to develop potential strategies. So very simple stuff so far - basically following the examples. I'm now beginning to dig into leveraging the "execution" side of flumine. I am struggling to fully understand but will keep digging and trying to ascertain what needs to be done. In the meantime, I wonder if anyone would be able to check my thinking. I'm looking at the example strategy at [https://liampauling.github.io/flumine/](https://liampauling.github.io/flumine/)

My current reading of this is:

• don't run the strat if the market is closed

• check every runner, if the ltp is less than 1.5, prepare a lay order at 1.01, for £2 (not liability, so .01 *2) 

• execute the orders, then every x millisecond (20?) reduce the order by 0.02 if it is still 2.00 and move it up a tick to 1.02

Apologies if that is way off the mark - will continue to read and experiment but if anyone has examples of hypothetical strategies and how that looks in even pseudo code, would be greatly appreciated.

*Tags: Strategies*

---

**Lee** - *19:40:42*

did you see this example too? [https://github.com/liampauling/flumine/blob/master/examples/strategies/lowestlayer.py](https://github.com/liampauling/flumine/blob/master/examples/strategies/lowestlayer.py)

*Tags: General Technical*

---

**liam** - *20:06:01*

It would be good to add to the examples and I am happy to code them up, what do you want to see / what would help explain?

*Tags: General Technical*

---

**ian mcneill** - *20:26:58*

Thanks [@U4H19D1D2](@U4H19D1D2) - one area I am looking at, IR, is when there is a build up of money on the back side of a runner at an XO during a particular relative % of time in the race, sustained over a few frames (in my head, a frame is 20ms) - there's more to the strategy than that but would be a good starting point for me to understand the framework and build up the rest from that.

*Tags: Strategies*

---

## 2021-05-22

**PeterLe** - *09:08:09*

Thanks [@U0128E7BEHW](@U0128E7BEHW) Im new to Python (and programming in general), so I guess I would create a variable using that formula, but where in the lowestlayer code would I insert it?Thanks

*Tags: Getting Started*

---

**Dave** - *09:12:58*

Oh I thought you were pretty pro with flumine already [@UQL0QDEKA](@UQL0QDEKA) so not sure if you're just pulling my leg here! Nevertheless, shove it in process_market_book :)

*Tags: General Technical*

---

**PeterLe** - *09:16:57*

Thanks [@U0128E7BEHW](@U0128E7BEHW) No all my stuff is written in C#, i have a programmer, but I has some time over the next few months to start learning Python myself.  I managed to get the market recorder working yesterday, so that was an achievement :grinning:

*Tags: Data Quality*

---

**liam** - *09:56:10*

Need some error catching on that 

*Tags: Errors Debugging*

---

**PeterLe** - *13:25:35*

Just a general question please about running the market recorded in the examples folder.

If I were to run it it on a live sub account (by that I mean an account that is already running a strategy not using Bflw/Flumine). This other program is submitting and matching thousands of bets a day.

the combined amount of markets subscribed to by both my own program and Bflw/Flumine would be less than 200.



Would this be seen in a bad light by Betfair, the fact that Im connecting to a single account by more than one program?

Thanks in advance

*Tags: Deployment, Strategies*

---

**PeterLe** - *15:05:34*

apologies for the very basic question again; If I wanted to set the market recorder to only record the inplay (horseracing win) data, is that easy to do?

I can see that this is already in the code:

strategy = MarketRecorder(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB", "IE"],

        market_types=["WIN"],

    ),

So that is OK.

I thought it may be by changing the in_play_only: bool value in filters.market_filter?

Or am i completely looking in the wrong place! (more lIkely)

Thanks

*Tags: Data Quality, Strategies*

---

**liam** - *19:35:59*

Agree with the above, flumine has flags for skippping pre play data when backtesting :wink:

*Tags: General Technical*

---

## 2021-05-23

**PeterLe** - *12:18:59*

Folks quick question please. If you were to calculate the round trip time from server to the betfair api and then the return response to be say 5ms plus the bet delay of say 1000ms ie a combined 1005ms, what would be the first leg to betfair (as it goes via the DDOS provider.  I think i remember [@UBS7QANF3](@UBS7QANF3) saying the first leg is longer than the return but just wondered if anyone had an approx idea please? ie does the first leg make up 60% of the total time etc. Thanks

*Tags: Deployment*

---

**thambie1** - *12:25:49*

[@UBS7QANF3](@UBS7QANF3) probably understands this stuff better, but here's how I would figure it out. If subscribing to the order stream, you can get the publish time of any orders you place. Thus you can figure out the matching time on betfair servers. The time between the matching time and when you got the return response, is the second leg. Though for the first leg, it includes both latency, bet delay, plus some part of the 50ms delay between matching cycles (which in theory on average should be 25ms).

*Tags: Performance, Deployment*

---

## 2021-05-24

**magiclevinho** - *08:36:35*

Dear Sirs, My problem is i can't get "traded volume" data. It is always empty. I tried both price projection setting ex_traded=True, and ex_traded=False too.

*Tags: General Technical*

---

**Scott** - *09:58:51*

Good Morning All! I was pointed this way by D C, If its the same person here as Twitter.. EXTREME Newbie to automation. Thrown together some python for very small programs, only to get them to produce the results I'm after. Starting Year 2 of a Data Science degree in September through the open uni as a side to creating my own betfair models. Never worked with an API before so as a complete beginner what Programming Fundamentals would you recommend diving into before even attempting to use bflw to automate my trading?

*Tags: Getting Started, Strategies*

---

**Mo** - *10:02:59*

It sounds like you are already capable of writing Python to some extent so personally I recommend you just start playing around with the bflw examples: [https://github.com/liampauling/betfair/tree/master/examples](https://github.com/liampauling/betfair/tree/master/examples). I think it's always best to learn by doing something related to your end goal

*Tags: General Technical*

---

**Scott** - *10:09:40*

makes sense [@UBS7QANF3](@UBS7QANF3), ill apply for a bf API key today and make a start, apologies in advance as I'm sure there will be many more newbie questions in the future, I will obviously try google first!

*Tags: General Technical*

---

**Peter** - *10:14:10*

[@U0223J9067R](@U0223J9067R) I doubt you'll find too much help on Google, so don't be afraid to ask here. There's a bit of an implicit assumption here that you have a basic level of competence, but with some Python experience and a year of DS behind you, you should be comfortably over that threshold.

*Tags: General Technical*

---

## 2021-05-25

**John** - *11:52:36*

Hi [@U4H19D1D2](@U4H19D1D2), looks this AWS/S3 object is quite beyond my level of coding skills/experience, sorry. I wonder if you have an example of saving the market catalogue to a local folder. Thanks again.

*Tags: Deployment*

---

## 2021-05-26

**AndyL** - *23:34:50*

Ive been running marketrecorder for 4days now a single flumine instance with 3 strategies, uk horses, au horses and uk greyhound, and as far as I can see and checking logs it's not hit the 200 limit which somewhat surprises me.. is the 200 limit on each strategy? And because each has not breached 200... eg.155 markets in cache 

So am I just teetering on the brink?!

*Tags: Strategies*

---

## 2021-05-27

**liam** - *07:52:19*

nah but streaming is so lightweight it would be pointless, however when backtesting you can skip data to speed things up [https://liampauling.github.io/flumine/quickstart/#listener-kwargs](https://liampauling.github.io/flumine/quickstart/#listener-kwargs)

*Tags: Performance*

---

**thambie1** - *09:39:33*

Despite the documentation, I found this to be untrue.

*Tags: General Technical*

---

**thambie1** - *09:41:33*

I believe I had a higher limit at the time, but yes. I was disconnected long after initiating the connection, with the error clearly specifying that the issue was the number of subscribed markets

*Tags: Errors Debugging*

---

**liam** - *11:08:55*

Now [https://github.com/liampauling/flumine/blob/6f0541d2ba57ab0cbf9389a560e9a41d1ad01cfc/examples/strategies/marketrecorder.py#L129|updated](https://github.com/liampauling/flumine/blob/6f0541d2ba57ab0cbf9389a560e9a41d1ad01cfc/examples/strategies/marketrecorder.py#L129|updated)

*Tags: General Technical*

---

**liam** - *11:09:48*

Simple [https://github.com/liampauling/flumine/blob/6f0541d2ba57ab0cbf9389a560e9a41d1ad01cfc/examples/strategies/priceblockage.py#L9|example](https://github.com/liampauling/flumine/blob/6f0541d2ba57ab0cbf9389a560e9a41d1ad01cfc/examples/strategies/priceblockage.py#L9|example) added, no grouping (needs to be added in the trigger func) but shows best practice on where to store the data, does this help?

*Tags: General Technical*

---

**liam** - *11:10:30*

Now [https://github.com/liampauling/flumine/blob/6f0541d2ba57ab0cbf9389a560e9a41d1ad01cfc/examples/strategies/lowestlayer.py#L39|added](https://github.com/liampauling/flumine/blob/6f0541d2ba57ab0cbf9389a560e9a41d1ad01cfc/examples/strategies/lowestlayer.py#L39|added) and stored in the order notes

*Tags: General Technical*

---

**liam** - *19:21:57*

Fairly sure betfair don’t have the ability to do this (unless things have changed) Are you sure it wasn’t a network/server disconnect and then a subscription error on reconnect?

*Tags: Errors Debugging, Deployment*

---

## 2021-05-28

**Beeblebrox** - *10:38:40*

I have a strategy that in backtesting works slightly better with best price execution turned off.  Is there a way to turn it off when running the strategy live, so that it only applies to that strategy?  I take it there isn't as it's an account-wide setting.  Is the only option to run it in a sub-account with the option turned off?  If so, how do I go about setting one up?

*Tags: Deployment, Strategies*

---

## 2021-05-29

**John** - *10:48:46*

Hi I was trying to setup my own log files (by doing logging.basicConfig(file_name) ) so to understand a bit deeper into the flumine. It turns out flumine still outputs the logs to the screen only. Guess I am missing something in flumine. Please could someone give me a pointer/example how I can output the logs to a file (ideally in the same time printing the logs on the screen/console too)? Many thanks.

*Tags: Getting Started*

---

**AndyL** - *19:54:12*

If i am paper_trade'ing a strategy, is there a way to check simulated order results like after a backtest run?

*Tags: Strategies*

---

**liam** - *19:57:16*

[https://liampauling.github.io/flumine/controls/|https://liampauling.github.io/flumine/controls/](https://liampauling.github.io/flumine/controls/|https://liampauling.github.io/flumine/controls/)

*Tags: General Technical*

---

**liam** - *19:57:55*

And a simple example [https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py|https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py|https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: General Technical*

---

**John** - *23:23:25*

Thanks [@U4H19D1D2](@U4H19D1D2). Here is the init function of my strategy class



```class strategy_test(BaseStrategy):





    def __init__(self, *args, **kwargs):

       BaseStrategy.__init__(self, *args, **kwargs)



       # date time

       now = datetime.now()

       dt_string = now.strftime("%Y%m%d")



       # setup logging

       path_file_log = os.path.abspath(os.path.join(cwd, "../log/", strategy_id + f"_{dt_string}" + ".log"))

       logging.basicConfig(format='%(asctime)s %(message)s', datefmt='%Y-%m-%d %I:%M:%S | ', level=logging.DEBUG,

                           filename=path_file_log)

       [http://logging.info|logging.info](http://logging.info|logging.info)("Log file: " + path_file_log)```

Any pointer to output a log file to a local drive will be appreciated.

*Tags: Getting Started, Errors Debugging, Strategies*

---

## 2021-05-30

**liam** - *09:10:12*

Logging is weird, this needs to be moved out of the strategy and into your main file (entry point) 

*Tags: Strategies*

---

**John** - *17:09:42*

Right I see. Yep moving the logging to the main file works well. flumine's logging format is very nice. I will try to use that. Thanks a lot [@U4H19D1D2](@U4H19D1D2)!

*Tags: General Technical*

---

## 2021-06-01

**Phil Anderson** - *12:12:22*

I'm sure this is something really simple - but I can't get list_current_orders to work. I've placed a few, some matched, some unmatched but when I use trading.betting.list_current_orders() the process completes but shows nothing.

From what i can see it should be as simple as this:

```trading = betfairlightweight.APIClient(username=my_username, password=my_password, app_key=my_app_key,

                                       certs=certs_path)



trading.login()



trading.betting.list_current_orders()```

Can anyone help out please?

*Tags: Strategies*

---

**Phil Anderson** - *12:49:36*

Thanks. I gave that a shot but get TypeError: 'CurrentOrders' object is not iterable

*Tags: Errors Debugging*

---

**Sam Asin** - *22:28:19*

So I got a random little modeling query if anyone is feeling helpful! I'm looking to experiment with some ML and so on, but before I do, I'm going back to basics with some manual linear regressions.

*Tags: Strategies*

---

**Sam Asin** - *22:32:45*

My understanding is that the R-squared is capturing in general what percentage of your variance is captured by your model, by doing something like 1 - (rmse / var(y) ) , so if two different regressions have the same y, then r-squared increasing is essentially analogous to rmse going down. Am I missing something here? Or am I right about the relationship between rmse and R-squared and there has to be some problem with my code somewhere?

*Tags: Strategies*

---

## 2021-06-02

**Mo** - *09:04:42*

I'm sure you're right [@U4H19D1D2](@U4H19D1D2) as I recall similar but impossible to find any reference to it in the documentation

*Tags: General Technical*

---

**Mo** - *12:36:18*

I use AWS

*Tags: Deployment*

---

**John A** - *13:19:43*

[@UBS7QANF3](@UBS7QANF3) is AWS located in London or Ireland?

*Tags: Deployment*

---

**liam** - *13:24:43*

you can run an EC2 micro for free for a year as well :slightly_smiling_face:

*Tags: Deployment*

---

**Mo** - *14:14:18*

I actually didn't know what Lightsail was, I thought it was a different cloud services provider. I see it's actually an AWS product. My immediate reaction is that the server types might be too restrictive for my use cases but it could well be worth me looking into

*Tags: Deployment*

---

**Taking Value** - *19:33:31*

Regarding the issue I had this morning regarding `r.totalmatched`  I am still trying to solve a raft of other issues associated with the problem. Gone over a few of the pro files I purchased from BF and they seem to have more than a few anomalies.

1. The first market update in each event seems to indicate money is already matched on the market. More odd is that no money appears to be matched on any of the runners

2. Whenever money is matched on a runner that delta is not what's added to the total amount matched on the market, which always goes up but by a different amount. 

3. Sometimes total amount matched on the market goes up without any runner receiving a delta. 

Very hacked off, its pro data that cost me some trading revenues. Wasn't expecting this, useless in its current form for what I have in mind. I have run the same code over files recorded with flumine and there are no issues. Also looking through flumine recorded files and the values for total matched on the market accurately reflect the sum of the amounts matched on each runner.

*Tags: Strategies*

---

## 2021-06-04

**John** - *08:46:19*

Hi [@UBS7QANF3](@UBS7QANF3), just found this useful function converting market_book to a dataframe. The Union type is quite beyond my Python level, and googling "Python Union type" doesn't help me really. I wonder if you can please give me a pointer how to make it work? Do I need to import a library for this Union type?

*Tags: Feature Engineering*

---

**Mo** - *08:50:23*

This is "only" for type hints; they are not necessary for this code (or any code that uses them) to run but it is best practice to use them as it makes your code clearer and your IDE will be able to analyse your code and ensure that e.g. the function is returning a value that is the correct type as the hint



In this case, the hint is saying that the `market_book` parameter is EITHER (Union) a `MarketBook` (defined in bflw) OR a `dict`. The intention is for the function to work whether you are using bflw in lightweight mode (generates `dict`s) or not (generates objects of the class `MarketBook`)

*Tags: General Technical*

---

**Mo** - *08:55:25*

[https://docs.python.org/3/library/typing.html#typing.Union](https://docs.python.org/3/library/typing.html#typing.Union)

*Tags: General Technical*

---

**Mo** - *15:33:32*

Almost certainly user error

*Tags: Errors Debugging*

---

**Phil Anderson** - *17:32:34*

Realised there's an error_code function. And it's throwing out an invalid odds error. So it's a problem with the excel calculations that I thought I'd sorted by rounding but for some reason it's still giving some results as multiple decimal places. I'll have to go back and find the problem. Or round it in python once I've extracted it. Thanks for the input, hopefully I can take it from here

*Tags: Errors Debugging*

---

**Phil Anderson** - *19:11:03*

[@U016TGY3676](@U016TGY3676) Thanks. I had something in place using the mround function and 'if' depending on which band the number fell in. A lookup table probably would've been easier! I think I found the problem anyway, there were a couple of numbers in the calculation that weren't rounded. I'll have to run it a bit to see if it's sorted.

*Tags: General Technical*

---

**birchy** - *19:17:00*

I've generally found that using the built-in rounding functions cause difficult to find price errors, so my go-to now is _always_ a lookup table as a sanity check.  In excel, I would simply port [https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/utils.py#L78|THIS](https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/utils.py#L78|THIS) to VBA

*Tags: Errors Debugging*

---

## 2021-06-05

**Phil Anderson** - *13:53:12*

[@U016TGY3676](@U016TGY3676) Yes you were right, even with everything rounded in excel it was still causing errors.  For example it was getting odds of 1.800000000000001. I've just extracted it from excel into python using round in python on top of the VBA rounding and seems to be working fine for now. I'll go the lookup table route if it fails again, but was easiest option with what I'd already done :crossed_fingers:

*Tags: Errors Debugging*

---

## 2021-06-07

**Steve** - *13:16:39*

Using flumine, I added a market to my script tonight. Got the message to say that the market had been added ("INFO:betfairlightweight.streaming.stream:[MarketStream: 2001]: 1.171635649 added, 2 markets in cache"). But when I got the first message for this market it included the market defintion info - but only gave me a very incomplete orderbook. On the website I can see that on the back side there are over 30 different prices - but the message I received only had 2 (and they weren't even the best prices for each side). Any ideas why I wouldn't be getting the full orderbook - when for all the other markets in my script I am? Is this a common problem?

*Tags: General Technical*

---

**John** - *15:41:14*

Hi on the just past 15:15 Lingfield race, I could see my testing script (using flumine) was running like crazy, even after the race had ended, it kept running for a minute, with some of the warning saying high latency, counting down from 45 seconds to finally 2 seconds when the script stopped... I have only one script running, the CPU usage is OK (about 50% with 2 CPUs), and this high latency happened only on some markets, not all. This high latency happens less in AU races. In the script (based on the flumine example strategy), I changed a little by adding a few lines for working out trigger prices, and calculating the stake sizes, so not too complicated. Does it mean there are too many stream events for the UK races (due to higher betting volume) to be coped by my script/VPS. Should I get a better VPS (with faster CPU), or I am missing something? Any comments will be appreciated.

*Tags: Performance, Strategies*

---

**liam** - *16:14:32*

50% would mean your cpu is maxed, what is this strategy doing? There is a leak in it 

*Tags: Strategies*

---

**John** - *16:49:28*

Many thanks both [@UFTBRB3F1](@UFTBRB3F1) and [@U4H19D1D2](@U4H19D1D2). What the strategy does is mostly based on the the example strategy: 1) calculate the trigger price - an arithmetic function of the bsp, to place an order; 2) calculate the stake size: another arithmetic function of the best available price. everything else is the same as the example strategy. I was trying to get lots of logs in a file, but found the log file size slows the script down a lot, so I removed the logs and comment out most of the print/logging I added.

*Tags: Performance, Strategies*

---

**Aaron Smith** - *16:56:15*

well, put it back in, if its 45seconds or 90seconds latency doesnt really make a difference, both is not viable. Personally i would probably even comment out a bunch of code and let it run, then add stuff step by step and see when it falls flat

*Tags: Performance*

---

## 2021-06-08

**Steve** - *00:47:05*

Yep. Run hundreds of markets using the same code, never a problem. Just one market where it seemed to be sending a change message rather than the whole orderbook.

*Tags: General Technical*

---

**Brian** - *12:01:25*

Hi guys, looking for a little assistance (bear with me as I'm still pretty new to this :slightly_smiling_face:)... I'm using Liam's very helpful error handling example script ([https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)) but I seem to have a bit on an issue in that once the loop has gone through all of the market books and starts over, it doesn't seem to be picking up updated odds from that point, like it keeps going over the stream it initially created at that point in time. Maybe this is expected behavior and there's still something I'm missing, but if there is I'd really appreciate some help figuring out what it is. I tried to move the start of the loop higher to incorporate the creation of the stream... which does work... but creates way too many logins per hour which Betfair aren't happy about. Any assistance would be hugely appreciated!

*Tags: Getting Started, Errors Debugging*

---

**liam** - *12:50:42*

At a guess it’s erroring on startup, logs tell you anything? It’s likely the thread / restart is hiding the error so remove the retry or try and catch it 

*Tags: Errors Debugging*

---

**Peter C** - *16:19:52*

I've got a quick question - Is it possible to add conflate_ms to a strategy based on the price recorder example so that I don't write every book update, or do I need to remove the unwanted rows from my file after I have written everything?

*Tags: Strategies*

---

**liam** - *17:51:24*

strategy init var 

*Tags: Strategies*

---

**Peter C** - *18:53:02*

```{"asctime": "2021-06-08 17:50:21,445", "levelname": "INFO", "message": "Adding strategy PriceRecorder"}

{"asctime": "2021-06-08 17:50:21,449", "levelname": "INFO", "message": "Creating new HistoricalStream (1000) for strategy PriceRecorder", "strategy": "PriceRecorder", "stream_id": 1000, "market_filter": "/recordings/1.183683101", "event_id": "30546561", "event_processing": false}

{"asctime": "2021-06-08 17:50:21,463", "levelname": "INFO", "message": "Starting flumine", "client": {"id": "926cb2e8", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x0000015AA2F2A130&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 0, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 13600)&gt;", "&lt;WriterThread(pydevd.Writer, started daemon 10536)&gt;", "&lt;ReaderThread(pydevd.Reader, started daemon 4432)&gt;", "&lt;_TimeoutThread(Thread-4, started daemon 12796)&gt;", "&lt;PyDBCommandThread(pydevd.CommandThread, started daemon 12004)&gt;", "&lt;CheckAliveThread(pydevd.CheckAliveThread, started 4428)&gt;"]}```

I can't see any mention of it in there - maybe I'm not adding it correctly?

*Tags: Deployment, Strategies*

---

**liam** - *18:53:36*

You need bflw logs 

*Tags: General Technical*

---

**Peter C** - *18:56:39*

Is this what I need to be looking at?

```{"asctime": "2021-06-08 17:54:42,433", "levelname": "INFO", "message": "Starting historical market '/recordings/1.183683101'", "market": "/recordings/1.183683101"}

{"asctime": "2021-06-08 17:54:42,435", "levelname": "INFO", "message": "[Register: 1000]: marketSubscription"}

{"asctime": "2021-06-08 17:54:42,438", "levelname": "INFO", "message": "[MarketStream: 1000]: \"MarketStream\" created"}

{"asctime": "2021-06-08 17:54:42,440", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.183683101 added, 1 markets in cache"}

{"asctime": "2021-06-08 17:55:39,023", "levelname": "INFO", "message": "Market 1.183683101 closed", "market_id": "1.183683101", "event_id": "30546561", "event_type_id": "7", "event_name": null, "market_type": "WIN", "market_start_datetime": "2021-05-23 14:15:00", "country_code": "IE", 

"venue": "Curragh", "race_type": "Flat", "orders_cleared": false, "market_cleared": false, "closed": true}

{"asctime": "2021-06-08 17:55:39,028", "levelname": "INFO", "message": "Market cleared", "market_id": "1.183683101", "order_count": 0, "client": {"id": "ac3d1f03", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x000001DC498CB130&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 14988)&gt;", "&lt;WriterThread(pydevd.Writer, started daemon 14100)&gt;", "&lt;ReaderThread(pydevd.Reader, started daemon 13280)&gt;", "&lt;_TimeoutThread(Thread-4, started daemon 1112)&gt;", "&lt;PyDBCommandThread(pydevd.CommandThread, started daemon 1512)&gt;", "&lt;CheckAliveThread(pydevd.CheckAliveThread, started 11800)&gt;"]}```

*Tags: Deployment, Strategies*

---

**Peter C** - *19:00:53*

OK thanks, I can sort that. Sorry for the confusion, thanks for you help - I didn't really realise how the conflate worked.

*Tags: General Technical*

---

## 2021-06-10

**Jono** - *09:02:09*

is it known if applications such as like bet angel make use of betfairlightweight or flumine? i watched a video of theirs talking about streaming data improving the speed and performance of their software and am curious if they have made use of whats available from these two packages

*Tags: Performance*

---

**D C** - *09:08:07*

There are 2 ways to get price data from betfair - streaming API and API-NG. Streaming is faster and more efficient - just becuase something uses streaming does not mean it uses BFLW - any application that has permission to use the streaming API can do so.

*Tags: General Technical*

---

**Jono** - *09:20:19*

ah right ok. i just read some earlier posts in this forum where someone mentioned  "it may be worth firing up betangel (if you have it ) in ladder mode so you can visualise what the data from bflw / flumine provides" . I was wondering if this was indeed the case and some of the data provided through betangel makes use of the two packages. i mean if you are going to stream betfair data why not use the best stuff thats already available, it makes sense.

*Tags: General Technical*

---

**Jono** - *09:29:03*

ah right perfect that makes sense [@ULDAVFDRP](@ULDAVFDRP) thanks for clarifying the specifics of your earlier comment. On a tangential note Is there an upper limit to the number of markets able to be streamed and placed on using flumine? if say for instance i wanted to run a strategy on all 300 soccer matches listed on the exchange would this be feasible using flumine or is there a limit caused by something like overuse of the trading object, or thread cap or something else entirely

*Tags: Strategies*

---

**Dermot** - *09:31:51*

There’s no technical limitation in flumine but betfair limit you to 200 markets (per subscription) by default. You can message them to get that increased though 

*Tags: General Technical*

---

**Jono** - *09:43:15*

when you get up to those large numbers 100, 150, 200+ has this been shown to impact speed or does flumine handle scaling up to this number of markets without slow down? I've seen quotes of iteration speeds near the 10-50ms running simultaneously on multiple markets. can speeds like this be expected when operating on hundreds of games

*Tags: Performance*

---

**Peter** - *09:51:02*

There's going to be some degradation, but it's not significant. A few months back [@U4H19D1D2](@U4H19D1D2)  posted information about the number of markets he was streaming on an AWS instance with a single CPU and it blew my mind (and saved me a few bucks at AWS).

*Tags: Deployment*

---

**Jono** - *10:00:17*

sounds fantastic could you possibly send me a quote i can search for in the slack group to find liams postings or the rough date of his postings. no problem if its a pain to find just let me know and ill go digging

*Tags: General Technical*

---

**Jono** - *10:14:27*

no problem thank you for digging into it

*Tags: General Technical*

---

## 2021-06-11

**VT** - *05:37:27*

Hi, I've been researching how I can get Betfair historical data to backtest, any tips on a good tutorial for beginners in Python? I would like to consult the live football game moneyline markets. I intend to consult the free Basic data, 1 minute intervals, I would like to convert the odds values ​​in each minute to a dataframe.

*Tags: Getting Started, Data Quality, Feature Engineering, Deployment*

---

**Peter** - *06:59:41*

[https://betfair-datascientists.github.io/api/apiPythontutorial/|Old but still a really good place to start](https://betfair-datascientists.github.io/api/apiPythontutorial/|Old but still a really good place to start)

*Tags: General Technical*

---

**VT** - *07:19:59*

Thanks but I know how the functionalities to consult markets and place bets work through the API, what I'm looking for now is to be able to manipulate the historical data of Betfair.

*Tags: Data Quality*

---

**Mo** - *10:40:44*

Generating a data frame from the prices files is simple though:



```from typing import Union



import betfairlightweight

import pandas as pd

import seaborn as sns

import smart_open

from betfairlightweight.resources.bettingresources import MarketBook

from betfairlightweight.streaming import StreamListener

from unittest.mock import patch





def market_book_to_data_frame(market_book: Union[MarketBook, dict]) -&gt; pd.DataFrame:

    if type(market_book) is MarketBook:

        market_book = market_book._data



    return pd.DataFrame(

        {

            'market_id': market_book['marketId'],

            'inplay': market_book['inplay'],

            'selection_id': runner['selectionId'],

            'side': side,

            'depth': depth,

            'price': price_size['price'],

            'size': price_size['size'],

            **({'publish_time': market_book['publishTime']} if 'publishTime' in market_book else {})

        }

        for runner in market_book['runners']

        for side in ['Back', 'Lay']

        for depth, price_size in enumerate(runner.get('ex', {}).get(f'availableTo{side}', []))

    )





def file_to_data_frame(path_to_betfair_price_file: str) -&gt; pd.DataFrame:

    trading = betfairlightweight.APIClient(username='', password='', app_key='')

    stream = trading.streaming.create_historical_generator_stream(

        directory=path_to_betfair_price_file,

        listener=StreamListener(max_latency=None, lightweight=True)

    )



    with patch("builtins.open", smart_open.open):

        g = stream.get_generator()

        return pd.concat(market_book_to_data_frame(mbs[0]) for mbs in g())```

*Tags: Feature Engineering, Performance, Strategies*

---

## 2021-06-15

**Scott** - *16:16:31*

Trying to run before I can walk again, I’m wanting to start collecting data whilst I learn enough python to understand what’s going off.. do I run marketrecorder and lowestlayer side by side or should I be referencing lowest layer somewhere in marketrecorder? 

*Tags: General Technical*

---

**river_shah** - *16:34:26*

lowest layer is just an example strategy. considering that you need to get your python skills up to speed first, my recommendation is don’t worry about recording data just yet. “buy” some of the free pro data and see if you replay back a market file. go into flumine github repo and follow the examples.



put a breakpoint after `def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:`see if the data structures make sense to you. get a good ide or editor such as pycharm or vs code. bring up python skills first and then the code will make a lot more sense.

*Tags: Performance, Strategies*

---

**Scott** - *16:37:58*

Makes sense. Thanks, I’ll work on python first before worrying about getting the data. 

*Tags: General Technical*

---

**Aaron Smith** - *16:40:12*

Were you planning to use lowest layer to place some bets in order to not get banned by betfair for only scraping data? I havent checked lowest layer and not sure if its usable for that. If you need to place a bet here and there to keep betfair happy while you get your data+python skills going, i would certainly still make a strat myself. Just put some idea that you think could very theoretically work into action. Make sure to only place bets on runners with low price spread and make sure not to place to many orders and 2 pound per bet obviously. You wont lose a lot and maybe gain some knowledge about how stuff works while keeping yourself safe.

*Tags: General Technical*

---

**Scott** - *16:43:39*

Yeah it was just to stop me from getting banned, Not sure where I would start without any data.. going to download some historic data, use that and then I’ll take your advice and try and come up with my own strategy when I understand python a little better. 

*Tags: Strategies*

---

**river_shah** - *16:54:19*

sure, just use a PRO data file that is free. dont get basic or advanced. This is strictly to get your feet wet with python and replaying data back (doesn’t matter that it is old at this stage)

*Tags: General Technical*

---

## 2021-06-17

**Matthieu Labour** - *12:52:23*

Hi, what is the reason there is `process_market_book()`  and`process_raw_data()` strategy callbacks. What would be some cases one would use`process_raw_data` over `process_market_book`? Thank you!

*Tags: Strategies*

---

**Crofty** - *15:50:18*

General question - when horses get beaten at 1.01, does anybody know if the majority losing backers tend to be manual players or automated ones?

*Tags: General Technical*

---

**Oliver Varney** - *15:52:45*

similar to my question a month or so ago about who is the fool in the situation, as there are a few guys in here that are happy to back into those prices, yet are not getting stung from what I understood (correct me if im wrong)

*Tags: General Technical*

---

**D C** - *16:08:02*

Probably, but one of the issues to contend with when LAYING at 1.01 is the queue position. Sometimes you see a loser matched for just a few hundred and sometimes tens of thousands. Even if you found some edge, you might never be able to guarantee getting matched yourself even if the runner was gubbed at 1.01 and for something with a low chance of happening, missing out on a few races like that could knobble the strategy completely. Same risks apply to anything with low risk/ massive reward payoffs. Even when the markets first open you see a few grand available to lay at 1.01 on all runners so it would always be tricky and that alone would make me look elsewhere.

*Tags: Strategies*

---

**Crofty** - *16:11:43*

Good point re the above, DC and opened up another question for me - when the markets are opened and that type of money appears at 1.01, is it Betfair customers like people in this group or is it the people who seed the markets?

*Tags: General Technical*

---

## 2021-06-18

**Mo** - *11:22:58*

I don't know about recently but late suspensions killed off our football strategy

*Tags: Strategies*

---

## 2021-06-22

**Adrian** - *06:44:31*

Hi all,

I'm just starting out with bwl, trying to wrap my head around it. Was hoping if someone could point me in the right direction.

In particular, I run this:

`available_to_back = runner.ex.available_to_back`

`print(available_to_back)`

and I get :

&lt;betfairlightweight.resources.bettingresources.PriceSize at 0x7fe4a0094af0&gt;

How do I turn that into price/amount?

Even better- how do I get the LTP?

Thanks in advance!

*Tags: Strategies*

---

**Adrian** - *07:32:18*

`runner.ex.available_to_back`

```[&lt;betfairlightweight.resources.bettingresources.PriceSize at 0x7fe4a0094af0&gt;,

 &lt;betfairlightweight.resources.bettingresources.PriceSize at 0x7fe4a0094a60&gt;]```

`runner.ex.available_to_back[0]`

```&lt;betfairlightweight.resources.bettingresources.PriceSize at 0x7fe4a0094af0&gt;```

`runner.ex.available_to_back[0]["prices"]`

```TypeError: 'PriceSize' object is not subscriptable```

What am I doing wrong?

*Tags: Errors Debugging, Strategies*

---

**liam** - *07:44:49*

It’s .prices in bflw 

*Tags: General Technical*

---

**mandelbot** - *07:58:28*

[@U01S1VB9X9P](@U01S1VB9X9P) check out flumine [https://github.com/liampauling/flumine](https://github.com/liampauling/flumine) and the docs [https://liampauling.github.io/flumine/](https://liampauling.github.io/flumine/)

*Tags: General Technical*

---

**Mo** - *08:49:42*

[@U4H19D1D2](@U4H19D1D2) have you thought about adding a `__repr__` method to `PriceSize` to help the newbies?

*Tags: General Technical*

---

**Aaron Smith** - *11:24:07*

it is. NearPrice is the bsp if the market was to start at that very moment, however i think near price is not calculated with every update, which probs makes it rather unreliable. There are documentations on how bsp is calculated, you could try and calculate it yourself, however i m not sure if its possible due to bets that are placed as a limit order but have persistance type == MarketOnClose, i dont think you can distinct them from bets without that persistant type. I guess theoretically you could reverse engineer  those bets when you get a new nearPrice and from there on calculate the bsp on your own until the next nearPrice shows up ? :smile:

*Tags: General Technical*

---

**Daniel Giannini** - *11:29:24*

May need to tweak my models to incorporate most recent prices in the EX_ALL_OFFERS as opposed to BSP !!

*Tags: Strategies*

---

**liam** - *11:39:23*

Good idea, confusion sometimes stems from resources / lightweight / flumine style mix 

*Tags: General Technical*

---

**Mo** - *12:23:27*

I don't think the 50ms is forbidden knowledge but it's probably one of those things buried deep in the API documentation. What I have more easily found is that, indirectly, we have:



• Virtual prices are updated 150ms after the non-virtual stream: [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Exchange+Stream+API#ExchangeStreamAPI-Marketdatafieldfiltering/MarketDataFilter](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Exchange+Stream+API#ExchangeStreamAPI-Marketdatafieldfiltering/MarketDataFilter) - i.e. they are on a 200ms cycle

• listMarketBook is not supposed to be called more than 5 times a second: [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listMarketBook](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listMarketBook) - because there won't be (virtual) updates more frequently than this

*Tags: General Technical*

---

**D C** - *12:26:03*

Thanks very much [@UBS7QANF3](@UBS7QANF3) very helpful as always.

*Tags: General Technical*

---

**Jonjonjon** - *14:36:58*

I haven't seen a function, but isn't it just in runner.sp.actual_sp?



[https://github.com/liampauling/betfair/blob/ee4488ddb4f4bee775450f7497c08ea0fb470e9b/examples/examplestreaminghistorical.py#L35-L51](https://github.com/liampauling/betfair/blob/ee4488ddb4f4bee775450f7497c08ea0fb470e9b/examples/examplestreaminghistorical.py#L35-L51)

*Tags: General Technical*

---

**mandelbot** - *15:34:21*

Yes for horse races thats the way I record SP, so not sure how to do it with greyhounds

*Tags: General Technical*

---

**liam** - *17:15:41*

yep it will be available here [https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/strategy/strategy.py#L122](https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/strategy/strategy.py#L122)

*Tags: Strategies*

---

**Sam Asin** - *17:18:24*

So I'm getting ready to turn on some new strategies with new code and such, and I was thinking of trying to limit how liquid my account funds are in case there are problems while I turn it on small. I was thinking for the next couple weeks while I'm testing it out, I could manually just go into a futures market that's far away, like next EPL season or NFL super bowl winner or whatnot, and make a big offer at 1.01 that would never be matched, where the market won't have a reason to change for like 12 months, and thus lower my account's liquidity so the automation can't do anything too nuts. Is this something that people do? Are there some downsides to this that I'm not thinking of? Do y'all do something similar or another trick for the same outcome when you are doing a ton of changes?

*Tags: General Technical*

---

**Paul** - *22:40:34*

I make more trading on exchange games than I do on sports at the moment. Hope to get some time at screen to fix that at the weekend.

*Tags: Errors Debugging, Strategies*

---

## 2021-06-23

**Adrian** - *01:42:31*

Hi - hope this is the right place to ask. Don't want to fill up the chat with my newbie questions. I have an active login with BFLW as per:

`trading.session_expired`

```False```

But when I run the example Flumine strategy as per:

`framework.run()`

```LoginError: API login: CERT_AUTH_REQUIRED```

That's the error I get. Is there a separate login class or function I'm not aware of? I'm using this for my credentials:

`trading = betfairlightweight.APIClient(username=user.name,`

                                       `password=user.password,`

                                       `app_key=user.key,`

                                       `certs=user.certs)`



`client = clients.BetfairClient(trading)`

`framework = Flumine(client=client)`

Which logs me in fine but not with Flumine. I am only using the demo API not the paid one so that's maybe affecting it since the Flumine example is trying to place a bet but the demo API doesn't have that functionality.

Thoughts?

TIA!

*Tags: Errors Debugging, Strategies*

---

**PeterLe** - *07:53:49*

[@U01S1VB9X9P](@U01S1VB9X9P) I may be wrong, but Im sure I had this error at one point; could it be two step auth??

*Tags: Errors Debugging*

---

**Adrian** - *08:20:40*

[@UQL0QDEKA](@UQL0QDEKA) Yes I have 2FA set up so that it adds the code to the end of my password. This works for `trading.login()` so perhaps Flumine is setup for non-2FA?

*Tags: Getting Started, Strategies*

---

**Adrian** - *08:25:17*

Hmm maybe you're right. If Flumine isn't using my active login then it won't see the full password, since I set that up via a separate function

*Tags: General Technical*

---

**Adrian** - *08:39:13*

Success! I got it to run. Got this output:

```starting strategy 'ExampleStrategy'```

Not sure what it's doing but it's a start! Thanks for your help everyone. Obviously BFLW and Flumine have their own login sessions/processes independent of each other

*Tags: Strategies*

---

**Adrian** - *09:16:06*

[@UQL0QDEKA](@UQL0QDEKA) on the surface yes but really it was Flumine requesting a new login when running `framework.run()` so the 2FA was de facto expired

*Tags: General Technical*

---

**Peter** - *10:54:13*

Couple of quick questions about the values in the info property of the trade class:

1. orders is a list, which has me wondering what I’m missing as I would have expected a trade to belong to a single order

2. Also wondering what offset_orders are?

*Tags: General Technical*

---

**Joe** - *13:30:31*

Hi all, is it possible to send orders to different markets from within the process_market_book function in flumine rather than just the one that called that function? I can send the orders using bflw as a workaround but that means they don't get published to the flumine orders list and can't work out how to pull them into the process orders function if not

*Tags: General Technical*

---

**Lee** - *13:33:44*

This might help [https://github.com/liampauling/flumine/blob/master/flumine/execution/transaction.py#L12](https://github.com/liampauling/flumine/blob/master/flumine/execution/transaction.py#L12)

*Tags: General Technical*

---

**Lee** - *13:46:56*

I've not tested but `market.flumine.markets.markets.get(market_id)` might work

*Tags: General Technical*

---

**Joe** - *13:57:42*

Works perfectly, thanks a lot, just had to add the other market into the streaming filter

*Tags: General Technical*

---

**liam** - *14:18:12*

Could do with a helper function on that 

*Tags: General Technical*

---

## 2021-06-24

**Adrian** - *03:22:23*

Hi it's me again. Just looking for some direction now that I can login. I've been working with historical data the last few months so if I can get the data in roughly the same format I'm golden. Wondering how does MarketRecorder output data and in what format? Also, what command should I use to call it? I'm thinking `framework.add_strategy(strategy)`  then point strategy at MarketRecorder? That way the parameters (country, event and type) are set via the strategy variable? TIA!

*Tags: Data Quality, Strategies*

---

**liam** - *09:01:11*

What do you mean? The marketrecorder is a strategy that records market data (streaming) for playback later 

*Tags: Strategies*

---

**liam** - *09:18:57*

So you would create your own strategy for that although recommend having some historical data for testing / prototyping 

*Tags: Data Quality, Strategies*

---

**Adrian** - *09:20:28*

Yes I've done that, using historical data. It was tricky, since the time intervals are not steady. But now i need live data

*Tags: Data Quality, Deployment*

---

**Adrian** - *09:33:55*

[@UBS7QANF3](@UBS7QANF3) thanks, what is the "standard format" and "prices file" you are referring to? Are these generated by Betfair or BFLW/Flumine?

*Tags: General Technical*

---

**Adrian** - *09:38:40*

Function/class for recording live data

*Tags: Deployment*

---

**Mo** - *09:40:19*

Have you looked at the market recorder example? [https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py)

*Tags: Data Quality*

---

**Mo** - *09:41:23*

Both the Betfair historic prices files and those files recorded by the market recorder are in the same format: a sequence of messages in an identical format to what you receive when connected to the streaming API

*Tags: Data Quality*

---

**Adrian** - *09:44:31*

Ohhhhhhhh my gawddd. I've been looking at the MarketRecorder.py strategy.. didn't realise there was a separate marketrecorder.py module.. :man-facepalming: only noticed it because of that link you shared. Man do I feel dumb

*Tags: Strategies*

---

**Adrian** - *09:46:41*

The market recorder is easy to understand. Thank you!

*Tags: Data Quality*

---

**Adrian** - *10:36:37*

[@UBS7QANF3](@UBS7QANF3) I am getting this error: ModuleNotFoundError: No module named 'strategies'

Any ideas?

*Tags: Errors Debugging*

---

**liam** - *10:37:31*

Error is telling you the problem

*Tags: Errors Debugging*

---

**Adrian** - *10:43:43*

is it part of a package i haven't installed or something? I've installed flumine

*Tags: Getting Started*

---

**mandelbot** - *11:00:36*

ie if you're running from flumine &gt; examples then put it in flumine &gt; examples &gt; strategies

*Tags: General Technical*

---

**D C** - *11:08:15*

I have a generic question that I can't answer because I don't request virtual prices off the stream currently. If I request virtual prices and XM is not active on the market, does the stream still push the bdatb/bdatl fields or does it suppress them? If it does send them out, are they simply identical to the batb/batl fields?

*Tags: General Technical*

---

**Adrian** - *11:12:24*

ok got it! Thank you! I had to create a new py file in the same directory as my script and pasted the market recorder strategy code in there. phew! I am now seeing a stream of data coming through

*Tags: Data Quality, Strategies*

---

**Adrian** - *11:13:56*

where i'm running my script from not the flumine directory

*Tags: General Technical*

---

**Adrian** - *11:22:49*

[@U4H19D1D2](@U4H19D1D2) thanks for your patience. I figured it out (with [@U010GM77S4W](@U010GM77S4W)’s help as well) and now have a folder full of streaming data files! yahoooo!

*Tags: General Technical*

---

**James** - *15:50:27*

Can anyone recommend a decent host for a mysql database? Thanks

*Tags: General Technical*

---

**Mo** - *15:50:44*

AWS RDS

*Tags: Deployment*

---

## 2021-06-25

**Adrian** - *00:45:11*

Is there a way to control the login/logout process with Flumine? It seems to be automatic when calling framework.run

Edit: I should add that I'm using the non-interactive login with certs. There are separate commands in bflw for this that I control with a GUI, so wondering if Flumine has the same functionality or is it inextricably linked with the run call

*Tags: General Technical*

---

**Peter** - *06:22:18*

It's controlled by injecting an appropriate client when you instantiate the framework

`framework = Flumine(client=client)`



If you inject the BetfairClient

`client = clients.BetfairClient(trading)`

that's designed to log you into Betfair and keep the connection alive. But if you're backtesting on previously recorded data, you inject the backtestclient,

`client = clients.BacktestClient()`

for which external login is disabled.

*Tags: Deployment, Strategies*

---

**flxbe** - *10:17:09*

I noticed that the flumine stream implementation uses two threads per stream:

1. The stream itself (MarketStream or OrderStream), since it subclasses threading.Thread

2. The output_thread within the stream, which pipes the updates into the flumine handler_queue

I am trying to understand the reasons for this design decision. Are there any benefits e.g. regarding error handling I am not seeing?

*Tags: Errors Debugging*

---

**liam** - *10:58:34*

It’s the logic to trigger a snap, handy on stale/low liquidity markets [https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/streams/marketstream.py#L65|https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/streams/marketstream.py#L65](https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/streams/marketstream.py#L65|https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/streams/marketstream.py#L65)

*Tags: General Technical*

---

**Joe** - *13:12:40*

Is it possible to add/remove markets from a strategy in flumine whilst it is running? I want to be able to manually enable/disable specific markets but not sure how to do it

*Tags: Strategies*

---

**liam** - *13:13:59*

[https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/strategy/strategy.py#L108|https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/strategy/strategy.py#L108](https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/strategy/strategy.py#L108|https://github.com/liampauling/flumine/blob/6d7ed2eed5a1a111949d0feb4ffe11f9be28efec/flumine/strategy/strategy.py#L108)

*Tags: Strategies*

---

**liam** - *13:14:17*

Just return False and process doesn’t get called on the strategy 

*Tags: Strategies*

---

**Will Morrison** - *16:46:23*

Heya guys! It's been ages since I messaged on here, but I've been doing lots of work with betfairlightweight lately and learning a lot. Thanks Liam! So I have a case where I want to look deep in a market book, but according to the API documentation ladderLevels can only be 1-10. That is, the way I'm currently subscribing to my market is:

*Tags: General Technical*

---

**Will Morrison** - *16:47:25*

market_data_filter = streaming_market_data_filter(

        fields=["EX_MARKET_DEF", "EX_BEST_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP"], ladder_levels=x

    )

where if I make x greater than 10, I still only get ten ladder levels in the market_book it creates. Is there a way for me to get deeper information?

*Tags: General Technical*

---

## 2021-06-26

**Javier Martín Pérez** - *00:34:29*

Thanks! I have worked in several bookmakers as an odds compiler and now I am learning coding with python to try to automate some strategies. As an odds compiler I was doing football but to try to trade football on Betfair looks a bit complicated without paying for a Runningball feed so I am giving a go to greyhound trading at the moment.

*Tags: Strategies*

---

**birchy** - *11:47:40*

Question for the stats guys here...

So I want to build a preplay model in order to create my own prices/book using historical data for each runner. Let's say I have calculated a handful of feature probabilities for each runner and have also manually given those features a weighting. For example, one of the features may be the bookie price, to which I might give a weighting of 0.20.

I'm assuming that all of the features for each runner would somehow need to be combined to create a single probability, then the probability for each runner would need to be normalised to create a 100% book.

So my question is... in layman's terms, what's the best way to combine all of this independent data to create a final probability for each runner?

*Tags: Data Quality, Feature Engineering, Strategies*

---

**Mo** - *12:05:02*

Add [https://github.com/slundberg/shap](https://github.com/slundberg/shap) to your learning list if you want to understand how a ML model came up with the number generated

*Tags: Strategies*

---

**Ivan Zhou** - *15:50:27*

Hey everyone. I have been working on a horse racing model and I am ready to give it a punt and automate the model. I am completly new to the betfair api and betfairlightweight. I'm asumming there is a beginner guide, if someone could point me that direction so I can have a go.

*Tags: Getting Started, Strategies*

---

**PeterLe** - *15:53:58*

[@U026A2YDLDQ](@U026A2YDLDQ) check out the github repo here : [https://github.com/liampauling/](https://github.com/liampauling/)

You need to take a look at BetfairLightweight and Flumine

All the docs are there, good luck

*Tags: General Technical*

---

**Dave** - *21:11:48*

[@U01DCR5PXDY](@U01DCR5PXDY) how's your euros performance been?

*Tags: Performance*

---

**Dave** - *22:26:49*

Majority of my strategies are passive so tighter markets make it more challenging to offer an attractive price to other participants. And for my taking I really only run it on leagues I have extremely good models for and Euros definitely ain't one of them

*Tags: Strategies*

---

## 2021-06-27

**Sam Asin** - *20:32:03*

i was wondering lately as i started to look at fighting, and rreading stuff written before the fight, to what extent a huge chunk of pro gramblers were basically betting whatever they are comfortable risking, and I was missing out on those sorts of things because i was ignorant and focused on (arguably harder) problem-spaces.

*Tags: Strategies*

---

## 2021-06-28

**Adrian** - *00:16:59*

Does the ExampleStrategy do anything? When I run it unmodified it seems to just get the market details and nothing more. Is it suppose to be placing bets?

I've changed the market id and runner id to an active market but still the same outcome

*Tags: Strategies*

---

**Oliver Varney** - *07:31:32*

with list_cleared_orders, what is the appropriate params / approach to attribute commission to each individual strategy assuming you run multiple strategies on one sub account. customer_strategy_ref in flumine is set to the host name I believe and customer order ref is None (with group by set to MARKET). The goal is to attribute the correct commission to each strategy. Do people just calc it off individual bets or am I having a blonde moment and missing the obvious?

*Tags: Strategies*

---

**liam** - *07:32:48*

I calculate locally off bets grouped per market/strategy in sql 

*Tags: Strategies*

---

**Peter** - *07:33:02*

When you say unmodified, the example strategy  is filtered to a specific market, which will be long closed and then to a specific runner in that market. So it wouldn't be expected to do anything.

*Tags: Strategies*

---

**Andrew** - *08:18:25*

Where there is an offsetting effect I view it as “gross” commission per strategy. That’s important in the event a specific strategy is ceased.

*Tags: Strategies*

---

**Unknown** - *13:34:24*

Hey, was just checking out the flumine github page and saw this screenshot of a notebook but couldn't find it in the repo. Can anyone point me to it? Thanks in advance!

*Tags: General Technical*

---

**Lee** - *13:35:05*

[https://github.com/liampauling/flumine/blob/master/examples/controls/analysis.ipynb](https://github.com/liampauling/flumine/blob/master/examples/controls/analysis.ipynb)

*Tags: General Technical*

---

**liam** - *20:19:17*

Just pass the trading object? Or am I misunderstanding your question 

*Tags: Strategies*

---

## 2021-06-29

**Adrian** - *01:07:46*

How can I exclude a single country from the strategy?

I tried: `country_codes=[not "GB"]` and `country_codes!=["GB"]`

*Tags: Strategies*

---

**Adrian** - *01:41:33*

[@U4H19D1D2](@U4H19D1D2) big shoutout to you dude. Thank you for all your hard work with this library. Incredible stuff. I have now ported over my strategy from another software and have this one up and running completely automated. So cool! Cheers. :clap::beers:

*Tags: Strategies*

---

## 2021-07-01

**Rob (NZ)** - *06:28:06*

Hey all , random question, does betfairlightweight by default output any logging of bets, i havent created any but just wondering does it write output anywhere....

*Tags: General Technical*

---

**liam** - *07:55:20*

Depends what you mean by logging? Have a read of flumine if you want to see how that does it (single logging queue with plug/play logging controls) as well as python logging using the jsonlogger library 

*Tags: General Technical*

---

**KG** - *08:21:01*

I think Flumine would be a good option for you [@UH9MMLXAS](@UH9MMLXAS)

*Tags: General Technical*

---

## 2021-07-02

**Rob (NZ)** - *04:51:51*

Justing doing pip install of Flumine and it came up with  Attempting uninstall: betfairlightweight

    Found existing installation: betfairlightweight 2.12.1

    Uninstalling betfairlightweight-2.12.1:

      Successfully uninstalled betfairlightweight-2.12.1

Successfully installed betfairlightweight-2.9.0  ,

is that all good?

*Tags: Getting Started*

---

**ShaunW** - *15:19:00*

Question to fans of the dishlickers, is there anything inherently different about morning dog racing? Time of day isn't something I'd usually factor in but for dogs, I might reluctantly have to. Getting the morning and the rest of the day to look similar resultswise has been like sqeezing a balloon, fix one part and another part bulges out of shape. The only thing I can think of is that you need to be pretty hardcore or not have much of a life to bet on the morning dogs, so maybe the significant participants have a different profile and it affects the overall behaviour? Is it to do with race quality? Maybe there's no difference and my issue is somewhere else. Any thoughts or similar experience?

*Tags: Errors Debugging*

---

**ShaunW** - *23:46:29*

Thx all, same here, stand back and the differences melt away.  I'll leave it alone I think. I don't like introducing exceptions unless I can see some sort of real world reason for them.

*Tags: Errors Debugging*

---

## 2021-07-05

**MacrcoPolo** - *01:06:54*

Usual questions: 1. Are you logged in to the website? 2. Are you using a delayed key?

*Tags: General Technical*

---

**Adrian** - *01:06:56*

Hey guys hoping someone could help me interpret this printout from process_market_book (flumine):

```pt: 1625442293716 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442294679 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442294788 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442296790 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442299818 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442300182 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442300901 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442301216 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442301310 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442301638 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442301720 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}```

As you can see, these are all from the same market/runner in the same update. But they are spread over 8 seconds. Which I can't wrap my head around, why would the ltp and volume return the same amount for 8 seconds? There should only be one update showing like this and the rest should be blank

(live key)

*Tags: Deployment*

---

**MacrcoPolo** - *01:08:02*

I don't use flumine, but I would have to guess that it's because the last traded price and size hasn't changed?

*Tags: General Technical*

---

**MacrcoPolo** - *01:08:27*

(Might be wrong here, I don't use flumine, but this would be my guess)

*Tags: General Technical*

---

**Will Morrison** - *01:10:20*

I'm doing it from betfairlightweight, not flumine. And 1: Yes, I'm logged in, 2: Active key (not delayed)

*Tags: General Technical*

---

**Unknown** - *01:25:26*

Hi MacrcoPolo, I just looked more, and I found that the the raw data part of the market book object has the correct info, but the fancier betfairlightweight one (the market_book.runners. etc one) which is where I was getting my data from, only has the 1.01 and 1000 odds that I described before. I've got a few pictures, hopefully I'm not spamming the group too much:

*Tags: General Technical*

---

**Adrian** - *01:39:53*

I think i found the reason for this and it is a design decision:

&gt; """"

&gt; django style `lazy` object creation, the main bulk of processing

&gt; is turning {priceSize} into &lt;PriceSize&gt; objects but this is

&gt; pointless a lot of the time as the object is not used or can be

&gt; handled as a dict instead.

&gt; The inclusion of slots further reduces the processing time as well

&gt; as reducing memory.

&gt; This optimisation will improve normal streaming as well as

&gt; backtesting, with more speed, less CPU + ram and minimal reduction

&gt; in usability.

&gt; """

*Tags: Performance*

---

**Adrian** - *01:41:27*

so that begs the question which method do i use now instead

*Tags: General Technical*

---

**birchy** - *07:39:02*

As [@UBS7QANF3](@UBS7QANF3) has said, are you requesting virtual prices to get the same display as the website? By default, bflw and Flumine display the actual prices rather than Betfair's virtual ones. The response will have 'atb' for actual prices and 'batb' if they're virtual.

*Tags: General Technical*

---

**D C** - *08:15:18*

I remember getting caught out by this myself when I first added streaming to my code base.

*Tags: General Technical*

---

**Peter C** - *08:50:14*

I wasn't aware of this, what is the reason for not showing the virtual prices by default? I've been using the basic market recorder example from the flumine docs, is there any way to set virtualise=true to request the virtual prices for the recorder? Do I put this in streaming_market_data_filter?

*Tags: Data Quality*

---

**Mo** - *08:59:36*

1. Better to use the raw stream live because of the latency of the virtual stream

2. Can always go raw -&gt; virtual, I would imagine it's impossible to go the other way

*Tags: Performance, Deployment*

---

**D C** - *09:05:21*

Starting to question everything I know/thought I know now. So does this mean that the virtual prices are calculated whenever new real prices change, or that the process itself is delayed and the delay refers only to the stream pushing the virtual prices?

*Tags: General Technical*

---

**Adrian** - *09:11:05*

[@U4H19D1D2](@U4H19D1D2) I have investigated and it looks like your market recorder uses cumulative traded volume. Whereas the historical (pro) data from Betfair does not use cumulative values.

*Tags: Data Quality*

---

**liam** - *13:22:22*

Not sure tbh but this is the description in the streaming docs, I assume some sort of concatenation takes place rather than 150 m/s delay on each update, send an email to Neil

```Please note: The virtual price stream is updated ~150 m/s after non-virtual prices. Virtual prices are calculated for all ladder levels.```

*Tags: General Technical*

---

**thambie1** - *19:26:27*

So many questions.. Does bet365 really have an official API? If so, why? And why would you want to use it?

*Tags: General Technical*

---

**Matthieu Labour** - *22:49:24*

Question with respect to Pinnacle. Wondering what is the maximum bet amount and if there is a difference between a market opening and right before the event. Does anyone have thoughts on that?

*Tags: General Technical*

---

## 2021-07-06

**ThomasJ** - *10:45:55*

I'm trying to understand *[https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions|StartingPrices ](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions|StartingPrices ) &gt;*[https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions|backStakeTaken](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions|backStakeTaken) ('spl' key in Runner Change msg.) using the Description in the Betting API but getting nowhere.

• The Type is a List

• The Description says "The total amount of back bets matched at the actual Betfair Starting Price."

• This implies that there is only ever 1 &lt;PriceSize&gt; as there is only 1 BSP per runner.

But I assume that to not be the case as the type is List AND betfairlightweight uses the same Available.update() method that is used for processing say ExchangePrices &gt; availableToBack  ('atb' key) which is definitely a list.



Can someone please de-befuddle me.

Thanks

*Tags: Strategies*

---

**ThomasJ** - *11:20:34*

Thanks Liam. Just so I am 100% sure.

Class RunnerBookSP self.back_stake_taken = [PriceSize(**i) for i in _backStakeTaken_] will only ever at most contain 1 element in the List?



BTW your effort in creating BFLW and maintaining it is simply amazing. Thank you.

*Tags: General Technical*

---

**liam** - *11:23:21*

theoretically, assuming non runner removals are handled correctly (which they should be), out of interest why are you looking this deep? Found a problem?

*Tags: General Technical*

---

**ThomasJ** - *11:32:11*

[@U4H19D1D2](@U4H19D1D2) No problem. I'm going over some [https://betfair-datascientists.github.io/historicData/jsonToCsvTutorial/#complete-code|code](https://betfair-datascientists.github.io/historicData/jsonToCsvTutorial/#complete-code|code) provided by Betfair in Oz that uses parts of BFLW to read historical data and new code to process it and write CSV. There is a section in the new code that seems to use 'spb' and 'spl' to calculate "calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))" which I cannot fathom.

This is performed using Final market. The code was written by a BF employee so I guess it must have some significance.



For info the specific code in question is...



`# returning smaller of two numbers where min not 0`

`_def_ min_gr0(_a_: float, _b_: float) -&gt; float:`

    `if _a_ &lt;= 0:`

        `return _b_`

    `if _b_ &lt;= 0:`

        `return _a_`



    `return min(_a_, _b_)`



`postplay_traded = [ (`

            `r.last_price_traded,`

            `r.ex.traded_volume,`

            `# calculating SP traded vol as smaller of back_stake_taken or (lay_liability_taken / (BSP - 1))`        

            `min_gr0(`

                `next((pv.size for pv in r.sp.back_stake_taken if pv.size &gt; 0), 0),`

                `next((pv.size for pv in r.sp.lay_liability_taken if pv.size &gt; 0), 0)  / ((r.sp.actual_sp if type(r.sp.actual_sp) is float else 0) - 1)`

            `)`

        `) for r in postplay_market.runners ]`

*Tags: Data Quality*

---

## 2021-07-07

**KG** - *06:10:02*

there's quite a lot of complexity to how BSP works - these are the resources I find valuable in helping to understand the logic:

• Overview: [https://docs.developer.betfair.com/pages/viewpage.action?pageId=4392566](https://docs.developer.betfair.com/pages/viewpage.action?pageId=4392566) 

• Detailed explanation: [https://promo.betfair.com/betfairsp/FAQs_theBasics.html](https://promo.betfair.com/betfairsp/FAQs_theBasics.html)

• Worked examples of the reconciliation process: [https://promo.betfair.com/betfairsp/FAQs_detailedWorkings.html](https://promo.betfair.com/betfairsp/FAQs_detailedWorkings.html) - particularly this one 

• How the projected odds work:  [https://promo.betfair.com/betfairsp/FAQs_projectedOdds.html](https://promo.betfair.com/betfairsp/FAQs_projectedOdds.html)

• FAQs in plain English: [https://www.betfair.com.au/hub/betfair-starting-price-faqs/](https://www.betfair.com.au/hub/betfair-starting-price-faqs/)

*Tags: General Technical*

---

**Neil T (Betfair)** - *12:16:01*

Yes, if a non-virtual update results in a corresponding virtual update calculated by the cross-matching algorithm.  Exception would be if cross-matching wasn't active on a market (this would be signified by the crossMatching bool field returned via the MarketDefiniton in the Stream API or listMarketBook.

*Tags: Errors Debugging*

---

**Mo** - *12:16:40*

Cheers, that's very helpful

*Tags: General Technical*

---

**Matthieu Labour** - *23:30:38*

Can we add and remove dynamically at runtime markets to a strategy market filter?

*Tags: Strategies*

---

**Peter** - *23:45:22*

There's no option in the Betfair API to do this, so no way to include the functionality in Flumine. However you can include code in your strategy's check_market_book method to ignore markets that you're not interested in.

*Tags: Strategies*

---

## 2021-07-08

**Adrian** - *03:33:47*

Could someone help me understand the following scenario? If I am retrieving the last price traded from market_book.runners every cycle and there is lag, how are the delayed messages handled? Are they all queued up and delivered once everything resumes, or are the delayed messages skipped over and only the last update sent through? E.g. in process_market_book, i save the ltp into a variable and 5 market updates are created but I don't receive them until the 6th update, do I only get the 6th update or will all 6 come through at once? Thanks

*Tags: Deployment*

---

**Adrian** - *06:00:21*

I'm using the live key. I just wanted to know how flumine queues up the market messages. I think I found the answer but it would be good to confirm. It seems there is a one-to-one ratio between loops and market messages, provided there is a message. Therefore if there is a delay and 5 messages are published, it will take 5 loops of the function to catch up, rather that catching them all up on the next loop.

*Tags: Deployment*

---

**Adrian** - *07:29:19*

Thanks, no need to apologise. I don't yet have the vocabulary for what I'm seeing so my questions are long winded

*Tags: General Technical*

---

**Ivan Zhou** - *09:58:06*

Does anyone know of a way to pull the distance of a horse race? Can't seem to find it in the documentation

*Tags: General Technical*

---

**Ivan Zhou** - *10:38:59*

not sure if I am doing something wrong im running

```import logging



import betfairlightweight





# setup logging

logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))  # change to DEBUG to see log all updates



# create trading instance (don't need username/password)

trading = betfairlightweight.APIClient("username", "password")



# race card login

trading.race_card.login()

# update

market_id = "1.185112813"



# race card request (provide list / returns list)

race_cards = trading.race_card.get_race_card(market_ids=[market_id])

print(race_cards)```

*Tags: Getting Started, Errors Debugging, Strategies*

---

**PeterLe** - *16:01:54*

Yes me too, If I can find out how to switch it on :grinning:

*Tags: General Technical*

---

## 2021-07-09

**Unknown** - *00:56:58*

[@U026A2YDLDQ](@U026A2YDLDQ) here's code I've used (in bflw) to split ANZ racing names to pull out race length:



```# splitting race name and returning the parts 

def split_anz_horse_market_name(market_name: str) -&gt; (str, str, str):

    # return race no, length, race type

    # input samples: 

    # 'R6 1400m Grp1' -&gt; ('R6','1400m','grp1')

    # 'R1 1609m Trot M' -&gt; ('R1', '1609m', 'trot')

    # 'R4 1660m Pace M' -&gt; ('R4', '1660m', 'pace')

    parts = market_name.split(' ')

    race_no = parts[0] 

    race_len = parts[1] 

    race_type = parts[2].lower() 



    return (race_no, race_len, race_type)```

*Tags: General Technical*

---

**Oliver Varney** - *08:17:54*

okay cool, yes that all I could see by searching the bflw resources. How long do marketbooks stay active post closing of the market?

*Tags: General Technical*

---

**liam** - *08:39:44*

pretty much, think it was the strategy and then sending that marketBook through to logging control

*Tags: Strategies*

---

**Mo** - *08:53:41*

To clarify I'm talking about the raw stream, obviously I'm not up to speed with what flumine does or even how bflw handles the cache

*Tags: Performance*

---

**James** - *13:16:10*

Only used it for a few days but betfairlightweight is the dogs nuts. Great work by Liam and all involved

*Tags: General Technical*

---

**liam** - *13:22:35*

wait till you use flumine

*Tags: General Technical*

---

**John** - *14:11:37*

Hi all, I have managed to collect some horse racing data (huge thanks to [@U4H19D1D2](@U4H19D1D2)’s flumine!) for the past month, and am trying to setup backtesting with flumine. I saw this backtest example [https://liampauling.github.io/flumine/quickstart#backtesting](https://liampauling.github.io/flumine/quickstart#backtesting) passing one market/file (1.170212754?) to the flumine. I was trying to find a way to pass the entire folder of recorded data to flumine, but with no luck. Please could someone give me a pointer? Many thanks.

*Tags: Getting Started*

---

## 2021-07-10

**Andrew** - *03:43:53*

Hi. I’m a non bflw user at the moment. My coding strength is C# and happily use the API - variations of that provided by Betfair. But I have a question on considering trying some bflw implementation. On RunnerChange trd is aggregate volume at price since start of market. I think I’m right. I can handle that OK but curious if bflw does that already to deliver change in trade volume at each change point? Simulating trading (how I do it) needs to know volume executed at a time point so how does bflw handle this?

*Tags: Deployment, Strategies*

---

**liam** - *07:35:14*

It’s a wrapper so doesn't, flumine is the framework and does via the simulated middleware 

*Tags: General Technical*

---

**Andrew** - *07:50:44*

Thanks for clarification. That’s probably what I was referring to.



And so a follow up simulation related question is for a rule of thumb for proportion of volume matched? It can’t be necessarily assumed you’d match against all traded volume in the historical record, even ignoring market impact effects, nor assume you’d always make top of the queue. Financial market modellers apply assumptions of a trade share as non-impacting and wondering what considerations may be made in this instance (obviously not in flumine).

*Tags: Strategies*

---

**liam** - *08:11:31*

Flumine replicates the queue and uses the delta, traded amounts per price to simulate matching. It isn’t perfect and as you mention double counting liquidity is possible and market impact is not replicated however it’s pretty good at giving you a base at reasonable stakes even inplay/volatile markets. This is ignoring XM which makes things even trickier!



Instead the framework has been designed so that you can use your own middleware if required to better handle conditions you might be simulating. 

*Tags: General Technical*

---

**liam** - *08:30:10*

Np, more info at the bottom of this page, probably needs to be moved to a dedicated page [https://liampauling.github.io/flumine/quickstart.html|https://liampauling.github.io/flumine/quickstart.html](https://liampauling.github.io/flumine/quickstart.html|https://liampauling.github.io/flumine/quickstart.html)

*Tags: General Technical*

---

**Unknown** - *11:47:46*

I'm no python guru but I used the  [https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|backtest_multi.py](https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|backtest_multi.py)  file uploaded by [@UPMUFSGCR](@UPMUFSGCR) in the files repository. Even if it doesn't do exactly what you need, I'm sure it will give you plenty of food for thought!

*Tags: General Technical*

---

**Dave** - *11:55:36*

With the above parallel example, wouldn't it be better to instantiate 24 flumine instances in parallel with n_markets/24 passed to each, rather than have 24 workers instantiate a flumine instance each time they are processing a market?

*Tags: General Technical*

---

**Jonjonjon** - *14:05:33*

My solution obviously has performance overhead due to spinning up the processes. I timed it at 4.5 seconds per 1,000 processes.

*Tags: Performance*

---

**Dave** - *14:27:57*

Fair enough, yeah you gotta be careful with memory leaks I suppose. When I backtest I split my markets into n_worker chunks, spin up n_workers and give them each a chunk. Avoids the overhead of flumine startup on each market basically

*Tags: Performance*

---

**liam** - *15:39:47*

I have found the optimum for me to be 8 processes (m1 8 core) and then chunk into 16 markets per flumine instance, there will be a slight memory leak if you go higher due to some of the loops so it’s better to keep markets per instance as low as possible (flumine startup time is negligible) 

*Tags: Performance*

---

**Michael** - *18:35:42*

I'm trying to use bfwl for the first time, whilst trying to login I get the following error "HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4024)')))". I presume this is coming from the certs being incorrect, I followed the xca certificate generation, has anything changed on this that I'm missing? [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA)

*Tags: Getting Started, Errors Debugging*

---

**Mo** - *18:48:17*

Definitely sounds like a certificate problem but I’m not a Windows guy so can’t help with the generation 

*Tags: General Technical*

---

## 2021-07-13

**J** - *10:45:51*

Hey, excited to join betfairlightweight. has anyone intergrated historical or live data with xcode or firebase?

*Tags: Deployment*

---

## 2021-07-14

**Andrew** - *03:20:17*

Have you read the general API specs on the Betfair API developer pages? What language do you want to use? Firebase is just your data store, if I’m right?

*Tags: General Technical*

---

**liam** - *06:36:48*

In that case you are probably better off looking at the app api, but as Andrew is probably hinting at this isn’t a problem to be solved using python or just firebase 

*Tags: General Technical*

---

**Newbie99** - *11:05:00*

This is an incredibly dumb question...but I can't figure out the syntax...how can I adjust the market_filter to include market_start_time (as its not include in the streaming_market_filter)?



So for example (I can't filter out after startup as there are &gt;1000 markets and I don't think I bring enough to the party to get unlimited!!!):



```strategy = ExampleStrategy(

    market_filter={**streaming_market_filter(event_type_ids=["2"]), **{'market_start_time':{"to": "{0}T23:45:00Z".format(datetime.today())}}},

)

framework.add_strategy(strategy)```

*Tags: Strategies*

---

**liam** - *11:06:13*

if only there where some helpful docs for this sort of thing

*Tags: General Technical*

---

## 2021-07-15

**KG** - *06:31:17*

hey guys, just a head's up that we have our next online ANZ analytical meet up next week, Wednesday AM your time, with a focus on greyhound modelling - you're welcome to join if you're keen :dog: [https://www.eventbrite.com.au/e/betfairs-digital-analytical-meet-up-greyhound-edition-tickets-160463756415](https://www.eventbrite.com.au/e/betfairs-digital-analytical-meet-up-greyhound-edition-tickets-160463756415)

*Tags: Strategies*

---

## 2021-07-16

**river_shah** - *11:04:13*

betfair down:

```betfairlightweight.exceptions.LoginError: API login: INTERNAL_ERROR```



*Tags: Errors Debugging*

---

## 2021-07-17

**Steve** - *09:05:46*

With flumine if I reference the pricesize attribute of a current order which has a status such "REPLACING" or "UPDATING" do I get the exist price and size - or what it is changing to? In other words do the attributes of an order object in flumine change when I send the instruction to replace/update or do they change when the exchange confirms that the change has been made?

*Tags: General Technical*

---

**Steve** - *09:14:11*

Also does anyone else have issues handling the max_selection_exposure parameter ?  Following on from my previous question - can I just confirm if this calculation is based on the orders.status has confirm by the BF. I keep finding my replacing orders (when I cancel and place simalteanously) violate this safeguard.

*Tags: General Technical*

---

**Jonjonjon** - *09:38:52*

If the place is sent before the cancel is confirmed, could there be a chance that both get filled? If so, that could be the cause.



There used to be a bug in exposure calculation when backing and then laying, but it was fixed a few months ago.

*Tags: Errors Debugging*

---

**Jono** - *09:49:10*

what would be the proper way to terminate a flumine session ie stop all strategies? say after 10 seconds of run time following the flumine.run() i wanted to halt and review the logs?

*Tags: General Technical*

---

**liam** - *09:50:44*

[https://github.com/liampauling/flumine/blob/master/examples/workers/terminate.py|https://github.com/liampauling/flumine/blob/master/examples/workers/terminate.py](https://github.com/liampauling/flumine/blob/master/examples/workers/terminate.py|https://github.com/liampauling/flumine/blob/master/examples/workers/terminate.py)

*Tags: General Technical*

---

**Oliver Varney** - *09:51:17*

could you not run it in debug mode and just stick breakpoints in if your looking to review the current state (whilst testing)?

*Tags: Errors Debugging*

---

**Jono** - *10:02:44*

yes but i was thinking i could just remove some of the logic and just have it run the "flumine.handler_queue.put(TerminationEvent(flumine))" line in order to halt the code. I would also alter the parameters in the BackgroundWorker to execute after 10s. Does this seem plausible or did you have something else in mind for the termination of the code?

*Tags: General Technical*

---

**Jono** - *10:08:07*

thank you for your help

*Tags: General Technical*

---

**Oliver Varney** - *10:09:51*

nw, [@U4H19D1D2](@U4H19D1D2) great design with flumine has thought of most things :+1:

*Tags: General Technical*

---

## 2021-07-18

**Juha Kiili** - *11:51:13*

I'm trying to implement a custom "market recorder" in Flumine that also records the relevant `inplayservice` events (or status) and is back-testable for a strategy that also uses the `inplayservice` data?



How would you approach this on a high-level?



I apologize for the vague question, but I'm not quite in-depth with the codebase and need a push to the right direction.

*Tags: Data Quality, Strategies*

---

**Juha Kiili** - *13:42:57*

Answering my own question. For now I think I'm going to...



In `inplayservice` callback

```for strategy in flumine.strategies:

    strategy.context["score"][market.market_id] = { #whatever score data }```

And in `MarketRecroder.process_raw_data()` i'm injecting it into the saved json

```json.dumps({"op": "mcm", "clk": None, "pt": publish_time, "mc": [data], "score": self.context["score"][market_id]})```

Then in my actual strategy backtesting, I need to parse it back somehow, but haven't looked at it yet.

*Tags: Strategies*

---

## 2021-07-20

**Greg** - *07:44:15*

Hi all. I have a simple issue that is driving me (chemo brain :slightly_smiling_face: )  nuts. the bot I have written is going ok from a functionality perspective but I am curious about the replace_order method. I have a cpl of questions but here is the code :

```def process_orders(self, market, orders: list) -&gt; None:

    for order in orders:

        if order.status == OrderStatus.EXECUTABLE:

            if order.elapsed_seconds and order.elapsed_seconds &gt; 5:

                if order.size_matched == 0:

                    market.replace_order(order, new_price=1.02)```

[1.]Can the new_price= be supplied with a variable (defined earlier in another function (I can't seem to get this to work anyway (unresolved reference error)but was curious re syntax violation or something)? e.g. new_price=best_lay_price (or other variable name). [2]. Does the replace order simply take the existing order and try again with the same stake at the new price(assume it does but just checking) [3.] How do you update the new_price to be at the best lay price or best back price (is this possible) ? .Thanks and apologies for the numpty coder here but I've hit the wall on an otherwise nice little bot (thanks Liam for this platform-its great). Cheers

*Tags: Errors Debugging*

---

**Mo** - *08:25:25*

A few points:



1. As previously discussed on here, the virtual price stream is delayed relative to the non-virtual one. You reduce latency by receiving the non-virtual price stream and doing the virtualisation yourself

2. The virtual price stream is limited by ladder levels so you cannot see the full order book

3. The virtual price stream has roll-up applied so you do not know the true best price if the volume available at that price is less than £1

4. I _believe_ that it is easier to remove your own orders from the order book using the non-virtual price stream then do the virtualisation yourself rather than trying to remove them from the virtual price stream

*Tags: Performance*

---

**KG** - *09:53:45*

really it comes down to how efficient you can be in calculating the 'cross matching' values yourself right, [@UBS7QANF3](@UBS7QANF3)?  I believe the latency associated with pulling the feed with virtual bets is ~100ms delayed compared to without

*Tags: Performance*

---

**Mo** - *10:26:12*

[@UUUAJQZ5J](@UUUAJQZ5J), sort of, and I think the latency is actually overstated in the documentation, based on my preliminary work on implementing the virtualisation. However, the other points are pretty important I think. For example, any kind of logic involving wanting to be exclusive best price/stepping inside the market is going to be thrown off by 3.

*Tags: Performance*

---

**KG** - *10:30:42*

very true, I think it comes down to a) do you want an identical view to what the Exchange is showing with virtual bets on, if that's relevant to your strategy and b) can you reliably recreate that yourself and if so then c) can you do it faster than by simply getting the same data from the API (my opinion is that yes, you should be able to replicate more efficiently yourself).

*Tags: Strategies*

---

**Jono** - *10:56:30*

Just getting started with flumine and there are few things I am wondering about the process_market_book() and process_orders() functions. First of all i'd just like to clarify what causes both of these functions execute. The documentation says that process_market_book logic executes upon "True" being returned from the check_market_book function but the process_orders function is a bit more ambiguous. Will this also run upon a market_book being recieved from the stream?



Secondly is there anything that stops multiple orders being placed on a single market when the market conditions are right? For example say i want to place a "back" order of stake £10 on a given market when the best back odds are &gt;3. Currently i dont understand how it wouldnt empty my account on repeated runs by placing many £10 orders whilst the conditions i have specified for placing are true (inside the process_market_book function). I'm sure this isnt the case but any clarification would be greatly appreciated. Thank you!

*Tags: Getting Started*

---

**liam** - *11:08:23*

`process_market_book`  will execute if True is retired on the check, the update itself comes from either an update from the market stream or a snap based on the strategy `streaming_timeout` which is handy on low update/liquidity markets



`process_orders` will be executed if there is an update through the order stream or is executed every 0.25s if there are live orders, see logic [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/streams/orderstream.py#L67|here](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/streams/orderstream.py#L67|here)



Order count is controlled by the runner context in the strategy, the default is a single live order per selection per market, var [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/strategy/strategy.py#L52|here](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/strategy/strategy.py#L52|here). And logic for validating that is [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/strategy/strategy.py#L138|here](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/strategy/strategy.py#L138|here) and executed on each place. Currently designed to be as safe as possible by default to prevent what you are describing. But you have backtesting and paper trading available to validate your strategy won't shit itself before you execute live :slightly_smiling_face:

*Tags: Deployment, Strategies*

---

## 2021-07-21

**Greg** - *03:46:52*

thanks again Oliver. My best mate is a coder and solved it with a get and set(C#/full stack coder he is but limited python) He solved for me in about 30 secs via screen share.Sigh..so much to learn from you lot :slightly_smiling_face: Anyway cheers for the offer.Might have to take advantage another time if that's ok.Cheers

*Tags: Deployment*

---

**Steve** - *11:59:54*

Does anyone know if there is a built in function in lightweight or flumine which calculates the matched profit if win for a selectionId that incoprorates the backs/lays which have been matched on other runners? The get_exposures function in the market blotter only seems to look at the backs/lay matched on a specific runner. I'm essentially looking for the website displays in the red/green numbers on the left hand side when a trade gets matched. Thanks

*Tags: General Technical*

---

**Aaron Smith** - *12:05:42*

[@UT6593JBH](@UT6593JBH) pretty sure this is not implemented within flumine, however it should be rather easy to calculate yourself. Just note that this only works on WIN markets (or any market with a single winner that is)

*Tags: General Technical*

---

## 2021-07-22

**RDr** - *09:51:49*

Hi, I am only getting started using betfairlightweight with python and looking for any existing link / shared codes about how to export all info from sample bz2 historical PRO data files into dataframes and csv files. The idea is more to get familiar with the data content (eg, visualise via excel or SQL) before deciding what transformations I need. Would anyone please give me some pointers? Thanks

*Tags: Getting Started, Feature Engineering*

---

**RDr** - *11:43:14*

Hi [@U4H19D1D2](@U4H19D1D2), Please I have a question about the example in [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py) . Where I can find the full list of exact names of the columns/details I can output to the text file? (...  other than the already listed ones in: output.write( ...  market_book.publish_time, market_book.market_id, market_book.status, market_book.inplay, runner.selection_id, runner.last_price_traded or "" ...). Would the names be different to what's shown in [https://historicdata.betfair.com/Betfair-Historical-Data-Feed-Specification.pdf](https://historicdata.betfair.com/Betfair-Historical-Data-Feed-Specification.pdf) ?

*Tags: General Technical*

---

**Mo** - *11:45:40*

Look at the definition of the MarketBook class: [https://github.com/liampauling/betfair/blob/6991a6a51363bae5fe5940f647b5cf7a4e7113cb/betfairlightweight/resources/bettingresources.py#L542](https://github.com/liampauling/betfair/blob/6991a6a51363bae5fe5940f647b5cf7a4e7113cb/betfairlightweight/resources/bettingresources.py#L542)

*Tags: Strategies*

---

**George** - *14:29:51*

Hi all. Just getting started with streaming. May be a stupid question but - what is the best way to stop streaming? Let's say I'm logging data for a single horse race and I only want to log it until the start of the race. What is the right way to end the stream? thanks.

*Tags: Getting Started*

---

**Oliver Varney** - *14:32:59*

are you talking about actual trading or just recording prices ? Is this bflw or flumine?

*Tags: Strategies*

---

**George** - *14:33:03*

bflw

*Tags: General Technical*

---

**liam** - *14:36:59*

```stream = trading.streaming.create_stream(listener=listener)



stream.stop()```

*Tags: Strategies*

---

## 2021-07-23

**Adrian** - *06:18:31*

does the market recorder cache the json messages and then bulk update the save file? Or does it update the save file every time? Reason I'm asking is if it's possible to use the raw json messages from betfair in real time or should we always be using runner_book/market_book?

*Tags: Data Quality*

---

**Mo** - *07:20:18*

I'm also really confused by the reference to market recorder and real time but note that there is a `streaming_update` field on the MarketBook: [https://github.com/liampauling/betfair/blob/6991a6a51363bae5fe5940f647b5cf7a4e7113cb/betfairlightweight/resources/bettingresources.py#L566](https://github.com/liampauling/betfair/blob/6991a6a51363bae5fe5940f647b5cf7a4e7113cb/betfairlightweight/resources/bettingresources.py#L566). This contains the raw message

*Tags: Data Quality, Strategies*

---

**Adrian** - *07:21:25*

awesome. thanks. that gives me options.

the second question i'm just trying to figure out if it's faster to read the file or get the price from market_book.runners

*Tags: General Technical*

---

**Adrian** - *07:22:37*

thanks Mo i don't really know how to interpret this code

*Tags: General Technical*

---

**Adrian** - *07:24:31*

i mean what does streaming update do

*Tags: General Technical*

---

**Adrian** - *07:26:55*

i see, so is the update to market book the first thing that happens upon streaming update? Just wondering if there's any point looking for something faster than market_book updates

*Tags: General Technical*

---

**liam** - *07:28:05*

The logic is heavily optimised however bflw is designed that you can pass in your own listener and read the raw updates or create your own logic 

*Tags: General Technical*

---

**Adrian** - *07:29:16*

speed isn't really my concern, i'm just trying to reduce any possible blocking from creating my numpy arrays from ltp and other vars

*Tags: Performance*

---

**liam** - *07:30:05*

I think you answered your own question, it won’t be bflw that is slow/blocking 

*Tags: Performance*

---

**Adrian** - *07:34:17*

i think what i mean to say is creating numpy arrays from market_book.runner data a legitimate way of doing it or is there a more flumine-like way

*Tags: General Technical*

---

**liam** - *07:35:09*

Any reason you need to use numpy?

*Tags: General Technical*

---

**Adrian** - *07:35:39*

for my models

*Tags: Strategies*

---

**liam** - *07:38:10*

So you can use the streaming update to tell you which runner has been updated, that can speed things up

*Tags: Performance*

---

**liam** - *07:39:17*

You also have the conflate var on streaming to slow down the number of updates if you don’t need every update 

*Tags: Performance*

---

**liam** - *09:17:59*

Regarding the streaming update filter, the simulation middleware does [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/markets/middleware.py#L40|this](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/markets/middleware.py#L40|this) you can actually take [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/markets/middleware.py#L169|this](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/markets/middleware.py#L169|this) function as is to get a list of runners that have been updated. Was shown to have a big impact on speeding up that code, quickest way to make code quick is to not execute it :wink:

*Tags: Performance*

---

## 2021-07-24

**Jono** - *09:22:41*

i was wondering the best way/practice to cancel any unmatched bets on a market using flumine? The reason i am asking is that although i am aware that the runner context should only allow 1 active bet per market per selection, upon running the strategy i am currently testing out it seems to place another order. Thus i would like to cancel the unmatched bet placed previously by the strategy so i dont unnecessarily increase my exposure whilst getting to grips with flumine. Below is the codfe i belived should have cancelled any outstanding orders i have but this is causing errors at "market.cancel_order(order)". Any help very much appreciated thank you!



`def process_orders(self, market, orders):`

`        [http://logging.info|logging.info](http://logging.info|logging.info)("Processing orders: %s"%(orders))`

        `for order in orders:`

`    `

            `try:`

`                [http://logging.info|logging.info](http://logging.info|logging.info)("Order: %s"%(order.__dir__))`

                `for key, value in order.__dict__.items():`

`                    [http://logging.info|logging.info](http://logging.info|logging.info)("%s : %s"%(key, value))`

            `except:`

`                [http://logging.info|logging.info](http://logging.info|logging.info)("Couldn't log order dict in method above")`





            `if order.status == OrderStatus.EXECUTABLE:`

`                [http://logging.info|logging.info](http://logging.info|logging.info)("Order executable")`



            `market.cancel_order(order)`                

`            `

`            [http://logging.info|logging.info](http://logging.info|logging.info)("Order cancelled supposedly")`

*Tags: Errors Debugging, Deployment, Strategies*

---

**liam** - *11:19:51*

What errors you getting? 

*Tags: Errors Debugging*

---

**Jono** - *11:34:24*

Traceback (most recent call last):

pythonbbauto-skrimming_1  |   File "/usr/local/lib/python3.9/site-packages/flumine/utils.py", line 233, in call_process_orders_error_handling

pythonbbauto-skrimming_1  |     strategy.process_orders(market, strategy_orders)

pythonbbauto-skrimming_1  |   File "/usr/src/app/syndicate_middleware/syndicateMiddlewareMain.py", line 232, in process_orders

pythonbbauto-skrimming_1  |     market.cancel_order(order)

pythonbbauto-skrimming_1  |   File "/usr/local/lib/python3.9/site-packages/flumine/markets/market.py", line 84, in cancel_order

pythonbbauto-skrimming_1  |     return t.cancel_order(order, size_reduction)

pythonbbauto-skrimming_1  |   File "/usr/local/lib/python3.9/site-packages/flumine/execution/transaction.py", line 69, in cancel_order

pythonbbauto-skrimming_1  |     order.cancel(size_reduction)

pythonbbauto-skrimming_1  |   File "/usr/local/lib/python3.9/site-packages/flumine/order/order.py", line 314, in cancel

pythonbbauto-skrimming_1  |     raise OrderUpdateError("Current status: %s" % self.status)

*Tags: Errors Debugging, Strategies*

---

**Jono** - *11:41:28*

Is flumine set up to be able to pick up orders already placed in the event of a stream ending error? Similarly is there a way of using it to process generic orders placed ie bets placed through the api not placed using flumine for a strategy or those placed on the exchange manually

*Tags: Errors Debugging, Strategies*

---

**liam** - *20:36:10*

That’s odd the only way you would get that error is if the order is not executable but you have an if statement handling that although it looks like it isn’t actually in the if? Or has the formatting on the message fucked it?

*Tags: Errors Debugging*

---

## 2021-07-25

**George** - *15:43:23*

Hi, just looking at logging market books using bflw streaming. In the listener.py file the logger is defined at the top as a global variable. I am finding this a bit inflexible, for example when I want to log two races into separate files with different logger 'names'. Is there a better way to do this? I was hoping maybe to feed a pre-prepared logger into the listener when creating the listener object?

*Tags: General Technical*

---

**George** - *16:03:56*

sorry not sure what that means. it's where the logger debug call is made so that is what determines where the messages are written to.

*Tags: Errors Debugging*

---

**George** - *16:04:31*

i don't know how to regain flexibility over that without subclassing the listeners, adding in a logger argument and rewriting every function that includes 'logger' to be 'self.logger'

*Tags: General Technical*

---

**liam** - *16:05:51*

So that logging call is for debugging the raw messages that come through the steam 

*Tags: Errors Debugging*

---

**liam** - *16:07:18*

From a user perspective logging would go [https://github.com/liampauling/betfair/blob/6991a6a51363bae5fe5940f647b5cf7a4e7113cb/examples/examplestreaming.py#L74|here](https://github.com/liampauling/betfair/blob/6991a6a51363bae5fe5940f647b5cf7a4e7113cb/examples/examplestreaming.py#L74|here) and you wouldn’t touch the bflw library logging but then I am not sure what you are trying to do? 

*Tags: General Technical*

---

**Mo** - *16:14:55*

Unless you've set the log level to debug for betfairlightweight it's not being logged anyway

*Tags: Errors Debugging*

---

**liam** - *16:19:26*

There is also a [https://github.com/liampauling/betfair/blob/6991a6a51363bae5fe5940f647b5cf7a4e7113cb/betfairlightweight/streaming/listener.py#L95|flag](https://github.com/liampauling/betfair/blob/6991a6a51363bae5fe5940f647b5cf7a4e7113cb/betfairlightweight/streaming/listener.py#L95|flag) on the listener as well to completely remove the overhead, even though debug might be disabled there is a cost in sending a large dict to the debug call 

*Tags: Errors Debugging*

---

**liam** - *16:19:48*

But yeah don’t debug in prod 

*Tags: Errors Debugging*

---

**George** - *16:22:48*

aren't they the same thing as what you'd get as the market_book.streaming_update

*Tags: General Technical*

---

**liam** - *16:29:33*

Most record the raw streaming data to a file for backtesting later 

*Tags: General Technical*

---

**George** - *16:30:48*

what's the difference between logging the raw market book delta updates and logging the raw streaming data

*Tags: General Technical*

---

**Oliver Varney** - *16:41:34*

Learning again :man-facepalming:. Id though the default would be False if its advisable to be off or am I thinking about it the wrong way round. Does anything need setting in flumine then. Its this just the raw stream logging, i.e. if listener_kwargs dict debug is false will the order logging continue ? currently  im not setting anything

*Tags: Errors Debugging*

---

**liam** - *17:12:23*

Flumine sets this to false by default and does a few other things to be quicker :wink:

*Tags: General Technical*

---

**George** - *17:13:08*

ok thanks for your help

*Tags: General Technical*

---

**Unknown** - *23:35:18*

Hi [@U012ZFYR19V](@U012ZFYR19V) and [@UPMUFSGCR](@UPMUFSGCR), it took me a while to make the backtest_multi.py work for me, but so far it's looking great, so thanks very much! I needed to add "`freeze_support()`" to make it work, otherwise it throws an error. I am not able to plug in the bz2 files directly (I got error message like "`UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 41: character maps to &lt;undefined&gt;`"), so have to decompress them manually. Guess I am missing something. Please could you give me a pointer?

*Tags: Errors Debugging*

---

## 2021-07-26

**George** - *11:03:54*

Just going back to this briefly [@U4H19D1D2](@U4H19D1D2) if you don't mind. I probably want to record what time I received the raw update. This would be useful because then I can deduct the publish-time from this and determine the latency. What would be the best way of doing this without using the listener.py logger?

*Tags: Performance*

---

**liam** - *11:05:45*

use flumine

*Tags: General Technical*

---

**liam** - *11:05:54*

or copy flumine

*Tags: General Technical*

---

**George** - *12:12:58*

just to log the latency!!

*Tags: Performance*

---

**liam** - *12:13:28*

[https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/examples/strategies/marketrecorder.py#L69](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/examples/strategies/marketrecorder.py#L69)

*Tags: General Technical*

---

**George** - *12:30:51*

ok, i've read through the code now. it's a completely different paradigm to bflw.

*Tags: General Technical*

---

**liam** - *12:35:14*

well its a fully functional trading framework, bflw is just an API wrapper

*Tags: Strategies*

---

**Matthieu Labour** - *14:55:54*

Hi, when running flumine / backtest, do the `runner_context.trade_count` and `runner_context.live_trade_count` get updated?

*Tags: Deployment*

---

**Matthieu Labour** - *18:31:17*

Would you be open that I open a PR with the following code change to [https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/baseresource.py#L23](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/baseresource.py#L23)

Add the following

```        elif isinstance(value, numeric_types):

            try:

                return datetime.datetime.utcfromtimestamp(value)

            except (ValueError, OverflowError, OSError):

                return```

*Tags: Errors Debugging*

---

**Matthieu Labour** - *20:14:09*

Here is my use case. On disk, I have market book data that look like `{"op": "mb", "clk": null, "pt": 1626986519.301084, "mc": [{"id": "1.185558123..."` . Please, do note pt. When replaying them w/ flumine/backtest, `publish_time` will be None [https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L587](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/bettingresources.py#L587). I tested the change above and it fixes the issue.

*Tags: Errors Debugging, Strategies*

---

**liam** - *20:39:04*

Why does it look like that (float)? Is this a flumine bug?

*Tags: Errors Debugging*

---

**Matthieu Labour** - *20:50:09*

I believe `1626986519.301084` is a valid epoch.  No, flumine has no issue. This is how I recorded the data.

*Tags: General Technical*

---

**George** - *22:12:46*

Latency. So i logged the raw updates directly from listener.py (as I was advised not to do!) and compared my timestamp to the Betfair publishTime value. The reason I did this is because the difference between my log timestamp and the Betfair publishTime should be equal to the network latency (assuming logging is negligibly fast).

*Tags: Performance*

---

**George** - *22:13:10*

What do we think is the theoretical minimum value for this network latency?

*Tags: Performance*

---

**Unknown** - *23:13:37*

Hi [@UBS7QANF3](@UBS7QANF3), thanks for your reply. I changed 3 things in the script (so I used the downloaded script and didn't add my own strategy): 1) added `from multiprocessing import Process, freeze_support` at the top; 2) added

```if __name__ == '__main__':

    freeze_support()```

at line 65; and 3) pointed `root_folder` to my data folder with the 3 bz2 files and removed (at line 77 ) `if x.endswith('.bz2')` . The error message I have from pycharm is:

`The above exception was the direct cause of the following exception:`



`Traceback (most recent call last):`

  `File "C:\betfair\API\bf_betting\backtest_multi_orig.py", line 96, in &lt;module&gt;`

    `all_dfs.append(future.result())`

  `File "C:\Users\User\AppData\Local\Programs\Python\Python39\lib\concurrent\futures\_base.py", line 438, in result`

    `return self.__get_result()`

  `File "C:\Users\User\AppData\Local\Programs\Python\Python39\lib\concurrent\futures\_base.py", line 390, in __get_result`

    `raise self._exception`

`UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 41: character maps to &lt;undefined&gt;`



If I switched to the folder with decompressed files, it is working fine (placing orders from the laylowest example strategy). So I asssume the file format/compression is the problem (I could well be wrong). Any pointers will be highly appreciated.

*Tags: Errors Debugging, Strategies*

---

## 2021-07-27

**Mo** - *05:35:19*

I think it's pretty clear the problem is you're trying to read a compressed file as an uncompressed file (or possibly assuming the wrong compression type). Why did you remove that condition from line 77?

*Tags: General Technical*

---

**Steve** - *06:00:08*

When I am processing orders in flumine with the def process_orders() function I can't figure out why when I iterate through the orders with [order.responses.current_order for order in orders] some orders have a current_order objects and others don't. Anyone know why?

*Tags: General Technical*

---

**James** - *13:23:17*

What's the fastest way to write/read/clear data with a time series in python? I've got a script that scrapes data every few seconds and stores it in a mysql database. It deletes data more than x seconds old so there's usually only 5 records of 2 fields. One is the scrape time the other is the data, a 20 digit number. Speed is important and using mysql seems OTT

*Tags: Performance*

---

**James** - *13:23:56*

...but then I'm relatively new to python

*Tags: Getting Started*

---

**Oliver Varney** - *13:26:39*

Can pandas do this? or do you need it accessible by other processes?

*Tags: Feature Engineering*

---

**liam** - *13:34:38*

Can you not keep it simple and just use python? Will be much quicker than anything involving a db

*Tags: General Technical*

---

**James** - *14:26:59*

Would storing in a dictionary be the best option using the python route?

*Tags: General Technical*

---

**liam** - *15:17:29*

Considering speed is important and the data size is tiny, yes, I wouldn't look to move this out to a dedicated k/v store until you start hitting the limitations of python or need multi process support etc. Easy to get lost and waste a lot of time on over engineering stuff like this (imho)

*Tags: Performance*

---

**PeterLe** - *18:54:17*

Folks, quick question please..First time I ran a live strategy (Whoop!)...but placed a bet deliberately low at 1.03..If I stop the program to edit the code and then restart it, does it somehow keep a live connection (ie am I in danger of  having too many connections if I stop and start it regularly? Thanks

*Tags: Getting Started, Deployment, Strategies*

---

**Unknown** - *18:59:31*

It seems an odd setup but my knowledge of python isn't strong enough for me to be confident. 



Script 1 (the one referred to above) scrapes data to the mysql database. 



Script 2 harvests links from emails using imap_tools and when a link is scraped calls Script 3 using os.system(f'start "window title" cmd /k "python scrape.py (link) &amp;&amp; exit"').



Script 3 then uses the data in mysql to scrape part of these sites.

*Tags: Getting Started*

---

**Dave** - *22:10:58*

I have generally accepted that if you have, say, more than 10 features then you might as well consider your XGBoost model a black box and it becomes quite difficult to interpret. I'd lump it in with neural nets with multiple hidden layers etc. Idk if anyone else here spends time interpreting their trees but IMO if it does well out of sample and you've been clean with your modelling approach then you are probably good to go. With that being said, the sklearn api for XGBoost exposes "feature importances" - I'd definitely analyse these and ensure they somewhat make sense.

*Tags: Feature Engineering, Strategies*

---

**Dave** - *22:12:15*

There are a billion and one tutorials written by kagglers which give decent tips on hyperparameter tuning and measuring model performance

*Tags: Performance, Strategies*

---

**Dave** - *22:26:56*

The "right" thing to do is to interpret your models of course and disect them, but unfortunately XGBoost doesn't make it easy. If you are keen on interpretable tree models, maybe look at Random Forest? Generally given the same set of features, a RF model and a XGB model will have similar predictive ability. It is quite easy to plot trees from RF though and visualize them with graphviz. Maybe it could be a good first step before moving onto xgb

*Tags: Feature Engineering, Strategies*

---

**Sam Asin** - *22:29:43*

For what it's worth, I did just run predictions, and then spot checked two points, and it does seem to me like the final prediction just has a constant/intercept term added of .5. Just in case others run into similar problems or get curious about how these things work. So, my model has all these negative leaf values, but the predictions are ultimately all of those + .5

*Tags: Strategies*

---

**Sam Asin** - *22:30:54*

In terms of "right" thing to do I'm not sure about best practice for modeling etc. with xgboost and all that, but for me as like a personal development "best practice" to be able to do a good job in general with these things and also get satisfaction out of doing them, I wanna have some idea what's going on I think. I'm sure there's plenty of room for people to have success in whatever way though :slightly_smiling_face:..

*Tags: Strategies*

---

## 2021-07-28

**Adrian** - *07:09:45*

[@U01J98RDHRN](@U01J98RDHRN) i'm in a similar camp. i think the hardest part about any ML is figuring out what it's trying to tell you. Building the models, feature engineering and all that is the easy part

*Tags: Feature Engineering, Strategies*

---

**Adrian** - *07:15:34*

applying the model in other words

*Tags: Strategies*

---

**bb** - *08:38:13*

[@U01J98RDHRN](@U01J98RDHRN) Another good way of getting a bit of insight into what is going on in "black box" models calculating/looking at the SHAP values. It gives a bit more of a detailed view than looking at the feature importances.



This library is a pretty good start for getting up and running



[https://github.com/slundberg/shap](https://github.com/slundberg/shap)

*Tags: Feature Engineering, Strategies*

---

**Oliver Varney** - *11:31:43*

May be worth taking a look at threading in python

*Tags: General Technical*

---

**James** - *11:40:09*

Thanks for all your help

*Tags: General Technical*

---

**Michael** - *16:02:54*

Has anyone managed to get their certs working on a MAC? I can create working certificates on windows with xca and openssl on but when I try to create one with either  openssl or xca on mac I keep getting certificate errors when loading them in the api. Is there any additional steps I'm missing (bar changing the cetificates uploaded on betfair) ?

*Tags: Errors Debugging*

---

## 2021-07-29

**Greg** - *08:05:56*

Hi all. Have done some searching but nothing stands out to me. (At the risk of overfitting ) I have a bot running but would like to exclude Trot and Pace races (my father in law has some experience as a bookie on harness races so there is good reason here) Is there a simple way to do this via Flumine/BFLW? thanks in advance

*Tags: General Technical*

---

**Steve** - *08:23:51*

If you have any order which has been placed on the exchange and is Executable but has a VIOLATION status - how do you cancel it from the exchange? I keep getting this error:

raise OrderUpdateError("Current status: %s" % self.status)

flumine.exceptions.OrderUpdateError: Current status: OrderStatus.VIOLATION

*Tags: Errors Debugging*

---

**Beeblebrox** - *09:38:02*

There's some helper functions shown here for just this: [https://github.com/betfair-down-under/knowledgeShare](https://github.com/betfair-down-under/knowledgeShare)

*Tags: General Technical*

---

**moseley82** - *10:04:43*

Hi guys hoping this is a simple one, looking at this doc here [https://betfair-datascientists.github.io/api/apiPythontutorial/#cancelling-orders](https://betfair-datascientists.github.io/api/apiPythontutorial/#cancelling-orders) and it gives instructions on how to cancel by Market ID, but does anyone know how to cancel by Bet ID or Selection ID?

*Tags: General Technical*

---

**liam** - *10:05:17*

You guys have picked a bad day for all these questions 

*Tags: General Technical*

---

**Oliver Varney** - *10:07:20*

[https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/order/order.py#L314](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/order/order.py#L314)

*Tags: General Technical*

---

**Oliver Varney** - *10:26:29*

I think the message on the raise could be a little clearer, maybe raise OrderUpdateError(f"Incorrect order status : {self.status}. Only orders with status: {OrderStatus.EXECUTABLE} can be cancelled")

*Tags: Errors Debugging*

---

**Oliver Varney** - *11:37:26*

the issue it seems is the status of the order if that the line code throwing the exception. Is the bet still unmatched in the market?

*Tags: Errors Debugging*

---

**Oliver Varney** - *11:39:42*

hmm, can you replicate it and log / view through the debugger?

*Tags: Errors Debugging*

---

**Steve** - *11:44:03*

I can try. I've never used the debugger before.

*Tags: Errors Debugging*

---

**PeterLe** - *19:30:57*

incoming newbie question..Took a while, but my first full day of running Flumine on an account :grinning: (small profit too). If I wanted to run the same strategy (slightly different triggers) on a second account , would I simply copy the program; give it a different file name and then run that separately? (the way I ran the file was : I just pointed to the relative file path and pasted it in file manager and it opened a terminal C:\windows\Py.exe. Im not sure if this is the correct way to do it (Seemed to work OK)? Sorry for the very basic question. Thanks in advance

*Tags: Strategies*

---

**liam** - *19:56:42*

Yep no problem with that, ideally you would pass the username as an argument into the script (sys.argv) so that you are keeping things dry, no point in copying a file just to change a single var 

*Tags: General Technical*

---

**liam** - *19:59:01*

This could be a bug where a previous cancel/update/replace was a violation although I think that was fixed recently 

*Tags: Errors Debugging*

---

**PeterLe** - *20:02:48*

Ok thanks Liam, I’ll check that out. Looking forward to experimenting with Flumine.  Hope the hangover has gone from last night :smile:

*Tags: General Technical*

---

## 2021-07-30

**Jono** - *07:20:05*

What constitutes a live order in flumine - is it an order that has a unmatched amount on a market yet to resolve? As i want to place a follow up bet each time the previous one is matched in the order back, lay , back, lay, back.... so on i was wondering will i have to adjust the max_live_trade_count to something very high in order to do this?



And are orders retrieved in the process_orders function listed in a p[articular way consistently eg placed time descending? Additionally if there is some consistent ordering does this also apply to selection_orders/strategy_orders orders retrieved via the market.blotter so i know when iterating? Thanks for any help everyone :grin:

*Tags: Deployment, Strategies*

---

**Peter** - *07:55:13*

This sounds like a scalping strategy, which would be low risk, but relies on the pairing of back and lay bets to keep it that way. Placing the back bet first keeps the risk down as the liability is smaller on anything with odds &gt; evens.

*Tags: Strategies*

---

**Jono** - *08:24:19*

Im viewing this as a problem of first rasing the exposure to a certain level with back bets then bringing it back to 0 with lay bets.



A question in a similar vain would be is there an acknowledged way to cash out from a market or selection using flumine/bflw

*Tags: General Technical*

---

**Peter** - *09:33:55*

No widely accepted way. It has been discussed in the past, and people broadly fall into three camps:

1. reverse engineer the website's cash out endpoint and hit that

2. write your own, not difficult and I believe that code has been posted here in the past

3. "never green up" (not really relevant to your strategy).

but nothing built into BFLW or Flumine.

*Tags: Strategies*

---

**Jono** - *10:06:51*

i think method 2 might be my port of call. if market conditions are looking good after the initial bet it might be worth placing odds at one tick better than whatever is currently listed. Ive written my own function for doing this but seems cumbersome and not very "flumine" like. do you know if there is a good way of correctly identifying the odds value which is the next tick along from what is offered?

*Tags: General Technical*

---

**Beeblebrox** - *10:19:19*

There's this function to return a price a certain number of ticks away from another price:



[https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/utils.py#L139](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/utils.py#L139)

*Tags: General Technical*

---

**Jono** - *11:59:21*

thank you for that section [@U9JHLMZB4](@U9JHLMZB4) it looks very helpful

*Tags: General Technical*

---

## 2021-07-31

**thambie1** - *21:11:27*

Question about the marketVersion in the placeOrders request. Does anyone know if the marketVersion check is performed after or before the bet delay?

*Tags: General Technical*

---

## 2021-08-01

**thambie1** - *11:37:36*

I was messing around with my simulated execution latency in backtests, and saw flumine defaulted to 0.12 for placing orders, and 0.17 for cancelling. Why would cancel latency be greater than place?

*Tags: Performance*

---

## 2021-08-02

**river_shah** - *13:34:56*

Second [@UBS7QANF3](@UBS7QANF3) I wasted plenty of time with advanced data. Don’t do it. For any serious modelling work you will need `PRO` or self recorded

*Tags: Strategies*

---

**Michael** - *18:18:36*

From a newbies perspective, seems like a large investment to make up front without understanding whether you have a strategy which will perform. How do you self record? I've got experience with web scraping but I imagine since betfair sell the data they would be against this and proxies etc can be just as expensive

*Tags: Strategies*

---

**thambie1** - *18:20:43*

I don't use flumine myself, but this is the example market recorder: [https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py)

*Tags: Data Quality*

---

**Michael** - *22:52:49*

And other than officially from betfair, there are no other ways I can get this data? Also, if you don't mind how many months of data did you require in order to backtest your strategy

*Tags: Strategies*

---

## 2021-08-03

**Mo** - *08:10:05*

I wouldn’t recommend someone just starting out to dump thousands on a load of PRO data for backtesting. Just put together a simple strategy so you are actually placing live bets then record the data yourself. This seems to be the canonical way to get started 

*Tags: Deployment, Strategies*

---

**thambie1** - *08:22:14*

Depends. If you're strategies require any kind of model building, I think it's worth paying pretty early on. Assuming you value your time, and are taking this seriously. I regret not spending the money sooner, would have saved me many weeks if not a month or two.

*Tags: Strategies*

---

**Jonjonjon** - *08:53:24*

(Is it permitted to share recorded files here, for debugging purposes?)

*Tags: Errors Debugging*

---

**Mo** - *08:54:57*

I think it depends on your background and initial starting resources. I can see how it would have been a good move for you [@U01DCR5PXDY](@U01DCR5PXDY) but you are an exceptional case in my opinion :wink:

*Tags: Errors Debugging*

---

**liam** - *16:13:43*

Yes however do a check first and this is assuming you are using python &gt;= 3.7

*Tags: General Technical*

---

**Jonjonjon** - *22:07:17*

Suppose I want to find the line of the PRO market data file in the flumine test cases, corresponding to datetime 2020-04-01 03:34:55.151000, How do I find it?



Using datetime.timestamp(), I get 1585708495, which is not in the file

*Tags: General Technical*

---

## 2021-08-04

**Jonjonjon** - *07:41:15*

It's the PRO data file for the flumine tests

*Tags: General Technical*

---

**Unknown** - *07:59:22*

Using betfairviz, I am not seeing any updates at that time, will check flumine now

*Tags: General Technical*

---

**Unknown** - *08:04:25*

The next update you can see this, flumine matches it with the traded volume at 5.3, I can only assume the volume at 5.4 and 5.5 was matched in the engine/xm, I don't see a problem?

*Tags: General Technical*

---

**liam** - *09:01:09*

FYI bflw [https://github.com/liampauling/betfair/blob/master/HISTORY.rst#2130-2021-08-03|2.13.0](https://github.com/liampauling/betfair/blob/master/HISTORY.rst#2130-2021-08-03|2.13.0) released to handle 09/08 API change, flumine also upgraded [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1198-2021-08-03|1.19.8](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1198-2021-08-03|1.19.8) (can now record order stream data)

*Tags: General Technical*

---

**Peter C** - *09:03:21*

If we don't update flumine before the API changes will it continue to work?

*Tags: General Technical*

---

**Jonjonjon** - *09:26:17*

[@UBS7QANF3](@UBS7QANF3) and [@U4H19D1D2](@U4H19D1D2): Thanks for helping me look at this. I will go away and ponder. It's definitely shown a good use case for betfairviz!

*Tags: General Technical*

---

**Jono** - *09:58:32*

Quick question,  the documentation in the link that [@UBS7QANF3](@UBS7QANF3) sent previously it states that:



*Please note:* EXECUTION_COMPLETE (fully matched) orders are only returned when transitioning from EXECUTABLE to EXECUTION_COMPLETE. The full details of EXECUTION_COMPLETE orders can only be viewed using listCurrentOrders/listMarketBook using orderProjections.



I was wondering if this applies to the stream that process_order func is triggered by? more specifically when an order transitions from EXECUTABLE to EXECUTION_COMPLETE i am assuming this causes process_orders to run but then in follow up runs are orders with an EXECUTION_COMPLETE status brought through in the "orders" variable or to access them is a request for either listCurrentOrders/listMarketBook required. Thank you!

*Tags: General Technical*

---

**river_shah** - *12:10:36*

[@U4H19D1D2](@U4H19D1D2) can we please discuss what `betfairlightweight[speed]` actually does / how it is compiled / any [https://github.com/liampauling/betfair/blob/master/docs/advanced.md|benchmarks](https://github.com/liampauling/betfair/blob/master/docs/advanced.md|benchmarks)? I have not made an effort to dig through this aspect of the code yet, but any direction from your side will help me understand better. Thanks

*Tags: Performance*

---

**liam** - *12:13:21*

Installs `ciso8601` for faster datetime [https://github.com/liampauling/betfair/blob/8ee78d0ff14a585bf49eb3712375c473e83180df/betfairlightweight/compat.py#L20|parsing](https://github.com/liampauling/betfair/blob/8ee78d0ff14a585bf49eb3712375c473e83180df/betfairlightweight/compat.py#L20|parsing) (uses c) and installs `orjson` for faster json [https://github.com/liampauling/betfair/blob/8ee78d0ff14a585bf49eb3712375c473e83180df/betfairlightweight/compat.py#L14|encoding/decoding](https://github.com/liampauling/betfair/blob/8ee78d0ff14a585bf49eb3712375c473e83180df/betfairlightweight/compat.py#L14|encoding/decoding) (uses rust)

*Tags: Getting Started*

---

**liam** - *12:14:13*

However since the major streaming refactor in 2.12, I haven't actually benchmarked speed

*Tags: Performance*

---

**river_shah** - *12:21:38*

No actual compilation needed in bflw itself. For a second I thought there was some cython like stuff going on

*Tags: General Technical*

---

**Peter C** - *15:39:56*

What is the most flumine way to check whether a runner has had a reduction factor applied? Will average_price_matched be updated when a runner is removed, or do I need to compare the market_book at placement/off and apply the reduction factors myself?

*Tags: General Technical*

---

**liam** - *15:49:09*

In stationary traffic at the moment so can’t check but cleared orders tell you a rf has been applied, not sure about currentOrders / streaming, have you checked the docs?

*Tags: General Technical*

---

## 2021-08-07

**thambie1** - *07:52:18*

Does blfw/flumine ever hit conflation issues when subscribing to a ton of markets?

*Tags: General Technical*

---

## 2021-08-09

**S G** - *16:12:21*

Hi guys, is there any java implementation similar to betfairlightweight?

*Tags: General Technical*

---

**S G** - *16:14:04*

I m more a Java guy, so finding it time consuming to use python library betfairlightweight

*Tags: General Technical*

---

**thambie1** - *16:14:42*

I'm using java myself. You can either spend your time recreating flumine (I did), or getting comfortable with python.

*Tags: General Technical*

---

**thambie1** - *16:16:40*

Which? My repo is private. Flumine, betfairlightweight, and betfair sample code; are all public.

*Tags: General Technical*

---

**S G** - *16:17:05*

good to hear that you recreated flumine in Java

*Tags: General Technical*

---

**S G** - *16:20:40*

I started playing with flumine and betfairlightweight. They are good libraries, but its just me taking time to adjust to python world

*Tags: General Technical*

---

**Mo** - *16:21:29*

Continue to adjust in my opinion (as someone whose original trading platform was written in Java and is now in Python)

*Tags: Strategies*

---

**Peter** - *16:21:52*

[@U02ABPZRT2N](@U02ABPZRT2N) I took the opposite approach. My PHP package is the goto in that community with more checkouts from github more than all the other PHP equivalents put together, but 2 years ago I concluded that it was worth learning Python just for betfairlightweight and Pandas. And then Flumine came along to confirm that I'd made the choice (at least for me).

*Tags: Feature Engineering*

---

**thambie1** - *16:25:58*

Harder to read code, harder to debug, harder to guarantee it works as you expect, slower

*Tags: Errors Debugging, Performance*

---

**S G** - *16:26:32*

I was about to ask diffs between Flumine and bflw. is Fulmine slower considering that only works with clients that has lightweight=False

*Tags: Performance*

---

**Mo** - *16:27:50*

A lot of these problems are solved with pyrsistent, type hints and a good IDE

*Tags: General Technical*

---

**Peter** - *16:27:52*

Betfairlightweight is a wrapper around the Betfair API. flumine is a trading framework that uses betfairlightweight to interact with Betfair.

*Tags: Strategies*

---

**S G** - *16:28:35*

sure, I was more interested in performance aspects of both libraries

*Tags: Performance*

---

**S G** - *16:32:37*

so, was wondering if anyone noticed any thing at all with Flumine

*Tags: General Technical*

---

**Mo** - *16:35:22*

I agree performance is a problem so think C/C++ is a reasonable alternative

*Tags: Performance*

---

**liam** - *16:35:28*

Flumine does other things to be quick [https://github.com/liampauling/flumine/commit/64ae76f0cb5571a040ce9904febf35df733931c4|https://github.com/liampauling/flumine/commit/64ae76f0cb5571a040ce9904febf35df733931c4](https://github.com/liampauling/flumine/commit/64ae76f0cb5571a040ce9904febf35df733931c4|https://github.com/liampauling/flumine/commit/64ae76f0cb5571a040ce9904febf35df733931c4)

*Tags: General Technical*

---

**admiral** - *16:48:24*

Where does flumine get its historical data for backtests from? I thought it costs money

*Tags: Data Quality*

---

**liam** - *16:55:28*

You can either purchase from betfair or use flumine to record your own 

*Tags: General Technical*

---

**S G** - *17:31:43*

With Flumine is it possible to configure strategy rules?

*Tags: Strategies*

---

**S G** - *17:31:54*

instead of writing code for every strategy

*Tags: Strategies*

---

**S G** - *17:41:24*

instead of inheriting BaseStrategy class everytime for a new strategy

*Tags: Strategies*

---

**S G** - *17:41:43*

is there any genericstrategy that supports configuration

*Tags: Strategies*

---

**Aaron Smith** - *17:42:08*

this can easily be done while inheriting BaseStrategy

*Tags: Strategies*

---

**Aaron Smith** - *17:42:34*

if you dont inherit BaseStrategy, you ll just end up having to write way more code to bascially write it yourself again

*Tags: Strategies*

---

**admiral** - *19:16:00*

Very noob question but if you wanted to record market data to use in backtest going forwards, would running a OrderDataStream class on an ec2 instance saving to S3 be a generic way of doing this?

*Tags: Deployment*

---

**mandelbot** - *19:21:37*

there's a marketrecorder strategy you could use to do just that [https://github.com/liampauling/flumine/tree/master/examples](https://github.com/liampauling/flumine/tree/master/examples)

*Tags: Strategies*

---

## 2021-08-10

**Peter** - *10:48:03*

Not sure is this is answering your question, but I have a strategy that encapsulates some code patterns that I use repeatedly. It inherits BaseStrategy and is in turn inherited by my individual strategies.

*Tags: Strategies*

---

**Peter** - *10:50:06*

Another pattern that works well for me is when strategies are similar, I parameterise them and pass the parameters in the context dictionary passed to each strategy when it's instantiated.

*Tags: Strategies*

---

**Jono** - *15:21:31*

bit of a strange one i would like to perform a single run of the logic in my "process_orders" function at the start of the flumine strategy run in order to perform some actions involving "EXECUTION_COMPLETE" orders. The reason being ive managed to get the logic of the strat im testing in a semi working condition and a large part of it involves placing follow up bets inside the process_orders func. However when im restarting the strat very often the orders that came before are matched and are therefore no longer causing this function to run. Additionally none of these bets are of the "EXECUTABLE" status so i cant rely on the 0.25s auto run. In essence this means no other follow up bets are placed for the strat. Any ideas how best to go about fixing this problem either through an initial sort of forced run of "process_orders" or otherwise? Cheers!

*Tags: Errors Debugging, Strategies*

---

**birchy** - *16:16:35*

Could run that in any of the functions that have the market object as an input. I would probably stick it in `check_market_book` and return False until blotter is initiated, then set a strategy flag like `self.initiated=True` or whatever.

*Tags: Strategies*

---

**Jono** - *16:25:42*

That sounds like a great way to tackle this problem. Thanks very much!

*Tags: General Technical*

---

**Newbie99** - *16:39:31*

Maybe I'm missing the obvious here, but if the blotter was genuinely 0 length (i.e. had no orders), then wouldn't the above mean the strategy never initialises?

*Tags: Strategies*

---

**birchy** - *17:49:13*

[@U4H19D1D2](@U4H19D1D2) does Flumine have a flag to tell us when the market orders have been loaded? Or is there a way to pause strategy calls until the orders have been processed?

*Tags: Strategies*

---

## 2021-08-11

**George** - *11:55:09*

Hi All! When using the Scores API there's no streaming alternative so I'm wondering whether anyone makes asynchronous requests. As far as I know it's not possible to do this using BFLW because the BaseEndpoint uses requests rather than aiohttp. Is that right? Has anyone else solved this problem / come up with a better idea?

*Tags: General Technical*

---

**George** - *13:53:54*

do you mean use multithreading rather than python 3's built-in async/await?

*Tags: General Technical*

---

**George** - *14:24:15*

maybe i wasn't clear about the use case. i know how to do polling in the background in a thread, but i want to do asynchronous polling to increase the frequency

*Tags: General Technical*

---

**George** - *14:31:07*

yes, i'm asking about the scores API, and the reason I am asking is because they don't have a streaming version so it's their own fault if they get too much traffic

*Tags: General Technical*

---

**George** - *14:37:11*

would I not have to rebuild a lot of BFLW to send the request myself

*Tags: General Technical*

---

**D C** - *15:08:38*

I've seen a few of you talking about Redis and from what I understand (limited) it is a RAM based database. Is it inherently thread safe?

*Tags: General Technical*

---

## 2021-08-12

**ThomasJ** - *10:28:01*

I've been doing the Flumingo Tango and the Middleware step has my head in a complete spin. My dance partner is Ms Historical Data from the Backtest Lowestlayer school. :smile:



PART A

So a Trade/Order Package is created and placed in the backtest handler queue.

After the appropriate delay the code winds it's way down to `class Simulated &gt; def place &gt; elif available_to_lay &lt;= price` which is where the matched/not-matched decision is made.

If a match occurs then that order is not looked at again from a matching perspective. All good.



PART B

But if a match does not occur then that order has a size_remaining != 0 and Middleware takes an interest.

For every marketbook update, Middleware calculates RunnerAnalytics, and I will focus on 'traded'.

In `class RunnerAnalytics &gt; def _calculate_traded` the traded values are calculated as deltas compared to prior marketbook update (or empty {} if runner is not included in latest marketbook update).

After RunnerAnalytics, because `order.status == OrderStatus.EXECUTABLE` and `size_remaining != 0`, the code ends up in `class Simulated &gt; def _calculate_process_traded`, which is where I get totally bamboozled.



`_traded_size_ = _traded_size_ / 2`

        `if _self_._piq - _traded_size_ &lt; 0:`

            `size = _traded_size_ - _self_._piq`

            `size = round(min(_self_.size_remaining, size), 2)`

            `if size:`

`                _self_._update_matched(`

                    `[`

`                        _publish_time_,`

`                        _self_.order.order_type.price,`

                        `size,`

                    `]  # todo takes the worst price, i.e what was asked`

                `)`

`            _self_._piq = 0`

        `else:`

`            _self_._piq -= _traded_size_`

            `logger.debug(`

                `"Simulated order {0} PIQ: {1}".format(_self_.order.id, _self_._piq)`

            `)`



1. Why are unmatched orders not just left in the backtest handler queue to wait for a match, or of course killed after 2 secs?

2. No doubt there is a good answer for Q1 so can someone please explain how the trade deltas matching works in middleware.

*Tags: Data Quality, Errors Debugging*

---

**liam** - *10:34:54*

tbh I don't really understand your question, however when simulating orders you have two parts, the initial place and check if a match occurs and then the constant check to see if an updated marketBook results in a match on the executable order



the code `_calculate_process_traded` is doing the latter, checking the piq (position in queue) and then the size remaining to see if `update_matched` needs to be called

*Tags: General Technical*

---

**George** - *10:40:50*

just seen this in BaseEndpoint:

```param Session session: Requests session to be used, reduces latency.```

so passing in an already active session reduces latency? by how much roughly do we think?

*Tags: Performance*

---

**George** - *10:42:53*

wow! anything else i should know for latency sensitive BFLW users?

*Tags: Performance*

---

**George** - *10:46:04*

so if i pass an aiohttp session to the client then maybe i can use async without rewriting BFLW?

*Tags: General Technical*

---

**Oliver Varney** - *10:55:47*

stupid question incoming, but does this mean I should be passing a session into the apiclient for flumine, not sure if I currently do but worth a check by the sounds

*Tags: General Technical*

---

**Oliver Varney** - *10:57:38*

okay so no need to set any params on the BFWL client passed into the flumine client (as per the getting started example)?

*Tags: Getting Started*

---

**Oliver Varney** - *11:04:45*

[https://liampauling.github.io/flumine/quickstart/](https://liampauling.github.io/flumine/quickstart/)

*Tags: General Technical*

---

**Oliver Varney** - *11:04:56*

```import betfairlightweight

from flumine import Flumine, clients



trading = betfairlightweight.APIClient("username")

client = clients.BetfairClient(trading)



framework = Flumine(client=client)```



*Tags: Strategies*

---

**liam** - *11:05:46*

you can still pass params such as locale etc. but flumine will create a session on any latency sensitive calls

*Tags: Performance*

---

**liam** - *11:08:04*

However flumine has been designed so that you can use your own middleware and simulation logic if you want to

*Tags: General Technical*

---

**Mo** - *11:23:51*

Cloudflare documentation states they keep connections open for 900 seconds after the last request. I suggest you test to confirm this 

*Tags: General Technical*

---

**Mo** - *11:25:36*

You seem to think courtsiding is a loaded term. I’m just using it as a colloquialism for latency arbitrage

*Tags: Performance*

---

**George** - *11:30:31*

I think this might be the answer? [https://docs.python-requests.org/en/latest/user/advanced/#keep-alive](https://docs.python-requests.org/en/latest/user/advanced/#keep-alive)

*Tags: Deployment*

---

**Mo** - *11:36:34*

You go to Betfair via Cloudflare so the important question is what Cloudflare does

*Tags: General Technical*

---

**Jan** - *21:04:12*

[@UBS7QANF3](@UBS7QANF3) in your presentation you said it would be nice if people had easy access to a hosted python environment, so they don't need to go through the struggles setting environments up. I think google colab serves that case pretty well.



I've set up the dashboard branch in a colab, so all non coders can basically just upload their market file and use your dashboard right away:

[https://colab.research.google.com/drive/1D362tW_TkPQdxX7UXqiLgTWZIZIcOmxr?usp=sharing](https://colab.research.google.com/drive/1D362tW_TkPQdxX7UXqiLgTWZIZIcOmxr?usp=sharing) (if you intend to use it, please make a copy for yourself, after all this is "colab", so people can see and interrup what you're doing).

*Tags: General Technical*

---

**Jan** - *21:05:51*

One q to you though, what kind of market files are you passing in? I'm only doing soccer and only working with the pro files, they always throw an error when being parsed.

*Tags: Errors Debugging*

---

**Michael** - *22:55:13*

[@U4H19D1D2](@U4H19D1D2) not using Flumine - is there any reason not to pass a session to the client?

*Tags: General Technical*

---

**liam** - *23:03:23*

You don’t care about latency and/or don’t plan on making another request 

*Tags: Performance*

---

## 2021-08-13

**Mo** - *06:59:31*

[@U029JM0BJLB](@U029JM0BJLB) I've not used collab before but looks like uploaded gives you a dictionary of filename to file contents so you need to change the dashboard creation cell to something like `betfairviz.create_dashboard(list(uploaded.keys())[0])`. While this "works" in that there is no error and there is clearly some processing involved, the cell output is empty

*Tags: Errors Debugging*

---

**Mo** - *07:20:26*

Looks like the problem is the graph even though I can get the graph working by itself. There are also some issues even when the dashboard can be brought up including:



1. The slider for navigating through the market doesn't work

2. None of the icons on the buttons display

*Tags: General Technical*

---

**Mo** - *07:23:35*

Icons solved by this workaround: [https://github.com/googlecolab/colabtools/issues/1302](https://github.com/googlecolab/colabtools/issues/1302)

*Tags: General Technical*

---

**Joe Fanning** - *08:13:18*

I'm having a similar problem:  except I'm getting 4046 instead of 4022:   SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:4046)    Is there documentation online for fixing these errors?

*Tags: Errors Debugging*

---

**Mo** - *08:34:39*

Looks like the same root cause even if the error code is different: some kind of problem with certificate generation

*Tags: Errors Debugging*

---

**Unknown** - *09:07:26*

Hi Mo Yes there must be some documentation online that explains these specific errors. Anyway you can still login without the certificate if you want to keep working on your code. Just use the `trading.login_interactive()`  function instead of  the `trading.loogin()` * *function* *and comment out* *`certs=certs_path` in the

```betfairlightweight.APIClient( username=my_username,

password=my_password,

app_key=my_app_key,

#certs=certs_path

)    ```

* *function. See screenshot image attached. Screenshot image is take from: [https://github.com/liampauling/betfair](https://github.com/liampauling/betfair)

*Tags: Errors Debugging, Strategies*

---

**Unknown** - *09:07:50*

Hi Mo Yes there must be some documentation online that explains these specific errors. Anyway you can still login without the certificate if you want to keep working on your code. Just use the `trading.login_interactive()`  function instead of  the `trading.loogin()`  function and comment out `certs=certs_path` in the

```betfairlightweight.APIClient( username=my_username,

password=my_password,

app_key=my_app_key,

#certs=certs_path

)    ```

 function. See screenshot image attached. Screenshot image is take from: [https://github.com/liampauling/betfair](https://github.com/liampauling/betfair)

*Tags: Errors Debugging, Strategies*

---

**Jan** - *09:51:28*

Okay thanks Mo for looking into this. Wasn't aware this is a colab issue exclusively. Will try on a local python setup later as well

*Tags: Getting Started*

---

**Jan** - *09:54:44*

[@U9JHLMZB4](@U9JHLMZB4) that issue is fixed in colab with mo's previous solution (selecting from the list), but that renders the cell empty

*Tags: Errors Debugging*

---

**Michael** - *10:05:36*

The error you shared earlier comes from a certificate error. I found it easier to create the certificates with OpenSSL rather than xca as I got these errors when I was using xca to generate certificates. I couldn't find the documentation which stated what the specific error meant, but got enough from google to understand it was certificates causing the problem.

*Tags: Errors Debugging*

---

**PeterLe** - *10:42:38*

morning folks, simple question please...for the price ticks away function in flumine utils, can you use the LTP as well as the back/lay? if so, would it be like so: new_target_price = price_ticks_away(runner.last_price_traded, 2) thanks in advance

*Tags: General Technical*

---

**Mo** - *10:52:33*

[@U9JHLMZB4](@U9JHLMZB4), I thought the problem with your one without prices was the controls weren't displayed, not the lack of prices?

*Tags: General Technical*

---

**Rach** - *11:27:38*

Mmm then I'll consider paying for the live_key soon. Can you use streaming with the delayed key?

*Tags: Deployment*

---

**Rach** - *11:29:06*

And how does streaming work in general for you? It's mandatory to implement it if you want to get tick/data right?

*Tags: General Technical*

---

**Peter** - *14:45:27*

Works for me too, at least on Colab. Local issues are likely because of the minimalistic nature of my set up rather than issues with the package. Still getting warnings about plotly when I install, which are easily solved by including listing it with betfairviz when I do the initial install, but would be useful for it to be a required package.

*Tags: Getting Started*

---

**Jan** - *15:13:32*

Error: Widgets module plotlywidget is not supported

*Tags: Errors Debugging*

---

**Michael** - *17:13:17*

Is there a minimum amount of betting activity I need to do in order to stream and collect data? Obviously I imagine there is some minimum as I doubt they will just let me scrape data, if so, how much do i need to do? All my models are crap but can't stomach paying for pro data lol

*Tags: Strategies*

---

## 2021-08-14

**Michael** - *11:31:28*

I guess then, in terms of volume how much are you guys all betting / day / week? Guess its dependent on strategy

*Tags: Strategies*

---

**liam** - *15:29:46*

Tbh I think the bar is very low, just make some bets and you will be fine, if you have problems send me a message 

*Tags: General Technical*

---

## 2021-08-15

**Peter** - *07:34:20*

[@UBS7QANF3](@UBS7QANF3) The plotly issues that I reported earlier may be due to a conflict in the version requirements for Flumine (5.0.3) and plotly (&gt;=6.2.0), which I now believe to be the source of the plotly errors that I encountered when running everything up in a fresh container.

*Tags: Errors Debugging*

---

**Steve Roach** - *23:01:07*

I’d like to start building a history of market data with Flumine. Can anyone point me some code to get me started?

*Tags: General Technical*

---

**birchy** - *23:35:53*

[https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py|https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py|https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py)

*Tags: General Technical*

---

## 2021-08-16

**Steve Roach** - *00:08:03*

Thanks for that, [@U016TGY3676](@U016TGY3676) . I was looking for something more standalone and, also, not just storing raw data but processed.



Maybe something like [https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py) but one that builds the ladder as well.

*Tags: General Technical*

---

**Peter** - *08:18:31*

Step 1 is obtaining the data, which most of us do using the marketrecorder to which [@U016TGY3676](@U016TGY3676) has linked.



Step2 is using Flumine to stream that data. It will build the market cache (including the ladders) and you can then extract the information that you think will be useful to whatever ideas you have for strategies.

*Tags: General Technical*

---

**Newbie99** - *11:23:13*

Does that help?

*Tags: General Technical*

---

**George** - *11:52:17*

i think the problem with the values they give is that they are stale.

*Tags: General Technical*

---

**Peter** - *13:17:41*

The market recorder does stream the data in order to record it. But I'd recommend streaming it again to process it into the form that works for your analysis, ad then again for your backtesting, and you'll almost certainly find yourself doing it over and over as you become more sophisticated and think of new approaches for the same markets, so it's really helpful to have a clean versio of the original data to keep going back to. Good luck.

*Tags: Data Quality*

---

**Raymond Heaton** - *19:08:47*

Thank you [@U02B9K2PFBN](@U02B9K2PFBN) for your post, its a good work around for the mean time. I got the same but with '4027' as the error.

*Tags: Errors Debugging*

---

## 2021-08-18

**D C** - *15:50:07*

Does anyone routinely check for the conflate flag set in the price stream?? If so do you consider it a problem?

*Tags: General Technical*

---

**D C** - *15:56:19*

Not sure how old my documentation is but thought I would bring it up incase anyone else is looking for "conf" instead of "con". If indeed anyone considers getting the conflate flag to be a problem at all

*Tags: General Technical*

---

**Jack** - *16:28:12*

Has anyone found a nice way to pass *.gz market files straight to the market_filter for backtesting a strategy?

Wondered if I could get around decompressing and deleting markets as I backtest them using smart_open or something similar.

*Tags: Strategies*

---

**Jack** - *16:50:42*

on further investigation this seems to work just fine:

```with mock.patch("builtins.open", smart_open.open):

    framework.add_strategy(strat)

    framework.run()```

*Tags: Strategies*

---

**D C** - *17:34:40*

Maybe someone can help me with this observation? Given my above post on the conflate flag, I now see that I am getting one response packet with the conflate flag set true at regular intervals roughly every 15 seconds. Can anyone suggest any plausible reason for this? It seems far too regular to be purely random. I still can't decide if this is a problem or not but the regularity of this message has dug into my OCD a little bit and I am struggling to find a sensible reason.

*Tags: General Technical*

---

**mandelbot** - *21:39:13*

How do I grab all orders across all strategies on a single market in a separate strategy?

*Tags: Strategies*

---

## 2021-08-19

**Peter** - *07:42:22*

When I made that request it was pointed out that I already had 1,000 markets per stream (to be fair I thought I only had 200, so that was useful information). But even that wasn't enough. So I split my streams into major and minor competitions and also filter on market types with less major competitions but more market types for them. If I want to make changes to the streaming for any reason, I also have to be careful to restart them on a Monday morning, when there are less matches in the pipeline, as it's the number of markets when you start the stream that matters.

*Tags: General Technical*

---

**liam** - *09:50:59*

Check out the blotter.py code, few helper functions in there for exactly that (on mobile so can’t link right now)

*Tags: General Technical*

---

**user34** - *10:09:10*

I have a non-Flumine BFLW implementation that checks the upcoming matches every 24 hours and then starts streams that only subscribe to the ones in competitions I'm interested in. It's possible start a separate stream for each 1000 markets so 5k markets is only 5 streams.

*Tags: General Technical*

---

**mandelbot** - *10:37:08*

I was trying to use that but was getting 0 for

```sum(

    [o.size_matched for o in market.blotter if

     o.selection_id == runner.selection_id and o.side == "BACK"])```

even though there were bets in the market, i thought maybe because I was trying to pull orders from a different strategy?

*Tags: Strategies*

---

**mandelbot** - *11:20:51*

Also the strategy was on a different thread, perhaps this is why?

*Tags: Strategies*

---

**Peter** - *18:56:09*

I achieve this by limiting both country and market types e.g.



```MAJOR = [

    "GB", # Great Britain

    'DE', # Germany

    'ES', # Spain

    'FR', # France

    'IT', # Italy

   ]



MINOR = [

    'BE', # Belgium

    'CH', # Switzerland

    'DK', # Denmark

    'FI', # Finland

    'GR', # Greece

    'IR', # Ireland

    'NL', # Netherlands

    'NO', # Norway

    'PL', # Poland

    'PT', # Portugal

    'RO', # Romania

    'RU', # Russia

    'SE', # Sweden

    'TR', # Turkey

    'AR', # Argentina

    'BR', # Brazil

    'MX', # Mexico

    'US', # United States

    'AU', # Australia

    'CN', # China

    'JP', # Japan

]



MARKET_TYPES =  ['MATCH_ODDS', 'HALF_TIME', 'CORRECT_SCORE', 'HALF_TIME_SCORE', 'BOTH_TEAMS_TO_SCORE', 'OVER_UNDER_15', 'OVER_UNDER_25', 'OVER_UNDER_35']



strategy = S3MarketRecorder(

    name="Minor Soccer Market Recorder",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["1"],

        country_codes=MINOR,

        market_types=MARKET_TYPES,

    ),

    stream_class=DataStream,

    context={

        "local_dir": "/tmp",

        "bucket": "betfair-flumine",

        "force_update": False,

        "remove_file": True,

    },

)```



*Tags: Data Quality, Strategies*

---

## 2021-08-20

**George** - *11:38:03*

This morning I tested BFLW's place_orders with no Session passed, and with a Session passed. As advised by Liam, the speedup was quite significant when passing a Session. I wonder whether there is a case for implementing this as the default behaviour in BFLW?

*Tags: Performance*

---

**Jeff Waters** - *11:48:04*

It is possible to profit based on models of what a market's price 'should' be - for example, 'based on Elo ratings, this tennis player is x% likely to beat that tennis player'? Or is it the case that there is generally a good fundamental reason when the market deviates from where the models say it 'should' be at?

*Tags: Strategies*

---

**thambie1** - *11:52:46*

[@U013K4VNB6D](@U013K4VNB6D) Yes, it's possible. If there's a good fundamental reason why the market deviates, then maybe you should incorporate that reason into your model...

*Tags: Strategies*

---

**Mo** - *12:02:29*

More fruitful to think - "OK, this market is being priced up by Elo. What does Elo miss that I can capture with my own model?"

*Tags: Strategies*

---

**Jeff Waters** - *12:13:21*

Fair point, thanks.



From the responses, it sounds like stuff that isn't represented in the stats - such as a player having a high price because he's just come back from a period of injury or sacked his coach - doesn't account for all deviations from the model prices, and there is still value to be had?

*Tags: Strategies*

---

**liam** - *15:28:44*

I do the latter (normally) and then just error handle any requests 

*Tags: Errors Debugging*

---

**Unknown** - *17:44:28*

Thought I’d been lurking here long enough and about time to contribute something. Just got multiprocessing working over backtests quite nicely and though I would share.

It should figure out how many cores you’re on and pick a good amount of markets for each process.

Implemented a counter as well so you can track progress in a memory safe way across processes (`check_market_book()` in the strategy file) :+1:

*Tags: Performance, Strategies*

---

**Jack** - *20:46:29*

Just added a fix for a memory leak

*Tags: Errors Debugging, Performance*

---

## 2021-08-23

**liam** - *08:39:09*

Hard to debug this, would have thought you would want this code in `strategy.process_closed_market` as this will be once the market has closed

*Tags: Errors Debugging, Strategies*

---

**Steve** - *09:52:45*

All good. It's just pulling the attributes of the order objects in the blotter into a dict, so pandas can convert it into a df.

*Tags: Feature Engineering*

---

**Steve** - *10:12:25*

i also run something similar under the process_orders function. It has the same problem.

*Tags: General Technical*

---

**liam** - *10:14:42*

Are you getting any latency warnings? You could be working off stale data

*Tags: Performance*

---

**Steve** - *10:16:49*

I'm in Australia so I always get latency warnings. But that isn't the issue because the df is never correct. the matched amount never shows up in the df.

*Tags: Performance*

---

**Steve** - *12:31:04*

Guess that's the fix - but it's frustrating not being able to find the underlying logic.

*Tags: Errors Debugging*

---

**Steve** - *12:33:02*

BTW. I'm definitively one of the people out there who want the buy a liam and the crew a beer. Hope you're earning some lucrative consulting gigs on the back of your work with lightweight and flumine. Thanks for your all effort.

*Tags: General Technical*

---

**liam** - *12:47:56*

replace is hacky, how are you linking flumine orders to betfair? BetId?

*Tags: General Technical*

---

**JK** - *14:57:59*

Hi all, I am writing my first flumine bot (been running bf bots using polling API for awhile but it's time to make the switch), but I can't seem to place an order. When I use the example from  [https://github.com/liampauling/flumine/blob/master/examples/example.py](https://github.com/liampauling/flumine/blob/master/examples/example.py), if i print the order object i get "Order None: None", and if i print the response from `market.place_order(order)`, i get `False`. Does anyone know what I am doing wrong? Thanks heaps

*Tags: General Technical*

---

**JK** - *15:22:29*

logs, very helpful!

*Tags: General Technical*

---

**birchy** - *15:47:38*

[@U4H19D1D2](@U4H19D1D2) does Flumine automatically handle the requests session?

*Tags: General Technical*

---

**birchy** - *15:50:29*

Oh right. I had just assumed it was automatic. What are the potential bottlenecks in Flumine? i.e. regarding market data and bet placement?

*Tags: General Technical*

---

## 2021-08-24

**Adrian** - *01:36:52*

Possibly another stupid question, but I've spent an hour searching through threads. How do I get the backtest to show me the actual profit for the runner/market? I know it's placing bets because the second last update is showing "Market cleared" "order_count": 13" and the last update showing "Market closed" "transaction_count_total": 13". But nowhere does it show the outcome of those bets :thinking_face:

*Tags: General Technical*

---

**Adrian** - *07:33:30*

Nope i didn't even consider there was an example. I copied the relevant code and it works a treat, thanks! Like i said, dumb question :laughing:

*Tags: General Technical*

---

**Adrian** - *07:54:34*

I've added

`from flumine.controls.loggingcontrols import LoggingControl`

`control = LoggingControl()`

`framework.add_logging_control(control)`

and imported the backtestloggingcontrol example, but i can't figure out how to call it (i.e. save the profit figure to disk)

*Tags: General Technical*

---

**liam** - *08:16:24*

So if you look at the code for `LoggingControl` you can see its just a base class (it doesn't do anything) you need to write your own, here is a simple example that uses one of the callbacks [https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: General Technical*

---

**liam** - *08:17:33*

For market data it will be your network or maxing out CPU, bet placement flumine tries to keep a few sessions active to reduce latency, the logic could be improved

*Tags: Performance*

---

**Adrian** - *09:14:41*

Yes thanks. I just had an idea to put

`BacktestLoggingControl = backtestloggingcontrol.BacktestLoggingControl`

`control = BacktestLoggingControl()`

`framework.add_logging_control(control)`

and it created the output.txt file i need.

there's probably a neater way to do it? but that's the step i was missing. how to call it from flumine

previously i just had control = LoggingControl()

*Tags: General Technical*

---

**Adrian** - *09:27:52*

I think where my brain disconnected is where you have in the Flumine docs  `import LoggingControl` but that doesn't do anything. only `backtestloggingcontrol.BacktestLoggingControl()` does something. I don't even need to import LoggingControl

*Tags: General Technical*

---

**Adrian** - *09:36:41*

I see... so when you say it's a base class, it's still being used by the BacktestLoggingControl? And does that mean base strategy is also the same, it gets used when certain parts of a strategy call it?

*Tags: Strategies*

---

**liam** - *09:39:33*

look up python inheritance

*Tags: General Technical*

---

**liam** - *09:50:33*

On my production code I have an API logging control which sends new markets/trades/orders to an API so that I can analyse later in the db, the control is just an abstraction for exactly this with the framework itself making the calls when required

*Tags: Deployment*

---

**Adrian** - *10:04:43*

ah yes. ok, i think this is what Mo was talking about. Rather than just saving to csv or whatever. he suggested postgresql

*Tags: General Technical*

---

**Jono** - *16:44:09*

hey everyone ive got a strategy built out in flumine that ive got to a stage where i'd like to increase the number of markets i place on to really test it out. So far ive kept it at one market for testing as this is my first flumine strat and very much needed to take the time to familiarise myself with the basics. I was wondering the following after recently attempting to place on two markets:



Does the process_orders function run independently for each market_id placed on or does it just run whenever there is an update on any executable order across all markets? I am looping through the "orders" list in this func currently and have been assuming only orders in the same market correlating to the update are being included with no check of my own. Will this be an issue if i dont include my own market_id check going forward inside process_orders?



Secondly i currently have the "max_live_trade_count" set to 1 and was assuming this would allow for 1 executable order per runner per market as suggested in the docs but im running into a problem that suggests im going over this from time to time. Am i wrong and does this variable actually apply across the whole strategy or should it function as i initially described?



Lastly, should the limits set up through "max_order_exposure= ", "max_live_trade_count=", "max_selection_exposure=" ... actually stop the placing of orders or cancel bets if their set values are exceeded?



Thanks for any help with the above its greatly appreciated! :grin:

*Tags: Deployment, Strategies*

---

**azevedo** - *19:22:08*

hey guys. testing the flumine market recorder on greyhounds. getting quite a few messages come through after more than an hour after market initially closed. seems to be mostly on FORECAST markets. I've investigated a few manually. Examples messages for one market below. If you have some editor like VS code to compare the two messages, you'll see that the only difference is the "pt" timestamp and that the latter message has "img": true attached to it. Everything else is the same. Any idea why this might be happing? there is of course the "force_update" field for this sort of thing, but seems that too many of these update are just the same initial closure. (no issue on the Flumine side, rather why Betfair might be doing this? and how are people setting the "force_update" and "market_expiration" parameters to tackle these sort of things?



```Initial closure message

{"op": "mcm", "clk": null, "pt": 1629820207534, "mc": [{"id": "1.186741004", "marketDefinition": {"bspMarket": false, "turnInPlayEnabled": false, "persistenceEnabled": false, "marketBaseRate": 5, "eventId": "30826229", "eventTypeId": "4339", "numberOfWinners": 1, "bettingType": "ODDS", "marketType": "FORECAST", "marketTime": "2021-08-24T15:48:00.000Z", "suspendTime": "2021-08-24T15:48:00.000Z", "bspReconciled": false, "complete": true, "inPlay": false, "crossMatching": false, "runnersVoidable": false, "numberOfActiveRunners": 0, "betDelay": 0, "status": "CLOSED", "settledTime": "2021-08-24T15:50:02.000Z", "runners": [{"status": "LOSER", "sortPriority": 1, "id": 38255840}, {"status": "LOSER", "sortPriority": 2, "id": 38255841}, {"status": "LOSER", "sortPriority": 3, "id": 38255842}, {"status": "LOSER", "sortPriority": 4, "id": 38255843}, {"status": "LOSER", "sortPriority": 5, "id": 38255844}, {"status": "LOSER", "sortPriority": 6, "id": 38255846}, {"status": "LOSER", "sortPriority": 7, "id": 38255847}, {"status": "LOSER", "sortPriority": 8, "id": 38255848}, {"status": "LOSER", "sortPriority": 9, "id": 38255849}, {"status": "LOSER", "sortPriority": 10, "id": 38255850}, {"status": "LOSER", "sortPriority": 11, "id": 38255852}, {"status": "LOSER", "sortPriority": 12, "id": 38255853}, {"status": "LOSER", "sortPriority": 13, "id": 38255854}, {"status": "LOSER", "sortPriority": 14, "id": 38255855}, {"status": "LOSER", "sortPriority": 15, "id": 38255856}, {"status": "LOSER", "sortPriority": 16, "id": 38255858}, {"status": "LOSER", "sortPriority": 17, "id": 38255859}, {"status": "LOSER", "sortPriority": 18, "id": 38255860}, {"status": "LOSER", "sortPriority": 19, "id": 38255861}, {"status": "LOSER", "sortPriority": 20, "id": 38255862}, {"status": "LOSER", "sortPriority": 21, "id": 38255864}, {"status": "LOSER", "sortPriority": 22, "id": 38255865}, {"status": "LOSER", "sortPriority": 23, "id": 38255866}, {"status": "WINNER", "sortPriority": 24, "id": 38255867}, {"status": "LOSER", "sortPriority": 25, "id": 38255868}, {"status": "LOSER", "sortPriority": 26, "id": 38255870}, {"status": "LOSER", "sortPriority": 27, "id": 38255871}, {"status": "LOSER", "sortPriority": 28, "id": 38255872}, {"status": "LOSER", "sortPriority": 29, "id": 38255873}, {"status": "LOSER", "sortPriority": 30, "id": 38255874}], "regulators": ["MR_INT"], "venue": "Hove", "countryCode": "GB", "discountAllowed": true, "timezone": "Europe/London", "openDate": "2021-08-24T12:57:00.000Z", "version": 3988988530, "priceLadderDefinition": {"type": "CLASSIC"}}, "_stream_id": 2001}]}



Another message after more than an hour after closure

{"op": "mcm", "clk": null, "pt": 1629825468969, "mc": [{"id": "1.186741004", "marketDefinition": {"bspMarket": false, "turnInPlayEnabled": false, "persistenceEnabled": false, "marketBaseRate": 5, "eventId": "30826229", "eventTypeId": "4339", "numberOfWinners": 1, "bettingType": "ODDS", "marketType": "FORECAST", "marketTime": "2021-08-24T15:48:00.000Z", "suspendTime": "2021-08-24T15:48:00.000Z", "bspReconciled": false, "complete": true, "inPlay": false, "crossMatching": false, "runnersVoidable": false, "numberOfActiveRunners": 0, "betDelay": 0, "status": "CLOSED", "settledTime": "2021-08-24T15:50:02.000Z", "runners": [{"status": "LOSER", "sortPriority": 1, "id": 38255840}, {"status": "LOSER", "sortPriority": 2, "id": 38255841}, {"status": "LOSER", "sortPriority": 3, "id": 38255842}, {"status": "LOSER", "sortPriority": 4, "id": 38255843}, {"status": "LOSER", "sortPriority": 5, "id": 38255844}, {"status": "LOSER", "sortPriority": 6, "id": 38255846}, {"status": "LOSER", "sortPriority": 7, "id": 38255847}, {"status": "LOSER", "sortPriority": 8, "id": 38255848}, {"status": "LOSER", "sortPriority": 9, "id": 38255849}, {"status": "LOSER", "sortPriority": 10, "id": 38255850}, {"status": "LOSER", "sortPriority": 11, "id": 38255852}, {"status": "LOSER", "sortPriority": 12, "id": 38255853}, {"status": "LOSER", "sortPriority": 13, "id": 38255854}, {"status": "LOSER", "sortPriority": 14, "id": 38255855}, {"status": "LOSER", "sortPriority": 15, "id": 38255856}, {"status": "LOSER", "sortPriority": 16, "id": 38255858}, {"status": "LOSER", "sortPriority": 17, "id": 38255859}, {"status": "LOSER", "sortPriority": 18, "id": 38255860}, {"status": "LOSER", "sortPriority": 19, "id": 38255861}, {"status": "LOSER", "sortPriority": 20, "id": 38255862}, {"status": "LOSER", "sortPriority": 21, "id": 38255864}, {"status": "LOSER", "sortPriority": 22, "id": 38255865}, {"status": "LOSER", "sortPriority": 23, "id": 38255866}, {"status": "WINNER", "sortPriority": 24, "id": 38255867}, {"status": "LOSER", "sortPriority": 25, "id": 38255868}, {"status": "LOSER", "sortPriority": 26, "id": 38255870}, {"status": "LOSER", "sortPriority": 27, "id": 38255871}, {"status": "LOSER", "sortPriority": 28, "id": 38255872}, {"status": "LOSER", "sortPriority": 29, "id": 38255873}, {"status": "LOSER", "sortPriority": 30, "id": 38255874}], "regulators": ["MR_INT"], "venue": "Hove", "countryCode": "GB", "discountAllowed": true, "timezone": "Europe/London", "openDate": "2021-08-24T12:57:00.000Z", "version": 3988988530, "priceLadderDefinition": {"type": "CLASSIC"}}, "img": true, "_stream_id": 2001}]}```

*Tags: Data Quality, Strategies*

---

## 2021-08-25

**liam** - *09:30:37*

Join github and I can help

*Tags: General Technical*

---

**liam** - *10:19:05*

`process_orders` is per market but note that if you have live orders in the market this will be called every 0.25s and every 5s regardless, logic [https://github.com/liampauling/flumine/blob/74114ed6c20e90289e5a0d2fac46d0969d46a158/flumine/streams/orderstream.py#L62|here](https://github.com/liampauling/flumine/blob/74114ed6c20e90289e5a0d2fac46d0969d46a158/flumine/streams/orderstream.py#L62|here). So you can assume each event will be a single market but you will need to handle which market if you have specific rules etc.



`RunnerContext` is per market per selection, need more info on any problems regarding this?



Yes the controls will prevent you placing any orders over exposure/count with a violation message.

*Tags: Deployment*

---

**Van** - *12:48:41*

could you try changing the URL in betfairlightweight/streaming/betfairstream.py ?

*Tags: General Technical*

---

**Pedro Lima Monteiro** - *15:04:35*

Hi again.

So I managed to get the stream up and running with the addition of my proxy's IP.

However, after 30 minutes or so of being up, I got the following error:

`betfairlightweight.exceptions.SocketError: [Connect: 2]: Socket The read operation timed out`

After which the client was broken and I had to restart it.

Any ideas on how to avoid this problem? Or should I just accept it and try to restart it automatically?

Thanks.

*Tags: Errors Debugging*

---

**liam** - *15:12:14*

any latency warnings before this? Are you using the error handling example? where are you located?

*Tags: Errors Debugging, Performance*

---

**Pedro Lima Monteiro** - *15:20:44*

Was not using such example. Trying it as we speak.

Was using the plain streaming example one can find on the website.

I will post further feedback if I get the same/other errors.

Thanks a lot.

*Tags: Errors Debugging*

---

**Peter** - *17:32:39*

Compared my stream to yours for this market. My last message was identical to your initial closure message with the exception of the __stream__id, which I believe is inserted by BFLW and isn't anything to do with Betfair and the timestamp (mine with 2 milliseconds later than yours. I didn't get a 2nd closure message.



This appears to support [@U4H19D1D2](@U4H19D1D2)'s hypothesis that it's a connection issue causing the whole market image to be resent.

*Tags: Errors Debugging*

---

**Peter** - *17:34:54*

As to how to deal with it, given that none of the data has changed, I'd simply not worry about it. I do have scripts that process the closed data, but they wouldn't care which of the closure messages the processed or whether the 2nd overwrote the first since the data is the same. Does this give you a specific problem?

*Tags: General Technical*

---

**azevedo** - *18:47:55*

Thank you very much [@U9JHLMZB4](@U9JHLMZB4) [@U4H19D1D2](@U4H19D1D2)! If I get a connection issue would that appear in the logs? (I don't see any warning, errors, or any "connection" containing messages)



I basically get the following patterns with Flumine re-adding the removed market later and then just having this one line with img true in the end.

```# Initial Closure

{"asctime": "2021-08-25 15:00:59,330", "levelname": "INFO", "message": "[MarketStream: 2001] 1.186826857 removed, 202 markets in cache"}

{"asctime": "2021-08-25 15:00:59,331", "levelname": "INFO", "message": "Market 1.186826857 closed", "market_id": "1.186826857", "event_id": "30831054", "event_type_id": "4339", "event_name": "Monmore (F/C) 25th Aug", "market_type": "FORECAST", "market_start_datetime": "2021-08-25 14:58:00", "country_code": "GB", "venue": "Monmore", "race_type": null, "orders_cleared": false, "market_cleared": false, "closed": true}

{"asctime": "2021-08-25 15:00:59,331", "levelname": "INFO", "message": "Closing market 1.186826857"}

... a few more messages saying successfully loaded to s3, then market_cleared etc.



# Then an hour later removing the market (I have left default 3600s market expiration setting)

{"asctime": "2021-08-25 16:05:40,133", "levelname": "INFO", "message": "Removing market 1.186826857", "client": ...}

{"asctime": "2021-08-25 16:05:40,133", "levelname": "INFO", "message": "Market removed", "market_id": "1.186826857"}

{"asctime": "2021-08-25 16:05:43,538", "levelname": "INFO", "message": "Removing: /home/ubuntu/betfair_market_recorder/a182664f/1.186826857.gz, age: 3884.06s"}

{"asctime": "2021-08-25 16:05:43,538", "levelname": "INFO", "message": "Removing: /home/ubuntu/betfair_market_recorder/a182664f/1.186826857, age: 3884.21s"}



# Then shortly after in 12 mins, it's adding the market again

{"asctime": "2021-08-25 16:17:49,782", "levelname": "INFO", "message": "Adding: 1.186826857 to markets"}

{"asctime": "2021-08-25 16:17:49,785", "levelname": "INFO", "message": "Market 1.186826857 closed", "market_id": "1.186826857", "event_id": null, "event_type_id": null, "event_name": null, "market_type": null, "market_start_datetime": "1970-01-01 00:00:00", "country_code": null, "venue": null, "race_type": null, "orders_cleared": false, "market_cleared": false, "closed": true}

{"asctime": "2021-08-25 16:17:49,785", "levelname": "INFO", "message": "Closing market 1.186826857"}

... few more lines saying market cleared, then removed etc.```

*Tags: Errors Debugging*

---

**azevedo** - *18:53:28*

As with regards to if it's giving a specific problem, it isn't really. I was just curious about these warnings I'm getting and if that could somehow have any data quality implications down the line.

*Tags: Data Quality*

---

## 2021-08-26

**Peter** - *07:53:31*

Yes [@U0122TPQABW](@U0122TPQABW) 2 milliseconds difference on the pt is exactly what I mean and I was surprised at that too, but probably shouldn't have been. The Betfair API allows quite a lot of control over the filters (whether we take advantage of that or not), so it makes sense that we have streams unique to the recipient, and that updates can't be collated and dispatched simultaneously.



I'm like you I like to track down each error message or oddity to understand what's going on in case it's a pointer to an underlying problem that I can solve. However, we also need a certain tolerance for data integrity issues, because we see a lot of them and I'd see this as being at the benign end of the spectrum.



Picking it apart though, markets are added to the stream by Betfair and picked up by Flumine when that happens. In the cases you're citing it looks like you're simply detecting actions taken at the Betfair so beyond your control. However they're happening after you've uploaded the recorded data to S3, so that file will not contain the extra img.

*Tags: Errors Debugging*

---

**Jono** - *08:13:41*

whats the best way to use an existing active login session after a crash or error has caused the most recent code run to fail or timeout? Ive seen you can pass the trading object a requests.Session that should allow for this but im not certain how i can pass this after an error that caused the code to halt and have to start from a blank slate. Is there a straightforward way of using the trading object from betfairlightweight to start things off again utilising an active session from the least information say login details, app_key and the session_token of the last successful login?

*Tags: Errors Debugging, Strategies*

---

**liam** - *08:28:21*

catch the errors? use flumine? the sessionToken is stored in the trading object (nothing to do with requests) so you would need to persist to disk or similar

*Tags: Errors Debugging, Strategies*

---

**Jono** - *08:33:34*

[@UBS7QANF3](@UBS7QANF3) betfair are getting on at me for logging in too many times and i dont blame them i need to go in and change the stuff i wrote back when i started and had no idea bflw existed. we are talking a session per request sort of business, nothing im too proud of. [@U4H19D1D2](@U4H19D1D2) so if i have the session token to hand would that be enough in a completely new environment to do something like:



trading = APIClient([mailto:abc@mail.com|abc@mail.com](mailto:abc@mail.com|abc@mail.com), password, app_key=APP_KEY, session_token=SESSION_TOKEN)

*Tags: Strategies*

---

**Jono** - *08:48:12*

you underestimate my past self's ability to write poor code. I hope so anyway, more just a pain in the arse to go back and have to dig through it all but im sure putting in the effort at this stage and properly swapping everything over to bflw will make my life much easier. So just to clarify would carrying out the following actually solve my problem of not having to log in again?



```trading = APIClient([mailto:abc@mail.com|abc@mail.com](mailto:abc@mail.com|abc@mail.com), password, app_key=APP_KEY)

trading.set_session_token(SESSION_TOKEN)```

Just did some quick tests there and it looks as if i dont have to perform either login() or login_interactive() afterwards so it looks as if it just might be doing what i wanted it to

*Tags: Strategies*

---

**Jono** - *08:56:03*

Im assuming that is only a problem after 8 hours and i havent ran keep_alive() at any point - as long as i stay on top of running keep_alive() will this be enough to get around any issues this might cause?

*Tags: Deployment*

---

**liam** - *08:58:58*

have a look at the [https://github.com/liampauling/betfair/blob/0dbd551272d66ea1e4efe8f8c695b54099567265/betfairlightweight/baseclient.py#L96|code](https://github.com/liampauling/betfair/blob/0dbd551272d66ea1e4efe8f8c695b54099567265/betfairlightweight/baseclient.py#L96|code) every time you set the sessionToken you reset the login time, yes making sure you keep_alive will prevent errors but just don't use that property if you are sharing tokens around trading objects

*Tags: Errors Debugging, Deployment, Strategies*

---

**George** - *09:55:53*

Morning guys, I am starting to use BFLW streaming to get market data and I am wondering about how connections are used / managed. I have heard somewhere that BF allows you to use up to 10 connections but I don't really know how to make the best use of that. I am assuming BFLW default behaviour is just to use one connection?

*Tags: General Technical*

---

**S G** - *10:18:09*

Just a follow up question on wide filter for a stream. Do this hit 200 market limit at all?

*Tags: General Technical*

---

**Pedro Lima Monteiro** - *16:54:24*

Hi again.

Taking advantage of you all being so kind here I am with yet another doubt.

I managed to successfully connect to the betfair streaming API using the streaming error handling example and I am currently running this example, with some small changes, without any problems.

The thing I am struggling with now is that I am processing the incoming data and storing it in a NoSQL document database similar to MongoDB (not using any type of async python code while doing so) and the timestamps I am getting from the incoming data are getting more and more behind in time the longer I keep the streaming running. Reaching the point where, after 15 minutes running, I was receiving messages with an 8 minute delay.

That being said, is there any way to avoid this situation? Maybe using async python code? As a not so experienced python programmer, every bit of help is appreciated.

Thanks in advance!

*Tags: Errors Debugging*

---

**liam** - *17:00:32*

Assuming you are using the queue to read the data it is likely you are exhausting your CPU, have you monitored to isolate the bottleneck? Easy alternative would be to record streaming data to a txt file (or redis) and then use a separate process to send to mongoDB

*Tags: General Technical*

---

**liam** - *17:05:57*

Assuming the delay isn’t a problem just create a thread pool to process the data / send to mongo although I would question why you just don’t record the data to a text file 

*Tags: General Technical*

---

**Pedro Lima Monteiro** - *17:07:36*

I am not against writing to a file that I am after capable of reading.

I just want to have the data stored in the database as fast as possible and was wondering if anyone had experienced the same issue and looking for a possible solution.

*Tags: General Technical*

---

## 2021-08-27

**JK** - *02:56:56*

does anyone have an opinion on the max amount of time you want to be processing the market_book using flumine before it starts to degrade performance?

*Tags: Performance*

---

**JK** - *07:32:20*

thanks [@UPMUFSGCR](@UPMUFSGCR) thats very helpful. Will try make it quicker than that.



[@U9JHLMZB4](@U9JHLMZB4) how do you choose to receive updates less frequently?

*Tags: General Technical*

---

**JK** - *09:28:10*

is the best way to find current total exposures on a selection using flumine to use `blotter.get_exposures()`? and if so, what does `matched_profit_if_win` and `matched_profit_if_lose` mean? e.g. if I have a matched back $2 stake @ $10, and a matched lay $1 stake at $10, would `matched_profit_if_win` return $18 profit, or $9 profit? and what would `matched_profit_if_lose` return? thanks in advance

*Tags: General Technical*

---

**liam** - *09:54:20*

Have a read of the [https://github.com/liampauling/flumine/blob/6f4782901eb7abdf1a68786d575be5dd111d79b8/flumine/markets/blotter.py#L103|code](https://github.com/liampauling/flumine/blob/6f4782901eb7abdf1a68786d575be5dd111d79b8/flumine/markets/blotter.py#L103|code)

*Tags: General Technical*

---

**Steve Roach** - *13:15:17*

Thanks for that, Mo. I'll see what I can do with that.



In the meantime, do you know if there is a Flumine example for the same result?

*Tags: General Technical*

---

**Steve Roach** - *13:42:42*

Well, I have a load of streaming files and I would like to run them through Flumine and convert them into a ladder. I'm going to store this in a database for model development.



Of course, it doesn't HAVE to be Flumine, but I would like to use that as my main platform at the moment so as to learn my way around it.

*Tags: Strategies*

---

**Mo** - *14:11:20*

It doesn't make sense to use Flumine to convert them

*Tags: General Technical*

---

## 2021-08-28

**Mo** - *07:36:55*

It's the wrong tool for the job. It takes in either the live stream or historic data files and runs a strategy or strategies against that. It's for live trading or backtesting. It's not for data transformation or analysis

*Tags: Deployment, Strategies*

---

**Jono** - *12:23:40*

whats the best way to delay a strategy from starting in order to ensure the market blotter is filled, add_market_middleware have run, and allow workers to first execute? As far as im aware the first two should run prior to the strategy logic starting up but ceratinly with background workers there a few i would to have definietly run once before starting. Delaying the process_market_book and process_orders would also be a perfect solution with something like



If seconds_since_strat_start &lt; 10:

       return

else:

      ....



 but im struggling to get a global timer variable to work. Any help greatly appreciated thanks

*Tags: Strategies*

---

**Lee** - *12:26:37*

yep and just make a strategy dependant on it existing/being set to the correct value

*Tags: Strategies*

---

**Jono** - *16:31:26*

how often does the betfair ui refresh? im placing using flumine for the first time and i just got wondering since not all the bets placed and alterations can be observed in the ui due to the speed - at least thats what it seems like so far

*Tags: Getting Started, Performance*

---

**Dave** - *17:41:17*

Pretty sure you can access the bet delay from the market object in flumine I think (or marketbook object, can't remember)

*Tags: General Technical*

---

## 2021-08-30

**Steve** - *12:07:29*

Just a 3 quick questions to clarify how the flumine queue works. Firstly, it seems that both the order stream and market stream fill the same handler_queue - so if I get an order update arrive just before a market update the order update will need to be processed before the process_market_book function starts processing the market stream update? Is that right? Or can orders and a marketbook be processed asynchronously? Secondly, is the best way to check if a backlog of updates is building up in the queue just to check with handler_queue.qsize() - or do I just have to rely on looking for a True conflation flag in streamingupdates? Finally, if a market_definition update is received. When will it be reflect in the attributes of the Market object in flumine - when that information is first received - or when that marketbook gets to the front of the handler_queue and is processed?

*Tags: General Technical*

---

**Mons___das** - *15:23:13*

A broad customer base is fundamental for any exchange. If a certain threshold is surpassed, the exchange would collapse into a dry desert pretty quickly. I think the problem is that betfair has little to no motive to push the exchange and the marketing campaigns from bookmakers are way bigger. Also some of main the advantages just can not be advertised by betfair itself. Get better odds? Kinda implies bad odds on the sportsbook, but sure this one goes. Dont actually get banned when winning? Well, this is a big advantage of the exchange, but no way betfair would say that :smile: I think its partially up to the players to showcase the exchanges advantages. As much as ppl like Caan (at least before his vids got a slightly negative touch towards betfair) and that guy selling courses and posing with his big winnings are doubted here, those guys are doing the big players a big favor with how many ppl they attract

*Tags: General Technical*

---

**Steve Roach** - *22:01:04*

Ok. Out of curiosity, say your strategy relied on knowing the state of the ladder in real time. How would you maintain that data? I’m thinking , each time a streaming record arrived, a process added it to the end of the ladder. Looks like flumine would struggle with this. Is there something else?

*Tags: Strategies*

---

## 2021-08-31

**Steve Roach** - *01:00:15*

Like this from Bet Angel. Not sure it would be practical to feed this in its entirety into a strategy every second or so, but certainly an extract of features from something like this.

*Tags: Feature Engineering, Strategies*

---

**Steve Roach** - *07:00:43*

I'm trying to find my way around the landscape here and some of the terminology is a bit unfamiliar. Please excuse me for asking a lot of dim questions.



The function you gave me (prices_file_to_data_frame) works very well; it takes a historic file and outputs a ladder to a flat file. It needs a bit of processing but it's close enough to do the job, so thanks for that. The next hurdle for me is to build the same ladder (which you call a market book) in real time and use that against any strategy I may have come up with. So, how do I get this market book? You have indicated that Flumine is not the thing for very good reasons, so where do I go for the market book? I have seen some bflw programs that mention it but the documentation is fairly sparse. Are you able to point me to an example program that converts the streaming data in real time in this way?

*Tags: Strategies*

---

**Mo** - *07:08:03*

You seem to be missing something here. The market book is the fundamental collection of data that any strategy works off so of course flumine gives you access to that. I'm not a flumine user but a quick look at the examples shows that to write a strategy you overload functions like `check_market_book` and `process_market_book`. Everything revolves around it. The only reason you might want to convert it to a data frame is if you want to perform operations on it that are easier to do in pandas or more generally with the data in a tabular format

*Tags: Feature Engineering, Strategies*

---

**liam** - *08:07:38*

1. Yes that is correct, FIFO queue

2. You can do that, note there is a warning [https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/baseflumine.py#L128|log](https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/baseflumine.py#L128|log) if the MarketBook is delayed by over 2s from publishTime. Conflation would be a separate issue where your connection (or CPU) is limited and unable to read off the socket quick enough, this would not be impacted or caused by the handler_queue size

3. Have a look at the [https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/baseflumine.py#L149|code](https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/baseflumine.py#L149|code), its called when processed by the queue and before strategies etc are called

*Tags: General Technical*

---

**Steve Roach** - *12:10:15*

Yeah, converting to a dataframe should be quite easy once I've got the basics in place - my problem is understanding the basics. I think my best be is to just plough through the github/Flumine documentation and see what I can understand. After that, I need to get dirty with the code, and probably ask a lot of dump questions on slack.



But, thanks for you advice here - it helps a lot.

*Tags: Feature Engineering*

---

## 2021-09-01

**Adrian** - *06:58:25*

Thanks Peter. It's quite interesting how that works. I don't fully understand it but that makes sense not to pass them in in the first place. My problem is I started the market recorder a couple months back without sorting the market types into separate folders, so I'll have to create some kind of script that creates a list of the needed markets. Or that sorts them into correct folders them

*Tags: Data Quality*

---

**liam** - *07:49:08*

As the [https://liampauling.github.io/flumine/quickstart/#event-processing|docs](https://liampauling.github.io/flumine/quickstart/#event-processing|docs) describe you can filter by marketType



```strategy = ExampleStrategy(

    market_filter={"markets": [..], "market_type": ["MATCH_ODDS"]}

)```

This is done before the listener when adding the market, [https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/streams/streams.py#L43|here](https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/streams/streams.py#L43|here).

*Tags: Strategies*

---

**liam** - *07:50:52*

Although Adrian its a 5 second job to read the first line of a streaming file to work out the marketType from the definition ([https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/streams/streams.py#L42|this](https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/streams/streams.py#L42|this) is how flumine does it anyway)

*Tags: General Technical*

---

**Michael** - *23:24:57*

Quick question on displaying data, in flumine for backtesting you use the market filter and pass in your files. How can I display that data say in a dataframe, given its in .bz2 format, can I use flumine or some other library?

*Tags: Feature Engineering*

---

## 2021-09-02

**JK** - *03:40:35*

What would be the best way to read in my prices from a CSV every x minutes using flumine? any suggestions where to put it? thanks in advance

*Tags: General Technical*

---

**liam** - *09:29:40*

trades have a [https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/order/trade.py#L48|date_time_created](https://github.com/liampauling/flumine/blob/f0e7e6542942d00685ceb6d72951456684998739/flumine/order/trade.py#L48|date_time_created) you can use

*Tags: General Technical*

---

**Ke** - *11:29:03*

To be precise, i'm looking for the matched price and matched size in last 5 seconds. As we are streaming data from betfair, we must have a timestamp for each matched order. I'd like to filter out the matched order based on the timestamp to calculate the moving average of the matched price.

*Tags: Feature Engineering*

---

**liam** - *12:21:03*

Easy to do what you are describing but you just need to do it yourself inside a strategy

*Tags: Strategies*

---

**liam** - *14:45:32*

Yeah you can either create some middleware and store in the context, this is handy if you have a few strategies that require the data or just do it in the strategy, worker wouldn't be suitable

*Tags: Strategies*

---

**Ke** - *20:36:55*

Can i make worker to be activated by event, say market close or open, rather than a fixed time interval?

*Tags: Errors Debugging*

---

**liam** - *20:39:26*

Not currently, it’s assumed anything like that is strategy specific rather than a worker, you could probably hack it with a queue triggered with events from a strategy though 

*Tags: Strategies*

---

## 2021-09-03

**Pedro Lima Monteiro** - *13:35:57*

Hi. I am facing a problem that is not specific of this library but I am looking for any help since I ran out of ideas as to why this is happening. My connection to betfair exchange stream api is reset by betfair every 30 minutes at approximately minute 2 and minute 32 of every hour. Any idea as to why this is happening? Any kind of help is appreciated. Thank you.

*Tags: General Technical*

---

## 2021-09-04

**liam** - *10:15:48*

Yep, currently no public property for you to store info, ideally we would add `RunnerContext.context` for this in flumine itself

*Tags: General Technical*

---

**Jonjonjon** - *21:08:49*

When starting a flumine session, what's the recommended way to identify whether or not the streams have connected or not yet?

*Tags: General Technical*

---

**Jonjonjon** - *21:29:53*

What if I want to do it from within Flumine in realtime, rather than after the event?

*Tags: General Technical*

---

**Lee** - *21:37:35*

You could probably do something like flumine.streams but why would you want todo that?

*Tags: General Technical*

---

**Jonjonjon** - *22:54:53*

I use non-standard deployment methodology, as I'm only interested in stuff a few minutes before the race starts.

*Tags: Deployment*

---

**Jonjonjon** - *22:57:49*

So I have a cronjob that runs every 10 minutes. It runs a Python script that watches markets that are starting in the next 10 minutes. When those markets close, the Python script is suppose to close too.



It closes them when it detects that flumine has zero open markets. i.e.



```if len(framework.markets.open_market_ids) == 0:

    [http://main_logger.info|main_logger.info](http://main_logger.info|main_logger.info)('No more open markets, terminating')

    framework.handler_queue.put(TerminationEvent(framework))```



*Tags: General Technical*

---

**Lee** - *23:09:54*

Streaming is lightweight

*Tags: General Technical*

---

## 2021-09-05

**Mo** - *08:45:04*

For previously discussed reasons, even if you get a connection limit per subaccount, if you end up using that many connections they will come down hard on you



It sounds like all of your problems go away if you use a coarse market filter and do the filtering locally as is best practice

*Tags: Multi Client*

---

**Jonjonjon** - *10:54:35*

What do people do if they run over 10 discrete strategies?



Do they have Python scripts that run more than one strategy?



Or do they use some sort of local server, with course filter, and sends the data to individual strategies running on other Python scripts?

*Tags: Deployment, Strategies*

---

**Oliver Varney** - *11:03:25*

For me at least strategies are associated with a sub account, and config is loaded from a database on start up, so I typically run a single process for an account which just loads in multiple strategies. Not sure if others do similar, I guess it may depend on where / how you code your logic

*Tags: General Technical*

---

**Mo** - *11:07:21*

I'd be shocked if someone is trading &gt;10 strategies where it makes sense to have a connection per strategy. 10 horse racing strategies can be easily served by one connection

*Tags: Strategies*

---

**Oliver Varney** - *11:07:41*

at a basic level with flumine isnt it as simple as flumine.add_strategy or something? so can you not just add some logic here to batch up strategies if you only have one account currently?

*Tags: Strategies*

---

**Newbie99** - *11:11:46*

I'm on a much more basic level than all of you I'd imagine and thats exactly what I do.



I have a json file which has various parameters, then on startup I loop through and add each strategy (this is using Flumine).

*Tags: Strategies*

---

**Jonjonjon** - *11:55:28*

[@ULDAVFDRP](@ULDAVFDRP) yes using add_strategy would make it easy to batch up a load of strategies into one script. However, if updating code for a strategy, I wouldn't want to disrupt the operation of other strategies.



My strategies trade in and out of positions quite heavily, so if something is interrupted while doing that it could leave me unhedged.



I like [@UBS7QANF3](@UBS7QANF3)'s idea. I'm wondering how easy it would be to make a flumine session point at a centralised component rather than Betfair.

*Tags: Strategies*

---

**Jonjonjon** - *12:17:45*

[@U4H19D1D2](@U4H19D1D2) do you have any thoughts on the cleanest way to do this with Flumine?

*Tags: General Technical*

---

**liam** - *12:20:36*

However I wouldn’t recommend the complication unless you are trying to solve something, I wouldn’t consider the connection limit a problem to solve 

*Tags: General Technical*

---

**Mo** - *12:26:47*

I have to say even though I do things this way I agree with [@ULDAVFDRP](@ULDAVFDRP) and [@U4H19D1D2](@U4H19D1D2) - it sounds like the problems you are currently experiencing have much simpler solutions for now

*Tags: General Technical*

---

**Dave** - *13:04:14*

This is the kinda thing we debated a while ago...I decided I liked the idea of having one centralised consumer for market data which was broadcasted internally to whatever processes wanted to subscribe to whatever markets. If you didn't want to go the redis route you could go down zmq instead which has less setup overhead (generally found it simpler). Didn't integrate it into flumine though.

*Tags: Getting Started*

---

**Dave** - *13:09:29*

(one advantage is that you can have a process per strategy rather than one process for N strategies and be hindered by GIL)

*Tags: Strategies*

---

**Mo** - *14:20:48*

Good to know, I’m still thinking of replacing Redis with ZMQ in certain parts of my stack for latency reasons

*Tags: Performance*

---

**Mo** - *15:25:02*

Global interperer lock. It means Python is inherently single threaded

*Tags: General Technical*

---

**Jeff Waters** - *15:52:47*

I've just purchased some historical data from Betfair containing thousands of JSON files. What I've done previously is to manually extract some of the files to a folder called 'Data', and then go:



*from os import listdir*

*from os.path import isfile, join*



*markets = [f for f in listdir('Data') if isfile(join('Data', f))]*

*for i in range(len(markets)):*

    *markets[i] = "Data/" + markets[i]*



*strategy = TestStrategy(*

    *market_filter={"markets": markets},*

*)*

*framework.add_strategy(strategy)*

*framework.run()*



However, that approach isn't viable when there are thousands of files in different folders in a WINRAR file. I've been told I can use match.patch:



*with mock.patch("builtins.open", smart_open.open):*

    *framework.add_strategy(strat)*

    *framework.run()*



However, where do I put the name and path of the tar file? Alternatively, is there a program that extracts all the JSON files to a single folder?



Thanks



Jeff

*Tags: Data Quality, Strategies*

---

**D C** - *16:06:22*

Yes that's what I am getting at. It has occurred to me that I need to shift from this single thread event loop model and rework things to a multi-process approach with a shared image of the market - rather like your redis type architecture

*Tags: Strategies*

---

**Unknown** - *16:39:22*

I've now gone with:



```strategy = TestStrategy(

    market_filter={},

    max_order_exposure=1000,

    max_selection_exposure=105,

)



with mock.patch("builtins.open", smart_open.open):

    framework.add_strategy(strategy)

    framework.run()```

I've also unzipped the tar file to the same folder that the code resides in.



However, none of the data files are being read. One of the messages I get is:



{"asctime": "2021-09-05 15:35:38,776", "levelname": "WARNING", "message": "No markets or events found for strategy TestStrategy"}



What am I doing wrong?

*Tags: Strategies*

---

**Jeff Waters** - *17:50:51*

Thanks Jack



I know how to access a particular file, viz something like:



markets = ["tests/resources/PRO-1.170258213"] (from [https://github.com/liampauling/flumine/blob/master/examples/backtest.py](https://github.com/liampauling/flumine/blob/master/examples/backtest.py))



What I'm not clear is how to get my program to go through the entire directory tree and extract every bz2 file. Would I need to do a traversal of the directories, using something like this? [https://www.pythoncentral.io/how-to-traverse-a-directory-tree-in-python-guide-to-os-walk/](https://www.pythoncentral.io/how-to-traverse-a-directory-tree-in-python-guide-to-os-walk/)

*Tags: General Technical*

---

**Jeff Waters** - *22:35:15*

I've tried to add filters to only include British races and win races (based on filters used in the the example code at [https://github.com/liampauling/flumine](https://github.com/liampauling/flumine)):



```strategy = TestStrategy(

    market_filter={"markets": marketsToProcess},

    country_codes=["GB"],

    market_types=["WIN"],

    max_order_exposure=1000,

    max_selection_exposure=105,

)```

However, I got an error message:



*TypeError: __init__() got an unexpected keyword argument 'country_codes'*



Presumably, that was because I wasn't using market_filter=streaming_market_filter, as in the example (as I was using historical data rather than live data).



Is there any way of filtering out particular types of races when using historical data, other than putting something like

```if market.country_code != "GB":

    continue```

in the process_market_book method?



Thanks



Jeff

*Tags: Data Quality, Errors Debugging, Deployment, Strategies*

---

## 2021-09-06

**mandelbot** - *05:40:48*

You have to create a market_filter

```marketsToProcess = betfairlightweight.filters.streaming_market_filter(

           event_type_ids=["4339"],

           country_codes=["GB"],

           market_types=["WIN"]

)```

and then use that in your strategy such as



```strategy = TestStrategy(

    market_filter=marketsToProcess,

    max_order_exposure=1000,

    max_selection_exposure=105,

)```

*Tags: Strategies*

---

**Jono** - *07:53:43*

Hey everyone, before using flumine in an older strategy i used to send a pandas dataframe of orders placed along with the ladder information at the time the bet was placed to a database api for cataloging. I would quite like to do this again specifically in a given flumine strategy iteration when a market order is placed make a note of the runner info (available to back/lay etc) for that given market_book and then send it back to my database api every 30 seconds or so. I was wondering the best way to go about this without using messy global variables. After reading up in the flumine docs my current plan is to store a pandas df in the market.context and access it in a background worker, then loop through "for market in flumine.markets:" appending each of the dfs to get all the orders and runner information in one large frame before sending it off to the db. Not had much experience with utilising the market[context] to store information between iterations so wondering if this sound like a plausible way of using it or is there a better way of going about storing odds at time of placing before sending this info off to an external api? Thanks

*Tags: Feature Engineering, Strategies*

---

**Mo** - *08:03:16*

I'm not a flumine user but I think a generic approach to this would be to have a queue that you push requests for database insertion on to and then a worker that pulls off the queue and does the actual insertion

*Tags: General Technical*

---

**Mo** - *08:04:08*

However really that worker needs to be a separate process because you don't want to hold things up while you make an expensive database insertion

*Tags: General Technical*

---

**Jono** - *08:10:39*

if the worker is only performing the post request to the db every 30 seconds or so surely if the process only takes 4-5 seconds it shouldn't cause strategy run time issues? Also to clarify the backin up concern that time window 4-5 second window is time spent waiting for a response rather than actually inserting the order information so nothing intensive is actually taking place in the worker, just something with a fair amount of blocking.

*Tags: Strategies*

---

**Mo** - *08:11:38*

If it's blocking I/O such that the thread can yield then it's OK. But if you block the entire application for 4 - 5 seconds you will have major problems

*Tags: General Technical*

---

**Jono** - *08:18:53*

yes ive seen previously on here that you have to mindful of requests as to not interfere with the actual flumine event loop. Assuming the blocking I/O request works as intended the last thing to clarify before pushing forward with this idea is: Is a pandas df an appropriate object to set up add rows to inside market.context eg market.context['bet_df'] ? I know you arent a flumine user so may not be able to give me a definite answer on this one so its no problem if i have to go trial this one myself

*Tags: Feature Engineering*

---

**Oliver Varney** - *09:31:05*

sending stuff to a database for cataloging can be done post market using flumine logging control (I think its a on market close event), unless you really really need it somewhere in a relatively decent time frame.

*Tags: General Technical*

---

**Peter** - *11:14:53*

Doesn't need to wait until market close, which carries the risk of losing order data if something interrupts the stream or the market closure doesn't come through for any other reason. I have a database logging control that inserts orders during _process_order() and updates them during _process_cleared_orders_meta()

*Tags: General Technical*

---

**PeterLe** - *13:35:06*

Thanks, but Ive had some great help from the folk on here

*Tags: General Technical*

---

**Jonjonjon** - *13:50:37*

[@U013K4VNB6D](@U013K4VNB6D) The time will depend a lot on how much processing your own code does. And how many orders it places.



If you place lots of limit orders, flumine will check them for fills on every market book update. That takes time.



You'll get up to 20 market book updates per second. So if you do time-consuming work in process_market_book, that will increase the time.



It also depends on your CPU and size of data. From memory, on a single-thread, and with a very simple strategy that does virtually nothing, I think my Ryzen 3950x CPU, with a Samsung EVO 970 PRO NVME drive, can process a single GB horse racing WIN file in under 3 seconds. It might even be under 1 second. You can do a crude comparison of your CPU, vs the 3950X, just by comparing the single thread ratings here: [https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+9+3950X&amp;id=3598](https://www.cpubenchmark.net/cpu.php?cpu=AMD+Ryzen+9+3950X&amp;id=3598)

*Tags: Performance, Strategies*

---

**Jonjonjon** - *13:51:19*

If you have plenty of cores and memory, you can also use Python's `multiprocessing` module to run backtests in parallel.

*Tags: Performance*

---

**Jeff Waters** - *13:55:17*

Thanks Jon



My code isn't doing a huge amount of bet placing. At the moment, I'm just trying to get something that works, so all I'm doing is offering a bet to the market 30 seconds before the off.



I wonder whether the problem is with the way I'm retrieving the data. I had previously accessed JSON files that I'd manually accessed directly, and my program ran quickly. I'm now using the following code to get data from an extracted WINRAR file:



```def get_markets(data_dir):

    markets = []

    for dir_, _, files in os.walk(data_dir):

        for file_name in files:

            rel_dir = os.path.relpath(dir_, data_dir)

            rel_file = os.path.join(rel_dir, file_name)

            markets.append("PRO\\" + rel_file)

    return markets





strategy = TestStrategy(

    market_filter={"markets": get_markets("PRO")},

    max_order_exposure=1000,

    max_selection_exposure=105,

)



with mock.patch("builtins.open", smart_open.open):

    framework.add_strategy(strategy)

    framework.run()```

Is there anything I'm doing wrong in that code that jumps out at you?

*Tags: Strategies*

---

**Jonjonjon** - *13:58:45*

Nothing jumps out.



But... How many markets are in your 1.7GB of data?



Given you have no idea what's happening, for future work I'd do some logging.



e.g. in your test strategy, just before (or after) placing orders, make it log a message saying it's placing a bet for market XXX. That way, you can see its progress.

*Tags: Strategies*

---

**Peter** - *15:49:18*

I use Colabs a lot for analysis. Love it. But backtesting isn't something that I would do there. There are a number of reasons: it's designed to be run in a browser, which just isn't how I run my backtests at volume, resources are limited, not guaranteed and prioritised for users using the service interactively, and you've got to get all that data up to Google somehow before you can start analysing it.



So my volume backtesting is done on a budget VPS with 12 processors, 48GB of memory and taking advantage of python multiprocessing to ensure that those resources are fully-utilised.

*Tags: Performance*

---

**Jeff Waters** - *16:07:12*

Thanks Mo.



My code for getting the data and running the tests is as follows:



```def get_markets(data_dir):

    markets = []

    for dir_, _, files in os.walk(data_dir):

        for file_name in files:

            rel_dir = os.path.relpath(dir_, data_dir)

            rel_file = os.path.join(rel_dir, file_name)

            markets.append("PRO\\" + rel_file)

    return markets





strategy = TestStrategy(

    market_filter={"markets": get_markets("PRO")},

    max_order_exposure=1000,

    max_selection_exposure=105,

)



with mock.patch("builtins.open", smart_open.open):

    framework.add_strategy(strategy)

    framework.run()```

Is there a quick and easy way for me to get that code to use multiprocessing, please?

*Tags: Strategies*

---

**Unknown** - *16:24:53*

I found this with the search: [https://betfairlightweight.slack.com/files/U01SCPDTV5J/F02C21VMQ1J/multiprocessing_backtest_py.py](https://betfairlightweight.slack.com/files/U01SCPDTV5J/F02C21VMQ1J/multiprocessing_backtest_py.py)

*Tags: General Technical*

---

**ShaunW** - *18:11:48*

I find just being organised is easier than a tech solution or spending money. I test a few strategies at the same time, log enough info to allow many ways to look at it afterwards and run them overnight locally. I'm not a flumine user so I can't compare but takes me about 15hrs to run 12months of UK horses with 2 or 3 fairly intensive strategies. I never find I'm sat waiting for it with nothing to do because each run creates more than a day's work.   Similar approach to the recent question about disk full warnings, the answer is to just be organised enough not to get them, scheduled archiving etc.

*Tags: General Technical*

---

**Jeff Waters** - *19:34:21*

Thanks Thambie



I'll have a go at multiprocessing (though it may be beyond my current Python abilities, which are rather modest!).

*Tags: General Technical*

---

**liam** - *19:39:47*

[@U013K4VNB6D](@U013K4VNB6D) you really want to be using [https://liampauling.github.io/flumine/quickstart/|listener_kwarg](https://liampauling.github.io/flumine/quickstart/|listener_kwarg)s to turbo charge the backtest 

*Tags: General Technical*

---

**Jack** - *23:30:59*

```strategy = ExampleStrategy(

    market_filter={

        "markets": ["/tmp/marketdata/1.170212754"],

        "listener_kwargs": {"inplay": False, "seconds_to_start": 600},

    }

)```

[@U013K4VNB6D](@U013K4VNB6D) I think [@U4H19D1D2](@U4H19D1D2) is talking about this.



If you are only placing a bet 30seconds before the off then you don't need Flumine to run all the way from the market opening to that point.

*Tags: Strategies*

---

## 2021-09-07

**liam** - *06:37:31*

How many cores do you have? Python/bflw/flumine will happily use all of them and max them out if backtesting 

*Tags: General Technical*

---

**Jeff Waters** - *08:44:23*

Thanks [@U01SCPDTV5J](@U01SCPDTV5J). I appreciate that. Just to clarify, will the win command I posted have the effect of only considering win markets?



Before I went to bed last night, I put the following filters in place:



```strategy = TestStrategy(

    market_filter={"markets": get_markets("PRO"),

    "listener_kwargs": {"seconds_to_start": 60},

    "market_types": ["WIN"]},

    max_order_exposure=1000,

    max_selection_exposure=105,

)```

The program is still running (I went to bed before midnight).



I'll  have a go at adding multiprocessing to my code (though it might be beyond my current Python abilities).



For now though, I suspect that my best bet might be to remove all but 1 day of the month of data from the dataset. If I find an approach that yields a profit on one day, I can test it on a larger dataset.

*Tags: Strategies*

---

**Jeff Waters** - *09:25:37*

Thanks [@U4H19D1D2](@U4H19D1D2). I have 4 cores (but no hyperthreading).



My (very limited) understanding of multiprocessing is that it causes Python to use multiple cores. However, given that it is already using all of the cores for back testing, will it give me a speed advantage?



Thanks



Jeff

*Tags: Performance*

---

**liam** - *09:29:23*

Just give it a handful of markets and see how long it is taking, chances are that something in TestStrategy is slowing it down

*Tags: Performance, Strategies*

---

**Stefan** - *09:45:46*

Your python code will be always slower, and it is not just about it is interpreted, but how python treat data, for instance what is representation of your api data, dictionary I think.



No problem, you should stick with programming language that best suits you. I use python sporadically, but I would not use it on this kind of projects.

*Tags: Performance*

---

**Stefan** - *09:46:40*

Have market data, similar you use with your python library, and what data the strategy is using depends on the strategy

*Tags: Strategies*

---

**Jonjonjon** - *09:47:24*

Is it the streaming data, with up to 20 updates per second?

*Tags: General Technical*

---

**Stefan** - *09:48:08*

It is data recorded by my app, yes from streaming api

*Tags: General Technical*

---

**Unknown** - *10:17:09*

I may consider to open source data analyzer part, as I use scripts for showing charts, so some users could be able to share such scripts, but really my experience is that it is just waste of my time. Well yes some people know how to use google, and they had found my github sporadically, but all that ends only asking me to build some bespoke bots for them, so none collaboration on source code.



App can execute scripts, like this one

*Tags: General Technical*

---

**Jeff Waters** - *10:20:28*

Do you think the reason that you don't get offers to help with source code is simply because F# is a pretty niche language that few people are able to program in?



For example, when I search for F# jobs in the UK on Indeed, I get 75 ads, whereas when I search for C# jobs, I get 8,064 ads.

*Tags: General Technical*

---

**Stefan** - *10:24:19*

[@U013K4VNB6D](@U013K4VNB6D) Bot sdk contains C# bot examples as well, and as my framework is written in .net you can use it from any programming language the .net offers, so even in iron python, you can simply recode for instance this code in any .net supported language:



[https://github.com/StefanBelo/Bfexplorer-BOT-SDK/blob/master/TestStreamingAPI/Program.fs](https://github.com/StefanBelo/Bfexplorer-BOT-SDK/blob/master/TestStreamingAPI/Program.fs)



When I am totally bored in my day job I can do it and recode that same code to Visual basic and C#

*Tags: General Technical*

---

**ThomasJ** - *10:24:23*

When an order is placed, the code in `class BetfairExecution(BaseExecution &gt; def execute_place` seems to do nothing when the response order status is "EXECUTION_COMPLETE" and waits for this order status to be processed via the flumine queue; specifically in `process.py &gt; def process_current_order` which sets the order status to `OrderStatus.EXECUTION_COMPLETE`.



Given that the API immediate response is the quickest way to determine an order status, I would have thought that an EXECUTION_COMPLETE order would be set thus in `class BetfairExecution(BaseExecution &gt; def execute_place` so that a strategy knows at the earliest possible time that an order is done'n'dusted.



Caveat: I am learning BFLW and Flumine so that above may be gibberish.

*Tags: Strategies*

---

**liam** - *11:22:18*

Change logging to critical to speed things up, error line 47 in the strategy, should be

```str(len(market.blotter))```

*Tags: Errors Debugging, Performance, Strategies*

---

**liam** - *11:28:56*

100 markets:



```start time = 07/09/2021 11:25:17

Total profit post-commission: 468.04



finish time = 07/09/2021 11:26:44```

Bugfix in the strategy, logging set to CRITICAL and this fix to the get_markets



```def get_markets(data_dir):

    markets = []

    for dir_, _, files in os.walk(data_dir):

        for file_name in files:

            rel_file = os.path.join(data_dir, file_name)

            markets.append(rel_file)

    return markets ```

*Tags: Errors Debugging, Strategies*

---

**Adrian** - *11:33:50*

I cannot for the life of me figure out how to get the underround calc into the notes. I'm using:

`underround = _calculate_underround(market_book.runners)`

                    `order2 = trade.create_order(`

                    `side="LAY",`

                    `order_type=LimitOrder(2, self.context["stake"],`

                        `persistence_type="LAPSE"),`

                    `notes=OrderedDict(underround=round(underround, 4))`

                    `)`

                    `market.place_order(order2)`

`def _calculate_underround(runners: list) -&gt; float:`

    `return sum(`

        `[`

            `1 / r.ex.available_to_lay[0]["price"]`

            `for r in runners`

            `if r.ex.available_to_lay`

        `]`

    `)`

But nothing ever appears in the notes in the logging csv output. There is a notes field already there but it's always empty.

*Tags: General Technical*

---

**birchy** - *12:01:19*

I run mine on an AWS Lightsail instance, overnight. Results are usually in by the time I wake up.

*Tags: Deployment*

---

**Unknown** - *13:38:54*

aaannndddd wwhhhaatttssss thhhehehehe problem?

*Tags: General Technical*

---

**Adrian** - *13:53:09*

maybe something you're adding in your strategy. i've just used all the examples. maybe i'm missing something

*Tags: Strategies*

---

**liam** - *13:53:50*

flumine version?

*Tags: General Technical*

---

**Adrian** - *14:16:10*

which i did when you asked which version but then i introduced another error after

*Tags: Errors Debugging*

---

**Jeff Waters** - *14:21:44*

I don't know if it's relevant, but when I ran the program I got loads of messages like the following:



*{"asctime": "2021-09-07 10:55:53,215", "levelname": "CRITICAL", "message": "Unknown error can only concatenate str (not \"int\") to str in process_market_book (1.145082711)", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\water\\anaconda3\\envs\\bet-project\\lib\\site-packages\\flumine\\utils.py\", line 204, in call_strategy_error_handling\n    return func(market, market_book)\n  File \"C:\\Users\\water\\PycharmProjects\\bet-project\\teststrategy.py\", line 47, in process_market_book\n    print(\"Number of bets placed so far: \" + len(market.blotter))\nTypeError: can only concatenate str (not \"int\") to str"}*



and





*{"asctime": "2021-09-07 10:55:53,215", "levelname": "INFO", "message": "Order status update: Violation", "market_id": "1.145082712", "selection_id": 17069875, "handicap": 0, "id": "138503049532150426", "customer_order_ref": "d0d1ab6db12de-138503049532150426", "bet_id": null, "date_time_created": "2018-07-01 07:42:37.024000", "publish_time": null, "trade": {"id": "2bac1299-0fca-11ec-91ad-00e62d0401a8", "strategy": "TestStrategy", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138503049532150426"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 42, "size": 10, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 10.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}*



and



*{"asctime": "2021-09-07 10:55:53,230", "levelname": "WARNING", "message": "Order has violated: STRATEGY_EXPOSURE Error: Potential selection exposure (410.00) is greater than strategy.max_selection_exposure (105)", "control": "STRATEGY_EXPOSURE", "error": "Potential selection exposure (410.00) is greater than strategy.max_selection_exposure (105)", "order": {"market_id": "1.145082712", "selection_id": 17069875, "handicap": 0, "id": "138503049532150426", "customer_order_ref": "d0d1ab6db12de-138503049532150426", "bet_id": null, "date_time_created": "2018-07-01 07:42:37.024000", "publish_time": null, "trade": {"id": "2bac1299-0fca-11ec-91ad-00e62d0401a8", "strategy": "TestStrategy", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138503049532150426"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 42, "size": 10, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 10.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": "Order has violated: STRATEGY_EXPOSURE Error: Potential selection exposure (410.00) is greater than strategy.max_selection_exposure (105)", "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}}*



I also got loads of 'market closed' and 'market cleared' 'INFO' messages.



Are they to be expected, or do they hint at a problem with the code?

*Tags: Errors Debugging, Deployment, Strategies*

---

**Jeff Waters** - *14:44:02*

No, they are stored in a folder on my C drive.



Because the files are in different folders within the PRO folder, I modified the get_markets method, as I was getting a 'file not found' error message:



```def get_markets(data_dir):

    markets = []

    for dir_, _, files in os.walk(data_dir):

        for file_name in files:

            dirName = Path(dir_).absolute()

            rel_file = os.path.join(dirName, file_name)

            markets.append(rel_file)

    return markets```

I wouldn't have thought that would slow things down, though?



Apart from that, you and I were running the same code.



The fact that you guys aren't seeing any major errors or omissions suggests to me the the slow speed is hardware-related?

*Tags: Errors Debugging, Performance*

---

**Jeff Waters** - *14:56:40*

I've included it in my market filter:



```strategy = TestStrategy(

    market_filter={"markets": get_markets("PRO"),

                   "listener_kwargs": {"seconds_to_start": 61},

                   "market_types": ["WIN"]},

    max_order_exposure=1000,

    max_selection_exposure=105,

)```



*Tags: Strategies*

---

**Lee** - *15:00:34*

Or just put your `now = datetime.now()`  after you have instantiated the strategy

*Tags: Strategies*

---

**Beeblebrox** - *15:04:55*

Also, set your logging level to CRITICAL.  You're going to be printing out a lot of messages with it set to INFO, especially as you're not checking the exposure in your strategy, so you're going to get lots of STRATEGY_EXPOSURE messages.

*Tags: Strategies*

---

**liam** - *15:44:20*

it would solve this problem but create many more

*Tags: General Technical*

---

**Jack** - *19:07:06*

Can anyone tell me when and why one would use paper trading in flumine :scroll:?

*Tags: Strategies*

---

**Beeblebrox** - *19:10:20*

I think the standard process for developing a strategy is: analysis -&gt; backtest -&gt; paper trade -&gt; live...

*Tags: Deployment, Strategies*

---

**Jack** - *19:20:16*

Thanks [@U01MPC0GUK1](@U01MPC0GUK1)  So I guess next question is what does paper trading offer you that a backtest doesn't?

*Tags: Strategies*

---

**Beeblebrox** - *19:22:22*

It's happening in real time and with live data.  In theory it should be the same as backtesting, but who knows what errors you might have made that won't show up with historic data, but will in live data?  It's just a safety net to stop you losing a load of money.

*Tags: Errors Debugging, Deployment*

---

**Beeblebrox** - *19:31:53*

Yeah, in theory it should be the same, and to be fair Flumine already has checks in-built to make it less likely that you'll blow your bank.  So you don't have to do it if you don't want to - up to you.



I've used it sparingly.  To be fair my process is a bit slap dash.  Rather than analysis -&gt; backtest -&gt; paper trade -&gt; live, mine's make something up in my head that I think's logical -&gt; back test it -&gt; live -&gt; analysis to refine it.

*Tags: Deployment*

---

**liam** - *19:49:44*

Yep it’s basically a final integration test that is using live data, handy if you are testing new markets where you don’t have data or in my case checking a change/update without accidentally fat fingering 

*Tags: Deployment*

---

**ShaunW** - *20:48:02*

[@U01SCPDTV5J](@U01SCPDTV5J) The data may be the same shape but the delivery method is different, so if a test is possible then the question becomes why not?  Personally I'd always do it for a while at least as it was a useful part of the belt and braces methodology I introduced at my old 'work'.  I like to treat my money with the same degree of respect I had to show towards other people's.:slightly_smiling_face:

*Tags: Deployment*

---

**Peter** - *21:24:19*

The data isn't really the same shape. When backtesting Flumine processes one market at a time by default (unless you ask it to group markets together e.g. by event). With paper trading you're getting all the open markets captured by your filter, at the same time, so the same structure as your live strategy will see. This is especially useful if your strategy crosses markets.

*Tags: Deployment, Strategies*

---

## 2021-09-08

**Jonjonjon** - *08:34:25*

[@U013K4VNB6D](@U013K4VNB6D) It depends on what you are doing. If you are planning on using Flumine with backtesting and multiprocessing, I would recommend a minimum of 1GB RAM per CPU thread.



So I have 32 CPU threads and 32GB RAM. I will occasionally run out of RAM when backtesting on 31 threads. That could be due to my bad coding, or maybe there's a memory leak (I have a feeling it might be a Pandas memory leak rather than BFLW/Flumine memory leak), but if I'd bought 64GB my life would be easier.

*Tags: Feature Engineering, Performance*

---

**Mo** - *08:42:36*

I don't think it's bad advice - sometimes much easier to throw money at a problem than try to solve a hard technical issue

*Tags: General Technical*

---

**Oliver Varney** - *08:46:07*

If you have plans for doing some big modelling / AI then definitely having large ram / large gpu ram are going to to help. Try and only buy once and go for something decent.  I just got bent over for a 3090 last month :face_with_rolling_eyes:

*Tags: Strategies*

---

**Jeff Waters** - *09:30:28*

I hate Macs. Whenever my mum asks me to help her with her Mac, I find myself muttering 'tool of the devil' under my breath! :joy:



That said, I'm sure they are perfectly fine if you're used to them, but I've been using Windows for literally decades.

*Tags: General Technical*

---

**Unknown** - *09:36:26*

[https://betfairlightweight.slack.com/files/UBS7QANF3/F012WG50U8K/image_from_ios.jpg](https://betfairlightweight.slack.com/files/UBS7QANF3/F012WG50U8K/image_from_ios.jpg)

*Tags: General Technical*

---

**Jono** - *10:35:05*

im currently using market.context to store a pandas df, 3 timetamps and a handful of lists. These are updated every 10 seconds or so but can increase in frequency occasionally being accessed and updated every market book. The process_market_book, process_orders, and a worker all access these variables and make changes to them as well as base decisions on their values. I was wondering best practices to avoid problems accessing or trying to update the same value at once? For example the dataframe is populated during process_market_book, the worker sends then sends said dataframe (market.context['pd_dataframe']) to an api and then attempts to clear the df. Would clearing the df in the worker cause conflicts if it at the same time the market.context is being accessed/updated at the same time in process_market_book? Sorry if this is a skewed understanding of the framework im still getting to grips with the package and threading. Thanks

*Tags: Feature Engineering*

---

**Mo** - *11:02:07*

Python is inherently single threaded so there will be no concurrent access to worry about 

*Tags: General Technical*

---

**Jono** - *11:16:00*

the worker is not strating its on thread and is waiting for the response. I didnt want want to mess with anything thread related as i wasnt sure if that would interfere with the rest of flumine

*Tags: General Technical*

---

**Unknown** - *11:24:47*

I've adapted [@U01SCPDTV5J](@U01SCPDTV5J)'s multiprocessing code to my back test code and I've excluded from consideration JSON files beginning with '2'. As a result, I'm now processing about 3 markets a second, which is fantastic.



However, the results summary code that I used previously no longer works, as the interpreter seems to think that there are no markets in the blotter. I've put the revised code on GitHub - [https://github.com/JeffW12345/betfair-backtesting/blob/main/runbacktests.py#L95](https://github.com/JeffW12345/betfair-backtesting/blob/main/runbacktests.py#L95).



I can't work out why, and when I try to use the debug tool I get an error message (attached).



Any suggestions would be much appreciated.



Thanks



Jeff

*Tags: Errors Debugging*

---

**Jono** - *11:25:19*

so it loops though the markets appending them to generate the full dataframe does

`[http://logging.info|logging.info](http://logging.info|logging.info)("Generating bet dataframe from all markets")`

`for market in flumine.markets:`

    `bet_df = bet_df.append(market.context['bet_df'], ignore_index=True)`



then makes the api call.

Finally my plan would be to loop through them again and clear them, ie something like





`[http://logging.info|logging.info](http://logging.info|logging.info)("Clearing bet dataframe for all markets")`

`for market in flumine.markets:`

    `bet_df = pd.DataFrame(columns=['placed_at', 'external_event_id', 'external_bet_id'])`



The reason i did it this way is i wasnt sureif there was a "strategy.context" of sorts that you can access from all markets and append to the same dataframe. Thus each market has a df that is joined together in the worker

*Tags: Feature Engineering, Strategies*

---

**Lee** - *11:47:08*

I'm not sure on the best approach here but with your current approach a unique key might be useful but i would try to limit / restrict where you're adding/removing/updating data to preferable a single place.

For example, how often will your worker run? will you only trigger bets based on your api response? maybe the worker could do all this logic adding to `market.context` and then `process_market_book` just triggers bets based on this.

This might not be a great idea / best practice but might give you some  ideas on how to approach it.

*Tags: General Technical*

---

**D C** - *12:51:48*

I think someone asked this recently but I can't find it anywhere in any thread. I've just changed from stream subscription using market ID list to a coarser filter. Yesterday it silently failed on me - the silence likely a bug in my logging setup but I suspect that the number of markets in my filter changed as new markets were added (seemed to fail around 6pm UTC). Because my logging failed I am not sure how to catch it so does anyone know what actually happens when this occurs. The socket was not disconnected so I presumably kept getting heartbeats and zero data (no market data files were produced after the "failure" time). Recommended solution was to ask for a bump in the market limit per connection which I will do but just curious as to what actually happens when this occurs as I am basically just making an assumption here.

*Tags: Getting Started, Errors Debugging*

---

**D C** - *12:58:54*

OK great. Thanks Mo. Problem must reside elsewhere then. Cheers.

*Tags: General Technical*

---

**Jono** - *15:49:45*

would running flumine.run() in separate thread to main cause issues? i'd quite like to use an azure app service to let my test strategies run and be able to send requests to it and get simple responses back indicating how its performing. Im having trouble keeping the keeping the endpoint open once the flumine loop has started and so this idea of moving it to a separate thread came about after digging through stackoverflow. Does anyone know if this is viable to move the long term running of the flumine strategies to a different (non-main) thread whilst main stays open to receive and process other requests/tasks. Also Im using the flask api python package to create the endpoint. Thanks!

*Tags: General Technical*

---

**Newbie99** - *15:54:43*

if you go down that route...make sure you set debug to False....otherwise you end up with 2 processes and duplicate bets....er not that I ever did that of course....:woozy_face:

*Tags: Errors Debugging*

---

**Jono** - *16:15:51*

sorry [@UFTBRB3F1](@UFTBRB3F1) could you clarify exactly what you mean? Is there a _debug=_ variable i havent been aware of all this time thats worth making a quick note of :scream:

*Tags: Errors Debugging*

---

**Newbie99** - *16:16:24*

for example:



```socketio.run(app, debug=False, host='0.0.0.0', port=1234)```



*Tags: Errors Debugging*

---

**Aaron Smith** - *18:58:59*

How do ppl go about updating their market_filter for a strategy? Do you guys just catch all markets for the day and terminate flumine at the end of the day? Or is it more efficient to update the market_filter regularly over the day?

*Tags: Strategies*

---

**Ke** - *19:34:29*

i'm trying to plug in my silly price prediction model and trade around it. where is the best place to put the price prediction in? should it be in middleware or strategy directly? any factors should be considered here?

*Tags: Strategies*

---

**liam** - *19:39:13*

Best practice would be within the strategy itself unless there is latency involved in the prediction itself in which case you want to push this out to a worker or external process (e.g redis)



Middleware would be suitable if lots of strategies require the prediction as it can be calculated once and then shared. 

*Tags: Performance, Strategies*

---

**Oliver Varney** - *19:43:58*

I abstract all of my data away from flumine native objects attributes / structures and store it in the strategy. Price prediction is done in a separate process though this may not be worth it

*Tags: Strategies*

---

**Ke** - *19:50:40*

Any drawback to put in middleware if not to share with different strategies? Would it affect latency? My prediction will use not just market data, would it be easier to plug in 3rd part data in strategy directly?

*Tags: Performance, Strategies*

---

**Oliver Varney** - *19:56:47*

Yes you would also need to update your stored price also (if in Middleware or where ever), and if your capturing historic prices as an input to your model these will also need to be adjusted

*Tags: Strategies*

---

**Jeff Waters** - *21:24:24*

I'm trying to use multi-processing for my backtesting. 



The challenge I am having is getting a total profit/loss figure for all the markets combined. I have four cores, meaning the display_results method is called four times, resulting in four different 'total profit and loss' totals. 



I have explored using global 'running total' variables and global lists containing profit and loss figures for each market, to no avail. This included creating a module with its own list, but that didn't work. Each time a new processor called the display_results method, the list in the module was wiped clean. I've also tried the same approach with a dictionary, with market ids as keys, but had the same problem.



Any suggestions?

*Tags: General Technical*

---

**Jack** - *21:40:52*

You want to use a logging control to write out to something persistent like a csv file, JSON, or to a database. 

Then filter that down or run queries over it to get the figures you are looking for. 

Might be overkill just for profit but when you start digging into things a little deeper you'll need this most likely.

*Tags: General Technical*

---

**Jeff Waters** - *21:42:04*

Thanks Jon.



The big challenge I'm having is aggregating the data.



To be honest, I might revert back to the non multi-processing approach (which, thanks to the help I've received in this forum, is now running at a speed that will allow me to run back tests reasonably quickly). I think that, with my current level of Python knowledge, I could easily spend days getting bogged down in multi-processing, when I could be spending that time creating indicators and running tests.



Saving the data to disk is an interesting idea, btw. One solution could be for me to append each core's results separately to a CSV file. It would then be easy to summarise the results using a spreadsheet or a program that imports the data.

*Tags: Performance*

---

**Jack** - *21:43:57*

There are a couple of example logging controls I think in the flumine repo :+1:

*Tags: General Technical*

---

**Unknown** - *23:35:02*

My thanks to [@U01SCPDTV5J](@U01SCPDTV5J) and [@UPMUFSGCR](@UPMUFSGCR) for the advice re. using output to a file (and to everyone else who has helped me with this program).



My program can now create two CSV files - one for markets and one for bets - which log all the data I need. Plus, it can do about 8-9 markets a second.



Light at the end of the tunnel! :grin: Cheers, guys!

*Tags: General Technical*

---

## 2021-09-09

**George** - *14:54:08*

If I'm using a Mac or Linux, do I still need to install using

```pip install betfairlightweight[speed]```

or is it enough just to do

```pip install betfairlightweight```

*Tags: Getting Started, Performance*

---

**liam** - *15:05:22*

`betfairlightweight` is enough, speed is for speed

*Tags: Performance*

---

**George** - *15:14:45*

If I was to remove the betfairlightweight and install the [speed], do you know what would speed up and roughly by how much?

*Tags: Getting Started, Performance*

---

**George** - *15:17:20*

wow that's a lot!

i'm only asking because I saw this in the docs:

"By default betfairlightweight will install C and Rust based libraries if your os is either linux or darwin (Mac), due to difficulties in installation Windows users can install them separately:

```$ pip install betfairlightweight[speed]```

"

which made me think that, since I only use Linux and/or Mac, there wasn't any benefit?

*Tags: Getting Started, Performance*

---

**George** - *15:22:50*

ah ok. call me a noob but i'm confused at how installing extra packages help speed things up? Because, surely if the code is identical to before, then the packages aren't being imported anyway?

*Tags: Getting Started, Performance*

---

**liam** - *15:23:19*

[https://github.com/liampauling/betfair/blob/eb08f1c400c0c5d61dacfd618aa93d5aceec15ac/betfairlightweight/compat.py#L11|magic](https://github.com/liampauling/betfair/blob/eb08f1c400c0c5d61dacfd618aa93d5aceec15ac/betfairlightweight/compat.py#L11|magic)

*Tags: General Technical*

---

**liam** - *15:24:02*

common design pattern in python to try and use certain libraries and then fall back to the included

*Tags: General Technical*

---

**George** - *15:24:24*

yep ok agreed that this is magic. all makes sense now! so i will just pip install those two and then i don't need to worry about uninstalling bflw etc

*Tags: Getting Started*

---

**liam** - *15:24:47*

or



```pip install betfairlightweight[speed] --upgrade```

*Tags: Getting Started, Performance*

---

**liam** - *20:23:40*

I currently do a bit of inbetween by locking an order whilst ‘executing’ and then taking the stream as the golden source, I feel this keeps things simple and removes the potential for race conditions.



However this makes async requests tricky and as you mention it introduces the potential for latency if the response is returned before the stream etc. 

*Tags: Performance*

---

**Sam Asin** - *22:32:40*

or if I should try to go through the help desk or something?

*Tags: General Technical*

---

## 2021-09-10

**liam** - *09:07:24*

[https://liampauling.github.io/flumine/quickstart/#event-processing](https://liampauling.github.io/flumine/quickstart/#event-processing)

*Tags: General Technical*

---

**liam** - *09:07:40*

The `Market` object contains a helper method for accessing other event linked markets:

```place_market = market.event["PLACE"]```

*Tags: General Technical*

---

**George** - *12:27:31*

Does BFLW's streaming code handle missed heartbeats and auto-resubscribe? Or does the user have to write some kind of checking?

*Tags: General Technical*

---

**George** - *12:38:47*

thanks. if you mean the error handling example using tenacity i've just implemented something very similar. is the socket timeout equivalent to missed heartbeats, or should I try to handle missed heartbeats separately?

*Tags: Errors Debugging*

---

**liam** - *12:45:36*

It was but has recently been bumped to [https://github.com/liampauling/betfair/blob/eb08f1c400c0c5d61dacfd618aa93d5aceec15ac/betfairlightweight/endpoints/streaming.py#L26|64s](https://github.com/liampauling/betfair/blob/eb08f1c400c0c5d61dacfd618aa93d5aceec15ac/betfairlightweight/endpoints/streaming.py#L26|64s) to solve another problem, you can reduce this to a value just above your heartbeat value (maybe we should add this dynamically after connection)



```betfair_socket = trading.streaming.create_stream(timeout=11)```



*Tags: Strategies*

---

**liam** - *12:58:36*

It needs to be above 15 to correctly get the timeout message on connection but this was to help poor networks where the socket was timing out when the server was still happy

*Tags: Deployment*

---

**Aaron Smith** - *17:55:42*

When should one use a market_filter and when a streaming_market_filter?

*Tags: General Technical*

---

**Peter** - *18:05:21*

If you're using Flumine or BFLW for streaming, use the streaming market filter. If you're just making API calls with BFLW use the market filter. In either case you can skip them all together and just pass a dict with the relevant parameters if you prefer.



There are two filters because the permitted parameters for streaming are more restrictive than for the API. So using the streaming filter will trigger an error if you try to use (for example) competition_ids, which is a valid parameter for API calls.

*Tags: Errors Debugging*

---

## 2021-09-11

**Stefan** - *06:47:02*

[@U013K4VNB6D](@U013K4VNB6D) and with what programming languages are you familiar? You know python, and what others?

*Tags: General Technical*

---

**Jeff Waters** - *16:54:08*

Java, Python, JS and C#. Been programming for 2 years, so still a relative newbie

*Tags: General Technical*

---

## 2021-09-12

**Stefan** - *10:21:26*

[@U013K4VNB6D](@U013K4VNB6D) interesting, so you are familiar with other programming language but you chose to use python. Are you familiar with  IronPython? It is good to ask you as you know C#.

*Tags: General Technical*

---

**Jeff Waters** - *10:51:04*

Hi Stefan



The reason I am using Python is because Flumine and Betfair Lightweight are written in Python. I figured it made far more sense for me to use programs that someone has created than spend countless hours reinventing the wheel by making my own from scratch in a language that I'm more familiar with. :slightly_smiling_face: Plus, increasingly my knowledge of Python could have other benefits - for example, if I wanted to do some machine learning.

*Tags: General Technical*

---

**Stefan** - *11:30:44*

[@U013K4VNB6D](@U013K4VNB6D) yes I can understand that but you are C# developer as well, and before python betfair api wrapper library betfair itself offered C# libraries, What you use now is not actually framework. Yes, I use python as well no some many but am able to code something to support my ML, actually to use ML you do not have to know any python. I tried to write today some code I did exactly in 3 other programming languages yesterday, but failed to do so, and it is just initialization of 2 objects, what I managed to do, but calling 2 async/Task method I failed, so I leave python for now as 4 example of using betfair framework.

*Tags: Errors Debugging*

---

**Jeff Waters** - *12:05:01*

Hi [@U013MLED3V1](@U013MLED3V1). I'm not sure I follow. Surely flumine is a framework (and Betfair Lightweight is a wrapper)? At this stage, I am purely doing back testing, and as I have a (nearly completed) setup that works, using flumine, it makes sense for me to stick with that. Plus, flumine is regularly updated and the support provided via this forum is excellent, so for me it's a no brainer.

*Tags: Getting Started*

---

**Mo** - *14:21:36*

Correct but not very Pythonic. e.g.



```def get_total_volume(runner):

    return sum(entry['size'] for entry in runner.ex.traded_volume)```

*Tags: General Technical*

---

**S G** - *14:26:48*

Another question, can we tell API to match bets at the best available price? for example backing a selection may result in less profit than laying opponent for the same stake?

*Tags: General Technical*

---

**S G** - *14:27:42*

or just write some python logic to deal with it?

*Tags: General Technical*

---

**admiral** - *17:55:00*

Apologies as this is probably a dumb question, but how do I get the market books from a 1.xxxxxxx file?

*Tags: General Technical*

---

**Ke** - *18:50:53*

what string i should use for fill or kill order? i can see in flumine's code that i can do that by setting time_in_force='FillOrKill', but betfair api takes the string "FILL_OR_KILL". I guess I should use 'FillOrKill' in Flumine, but just want to double check.

*Tags: General Technical*

---

**jhaa** - *18:50:56*

anybody else having issues with high latency this weekend or is this an issue on my side?

*Tags: Performance*

---

## 2021-09-14

**George** - *12:10:06*

specifically:

AttributeError: 'StreamReader' object has no attribute 'decode'

*Tags: Errors Debugging*

---

**George** - *12:15:21*

but it's failing on the `decode` with the AttributeError

*Tags: Errors Debugging*

---

**EJono** - *14:31:52*

Occasionally I'm missing executable orders in flumine upon restarting my strategy. I believe this is because the "config.hostname"  changes for me each time I rebuild (I'm running flumine on a cloud service VM and each deployment it changes). I'm wondering if I could use the same middleware method for accessing EXECUTION_COMPLETE orders but substitute in EXECUTABLE to the order_projection field of list_current_orders, would this be viable? I'm also filtering list_current_orders on market id. Other strategies bets being pulled through are not a problem as I'm only placing on this one strategy at the moment



Additionally should the market.blotter be filled with Strategy relevant executable orders at the time of add_market_middleware running?

*Tags: Deployment, Strategies*

---

**liam** - *14:34:10*

Better practice would be to hardcode a ref:



```from flumine import config



config.customer_strategy_ref = "jono001"```

*Tags: Strategies*

---

**EJono** - *14:46:36*

Then the regular identification of orders with EXECUTABLE status should take place? Would these be automatically picked up with no middleware intervention? and I only have to look for EXECUTION_COMPLETE bets in the middleware using 



customer_strategy_refs=["jono001"],

order_projection="EXECUTION_COMPLETE",

market_ids=[market.market_id]

*Tags: Strategies*

---

**liam** - *14:53:42*

yep, streaming will pull in Executable bets

*Tags: General Technical*

---

## 2021-09-16

**Adrian** - *08:43:05*

Are the notes recorded by the logging control example persistent from the time of placing the bet? Or do they get updated when the order status changes? It seems like an obvious question but there's only one entry per bet and that's with "execution complete", so I was wondering if the notes get updated at the time of status change, or they only get created once upon order creation and not modified beyond that point?

*Tags: General Technical*

---

**liam** - *08:48:28*

not sure I understand, they default to {} unless you provide data on creation or add to it later, flumine doesnt touch them

*Tags: General Technical*

---

**Adrian** - *08:52:21*

ok i think i get it. since it is recorded in `_process_cleared_orders_meta` i thought Flumine might have been updating them

*Tags: General Technical*

---

## 2021-09-17

**liam** - *08:45:03*

flumine [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1200-2021-09-17|v1.20.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1200-2021-09-17|v1.20.0) now released, updated to use bflw 2.14.0 and moves the `listener_kwargs` optimisation down a level in the stack, this results in massive reduction in function calls, I am seeing roughly 2x speed up in backtesting when restricting to inplay only with no loss in functionality :rocket:



Let me know if you have any issues, especially those that have integration tests on backtest markets :thumbsup:

*Tags: Performance*

---

## 2021-09-20

**Jeff Waters** - *08:57:22*

When doing backtesting with Flumine, is it possible to combine BSP and non-BSP bets?



When I write the following code, only the non-BSP bets appear in my results spreadsheet:



```def process_market_book(self, market, market_book) -&gt; None:

    for runner in market_book.runners:

        update_lists.vol_change_actions(runner)

        if runner.status != "ACTIVE" or not runner.last_price_traded:

            continue

        else:

            runner_context = self.get_runner_context(

                market.market_id, runner.selection_id, runner.handicap

            )

            if 360 &gt; market.seconds_to_start &gt; 0:

                # Bets are possible when no bets have been placed previously for this selection.

                able_to_proceed = True

                for order in market.blotter:

                    if order.selection_id == runner.selection_id:

                        able_to_proceed = False

                if able_to_proceed:

                    # LAY NON-BSP BET

                    # create trade

                    trade = Trade(

                        market_book.market_id,

                        runner.selection_id,

                        runner.handicap,

                        self,

                    )

                    # create order

                    price = get_price(runner.ex.available_to_lay, 3)

                    order = trade.create_order(

                        side="LAY",

                        order_type=LimitOrder(1000, 2)

                    )

                    # place order for execution

                    market.place_order(order)

                    print("Lay placed")

                    # NOW BSP BACK

                    # Create trade

                    trade = Trade(

                        market_book.market_id,

                        runner.selection_id,

                        runner.handicap,

                        self,

                    )

                    # create order

                    order = trade.create_order(

                        side="BACK",

                        order_type=LimitOnCloseOrder(2, 1.01)

                    )

                    # place order for execution

                    market.place_order(order)

                    print("BSP back placed")```

Am I doing something wrong?



Thanks



Jeff

*Tags: General Technical*

---

**liam** - *09:03:25*

It will be due to the runner_context stopping the second order (the logs would tell you this), you can let two orders go through by



```strategy = LowestLayer(

    market_filter={"markets": markets, "listener_kwargs": {"inplay": True}},

    max_live_trade_count=2

)```

Or putting them under the same trade

*Tags: Deployment, Strategies*

---

**liam** - *09:04:09*

ie flumine as already handling your `able_to_proceed` logic

*Tags: General Technical*

---

## 2021-09-21

**ThomasJ** - *10:51:36*

In Flumine, is it possible for a worker thread to have access to MarketBookCache/RunnerBookCache attributes? (Why is a long story :smile: )



If yes then how do I pass the reference via class BackgroundWorker &gt; def run to the target function?

*Tags: General Technical*

---

**Oliver Varney** - *11:47:18*

```framework.add_worker(

    worker=BackgroundWorker(

        client=client,

        function=worker.test_func,

        interval=300,

        start_delay=30,

        func_kwargs={

            'flumine': framework

        }

    )

)```



*Tags: General Technical*

---

**liam** - *11:50:11*

You already get the framework as the second var so you don't need the extra



[https://github.com/liampauling/flumine/blob/3674986e14d266acc739a8288d6c50183d42db14/examples/workers/inplayservice.py#L13](https://github.com/liampauling/flumine/blob/3674986e14d266acc739a8288d6c50183d42db14/examples/workers/inplayservice.py#L13)

*Tags: General Technical*

---

**Oliver Varney** - *11:54:07*

haha no I think I adapted your worker code for something else and didnt extend flumine code

*Tags: General Technical*

---

**ThomasJ** - *12:19:16*

I want to record to a DB the LTP movements during a race for use by another app. I can do that using a worker and the data in Flumine's  &gt; markets &gt; market &gt; runnerbook  every X secs. no problem.



But I want to speed things up by minimizing the IO to the DB by setting a 'price_changed' flag in the RBcache so that I only do a DB method when something has changed. This attribute ends up in RunnerBook OK.



(I have added attributes to the appropriate classes and have changed serialise/d() to propagate the value of the flag from the RBcache to the RB along with the changed data.)



So after the flag is 'used' I want to reset it to false in RBCache so an old value does not end up in RB.

*Tags: Performance*

---

**liam** - *12:26:46*

Why not just use a strategy and store the flag in the context? You have the raw streaming_update in the MarketBook 

*Tags: Strategies*

---

**ThomasJ** - *12:31:54*

OK thank you. Definitely using a strategy. So the context can store miscellaneous 'extra' stuff. Fantastic.

*Tags: Strategies*

---

**liam** - *13:18:59*

You just need to watch out for latency if you are doing anything in the strategy as it is blocking

*Tags: Performance, Strategies*

---

**Matthieu Labour** - *14:07:07*

Hi, Is there a hook in flumine that I can attach a custom callback that would get called whenever an order changes state (PENDING to EXECUTED for example). Thank you.

*Tags: General Technical*

---

**Matthieu Labour** - *15:33:08*

Will do. BetFairLightWeight does implement the Order stream API correct?

*Tags: General Technical*

---

## 2021-09-22

**ThomasJ** - *04:43:37*

I am storing LTP history at `update_cache &gt; if "ltp" in new_data`...(as I want to minimize looping thru runners on every update in `_process_market_books` to see if a price change occurred) but I'm darned if I can find a reference to the flumine instance at that point (in order to get the strategy context).

*Tags: Strategies*

---

**liam** - *08:49:16*

this help?



```    def process_market_book(self, market, market_book):

        for runner in market_book.streaming_update.get("rc", []):

            if "ltp" in runner:

                print(" ", runner["id"], runner["ltp"])```

*Tags: General Technical*

---

**ThomasJ** - *12:07:49*

Thanks Liam very much.



 `process_market_book` is dependant on `check_market_book` whereas I want all LTP history. I settled on updating the history as per my original choice of "at `update_cache &gt; if "ltp" in new_data`".



When I posted my original Q I did not realize that very deep with a flumine instance there lurked the RunnerBookCache, so I propagated the 'LTP history' to the RunnerBook.



After my enlightenment I use the cache everywhere.

*Tags: General Technical*

---

**S G** - *16:02:22*

Hi All 

How are you maintaining cert files required for betfair login in cloud? Considering the boxes are choosen at random/or a free box is choosen. For example in aws

*Tags: Deployment*

---

**S G** - *17:45:30*

Betfair recommends creating certs, from what i understand from your previous msg, its not needed and bflw/flumine still works smoithly without certs?

*Tags: General Technical*

---

## 2021-09-24

**Finn** - *05:06:37*

Hi everyone! Loved reading back through the chat. I've just started using BFLW, and I was wondering what the strategy is for filtering streams. Even if I filter (streaming_market_filter) to the minimum of what I want (Horse Racing, AU, win &amp; place), I still have 317 markets, far over my allowed 200. Should I open one stream for win markets, and one stream for place? Plenty more stupid questions to come I'm sure, cheers.

*Tags: Strategies*

---

## 2021-09-25

**Unknown** - *16:19:46*

I was just doing a sanity check over my live strategy to check it was on-course and I just wanted to give a shout-out to how solid the back-testing is in flumine :chart:.

Top graph is actual behaviour over the last couple of weeks, and bottom graph is the results over the same time using back-testing.

*Tags: Deployment, Strategies*

---

**AndyL** - *17:39:06*

Given my recent performance and reliance on strategies born from backtesting, I'm not sure it matters, quite happy to get i told you so from [@UGV299K6H](@UGV299K6H) on that one!

*Tags: Performance*

---

**Jack** - *17:40:16*

Recent performance not been too good?

*Tags: Performance*

---

**Jack** - *19:56:56*

[@U4H19D1D2](@U4H19D1D2) could you explain what you mean by this please?

&gt; min bet round down sub 2 (closure)

&gt; 

Can you only stake £0.50 per bet if you want then no problem?

*Tags: General Technical*

---

## 2021-09-26

**ShaunW** - *21:54:05*

Thanks Jack. I was just interested in whether the final difference was a slow accumulation or a series of bigger diffs. I think that answers the question pretty clearly.

*Tags: Performance*

---

## 2021-09-27

**thambie1** - *22:29:20*

I've been seeing some consistent (but low levels) of conflation with my system, and after a lot of testing, I'm starting to think the problem may be on Betfair's end. I tried out bflw to see if the problem was just with my system, and also getting conflation with that too. Anyone else monitoring conflation and getting better results for &gt; 300 market updates per second?

*Tags: General Technical*

---

## 2021-09-28

**JC** - *13:17:32*

Hi everyone, I've been running an old version of the S3 Market Recorder and am trying to convert all of my old recorded zip files to gzip on S3. EC2 bash script seems like potentially the best option, but will need a larger instance as inflating and zipping takes ages. Anyone had any experience with this or got an easy/serverless way to do it? Cheers, Joe

*Tags: Data Quality, Deployment*

---

**liam** - *13:22:57*

```import os

import sys

import zipfile

import gzip

import boto3

from io import BytesIO

from concurrent import futures

from tqdm import tqdm



"""

Move s3 to s3

"""



s3_client = boto3.client("s3")



BUCKET = "flumine"

NEW_PREFIX = "marketdata/marketBook"

OLD_PREFIX = "marketdata/streaming/{0}"

# threading

WORKERS = 4

CHUNKS = 10





def chunks(l: list, n: int) -&gt; list:

    for i in range(0, len(l), n):

        yield l[i : i + n]





def upload_object(object, key, metadata):

    return s3_client.put_object(

        Body=object,

        Bucket=BUCKET,

        Key=key,

        Metadata=metadata,

    )





def create_new_compressed_file(zf, contained_file):

    with zf.open(contained_file, "r") as f:

        file_compressed = gzip.compress(f.read())

    return file_compressed





def process_files(files):

    for file in files:

        # download file

        response = s3_client.get_object(Bucket=BUCKET, Key=file["Key"])

        _zip_data = response["Body"].read()

        try:

            with zipfile.ZipFile(BytesIO(_zip_data)) as zf:

                for contained_file in zf.namelist():

                    market_id = contained_file

                    gz_object = create_new_compressed_file(zf, contained_file)

                    key = "{0}/{1}.gz".format(NEW_PREFIX, market_id)

                    upload_object(gz_object, key, response["Metadata"])

        except Exception as e:

            print("Error in market {0}".format(market_id), e)





def process(event_type_id):

    print("Starting process on eventTypeId: {0}".format(event_type_id))

    old_prefix = OLD_PREFIX.format(event_type_id)

    paginator = s3_client.get_paginator("list_objects_v2")

    page_iterator = paginator.paginate(

        Bucket=BUCKET, Prefix=NEW_PREFIX

    )

    processed = []

    for page in page_iterator:

        for object in page["Contents"]:

            file_path = os.path.basename(object["Key"])

            processed.append(file_path[:-3])



    print("{0} files already processed".format(len(processed)))



    event_type_ids = [7]

    for event_type_id in event_type_ids:

        print(event_type_id)

        pcs, skp = 0, 0

        page_iterator = paginator.paginate(

            Bucket=BUCKET, Prefix=OLD_PREFIX.format(event_type_id)

        )

        for page in page_iterator:

            for object in page["Contents"]:

                file_path = os.path.basename(object["Key"])

                if file_path[:-4] not in processed:

                    pcs += 1

                else:

                    skp += 1

        print("{0} files to process ({1} skipped)".format(pcs, skp))

    return



    page_iterator = paginator.paginate(

        Bucket=BUCKET, Prefix=old_prefix

    )

    skp = 0

    files_to_process = []

    for page in page_iterator:

        for object in page["Contents"]:

            file_path = os.path.basename(object["Key"])

            if file_path[:-4] not in processed:

                files_to_process.append(object)

            else:

                skp += 1

    print("{0} files to process ({1} skipped)".format(len(files_to_process), skp))



    with futures.ThreadPoolExecutor(max_workers=WORKERS) as executor:

        _process_jobs = []

        for files in chunks(files_to_process, CHUNKS):

            _process_jobs.append(executor.submit(process_files, files=files))

        for job in tqdm(futures.as_completed(_process_jobs), total=int(len(files_to_process)/CHUNKS)):

            job.result()  # wait for result

    print("{0} files processed".format(len(files_to_process)))





if __name__ == "__main__":

    event_type_id = sys.argv[1]

    process(event_type_id)```

*Tags: Errors Debugging*

---

**JC** - *13:29:25*

Thanks so much, did you run this on a decent EC2 instance for speed?

*Tags: Performance, Deployment*

---

## 2021-09-29

**Oliver Varney** - *10:56:17*

Is the streaming_market_filter set up at the strategy level (and any necessary streams)? Will having a single flumine instance running strategies with just WIN GB and IRE markets filter and another strategy with PLACE GB and IRE filter cause any issues?

*Tags: Strategies*

---

**liam** - *10:58:05*

[https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/streams/streams.py#L83](https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/streams/streams.py#L83)

*Tags: General Technical*

---

**Oliver Varney** - *11:01:19*

Cool so does that mean essentially that it will create a separate stream for the win strategy based markets and then another for place strategy markets. I guess what I want to  double check, is that if I have 3 strategies that are win and 1 that is place that, the 3 win only process market books for win markets and the place strategy only processes market books for the place markets. Or do I need to add code for that ?

*Tags: Strategies*

---

**liam** - *11:03:28*

[https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/strategy/strategy.py#L91](https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/strategy/strategy.py#L91)

*Tags: Strategies*

---

**liam** - *11:06:40*

flumine handles it, few edge cases but it trys [https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/markets/middleware.py#L76](https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/markets/middleware.py#L76)

*Tags: General Technical*

---

**liam** - *11:14:06*

right, yes if you are recording prices (not orders) and backtesting out of flumine then yes you would

*Tags: General Technical*

---

**Oliver Varney** - *11:20:46*

Im sure someone will correct me, you either adjust a stored price (old market book) for every new non runner as they come through, or at the end with the product of adjustment factors of removed runners. Flumine adjusts orders straight away, so with a custom middleware you could call your own function and adjust the old market_book for example.

*Tags: General Technical*

---

**Oliver Varney** - *11:34:32*

an old price will need adjusting when a runner is removed to have any kind of comparable value. imagine a three horse race with Frankel and two very poor horses. Say you have come up with a price about an hour ago which you want to lay the poor horses at and stored it somewhere (like flumine context). Frankel is a late non runner. If you dont adjust that price of the two runners left then you may think your getting an amazing lay price when in reality it may not be with Frankel removed.

*Tags: General Technical*

---

**Jono** - *13:12:20*

Occasionally when altering an order either by cancelling entirely or adjusting the odds in my strategy, something causes the instruction to not run as expected and result in an order of status VIOLATION being produced. It seems to me to be indicative of the original bet being partially matched as i try to submit the cancel request but could very well be something else im not aware of. The new order correlating to the remaining unmatched amount of the original stake tends to be quite small, usually no more than a few tens of pence but then is unable to be cancelled in order to start the strategy cycle successfully again. The strategy heavily relies on making use of the max_live_trade_count = 1 in order to stay in check and not over place but orders of status VIOLATION contribute to this count resulting in progress stagnating until the remaining amount is matched as i cannot seem to cancel it.



Is there a way to deal with orders of status VIOLATION either by forcing cancellation or ignoring them in the max_live_trade_count or some other way i'm missing?



Additionally what tends to cause them so i can avoid generating these awkward orders?



Thank you

*Tags: Deployment, Strategies*

---

**Jono** - *13:35:57*

sorry i dont have anything particularly insightful to hand log wise other then times of different market transactions taking place and a list of the different orders placed . Need to make better use of the logging worker. the documentation has a comment inside the OrderStatus class stating beside VIOLATION definition that # order never placed due to failing controls

could you possibly elaborate on what this means? Is it something that is generated from both failed placings on betfair due to incorrect instructions sent to the api and/or breaches of the strategy variables ie max_live_trade_count?

*Tags: Errors Debugging, Deployment, Strategies*

---

**liam** - *13:56:44*

Need to see python logs as per the examples (this isn't the loggingcontrol) Could have failed on any of the default controls, have a look at the controls directory to see them

*Tags: Errors Debugging*

---

**Jono** - *14:18:29*

hmmmm potentially the third one although having seen these errors flag up before i put in some measures to stop them from happening with getValidBetSize checks adnd imported fumine util funcs like price_ticks_away, get_nearest_price etc but at some points there may be exceptions slipping through so ill be sure to tighten up the logging i have in place to identify when the controls are breached. Once an order is of status VIOLATION does exist tho, what are the consequences of utilising "force=True" specifically when all i want to do is simply cancel the remaining amount? basically if order.status == VIOLATION:  "market.cancel_order(order, force=True)"  exactly as liam originally posted. Could this have consequences i should be aware of? i know the validate control checks are there for good reason

*Tags: Errors Debugging*

---

**liam** - *14:23:55*

the controls are there to protect and prevent something stupid, by forcing you are overriding the controls so you are on your own, I would enable logging and fix the issue

*Tags: Errors Debugging*

---

**Peter** - *14:43:46*

To force you also need a fairly recent version of Flumine - at least 1.19.7, I believe.

*Tags: General Technical*

---

**Finn** - *14:45:07*

What’s the recommended method to record prices into a custom DB? Flumine middleware? Reading this talk about adjustment factors made me worried I’d miss some element of the data (such as recording scratching times, something I now know I need to add), and I’d be unable to test my strategies even slightly accurately.

I think it matters that I am interested in fundamentals, and I would just like to see when I can place $20-50 bets at a particular price. Nothing high performance required.

*Tags: Performance*

---

**Peter** - *14:49:48*

Recommended method is to persist live market streams, including price data, to files and then re-stream from there as many times as you need to test different placing strategies, at which time you could use logging control to write to a database - though I suspect many will write to a csv file or similar for ingestion into a Pandas dataframe.

*Tags: Feature Engineering, Deployment*

---

**Finn** - *15:33:58*

I think I’m working against the Flumine ethos here trying to store stuff with sql. I guess if I store the raw stream and process into a (flawed) db I can always replay it later.

*Tags: General Technical*

---

## 2021-09-30

**Jono** - *08:57:12*

qq whats the best way to limit the number of markets placed on for a strategy in flumine to a set number say 10 over long run times? at the moment im manually selecting a set number of market ids in the streaming market filter but once these games have concluded i have to select more and restart the strat. Im at thte stage where i'd like to keep it running for a few days but on a small number of markets and not on 100s of markets or a whole event_type_id. Thanks

*Tags: Strategies*

---

**Oliver Varney** - *09:00:44*

If your manually picking markets with no consistent methodology then theres not much you can do, unless you use a background worker to some kind of api/database (where you manually set) which return a list of market ids for which again your would use to filter out with a wide filter

*Tags: General Technical*

---

**Oliver Varney** - *09:40:55*

bankroll management?

*Tags: General Technical*

---

## 2021-10-01

**Oliver Varney** - *09:44:16*

[@U4H19D1D2](@U4H19D1D2) how does the event linking work in market.event? I want to link a WIN market to a PLACE market (essentially know the market id and a few other bits). Does this work by default or do I need certain setting on filters? Im guessing I need to have both a strategy with a WIN streaming filter and a place streaming filter?

*Tags: Strategies*

---

**liam** - *09:45:30*

works out the box, [https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/markets/market.py#L103|code](https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/markets/market.py#L103|code)

*Tags: General Technical*

---

**liam** - *12:43:33*

Identical, check out the code in streams.py, it try’s to be helpful but it’s not that clever that it will combine / share certain markets etc 

*Tags: General Technical*

---

**Finn** - *14:26:58*

I promise my questions will improve after a few weeks!

*Tags: General Technical*

---

**George** - *16:47:23*

I've been looking at the Order Streaming today and it seems like BFLW builds a cache, adds any new orders to it and returns the whole lot. However I can imagine if you are sending a lot of orders that it might be more convenient to just return the most recent order only - this would avoid the list of orders growing huge very quickly. Is there an option to do this in BFLW? Something like cache=False?

*Tags: General Technical*

---

**liam** - *16:53:11*

Ah I see, you have streaming_update available so you can see what was updated 

*Tags: General Technical*

---

**George** - *16:54:43*

true. i did think of that but wondered if there was a cleaner way to do it without having to unpick the strange acronyms myself which BFLW does very nicely. (e.g. with a cache=False option). but if not then fair enough

*Tags: General Technical*

---

## 2021-10-02

**PeterLe** - *11:56:10*

Morning, I've been running Flumine a good month now...are there any housekeeping activities i should be doing; ie clearing out logs etc, or can i just run each day and forget? Thanks

*Tags: General Technical*

---

## 2021-10-05

**Lee** - *10:40:25*

They have a priority, so i believe you'll only get one based on this order [https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/cache.py#L151](https://github.com/liampauling/betfair/blob/master/betfairlightweight/streaming/cache.py#L151)

*Tags: General Technical*

---

**liam** - *11:06:44*

I would email bdp/Neil as he might be able to share more details and yes things are a bit hacky when it comes to requesting both as bflw will serialise based on the priority Lee has shared, this was meant to be temporary but I am not sure how best to output to the user whilst matching a conventional marketBook request. However the data for both is in the cache if you want to pull it out.



Have you tried testing yourself with a quiet market? Set debug to true on the logs and see the updates coming in

*Tags: Errors Debugging*

---

**Oliver Varney** - *11:10:05*

Ive always just used the flumine default /  non-virtualised filter which would of always resulted in the first if condition returning true. I just wanted to experiment with a few things, so naively its the first time ive given it any serious thought :man-facepalming: Cool yes Ill email Neil and have a go on a quiet market

*Tags: Getting Started*

---

**liam** - *11:16:52*

if you do this in flumine watch out for potential race conditions on the `Market` object and `SimulationMiddleware` (untested)

*Tags: General Technical*

---

**liam** - *11:20:53*

both, for live you will have a single `Market` object with two different MarketBooks being sent through it (same marketId but different data because of the data filter) something to be aware of that might not impacting anything and tbh its on my list to isolate and fix the potential issue

*Tags: Errors Debugging, Deployment*

---

**EJono** - *14:38:34*

Can any action be taken of orders of status: VIOLATION ? Can funds be tied up or be unable to be cancelled as the result of an order having this status? 



I'm working on fixing the issues that cause this order status occurring but when they do it seems they cannot be cancelled through market.cancel_order(order, force=True)

*Tags: Errors Debugging*

---

**EJono** - *14:53:42*

So the most recent time was due to market suspension so nothing suspicious there but there was an occasion where it looked as if there was a small amount ~70p tied up in a partially cancelled bet of status VIOLATION, partially matched £24.30. This then filled the maxlivetradecount=1 limit and I could not move on as I was unable to cancel the remaining amount. I was wondering if this was something expected to be encountered or if it is more likely a one off fuck up my side (misinterpreting my logs). I've made changes over the past week and haven't witnessed this problem since but am curious if i should have violation checks in place for bets that cannot be altered but have small remaining amounts

*Tags: Deployment*

---

## 2021-10-07

**Kunal Maneck** - *07:14:41*

Does Flumine keep orderbook in memory for query, and can it generate orderbook from cricket tick data?

*Tags: Performance*

---

## 2021-10-08

**George** - *16:12:40*

This is a very strange question but I think it is relevant nonetheless...

When you login with BFLW you run the following code:

``` response = [http://session.post|session.post](http://session.post|session.post)(

                self.url,

                data=self.data,

                headers=self.client.login_headers,

                cert=self.client.cert,

                timeout=(self.connect_timeout, self.read_timeout),

            )```

And `self.data` looks like:

```{"username": "TestUsername", "password": "TestPassword123!"}```

*Tags: General Technical*

---

**George** - *16:13:06*

My problem is: TestPassword123! gets turned into TestPassword123%21 during the [http://session.post|session.post](http://session.post|session.post)() function.

*Tags: General Technical*

---

**George** - *16:15:28*

I looked into this and the StackOverflow answer says - don't pass `self.data` as a dictionary, pass it as a string, and this resolves the issue.

But, I don't have the option to do this with BFLW. So what should I do?

*Tags: General Technical*

---

**Matthieu Labour** - *22:01:38*

Hi,

For Tennis, I see markets that start w/ a `1` such as `1.188732649"` . Is it always  `1`?

I read the following for the getAccountStatement / StatementLegacyData betfair API.

```eventId: Please note: this is the Id of the market without the associated exchangeId```

Where can I get the exchangeId or can i assume it will be `1`? Thank you for your help and guidance.

*Tags: General Technical*

---

## 2021-10-09

**AndyL** - *20:43:09*

[@U4H19D1D2](@U4H19D1D2) hi, question on size reduction cancel_order, i seem to occasionally see a cancel execute, instruction report says success, i then see order status update: executable, but the size_remaining is still the old size and size_cancelled is 0. No subsequent order updates occur...

Can i force a query of order in betfair, to check actual status?

*Tags: General Technical*

---

## 2021-10-11

**liam** - *08:19:39*

Can you create a github issue for this and we can look to fix it?

*Tags: Errors Debugging*

---

**thambie1** - *09:02:20*

No clue. [@U0128E7BEHW](@U0128E7BEHW) is the only person I'm aware of that does fundamental football modelling. Perhaps he has a better answer.

*Tags: Strategies*

---

**liam** - *10:29:48*

it will never be placed as you will get an error due to the market not being open on placement

*Tags: Errors Debugging*

---

## 2021-10-12

**Ben** - *14:11:38*

Thanks for the help guys, will have a look at these

*Tags: General Technical*

---

**Javier Martín Pérez** - *15:15:22*

listener = betfairlightweight.StreamListener(output_queue=output_queue)

*Tags: General Technical*

---

**Javier Martín Pérez** - *15:15:34*

stream = trading.streaming.create_stream(listener=listener)

*Tags: Strategies*

---

**Javier Martín Pérez** - *15:16:37*

INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: 1.189088929 added, 9892 markets in cache

INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: 1.189089012 added, 9893 markets in cache

WARNING:betfairlightweight.streaming.stream:[MarketStream: 1]: Latency high: 48.739455461502075

WARNING:betfairlightweight.streaming.stream:[MarketStream: 1]: Latency high: 48.65541315078735

WARNING:betfairlightweight.streaming.stream:[MarketStream: 1]: Latency high: 48.55740690231323

WARNING:betfairlightweight.streaming.stream:[MarketStream: 1]: Latency high: 48.459402322769165

*Tags: Performance*

---

**Javier Martín Pérez** - *15:17:01*

Any ideas on what am I doing wrong? Thanks for your help :).

*Tags: General Technical*

---

**liam** - *21:13:01*

Can you share some code to replicate? It’s odd that you are calling stop at all tbh when it comes to streaming unless you are trying to gracefully shut everything down 

*Tags: General Technical*

---

## 2021-10-13

**Javier Martín Pérez** - *09:44:52*

As you can see I have used the "examplestreaming" and adapted it to get a snap of the odds for my use. Probably an odd use of the streaming too but seems to do the job other than that difficulty to stop the stream after checking the odds.

*Tags: General Technical*

---

**liam** - *09:45:18*

dont use streaming for this

*Tags: General Technical*

---

**Jono** - *14:58:42*

Related to a flumine based question i asked a while back, ive recently got the streamable market limit increased from 200 to 1000 and off the back off advice from [@ULDAVFDRP](@ULDAVFDRP) kept my market filter wide for my startegy. I then work out which markets to place on using logic inside check_market_book, and in middleware etc beyond the initial strategy set up which specifies the only the event_type=1 and market_type=MATCH_ODDS. I was wondering the best way to manually stop market books being streamed from markets i am not interested in, after framework.run() and prior to them closing automatically at market close. At the moment i am capping the number markets desired for placing to a fixed number currently 20 and then using



self.remove_market(market.market_id)



inside check_market_book to halt the streaming information correlating to these excess markets. However i can still see in a background worker using len(flumine.markets) that there still remains ~600 markets and so I was wondering if im actually correctly removing them, and if im not what is the best way to do so? and best location ie inside middleware, worker, etc as opposed to in check_market_book? Furthermore if i wanted to start up streaming a new market post run() how would i go about this?



Thank you!

*Tags: Errors Debugging, Strategies*

---

**Jono** - *15:17:18*

should streaming ~1000 markets but only actively running pmb on 20 have any noticeable effect on the performance of the strat on these 20? I know [@U4H19D1D2](@U4H19D1D2) you have mentioned before that number of markets shouldnt noticebly effect performance but now i am streaming 2 orders of magnitude more markets than last month i wanted to be certain since im still wrestling with latency issues ~0.5-1 seconds, even when all that is occurring in process_market_book is a log message

*Tags: Performance*

---

**liam** - *15:25:07*

the heavy CPU is done in bflw so removing from flumine doesn't do much and as you are finding out anything remove gets re-added as it gets pushed from bflw. The latency issues will probably be due to some lazy code?

*Tags: Performance*

---

## 2021-10-14

**birchy** - *13:34:50*

`"""Returns strategy/selection exposure, which is the worse-case loss arising

        from the selection either winning or losing. Can be positive or zero.

            positive = potential loss

            zero = no potential loss

        """`

Seems obvious that it returns the "worst-case" loss for the runner winning or losing, but in my example above, the worst-case loss is £0 and not the £5 I'm seeing? Maybe I'm overlooking something obvious?

*Tags: Strategies*

---

**Aaron Smith** - *15:36:40*

[@U016TGY3676](@U016TGY3676) not sure if i fully understand, but the selection_exposure only considers orders on this specific selection, as it only loops over

```for order in self.strategy_selection_orders(strategy, *lookup[1:]):```

therefore it will give you the exposure for this selection if this selection was the only selection with any orders to it.

*Tags: Strategies*

---

## 2021-10-16

**Mo** - *12:09:02*

Not sure [@ULDAVFDRP](@ULDAVFDRP), the missing market and event names were causing fatal errors all over my platform. A silly thing not to be robust against but it does reflect how unusual it is (haven't had to deal with such an issue in over three years)

*Tags: Errors Debugging*

---

**liam** - *12:10:03*

Hmm, I haven’t had any errors on this, probably concerning 

*Tags: Errors Debugging*

---

**Mo** - *12:10:27*

Depends how reliant you are on market catalogues, sounds like streaming is unaffected due to lack of names there

*Tags: General Technical*

---

**D C** - *12:13:57*

I was getting 503 errors on the stream about 10 mins ago and killed my bots. Not tried since will restart just before racing starts

*Tags: Errors Debugging*

---

**liam** - *12:19:33*

I normally have issues with other parts like running out of the free tier for sentry due to the number of errors 

*Tags: Errors Debugging*

---

## 2021-10-17

**Mo** - *07:26:38*

Any idea what the latency is like with sportsmonks?

*Tags: Performance*

---

## 2021-10-18

**rjj** - *18:57:08*

Question what would be the most appropriate place in a strategy to place a 'sibling' bet on another exchange like smarkets or matchbook. I've tried doing it in process_market_book but if the other exchange is slow in responding then the whole framework complaina of latency. If I try and background the call in a separate thread this seems to cause all manner of difficult to see issues. Any ideas?

*Tags: Performance, Strategies*

---

**rjj** - *21:09:03*

I just get ever increasing latency warnings. Yeah im logging the returned bet ids in one message hence I need to join them at some point. Maybe logging is the issue I will try and surround it with a lock see if that helps

*Tags: Performance*

---

**liam** - *21:13:12*

How many threads / bets? You might want to look at a pool instead. Profiling will help tell you the issue although it can get very tricky with threads 

*Tags: General Technical*

---

**Ke** - *22:40:42*

Is it possible to store the streamed data to a proper database, rather than individual files, which can be further used for back testing?

*Tags: General Technical*

---

## 2021-10-19

**ThomasJ** - *02:13:10*

[@U01UN2R5LM8](@U01UN2R5LM8) If my memory serves me correct...the consensus from prior discussions here is that it's best to store the stream as raw data (code is provided by Flumine) and then you can do as you please with that. Eg in your case, read the stored raw data, apply some code to it, store whatever you like in a DB.

*Tags: Performance*

---

**Peter** - *12:20:27*

[@U01UN2R5LM8](@U01UN2R5LM8) What Thomas said. Re-streaming the raw data from a file is pretty fast, and very flexible. Once you start storing the data in a database you're either losing information or adding complexity if you go the relational route, or having to pull lots of records (which is slower than re-streaming) if you go schema-less / document-based.



My approach is a compromise. I have a daily job that re-streams the previous day's files to extract meta information and adds it to a database-held index of markets. Then when I want to backtest, I query the market index and pass the files matching my criteria to my backtesting script.

*Tags: Performance*

---

**Alessio** - *13:05:08*

ah yes, i guess they also cost non trivial amount of money. in this case it's enough for me because it's a lot of historical data and my algorithm is not too sensitive around there (getting goals is basically 'state change', and that's it).

*Tags: Data Quality*

---

**Mo** - *13:08:34*

I'm asking because I'd expect high latency to be most punishing when you don't cancel in time and maybe this is not something you care about

*Tags: Performance*

---

**Alessio** - *13:36:53*

Indeed, i'm not at the level yet where i can invest in lower latency feeds fees, so I spend quite some time on my side to detect the risk of having to cancel pretty soon and anticipate that

*Tags: Performance*

---

## 2021-10-20

**ThomasJ** - *12:19:16*

[@U01UN2R5LM8](@U01UN2R5LM8) You said "But if doing so does not speed up the backtesting, i guess that is pointless." ... well it might speed things up if you summarize data...but what to summarize? You'll find that over even a short time of backtesting you'll want to look at data in many different ways and you'll keep on producing different summaries. So the production of the summaries and then testing on them will probably be way longer than just reading from the raw data, putting it into memory via, as liam mentioned a Pandas DataFrame (I presume it's Pandas), and then you can look at the data in an infinite number of ways directly, and super fast.

*Tags: Feature Engineering, Performance, Deployment*

---

## 2021-10-21

**Mike Walpole** - *12:31:43*

In a way, this strategy kinda likes when prices of my horse soften (“the worse the fill the better” as Soros once said) 

*Tags: Strategies*

---

## 2021-10-22

**Vinny Banks** - *10:47:30*

Morning guys. New to slack so cut me some if I'm barking up the wrong tree. I am trying to build an auto bet machine using Python. I have created the certs and exported them but keep getting the following error message



APIError: None

Params: None

Exception: HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:3991)')))



Also, when I try to upload it to my security area on my betfair account it is saying it's invalid. Is the Cert generation out of date and I'm missing something? Any hints or tips would be appreciated

*Tags: Getting Started, Errors Debugging*

---

**Vinny Banks** - *11:32:05*

Now I have a login error

*Tags: Errors Debugging*

---

**Vinny Banks** - *11:36:25*

LoginError: API login: INVALID_USERNAME_OR_PASSWORD

*Tags: Errors Debugging*

---

**PeterLe** - *12:51:01*

[@U02KDRHR8AC](@U02KDRHR8AC) I (like many!) had issues getting this set up initially..I made a note in the issues channel which may help a little

[https://betfairlightweight.slack.com/archives/C4H05ML2E/p1590763027445700](https://betfairlightweight.slack.com/archives/C4H05ML2E/p1590763027445700)

Dont forget; if you have two set set up you need to make provisions for that too

*Tags: General Technical*

---

## 2021-10-24

**Adrian** - *06:34:34*

What's the most Flumine way to track a specific bet? Let's say I place 3 bets with order id's 1, 2 and 3. I only want to place a fourth bet if order id 2 is fully matched? Only thing i can think of is store the order id's in a custom class.

TIA!

*Tags: General Technical*

---

**liam** - *07:11:18*

strategy.context 

*Tags: Strategies*

---

## 2021-10-25

**Aaron Smith** - *10:13:21*

I was referring to the previous question, so only talking about IP and took 1 second as an example, maybe i should ve been more clear about that.

*Tags: General Technical*

---

## 2021-10-26

**Lennart** - *16:11:50*

Hi everyone, wondering if anyone here can recommend a live data provider for NBA games or Basketball in general? Of course, ideally, I'd love a free one. Also wondering - anyone else here trading on Basketball markets?

*Tags: Deployment, Strategies*

---

**Mo** - *16:28:10*

We were trialing I think the BetRadar feed which at the time was the official feed with the lowest latency at a cost of $250,000 per season. This was prior to the opening up of the US market so I wouldn’t be surprised if you’d pay a lot more for the equivalent feed today

*Tags: Performance*

---

**Mo** - *21:29:30*

Perfect question for [#C02J4CBA0H5|betfair-news](#C02J4CBA0H5|betfair-news) 

*Tags: General Technical*

---

## 2021-10-27

**Lennart** - *09:16:49*

Thanks, Mo. "Data feed" was perhps a bit broad - as a starter I'm looking for accurate and low-ish latency game scores and game clock. Low latency compared to BF's inplay service which seems to be often delayed by 10-20 seconds.

*Tags: Performance*

---

## 2021-10-28

**EJono** - *11:12:44*

I've had an increase recently in the number of markets I am able to stream from 200 to 1000 after being advised here to email Betfair for an increase which has helped tremendously with testing out one of my strategies on all football games listed. During the week this is more than enough to cover all market_type=MATCH_ODDS,  event_type_id=1 markets set up in the streaming filter but at the weekend the number of FB matches often goes above this to ~1100-1300. At such times unless I'm specifying the marketIds I would like to place I get an error message from flumine suggesting I am attempting to exceed my market stream allowance of 1000. I was wondering if there is a good way of closing markets or setting up an additional filter to cap the number of markets to 1000 (in closest to market open order) whilst I wait for Betfair to get back to me about further increasing my streamable markets?



Cheers!

*Tags: Errors Debugging*

---

**Peter** - *11:20:24*

I have 1000 too and so divide my soccer streaming into two groups determined by country code. The first has GB, DE, ES, IT and FR and the second has everything else.

*Tags: General Technical*

---

**EJono** - *11:24:39*

Does this require two instances of flumine running? More specifically, two seperate trading objects with distinct login sessions that you then run the exact same strategy on barring the country code filter?

*Tags: Strategies*

---

**EJono** - *12:17:47*

Really that's incredible, do you think this could be taken advantage of? Say for instance start up flumine deliberately delay the full connection of streamed markets for a minute or two then begin adding connections.

*Tags: General Technical*

---

**Lee** - *12:30:02*

You can just have two market_filters, flumine will create two connections and handle this for you

```framework.add_strategy(

    strategy=StrategyOne(

        market_filter=streaming_market_filter(

            event_type_ids=["1"],

            market_types=["MATCH_ODDS"],

            country_codes=["GB", "DE"],

        ),

    )

)



framework.add_strategy(

    strategy=StrategyTwo(

        market_filter=streaming_market_filter(

            event_type_ids=["1"],

            market_types=["MATCH_ODDS"],

            country_codes=["ES", "IT"],

        ),

    )

)```

*Tags: Strategies*

---

**EJono** - *12:49:15*

Thank you very much guys all the solutions are incredibly helpful

*Tags: General Technical*

---

**liam** - *14:22:13*

Maybe we should add some code to handle multiple filters, if you create an issue we could add it, e.g:



```framework.add_strategy(

    strategy=StrategyOne(

        market_filter=[

            streaming_market_filter(

                event_type_ids=["1"],

                market_types=["MATCH_ODDS"],

                country_codes=["GB", "DE"],

            ),

            streaming_market_filter(

                event_type_ids=["1"],

                market_types=["MATCH_ODDS"],

                country_codes=["ES", "IT"],

            ),

        ]

    )

)```

*Tags: Strategies*

---

## 2021-10-29

**ThomasJ** - *10:56:39*

In *back testing* I am trying to get some values from EX (runner) at market close. However Betfair, in the *2nd last stream*, send an "mc" that contains all vols == 0 and so BFLW makes all EXs == [].



I've been lurking about here for some months and don't recall this being mentioned, but of course I could be wrong. So this leads me to conclude I am not understanding something about collecting EX data at market close.

*Tags: General Technical*

---

**Oliver Varney** - *21:21:00*

I spoke to at least one BA user last night. He was a nice guy and was  looking to get into python and bflw, so the post was worth while :+1:

*Tags: General Technical*

---

**Tom** - *21:45:10*

Hi Does anyone have the json output of runners? I can't seem to find an example on the repo for getting the odds dict or list.

```@shared_task

def parse_runners(market, items):

    """Parses runners from MarketCatalogue object"""

    runners = []

    for runner_item in items:

        with open('runners.json','w') as outfile:

            json.dumps(items,outfile)

        with open('runners2.json','w') as outfile:

            json.dumps(runner_item,outfile)

        runner, created = Runner.objects.update_or_create(

            selection_id=runner_item['selectionId'],

            defaults={

                'market': market,

                # default

                'name': runner_item['runnerName'].upper(),

                'sort_priority': runner_item['sortPriority'],

                'handicap': runner_item['handicap'],

                'runner_id': runner_item['metadata']['runnerId'],

            }

        )

        if created:

            [http://logger.info|logger.info](http://logger.info|logger.info)(f'Created {runner}')

        else:

            logger.debug(f'Updated {runner}')

        runners.append(runner)

    return runners```

Using the above code I have I can get runner_id's and save to my db.  I can't seem to print the json object and see the response to traverse through. Does anyone have a sample json response of list marketcatalogue so I can see where the odds are hiding?

*Tags: Errors Debugging*

---

## 2021-10-31

**casper** - *17:58:13*

in streaming, if no message of any kind is received (i.e. no data update or ChangeType.Heartbeat messages) for more than heartbeat interval, would an error be raised?

*Tags: Errors Debugging*

---

**Oliver Varney** - *17:58:57*

There is a snap function in flumine

*Tags: General Technical*

---

**casper** - *18:00:03*

for example, in the examplestreamingerrhandling.py would this throw BetfairError and attempt to resubscribe using latest clk?

*Tags: Errors Debugging*

---

**liam** - *18:10:11*

Yeah socket would timeout and raise an error causing the resubscribe 

*Tags: Errors Debugging*

---

## 2021-11-01

**C0rnyFlak3s** - *21:45:30*

Hello! I am new to this Slack and I really like it so far. I am not sure if this is the right sub channel to ask my question, however here it goes: Which libraries do you guys use for plotting your streaming data? Odds histories, ladder informations ect. Is there any good plotting library out there which offers this functionality?

*Tags: Getting Started*

---

## 2021-11-02

**Alex Morton** - *06:48:19*

Hi, I’m trying to print out the best back/lay size and price for a particular race. The problem I’m running into is it seems to have a minimum size of about $18 AUD (which is 10 GBP). I can see what its doing is if theres not at least size 10 for for the best price it will go to the next best price on the ladder and add the sizes for each price until it hits 10 GBP. I would like to remove this 10 GBP limit thing so I can just see the best price regardless of the size. Right now I’m just using the tutorial code to print out runner books. If anyone knows why this happens and how to stop it please let me know. Thanks in advance.

*Tags: General Technical*

---

**Alex Morton** - *08:01:51*

[https://betfair-datascientists.github.io/api/apiPythontutorial/](https://betfair-datascientists.github.io/api/apiPythontutorial/) I used this tutorial. The code I used was pretty much copied directly from the from “Get Market Books” section on this page.

*Tags: General Technical*

---

**Alex Morton** - *08:14:18*

`price_filter = `betfairlightweight`.filters.price_projection(`

    `price_data=['EX_BEST_OFFERS']`

`)`

*Tags: General Technical*

---

**ThomasJ** - *10:18:41*

There's a lot of code in BFLW and Flumine associated with the "clk" data in the stream which I have not looked at in detail as I presume it is about catching up with missed data during a disconnection and then used by the reconnect code to "catch up". Does that about sum it up or do I need to look at it in more detail to be a top notch BFLW and Flumine user?

*Tags: General Technical*

---

**Aaron Smith** - *11:09:47*

looking at that picture i feel a lot less confident using bflw/flumine and blindly trusting that liam did a good job :smile:

*Tags: General Technical*

---

**Aaron Smith** - *11:12:35*

with his finger hovering the enter button you know he just pushed an update to flumine and his face tells you that he is now waiting for the disasters to happen for the flumine users while he sips his tea

*Tags: General Technical*

---

## 2021-11-03

**Alex Morton** - *07:53:23*

`mcf = betfairlightweight.filters.market_filter(`

    `market_betting_types=["ODDS"],` 

    `race_types= ["Flat"],`

    `market_countries=['AU'],`

    `event_type_ids=['7'],`

    `market_type_codes=["WIN"],`

    `market_start_time={`

            `'to': (datetime.datetime.utcnow() + datetime.timedelta(days=0.25)).strftime("%Y-%m-%dT%TZ")`

        `}`

    `)`



`market_catalogues = trading.betting.list_market_catalogue(`

    `filter=mcf,`

    `max_results="40",`

    `sort = "FIRST_TO_START")`

*Tags: Strategies*

---

**Alex Morton** - *07:54:05*

`price_filter = betfairlightweight.filters.price_projection(`

    `price_data=['EX_BEST_OFFERS']`

`)`

*Tags: General Technical*

---

## 2021-11-04

**captainonionhead** - *10:46:13*

Morning, if I have a simple strategy to allow recording of data (as recommended multiple times here), do I need to bet in all the market types that I've subscribed to in order to avoid upsetting betfair?  E.g. if I've subscribed to both "win" and "place" horse racing markets for recording do I need to bet in both of those, or is it OK to just bet in one of them (e.g. win).

Also, from watching markets, I think my initial strategy will only bet in about 10% of the markets (or only 5% of those subscribed if it is OK to subscribe to two markets per event).  I'm guessing this will be just a handful of bets per day - is this too few or should I be OK?

Thanks!

*Tags: Strategies*

---

**Mr West** - *11:28:26*

Quick question guys, I know how much has been traded on a selection but is there a way to find out if it’s a back or lay trade?

*Tags: General Technical*

---

**liam** - *11:32:47*

flumine does it to calculate simulated matching via middleware

*Tags: General Technical*

---

**C0rnyFlak3s** - *17:13:38*

So according to [@U01NJ85MP7F](@U01NJ85MP7F) there may be issues if I only fetch streaming data without trading in the markets for a longer period of time? What will happen in the case I stream data for multiple days to create my own historical data? Is this unwanted behavior by the betting exchange, and what will happen if I do so? Answers are greatly appreciated before I finally start my data stream :slightly_smiling_face:

*Tags: Data Quality, Strategies*

---

**C0rnyFlak3s** - *17:16:16*

Thanks for replying to the question and giving this advice, however it lacks the answer to the question what would happen in the case I just stream data without placing bets. Would the account be terminated, the API access blocked for a limited amount of time or will nothing happen at all? Is there any source on this issue?

*Tags: General Technical*

---

## 2021-11-05

**Steve** - *03:59:14*

If I want to run concurrent entirely independent instances of flumine on the same machine but don't want one to be picking up the orders from the other what do I need to change? Is it the betfairlightweight config hostname? Or the strategy name? Or something else?

*Tags: Strategies*

---

**Aaron Smith** - *12:02:42*

was this not changed to config.customer_strategy_ref?

*Tags: Strategies*

---

**Ruben** - *13:34:02*

[@U01PNBCPSKT](@U01PNBCPSKT) its from the betfair documentation webpage, the section about the market data filter

*Tags: General Technical*

---

## 2021-11-06

**captainonionhead** - *10:26:16*

Please excuse the newbie questions, only just getting back to starting on this due to a long hiatus when real life got in the way...  Do people have strong views on whether to leave strategies (including recording) running 24-7, even if only betting on UK/IRE markets, for example.  Or should I use something like the terminate worker and kick everything off again the following morning?

*Tags: Strategies*

---

**Lewis** - *15:30:11*

Hi all,

Apologies if this is a stupid question, but is the customerRef field in the placeOrders function the same thing as the customerOrderRef that is seen elsewhere in the both the API-NG and the exchange stream api?

And so if I place an order with a customerRef set should I expect to see the rfo field in order changes messages in the exchange stream being populated with whatever I set it to?

*Tags: General Technical*

---

**Jeff Waters** - *19:45:22*

I have a 'for' loop which goes through the orders in my blotter for backtesting with Flumine - _for order in market.blotter._



I had been under the impression that _order.order_type.price_ was the requested price for the order in question, and _order.average_price_matched_ was the average price matched. However, there are instances for back bets where the requested price is higher than the average price matched. I don't understand that. Surely a back bet's average price matched for an order cannot be lower than the requested price?



I'm probably missing something obvious. Can anyone help please?



Thanks



Jeff

*Tags: General Technical*

---

**Jeff Waters** - *20:24:43*

On another matter, is there any way of checking if a market is in-play when backtesting in Flumine? I've tried looking for something like market.is_in_play or finding the actual off time so I can check if the current time is after then, but I've had no joy.

*Tags: General Technical*

---

**Lee** - *20:27:28*

Setting a breakpoint and inspect the objects will be helpful to find what you need

*Tags: General Technical*

---

## 2021-11-07

**Ruben** - *09:21:03*

Good morning everyone, in Flumine, is there any place in which I can store a variable and still have it available even after the stream re-starts? Context: I am trying to store the currency exchange rate to convert my bets into GBP. I have already created a worker to poll the endpoint, but I noticed that in case the stream is restarted the market.context (which is where I am currently storing the rate) gets cleaned. Ideally, I would find a place to store the exchange rate which will not get deleted/cleaned.

*Tags: General Technical*

---

**liam** - *09:51:57*

I am hybrid, recorders always running along with my strategies that work across all countries however I start UK/IE up each morning via AWS lambda / portainer. Reason being that I use quite a few instances, some are heavy on CPU so would prefer to limit markets / time running but tbh I could probably just keep them all running for a few £ each month and less complication

*Tags: Deployment*

---

## 2021-11-08

**Ke** - *22:02:59*

Anyone knows how to get the ranking of the racing. In greyhound, it looks the Ranking is stored as part of event name in Win market, but the name seems to be empty in flumine.

*Tags: General Technical*

---

## 2021-11-09

**Peter** - *16:58:14*

Seem to remember a discussion a while back about how many runners had gone to 1000 and then won. The answer (I can't verify if it's true or not) was only 3 since Betfair started. If so, laying at 1000 would surely be +EV. The biggest challenge would be having the stones to run a strategy doing it.

*Tags: Strategies*

---

**Aaron Smith** - *17:35:09*

[@U013ZS16QJZ](@U013ZS16QJZ) i dont think there is (only a filter for it it will go in_play at all), a market_book however will tell you. Best practice here i think is to subscribe to the markets you eventually want to bet in and then in check_market_book (which is always checked when receiving a market_book and if it returns True, it will go into process_market_book of your strategy) filter out all market_books that are not inplay.

*Tags: Strategies*

---

**Aaron Smith** - *18:40:40*

this is the full function, once you put it into the right place, you ll probably see clearer. The "check_market_book"-function is a method already existing in the BaseStrategy class from flumine. (I realize here i assumed that you are using flumine, which you didnt specify you do, so apologises for the confusion if you dont :smile: , in any case, if you dont maybe check it out anyway to see how it works there). Basically a strategy subscribes to some markets, whenever flumine gets a market_book event for a market this strategy has subscribed to, it will call the "check_market_book" function of this strategy. If this function returns True, it will then call the "process_market_book" function of this strategy

*Tags: Strategies*

---

**VT** - *19:44:55*

Thanks, I'm not using Flumine, just using betfairlightweight to connect the API, I need to study the codes.

*Tags: General Technical*

---

**Newbie99** - *19:50:44*

Are you streaming [@U013ZS16QJZ](@U013ZS16QJZ) or using the REST API?

*Tags: General Technical*

---

**VT** - *20:41:54*

I only use the REST API for now, I'm learning everything, I'm a trader who works for years in football markets, but only manually through GeeksToy software.



Is the streaming functionality only available to those who have an account with the correct 'vip' token? Last time I researched it cost a few hundred pounds.

*Tags: General Technical*

---

**Newbie99** - *20:46:24*

There are ways around that...if you're placing bets then [@U4H19D1D2](@U4H19D1D2) may be able to help you out.



As for your REST script, I think I might have some code for you, one sec...

*Tags: General Technical*

---

**Newbie99** - *20:55:44*

```import betfairlightweight as bf



''' Enter parameters here '''

market_ids = '1.190544753'

accname = 'account_name'

accpass = 'account_password'

acckey = 'account_key'

path = 'certificate_path'



trading = bf.APIClient(accname, accpass, acckey, certs=path)

trading.login()



filters = bf.filters

market_books = trading.betting.list_market_book(

            market_ids=[market_ids],

            price_projection=filters.price_projection(

                price_data=filters.price_data(ex_all_offers=True, sp_traded=True, sp_available=True)

            ),

            order_projection="ALL",

    lightweight=True

        )



for market_book in market_books:

    if market_book['inplay'] is True:

        print(market_book)```

*Tags: Strategies*

---

## 2021-11-10

**Newbie99** - *00:16:26*

```import betfairlightweight as bf



''' Enter parameters here '''

# market_ids = '1.190544753'

accname = 'account_name'

accpass = 'account_password'

acckey = 'account_key'

path = 'certificate_path'

event_type_ids = ['7']

market_types = ['WIN']

country_codes = ['US']



trading = bf.APIClient(accname, accpass, acckey, certs=path)

trading.login()

filters = bf.filters





market_catalogues = trading.betting.list_market_catalogue(

                    filter=filters.market_filter(event_type_ids=event_type_ids,

                                                 market_type_codes=market_types,

                                                 market_countries=country_codes,

                                                 ),

                    market_projection=[

                        "MARKET_START_TIME",

                    ],  # runner description required

                    sort="FIRST_TO_START",

                    max_results=5,

                )



market_books = trading.betting.list_market_book(

            market_ids=[m.market_id for m in market_catalogues],

            price_projection=filters.price_projection(

                price_data=filters.price_data(ex_all_offers=True, sp_traded=True, sp_available=True)

            ),

            order_projection="ALL",

    lightweight=True

        )



for market_book in market_books:

    if market_book['inplay'] is True:

        print('IN PLAY','\n',market_book)

    else:

        print('NOT IN PLAY','\n',market_book)```

*Tags: Strategies*

---

**VT** - *04:04:07*

Thanks for the code but it didn't work here, besides being a solution that seems inefficient to me, I still received:



'errorCode': 'TOO_MUCH_DATA'



I would like to list all football games that are in-play, live with the ball in play. I would like to get the ID of these events only.



It would be the same events found on the Betfair website when we click on in-play &gt; football



If anyone can help, I still haven't found a solution.



Thanks

*Tags: Errors Debugging, Deployment*

---

**Newbie99** - *09:44:57*

My code is correct, its just that there are data limits.



What data do you need for the market_book?



I'll re-do it quickly so it doesn't error, but it won't show you much!

*Tags: Errors Debugging*

---

**Newbie99** - *09:48:20*

This will return 100 markets (just change the max results, but it will cut off around the 200 mark):



```import betfairlightweight as bf



''' Enter parameters here '''

accname = 'account_name'

accpass = 'account_password'

acckey = 'account_key'

path = 'certificate_path'

event_type_ids = ['1']



trading = bf.APIClient(accname, accpass, acckey, certs=path)

trading.login()

filters = bf.filters





market_catalogues = trading.betting.list_market_catalogue(

                    filter=filters.market_filter(event_type_ids=event_type_ids,

                                                 ),

                    market_projection=[

                        "MARKET_START_TIME",

                    ],  # runner description required

                    sort="FIRST_TO_START",

                    max_results=100,

                )



market_books = trading.betting.list_market_book(

            market_ids=[m.market_id for m in market_catalogues],

            order_projection="ALL",

    lightweight=True

        )



for market_book in market_books:

    if market_book['inplay'] is True:

        print('IN PLAY','\n',market_book)

    else:

        print('NOT IN PLAY','\n',market_book)```

*Tags: Strategies*

---

**Newbie99** - *15:39:58*

For what you're trying to do, you can either:



1. Use the REST API approach, but make multiple calls (as you are limited with the amount of data you can return per call)

2. Use streaming and setup a filter for football

However, using either approach, there is no 'in-play' filter, you still need to check:



```if market_book.status == 'OPEN':

    if market_book.inplay:

        return True```

At some point.

*Tags: Getting Started*

---

## 2021-11-11

**ThomasJ** - *03:14:33*

In Flumine back testing when I want to 'get' a runner directly without iterating over all runners I use the following long winded technique to access the marketbook cache and not the marketbook as I do not see a way to do that in the marketbook. I wonder if I am missing something simple.



So to get say the best available to back price what I do is...

Get the stream's index

`stream_idx = mystrategy.stream_ids.index(market_book.streaming_unique_id)`

and then...

`mystrategy.streams[stream_idx]._listener.stream._caches[market_book.market_id].runner_dict[(selection_id, 0)].available_to_back.serialised[0]["price"]`



It just don't feel right!

*Tags: Strategies*

---

**liam** - *06:33:19*

`from flumine.utils import get_runner_book`

*Tags: General Technical*

---

**ThomasJ** - *09:18:51*

No I'm a relative newbie to Python. OK that's great to hear. Thx.

*Tags: General Technical*

---

**ThomasJ** - *09:38:16*

That's an amazing offer Liam and I thank you for it (no... I'm blown away actually)

The weight mainly consists of me storing price history, avail history,  and such and then doing calculation on them every .X of a second.



I set strategy.context to various values to control what I need and I can see exactly what hurts.



I store the accumulated stuff in market.context  whose code is called from  cache.py &gt; _def_ update_cache



I have zero secret sauce unfortunately, but many many lines of code, and countless hours. Ah the common cry of the beginner. :rolling_on_the_floor_laughing: :rolling_on_the_floor_laughing: :rolling_on_the_floor_laughing:

*Tags: Getting Started, Strategies*

---

**Peter C** - *16:28:12*

I've been running a couple of horse racing strategies on flumine for a few months now, and been working hard on refining them. In the last couple of weeks I feel like I've hit a bit of a wall - I can't make my strategies any better, and I'm struggling to look at horse racing through a different lens - I struggle to not use the tools I have already developed or approach the problem in the same way. I'm in a bit of a rut and wondering if anyone else has any advice for kicking myself out of it? I'm feeling frustrated because the last few months I've been motivated by seeing my returns increase, but now I've peaked.

*Tags: General Technical*

---

**Mo** - *16:42:13*

Read something off this list:



[https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723|https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723](https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723|https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723)

[https://www.amazon.co.uk/Trading-Bases-Fortune-Betting-Baseball/dp/0451415175|https://www.amazon.co.uk/Trading-Bases-Fortune-Betting-Baseball/dp/0451415175](https://www.amazon.co.uk/Trading-Bases-Fortune-Betting-Baseball/dp/0451415175|https://www.amazon.co.uk/Trading-Bases-Fortune-Betting-Baseball/dp/0451415175)

[https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew/dp/1079013458|https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew/dp/1079013458](https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew/dp/1079013458|https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew/dp/1079013458)

[https://www.amazon.co.uk/Precision-Statistical-Mathematical-Methods-Racing/dp/1432768522|https://www.amazon.co.uk/Precision-Statistical-Mathematical-Methods-Racing/dp/1432768522](https://www.amazon.co.uk/Precision-Statistical-Mathematical-Methods-Racing/dp/1432768522|https://www.amazon.co.uk/Precision-Statistical-Mathematical-Methods-Racing/dp/1432768522)

[https://www.amazon.co.uk/Efficiency-Racetrack-Scientific-Financial-Economics/dp/981320351X|https://www.amazon.co.uk/Efficiency-Racetrack-Scientific-Financial-Economics/dp/981320351X](https://www.amazon.co.uk/Efficiency-Racetrack-Scientific-Financial-Economics/dp/981320351X|https://www.amazon.co.uk/Efficiency-Racetrack-Scientific-Financial-Economics/dp/981320351X)

[https://www.amazon.co.uk/Kelly-Capital-Growth-Investment-Criterion/dp/9814383139|https://www.amazon.co.uk/Kelly-Capital-Growth-Investment-Criterion/dp/9814383139](https://www.amazon.co.uk/Kelly-Capital-Growth-Investment-Criterion/dp/9814383139|https://www.amazon.co.uk/Kelly-Capital-Growth-Investment-Criterion/dp/9814383139)

[https://www.amazon.co.uk/Man-All-Markets-Beating-Street/dp/1786071975|https://www.amazon.co.uk/Man-All-Markets-Beating-Street/dp/1786071975](https://www.amazon.co.uk/Man-All-Markets-Beating-Street/dp/1786071975|https://www.amazon.co.uk/Man-All-Markets-Beating-Street/dp/1786071975)

*Tags: Strategies*

---

## 2021-11-12

**river_shah** - *10:14:56*

When in doubt, improve the code. A stream of dopamine hits achieved from writing prettier code can be essential to keep morale up during tough times. Improving code also usually reveals sub-optimal components in the strategy / thought process behind the models.

*Tags: Strategies*

---

**Peter C** - *11:29:25*

Thanks, I think this is the route I'm going to go. Step away from direct strategy development and focus on improving my coding/code

*Tags: Strategies*

---

**liam** - *11:54:48*

Is this once per market? I would just chuck something in



```strategy.start```

in addition to the worker, this is [https://github.com/liampauling/flumine/blob/6216bcc233326cf07852fca9c7d39a18cee265ad/flumine/strategy/strategy.py#L102|called](https://github.com/liampauling/flumine/blob/6216bcc233326cf07852fca9c7d39a18cee265ad/flumine/strategy/strategy.py#L102|called) before the streams start

*Tags: Strategies*

---

**liam** - *11:58:04*

this is a good base [https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: General Technical*

---

**mandelbot** - *12:04:43*

so in order to add this to a framework it's just:

```from flumine.controls.loggingcontrols import backtestloggingcontrol



control = backtestloggingcontrol()



framework.add_logging_control(control)```

?

*Tags: General Technical*

---

**mandelbot** - *12:31:50*

Im running from flumine strategies folder but no CSVs there...

*Tags: General Technical*

---

**Newbie99** - *12:40:29*

Just in case it helps, this is what I started writing (got side-tracked, so haven't actually started using this yet, but you should be able to adapt):



```def check_and_create_json(file_path, json_input):

    file_exists = os.path.exists(file_path)

    if file_exists is not True:

        with open(file_path, 'w', encoding='utf8') as outfile:

            json.dump(json_input, outfile, sort_keys=True, indent=4, ensure_ascii=False, default=str)



    def _process_cleared_orders_meta(self, event):



        orders = event.event



        if len(orders) &gt; 0:

            market_id = orders[0].market_id

            file_path = market_closed_report_path + '/' + str(market_id) + '_order_data.txt'



            check_and_create_json(file_path, [format_betfair_order(order) for order in orders])```

*Tags: General Technical*

---

**mandelbot** - *20:18:55*

But your worker would block your strategy if both operating on one core?

*Tags: Strategies*

---

**Aaron Smith** - *20:36:16*

while i can not provide the technical details that Derek can, maybe a conformation by liam can provide a feeling of security that you can add workers with no trouble :smile: jump to: [https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1633775745333200](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1633775745333200)

*Tags: General Technical*

---

**D** - *20:44:26*

I don’t have the details to hand but for the vast majority of time, your cpu is just waiting - waiting for disk, memory, network packets etc. I’m not an expert on flumine and obviously it depends what you’re worker is doing…

*Tags: Performance*

---

## 2021-11-13

**Jono** - *13:19:26*

qq in flumine is there a way to snap /make process_market_book run on a market if there hasnt been a market_book published after a certain amount of time? Something along the lines of: last recorded market_book timestamp &gt; 60 seconds ago there for force run of process_market_book. Cheers

*Tags: General Technical*

---

**liam** - *16:34:26*

`strategy.streaming_timeout`

*Tags: Strategies*

---

## 2021-11-14

**Finn** - *14:29:34*

If I want to bring in data from another source through a socket into flumine, can I use a background worker? Do I need to extend some of the streaming infrastructure instead? Mo once mentioned extending StreamListener, BaseStream and BaseResource for adding GPS data, I’m looking to do something similar I think. Couldn’t afford GPS though haha

*Tags: General Technical*

---

**Mo** - *15:04:28*

The higher the risk the more you should expect to get paid. The further you are from the off the more uncertainty there is about the true price and the wider the spread. Ditto third division Thai football versus EPL. In running you have the risk of being picked off by latency arbers

*Tags: Performance*

---

## 2021-11-15

**liam** - *08:14:57*

You can use a background worker and then a custom event or utilise the streaming infra

*Tags: General Technical*

---

**liam** - *09:45:41*

ideally but not fixed

*Tags: Errors Debugging*

---

**liam** - *09:47:43*

custom, betfair api is very specific although the streaming/listener design suits most

*Tags: General Technical*

---

## 2021-11-16

**VT** - *03:42:31*

Hi everyone, I've noticed that most people are using Flumine, and apparently it's a more advanced tool. I'm a beginning programmer, I understand Python for working with data science, statistically analyzing data and things like that. I have a betting system using the platform before betfairlightweig and everything works fine, it's a simple system that I was able to develop just using the documentation.



Anyway, I'm trying to learn Flumine but it's challenging for me, I haven't found any real and complete tutorial or practical example. The content of the documentation is quite confusing for a beginning programmer, without practical and simple examples, at least for me.



I would like to know where to start, what I need to study for how to understand the system because I don't even know where to start.



Someone has a basic code, like for example checking the odds prices for the over 2.5 goals of Liverpool's next EPL game and placing a bet.



Thanks

*Tags: Deployment, Strategies*

---

**ThomasJ** - *04:35:40*

[@U013ZS16QJZ](@U013ZS16QJZ) If you know some Python then that's a huge help.



Rather than start off with an objective in mind how about starting off with one of the examples. Any one will do. Work your way thru it thoroughly over weeks. Yes weeks or months.



There is no fulsome documentation as it would take a huge amount of time to create for a free product. And then there's the maintenance.



I did this, it was a hell of a grind. Took several/more? months. At the end of the day I am comfortable with Flumine and Betfairlightweight without being an expert. I can use things out of the box and I can make changes/hacks to core code to get what I want. Shhhh don't tell Liam :slightly_smiling_face:



I don't think there is any other way that will be of true use in the long run.

*Tags: General Technical*

---

**Mo** - *06:09:24*

Good advice [@U01U24AG35W](@U01U24AG35W). [@U013ZS16QJZ](@U013ZS16QJZ) here is a full strategy right here: [https://github.com/liampauling/flumine/blob/master/examples/strategies/lowestlayer.py](https://github.com/liampauling/flumine/blob/master/examples/strategies/lowestlayer.py)

*Tags: Strategies*

---

**ThomasJ** - *06:25:34*

Oh I am a beginner Pythonista and seeing how the code is designed, constructed, executed etc has taken me to a whole different level of Python understanding.

*Tags: Getting Started*

---

**VT** - *06:41:10*

Thanks [@UBS7QANF3](@UBS7QANF3), but this code is really hard to understand for those who just arrived, it will take me many hours to understand it, maybe days, weeks. All these external functions and imports, as our [@U01U24AG35W](@U01U24AG35W) said this is a lot of work when we don't have extensive documentation. It's not a matter of laziness. I'll probably give up on Flumine before I even get started, I'll stick with the most basic lightwheith options.

*Tags: General Technical*

---

**ThomasJ** - *06:45:54*

[@U013ZS16QJZ](@U013ZS16QJZ) Please don't do what you say you're going to do. It will be well worth your while at the and there are many experts here to help. Just ask detailed and precise questions. People here love to help. Anyway think about it for a few days.

*Tags: General Technical*

---

**Paul** - *06:55:48*

Can I ask [@U013ZS16QJZ](@U013ZS16QJZ) - if you have something that works with bflw, what your objective in moving to flumine is? Is that you want to make use of paper trading, historical data stuff, etc.? If so, make that your end goal, don't worry about all the functionality and having a complete picture and just chip away towards your goal

*Tags: Data Quality, Strategies*

---

**Paul** - *06:58:24*

I've worked with many less experienced developers who think they need to understand everything in a tool or framework to make progress. Very common if they have a maths/physics background. That's not how this works: you need to know just enough to get the next step done, and only that. If that's you, just trust what was said above and trust it: take an example strategy and go from there.

*Tags: Strategies*

---

**Paul** - *07:00:07*

If you want a bit more confidence as a programmer, loads of python beginner books out there, the CS50 course gives a lot of people a solid foundation, loads of options.

*Tags: Getting Started*

---

**mandelbot** - *07:09:00*

[@U013ZS16QJZ](@U013ZS16QJZ) +1 what Paul said. I'm a novice programmer and had no python skills coming into Flumine yet I've managed to port all my strategies from Bet Angel to Flumine. Not as daunting as you think to get started as [@U4H19D1D2](@U4H19D1D2) has made things pretty simple. People on this slack are very helpful and have answered on many occasions my stupid/basic questions.

*Tags: General Technical*

---

**liam** - *08:05:42*

A tutorial start to finish is certainly on my todo list however there would be the assumption of having an intermediate knowledge of python when it comes to classes/inheritance/design patterns etc.



But because its a framework once you have things figured it can be a few lines of code to setup a variation of your strategy or quickly backtest/paper trade something new. When time is at a premium this can be extremely valuable for anyone starting out.

*Tags: Getting Started, Strategies*

---

**Newbie99** - *09:27:30*

[@U4H19D1D2](@U4H19D1D2) should line 76 in that example be:



1 / get_price(runner.ex.available_to_lay, 0)



rather than:



1 / r.ex.available_to_lay[0]["price"]



As won't the latter produce an Error if there is no price for that runner on the lay side?

*Tags: Errors Debugging*

---

**Newbie99** - *09:29:50*

Actually that could still produce a TypeError if None

*Tags: Errors Debugging*

---

**birchy** - *13:41:55*

[@U013ZS16QJZ](@U013ZS16QJZ) I have been "botting" for 15+ years but am not a developer or programmer. Up until about 12 months ago, I was using a self-created Betfair API polling library ([http://www.bespokebots.com|bespokebots.com](http://www.bespokebots.com|bespokebots.com)) but decided I needed to upgrade to the streaming API. It didn't make sense for me to code it myself when [@U4H19D1D2](@U4H19D1D2) had already developed a tried and tested library. Despite "knowing" python, I struggled to get to grips with Flumine and am still discovering new features and functions. As stated above start off with using the examples and get one of them working. I'd recommend getting the marketrecorder running as that should work "out of the box", so you only need to fathom out the login procedure. Then look at strategies, preferably via the lowestlayer example and backtesting feature. For that, you'll need data, which the marketrecorder will provide.

*Tags: Feature Engineering*

---

## 2021-11-19

**liam** - *09:10:41*

```I had a long discussion about this with an HFT friend today and ultimately I don't think it really makes a big difference```

But isn't roundtrip the entire problem / focus when it comes to HFT?

*Tags: General Technical*

---

**Unknown** - *15:59:43*

Hey guys, running through the examples @ [https://github.com/betfair-datascientists/API/blob/master/python/API%20Tutorial.ipynb](https://github.com/betfair-datascientists/API/blob/master/python/API%20Tutorial.ipynb) -- looking at tomorrow's horse racing I'm unable to get Total Matched to appear for any individual runner - i've checked the specific race and money is matched on most runners.

market_book.runners[0].total_matched



1. trading.betting.list_runner_book(

2.     market_id='1.190955523',

3.     selection_id=41749336,

4.     price_projection=price_filter_traded

5. )

tried both EX_TRADED and EX_BEST_OFFERS price data projection filters

*Tags: Strategies*

---

**C0rnyFlak3s** - *20:30:32*

Has anyone experience with purchasing the streaming data (PRO) off of betfair for 230$/month (soccer)? Is the data noisy or clean and more importantly is it worth the $ to get let’s say 2 years of data for backtesting strategies? On which data do you guys test your strategies? Self-monitored data via streaming API or purchased data from betfair or somewhere else? :slightly_smiling_face:

*Tags: General Technical*

---

**C0rnyFlak3s** - *20:36:50*

One other question, sharing data (for private use only) is probably not allowed by betfair’s terms of service, is it?

*Tags: General Technical*

---

**Sam Asin** - *23:20:18*

Ah sorry, maybe there are too many jargony words I use when discussing with my friends.



I guess the real question would be about how you evaluate the success/failure of given trades. I.e. If you backed a horse at 2.4 odds, but right before the event starts, the price has shifted to 2.8 odds, but then the horse wins, what sorts of things do you look at to gauge whether you thought the trade was good.



I didn't know what "greening up" means ironically, but it looks like closing out positions/hedging.



I guess for me I was thinking about when you're making/active in a market where you don't have an opinion, but maybe that's not your style.

*Tags: General Technical*

---

## 2021-11-20

**Unknown** - *02:13:56*

"But isn't roundtrip the entire problem / focus when it comes to HFT?"



Yeah I think so on some level. But there was some question as to whether you should differentiate your trades between once they've completed a roundtrip, vs. "they look good according to my current valuations."



You could frame it with categories like "my roundtrip profit",  "size of my net exposure" , and "my valuation of my net exposure."



Or you could say that you don't care about the part of the profit that's actually completed a roundtrip, and only look at "my valuation of all my trades marked to now (aka total PNL)" and the "size of my net exposure".



"valuation of all my trades marked to now" is something like "roundtrip profit + valuation of my net exposure now" so they're sooort of the same thing. But my friend was suggesting focusing on "roundtrip profits (aka closed PNL)" is a mistake, it's like misparameterizing the problem, and might make you make mistakes. So instead he'd just have it summed with marking your current exposures, as your "total PNL" and use that.



Hopefully that makes it more clear/interesting! But yeah, maybe you guys are more focused on finding the right horses, while he is only focused on playing markets well without having a long term opinion.

*Tags: General Technical*

---

**Mo** - *06:53:25*

Counter argument is that it reduces volatility which can be good for growing your bankroll when you first start out

*Tags: General Technical*

---

**Oliver Varney** - *06:57:21*

Just think about your "exit" trade (second bet), its either -EV,0EV or +EV. If you completely forget about the initial entry, would you take on the second trade if its -EV (nope). If its +EV, then if you forget the initial trade again, you just have a strategy that is taking +EV bets (similar to the one you are entering with which should be determined by your predicted price). I guess the argument in an ideal world is that its 0EV and your reducing volatility, but this is unlikely as your probably having to jump the spread to offload.

*Tags: Strategies*

---

**Oliver Varney** - *06:59:37*

If you already have a back to lay strategy with historical bet results available, sum up the profit on the exit hedge lay bets only, its likely your losing money on them

*Tags: Strategies*

---

**ThomasJ** - *07:02:49*

Very sad to report that I don't have a strategy that comes close to breakeven and that's without commission reduction. But working on it...

*Tags: Strategies*

---

**Mo** - *07:05:40*

To be clear, the comment I quoted was a comment that [@U4H19D1D2](@U4H19D1D2) has previously made. My personal view is that generally - but not always - you want to avoid greening up because it will lead to lower overall profits. So my advice is not "never green" but simply that you fully understand the implications and then decide what is the best approach for your personal circumstances. This includes having the humility to question whether you do _fully_ understand the implications

*Tags: General Technical*

---

**Oliver Varney** - *07:06:17*

I do wonder though (havent looked at the maths) on a back to lay strategy with a very high strike rate (80%+)and a super aggressive bankroll on long odd horses (20+), where you can use your money multiple times a day(s) , whether you can make a case for hedging. Market capacity problem would put this dead in the water once the size of your bank has increased

*Tags: Strategies*

---

**liam** - *07:20:47*

I think the big problem is this idea of ‘trading’ and ‘greening’ is the way you make money whilst ignoring value, you cannot make money without getting value 

*Tags: Strategies*

---

**birchy** - *07:55:12*

IMO, if your model places bets on both sides of the market that are _independently_ +EV, then "unintended" greening is fine as it reduces exposure and frees up money for further bets. Actively looking for exit positions without assessing the EV of the bet is leaking value.

*Tags: Strategies*

---

**James T** - *09:27:49*

I’ve mentioned before that I do have strategies where my closing bet does seem to be +EV for some reason. Not only that, in a way, it’s dependent on my opening bet because I’m staking based on what my opening position is (I haven’t actually tried working out how to stake without that opening bet). In the chart below opening is blue, closing is orange. 

*Tags: General Technical*

---

**Oliver Varney** - *09:34:01*

[@U01U24AG35W](@U01U24AG35W) People may have strategies to copy large orders as thats seen as a signal maybe. If too many people copy this strategy, it will lead to overbacking and create a +EV exit trade

*Tags: Strategies*

---

**James T** - *09:34:50*

In this particular strategy, yes. 

*Tags: Strategies*

---

**James T** - *09:39:52*

I think it comes back to what you were talking about yesterday. I think anything fundamental tends to find higher value early, but no liquidity. But then low edge later when there is liquidity. My model isn’t good enough near the off yet. 

*Tags: Strategies*

---

**James T** - *09:42:24*

Conclusion: I basically have to improve my model. 

*Tags: Strategies*

---

**ThomasJ** - *09:44:12*

[@USYQKE5HN](@USYQKE5HN) You said "And maybe I’m moving the market too which the backtest also does reflect." Are you saying Flumine backtest is able to move the market?

*Tags: General Technical*

---

**James T** - *09:45:11*

Sorry “doesn’t reflect”. Typo I fixed now. 

*Tags: Errors Debugging*

---

**S G** - *12:34:21*

I am new to modelling and strategies and was wondering if anyone can suggest a good book to kick start creating a working sport model.Any thoughts on the below book? More focussed on stocks etc but i guess priciples can be applied to sports. 

[https://www.amazon.co.uk/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715/ref=asc_df_1839217715/?tag=googshopuk-21&amp;linkCode=df0&amp;hvadid=430825732529&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15520940606181387990&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=m&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9045936&amp;hvtargid=pla-1123856727832&amp;psc=1&amp;th=1&amp;psc=1&amp;tag=&amp;ref=&amp;adgrpid=101598704058&amp;hvpone=&amp;hvptwo=&amp;hvadid=430825732529&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15520940606181387990&amp;hvqmt=&amp;hvdev=m&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9045936&amp;hvtargid=pla-1123856727832|https://www.amazon.co.uk/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715/ref=asc_df_1839217715/?tag=googshopuk-21&amp;linkCode=df0&amp;hvadid=430825732529&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15520940606181387990&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=m&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9045936&amp;hvtargid=pla-1123856727832&amp;psc=1&amp;th=1&amp;psc=1&amp;tag=&amp;ref=&amp;adgrpid=101598704058&amp;hvpone=&amp;hvptwo=&amp;hvadid=430825732529&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15520940606181387990&amp;hvqmt=&amp;hvdev=m&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9045936&amp;hvtargid=pla-1123856727832](https://www.amazon.co.uk/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715/ref=asc_df_1839217715/?tag=googshopuk-21&amp;linkCode=df0&amp;hvadid=430825732529&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15520940606181387990&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=m&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9045936&amp;hvtargid=pla-1123856727832&amp;psc=1&amp;th=1&amp;psc=1&amp;tag=&amp;ref=&amp;adgrpid=101598704058&amp;hvpone=&amp;hvptwo=&amp;hvadid=430825732529&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15520940606181387990&amp;hvqmt=&amp;hvdev=m&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9045936&amp;hvtargid=pla-1123856727832|https://www.amazon.co.uk/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715/ref=asc_df_1839217715/?tag=googshopuk-21&amp;linkCode=df0&amp;hvadid=430825732529&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15520940606181387990&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=m&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9045936&amp;hvtargid=pla-1123856727832&amp;psc=1&amp;th=1&amp;psc=1&amp;tag=&amp;ref=&amp;adgrpid=101598704058&amp;hvpone=&amp;hvptwo=&amp;hvadid=430825732529&amp;hvpos=&amp;hvnetw=g&amp;hvrand=15520940606181387990&amp;hvqmt=&amp;hvdev=m&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=9045936&amp;hvtargid=pla-1123856727832)

*Tags: Getting Started, Strategies*

---

**Mo** - *12:35:07*

If your focus is models for sports then much better to read academic papers on models for sports IMO

*Tags: Strategies*

---

**Mo** - *12:35:43*

Decent book for beginners though: [https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew-ebook](https://www.amazon.co.uk/Statistical-Sports-Models-Excel-Andrew-ebook)

*Tags: Getting Started, Strategies*

---

**mandelbot** - *12:48:33*

just an AWS question here, do stopped instances incur charges? Can they be restarted with files intact?

*Tags: Deployment*

---

## 2021-11-21

**liam** - *09:33:50*

The latter, flumine has a control for this 

*Tags: General Technical*

---

**Ruben** - *09:38:02*

flumine to the rescue, I love it

*Tags: General Technical*

---

**captainonionhead** - *09:45:11*

:thumbsup: Me too!

Having only just got properly started doing this, I'm rapidly learning how many barriers to entry there are.  Having flumine as a place to start removes an enormous one, for which I'm extremely grateful, thanks!

*Tags: General Technical*

---

## 2021-11-22

**Amit Patel** - *21:07:14*

hey guys! I'm running back tests with a simple strategy for horse WIN markets and it takes around 25 mins for a weeks worth of data. Is this expected and is there anything I can do to improve performance other than buying a better pc? :slightly_smiling_face:

*Tags: Performance, Strategies*

---

**Amit Patel** - *21:13:03*

thanks for the swift reply! I have quad core but not sure if its using them all, running in jupyter notebook, anything I need to configure? also what the hell is a listener_kwarg? sorry for the noob questions

*Tags: General Technical*

---

**liam** - *21:17:29*

[https://liampauling.github.io/flumine/quickstart/#listener-kwargs|https://liampauling.github.io/flumine/quickstart/#listener-kwargs](https://liampauling.github.io/flumine/quickstart/#listener-kwargs|https://liampauling.github.io/flumine/quickstart/#listener-kwargs)

*Tags: General Technical*

---

**liam** - *21:18:06*

Jupyter won’t help things 

*Tags: General Technical*

---

**Amit Patel** - *21:18:51*

ok will do! really appreciate the help :slightly_smiling_face:

*Tags: General Technical*

---

## 2021-11-23

**Michael** - *17:02:10*

[@USYQKE5HN](@USYQKE5HN) re your curious result, I find it unlikely that this could be accounted for by your individual effect on the market, to my mind it's more plausible that you're part of a larger phenomenon. For instance it could be that other participants are overweighting the same variables that you are using in your model.



Alternatively it could be that early momentum which you are part of tends to get over weighted by later participants.



Other people will undoubtedly be identifying the same early value opportunities that you are and probably the same signals. I'd expect it to be something to do with that, although that doesn't account for the result not showing up in a back test.



I suppose another explanation would be that your participation increases competition for the value bet, forcing other participants with inferior models to take a worse price and thereby overbetting the selection. That would account for the back test result.



I'm sure you've had these type of thoughts already - what do you think?

*Tags: Strategies*

---

**James T** - *17:20:43*

Yeah, exactly Michael. I don’t think it can just be me moving the market, but perhaps a ripple effect. I tend to offer in order to get more value, and often undercut the market early in the day to be at the front of the queue, however backtests can’t know if someone would subsequently decide to undercut me or not.



Like you say, it’s likely me and other people using similar factors in our models. I haven’t looked at this particular strategy in a while to be honest, so intend to revisit it. Really I just need to improve the model rather than execution I think. 



Also might look at taking prices only which I suspect is what most bigger fundamental players will do. But again, for that to work I need to be nearer the off with tighter spreads. 

*Tags: Strategies*

---

**James T** - *17:37:09*

Yeah, that’s all I can think of really. 



Sometimes I wonder whether if I don’t place the opening bets whether the bet at SP will still be profitable. But I don’t know how to do that since I’m staking my SP bet based on my open position. I guess I could simulate opening a position. Or else just develop a strategy around a reversed signal at the off. But given both legs are profitable I’ve just left it as it is. And obviously it’s very consistent betting both legs. It just doesn’t scale well so overall profits are tiny. 

*Tags: Strategies*

---

## 2021-11-24

**mon mon** - *13:26:02*

I seem to have the same issue with backtest_multi.py example that few others have mentioned, in that I get a "UnicodeDecodeError: 'utf-8' codec can't decode byte......"  error, even though the example uses smart_open.

Has anyone pinned down a solution to this? It works fine if I'm doing a standard (not 'multi') backtest.

*Tags: Errors Debugging*

---

**Peter C** - *13:38:32*

Are you running from .zip files? If so I think that this edit to the above file might solve the problem:

```with patch("builtins.open", smart_open.open):

    framework.add_strategy(strategy)

    framework.run()```

*Tags: Strategies*

---

**Peter** - *16:38:27*

Easy to get confused about this. The market recorder used to store the data in zip archives, but fairly recently [@U4H19D1D2](@U4H19D1D2) converted it to save the data in gzips instead - a significant improvement in usability.

*Tags: Data Quality*

---

**Peter** - *16:42:07*

On the original question from [@U02J5GMLJF6](@U02J5GMLJF6), I have had this problem too. On one occasion it proved to be because my gzip files were double gzipped, i.e. unzip them and the result was still a gzip. But that's not the only reason this occurs. I'm still looking for a solution myself or some of the files affected by this issue. Irritatingly, if I download a new version from S3, it sometimes works, once, then starts failing again, making me wonder whether the files are somehow not being closed properly at the end of the streaming.

*Tags: General Technical*

---

**birchy** - *19:08:23*

[@U9JHLMZB4](@U9JHLMZB4) has nailed this. I had the same problem when using the old marketrecorder. The zip files are archives that contain the file. I ended up running a python script to unzip the files and then re-save as .gz format. There's probably a nicer way of doing it though...

*Tags: General Technical*

---

**mon mon** - *20:58:00*

I'm using bz2 files from BF's PRO set - I had a look and they're single zipped (or bzipped), and the code has the patch section that  [https://betfairlightweight.slack.com/team/U01LD279D16|Peter C](https://betfairlightweight.slack.com/team/U01LD279D16|Peter C) mentioned - I'm using the backtest_multi.py (here [https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1637689688427200?thread_ts=1637615931.411600&amp;cid=C4HL6EZTQ](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1637689688427200?thread_ts=1637615931.411600&amp;cid=C4HL6EZTQ))

*Tags: General Technical*

---

**thambie1** - *23:20:37*

When debugging a historic market, occasionally I'd like to watch footage of the match and line it up with the market movements. Anyone have a suggestion on where to get such footage for football matches?

*Tags: Errors Debugging*

---

## 2021-11-25

**Mike Walpole** - *06:40:09*

Hi guys &amp; gals, not sure if this is the right channel for that. 



I'm running Randwick Group, a sports &amp; racing trading company in Sydney. Founders come from hedge fund and prop FX trading background. We treat sports trading as just another asset class &amp; run the business just like any other high quality hedge fund. 

We currently have 4 traders across racing, basketball and esports. 

We’ve raised additional capital from our investors and are now looking to hire more traders (ideally trading soccer, cricket, UK racing and NFL) to allocate capital to. 



We don't need you to work on this full time, and we're happy for you to trade on personal account as well. We offer market leading profit share of your trading. We respect your IP and don't require you to share your secret sauce. We provide you with a funded account and you trade any way you like (within risk parameters!) 



What we need is a verifiable track record (ideally on BetFair) of deploying money - so no backrests please. DM for a job description or if you have any questions. 

[http://www.RandwickGroup.com|www.RandwickGroup.com](http://www.RandwickGroup.com|www.RandwickGroup.com)

*Tags: Deployment, Strategies*

---

**mon mon** - *10:44:04*

I got it working -  the solution seems quite simple (once you find it):



Lines in def `run_backtest()` were:

```    framework.add_strategy(strategy)

    with patch("builtins.open", smart_open.open):

        framework.run()```

....but works with:

```    with patch("builtins.open", smart_open.open):

	    framework.add_strategy(strategy)

        framework.run()```

*Tags: Strategies*

---

**Mike Walpole** - *11:15:59*

Good question James, and one we get asked quite regularly. 

I would say that the main benefit we offer is a chance to accelerate your career as a sports trader with investor capital, similar to the way that many traders have done with equities and bonds. If you love what you're doing and think you can build a truly professional trading book, we support you through the process with everything from capital, to trade support, to understanding your actual risk position. 



We hire top tier individuals and help them become world class traders. For example, you will benefit from:



• sessions with sports/performance psychologist to help you unlock your trading potential

• speakers from best equities, FX, commodities trading outfits (Goldman Sachs, Morgan Stanley, Optiver)

• regular debrief on how you're tracking against expectations and what to “do more of”, “stop” and “try”. You will know how you are performing and won’t have to second guess yourself or waste time guessing.

 

We have intentions to list on the stock exchange within 36 months and you have a chance to join us on the ground level of what we are confident will be a truly global trading operation.

*Tags: Performance, Strategies*

---

**Mike Walpole** - *12:05:23*

You are right, it's ideal for people with track record who want to take their career to the next level. 



I don't think I'd agree with the second paragraph. I've been trading systematic strategies in equities space and I can tell you that psychology played a significant role - from the types of strategies I was designing, to the questions when is it right to switch off your strategy after a losing streak, etc. 



I actually prefer systematic strategies. After all that's why I posted it on this channel. 

*Tags: Strategies*

---

**James T** - *14:30:09*

Capital just isn’t a problem if you even have a vague chance of an edge. 



But at 25% profit, on top of presumably paying 40% PC, means the trader takes home 15% of their gross winnings, and then presumably has to pay income tax on top because it becomes part of a job. 



Can’t see how you’ll attract anyone who stands a chance for you? And that’s not even talking about reaching a scale that creates a substantial business. 

*Tags: General Technical*

---

**Aaron Smith** - *14:47:45*

The problem compared to a usual hedge fund will be that in usual trading you are dealing with tiny advantages that need a huge scale to have a significant impact, while in sports betting sizes are lower, money comes back to you quickly (after the event ends) and advantages are usually bigger. I dont think anyone would ever consider giving up 75% of their profits (unless those "profits" are losses)

*Tags: Strategies*

---

**James T** - *14:48:45*

I can see that, excluding the capital, some aspiring traders will be interested in the mentoring side of things that you say you offer, but it’s hard to know what expertise your team has in the sports betting front. 



But even for that, the cost seems a bit high when it’s not that hard to find accomplished people willing to help and mentor for free. 

*Tags: Strategies*

---

**Aaron Smith** - *14:53:34*

I do accept that, but those ppl also wont be successful sports bettors. When you have a successful strategy, you need like 2k capital to get it rolling, as the money comes bakc quickly anyway. Its not like the stockmarket where you are having a 0.001% advantage and throw in 10 million to make it worthwhile

*Tags: Strategies*

---

**James T** - *14:55:25*

Anyway, it’s not for us to judge your business model. I wish you the best of luck and cross my fingers you don’t lose all the investors’ money. In the end it’s all a zero sum game (once you remove the entertainment factor), so it’s no good thing and it’s no bad thing. 

*Tags: Strategies*

---

**Michael** - *16:22:34*

I will also watch this with an interested but sceptical eye. It's almost a standing joke amongst successful players that we get used to politely explaining to friends and acquaintances why we don't need their investment. Even a modestly successful player will have more funds than they can deploy and a frequent question here is _"how can I scale up - it doesn't seem to work."_



We've also got used to seeing "professional traders" advertising that they need investment but that's always met with derision. _"System working, send more money"_. Yeah.



So it seems like this lot are looking for that lot - the winners who haven't won. Why? Are there really investors queueing up to back low ranking gamblers recruited online who couldn't cut the mustard with a couple of grand but can safely be trusted with (presumably) a much bigger fund? People who see a website with literally nothing on it except a claim to be a "millionaires factory" and say _"yeah, that's for me"._ 



I can think of a few ways this can be made to work for the business owners if they have something slightly different in mind to what they're advertising. They wouldn't be the first people to have that idea, but this as a straight proposition? I wouldn't bet my own money on it - but hey, maybe I could use theirs.....?

*Tags: Deployment*

---

**birchy** - *18:48:32*

I would agree with everything the others have said and add that the kind of people you want to attract and the people you will attract are very different beasts. Sports betting is a game where finding edges is fairly easy but _getting matched_ for sizable sums is near impossible, particularly if you're looking to play with 4 (or even 5) figure sums. There's just not enough liquidity or "dumb" money available for big volumes.

And those that _do_ get matched have already built up a bankroll larger than they can utilise. Small players who have an edge will quickly scale up due to the quicker returns...and then they're in the same boat as the big players. So that only leaves the wannabes who haven't quite made it yet, and they're not really the kind of people you're looking for?

*Tags: Strategies*

---

**D C** - *19:10:35*

Heavily implied but not seen it explicitly stated that Betfair is the sole place to get matched though. I agree with pretty much every sceptical comment here but if using bookies instead of Betfair it would alleviate the concerns about PC and getting matched  (assuming that have accounts with bookies who will take large amounts of action). That said, I cannot imagine why anyone making money with a profitable method would want to give that up just to use larger stakes and paying tax on their income but I suppose it really does depend on the nature of the sport and markets in question. [@UGV299K6H](@UGV299K6H) spot on with the website though - Millionnaire factory is just going to attract people with a spectulative profit making system/model that lack the resources of the bottle to go for broke on their own two feet.

*Tags: Strategies*

---

**birchy** - *20:14:16*

I think it's a great idea. I can easily create a strategy that loses £100 a day, so given the 25/75 split, -£100*0.25= -£25, so I'd be £75 a day better off. In fact I could easily scale that to £1000 a day. Eureka!

*Tags: Strategies*

---

## 2021-11-26

**liam** - *09:20:28*

Just added a '[https://liampauling.github.io/flumine/performance/|performance](https://liampauling.github.io/flumine/performance/|performance)' tab in the flumine docs as it seems to be a common question, let me know if I have missed anything

*Tags: Performance*

---

**Javier Martín Pérez** - *13:02:29*

Sorry to be a dischordant note here but I believe in this project. As an example I am 50% manual gambler and 50% automatic trader in football and I am collaborating with another company that allows me to place my manual bets with them too and is working pretty well so far. I am still risking the same stakes of my own money but if it goes well I get the gains from this fund. About Mike and randickgroup I was speaking to him yesterday and it sounds legit, and the way it will work it looks unlikely to be a scam. I can explain why I think it can work for most people who have an edge. It doesn´t matter what your edge is, there is always ways to squeeze it harder by increasing risk. So for example, if I have a strategy where I bet 20.000 bets per year with a ROI of 0.5%, why not try to bet 20.000 more with a ROI of 0.2%? In my case I prefer to separate the riskier projects, so I use different accounts, platforms, etc, and I think a company like this will help me increase the profit without increasing the risk. 25% sounds fair to start, in my opinion. Think that many traders will cause them losses, but I think that if you are a successful one you will probably be able to negotiate different conditions with time. Also the fact that is a company where you can invest makes it interesting, but that is because I like the stock exchange and there are not many similar products in the market.

*Tags: Strategies*

---

**Michael** - *13:33:25*

As you increase your stakes your edge must diminish because you have to accept a worse price. That's what makes scaling such a wicked problem.

*Tags: General Technical*

---

**Javier Martín Pérez** - *13:47:40*

Not sure about that. But if you think about it, it doesnt really matter because then I can use the strategy the way I want and it doesnt have to be necesarily with the same strategy I used to get those results. So I think there will have to be a test phase anyway where the funds and the stakes are limited and then you will have to proof that the strategy works.

*Tags: Strategies*

---

**Michael** - *13:54:27*

Actually no - whilst what you say about being able to use a different strategy is true, if I was them I would certainly want to verify that you are operating at good profits.

*Tags: Strategies*

---

**birchy** - *14:13:09*

Many professional poker players are bankrolled, so it's the same sort of format but obviously there would need to be confirmed success in order to get investors interested.

*Tags: General Technical*

---

**Unknown** - *16:00:16*

I was just testing connection time to the API..comparing my home PC (Quite powerful system) v AWS eu-west-1a (Low powered but adequate : 2Gb 4 vCpu's)

from previous threads on here i know that the connection to betfair is asymmetric (as it goes via prolexic)

So I thought i check it out...

Same strategy on each system, simply fires in a lay order at 1.01 as the market signals it is in-play

In 3 out of 4 tests the home PC was able to receive the price data, and obtain a bet reference faster than AWS(as my strategies react to market prices, I thought this was the best way to measure and not worry about the asymmetry)

Would you say this was a reasonable test?

Just putting aside the other benefits of running on AWS, If speed to market is the main important factor is AWS worth it?

*Tags: Performance, Deployment, Strategies*

---

**Mo** - *16:40:55*

Would you decide if a strategy was profitable on the basis of four bets?

*Tags: Strategies*

---

**ShaunW** - *16:46:44*

Same here [@UQL0QDEKA](@UQL0QDEKA) but I'm not seeing a consistent difference. In fact some days it's the opposite. For the record I'm not a Flumine guy so maybe that helps to narrow down or eliminate any likely cause.

*Tags: General Technical*

---

**ShaunW** - *17:00:59*

4 does seem tiny given it can vary quite a bit from one order to the next .... I ran mine all day and averaged, running on both local and aws simultaneously to rule out any daily variations in the backbone in general. I couldn't find any tangible benefit to running locally so for me, the pros of being on aws outweigh the cons and performancewise it's much of a muchness.

*Tags: Performance, Deployment*

---

**PeterLe** - *17:27:00*

Yes think I knew you was [@UBS7QANF3](@UBS7QANF3) :smile:. when I first started using AWS years ago there was a significant advantage I recall, anyway I’ll see how it plays out

*Tags: Deployment*

---

**Peter** - *19:31:02*

“Sports betting is a game where finding edges is fairly easy but _getting matched_ for sizable sums is near impossible”



This is exactly the problem and why sports hedge funds don’t work. Capital isn’t the problem, deploying it is.



I’ve almost lost count of the number of firms that have started and failed in similar manners. In aggregate they have lost millions.



I really wish the markets were ten times bigger, but they are not. So I’ve never felt the need to raise capital and neither has any other successful trader I’ve met.



It’s a self defeating proposition in my mind. You give away an edge and get a fraction of the return in lieu of using money you don’t need.

*Tags: Errors Debugging, Deployment, Strategies*

---

**D C** - *19:52:36*

Surely latency is not the only concern is it? Internet down time at home is a lot more frequent than it is if you use AWS surely?

*Tags: Performance, Deployment*

---

**Javier Martín Pérez** - *22:16:48*

Its not exactly that I don't like it. For example if I set a strategy following Kelly with a bankroll of 10k and i stick to the stakes Kelly is giving me, I will progressively stake more. But if I test the same strategy with someone elses money there wont be any possible damage to the first strategy while I will increase my earnings if there are any.

*Tags: Strategies*

---

## 2021-11-27

**D C** - *10:10:13*

Yes good point about Betfair going down. We have Virgin and it drops out a few times a month. Not the best but I don't green anymore either so its less of a problem than it used to be.

*Tags: General Technical*

---

**mon mon** - *13:20:52*

Is there a way to limit the number of markets subscribed to when using streaming_market_filter?

E.g. I would usually filter markets from the market catalog by time (e.g. only stream markets starting in the next 60 mins) and then set the stream to use only those market ids.

I could do this in flumine when it forst starts up,  but I can't see how I would update the strategy to use this new list of market id's (e.g. how to update the list of market id's every so often).

*Tags: Strategies*

---

**Mo** - *13:25:18*

No there isn't but streaming is cheap. Filter locally. Subscribe using the available filters then use `check_market_book` to ignore any market whose start time is greater than 60 minutes in the future

*Tags: General Technical*

---

**birchy** - *13:35:29*

[@UQL0QDEKA](@UQL0QDEKA) so I guess the logical solution is to run several servers, i.e. AWS, Simply (Tagadab), home pc, etc and let it average out over time?

*Tags: Deployment*

---

**mon mon** - *13:44:41*

That's true - the issue is that I'm hitting the 200 subscription limit.

Looking back through old posts the common fix seems to be 'ask BF for more' !

*Tags: Errors Debugging*

---

## 2021-11-28

**PeterLe** - *09:11:25*

So for what it is worth, the home PC was quicker to receive betid on every market i tested yesterday.

I was using lightsail rather than EC2, which has a smaller bandwidth than EC2 if i remember correctly. (there is also some throttling (bustable zones) but I wouldn't have thought that would have come into play at the beginning of the race etc

The differences were much bigger too than the screenshot I posted above, ie in a couple of cases, betid was up to 50K.

Anyway make of it what you will, only posted as it may help someone. For me, Ill probably just run locally.

*Tags: Deployment*

---

**Mo** - *09:19:18*

Oh, it's an AWS product?

*Tags: Deployment*

---

**D C** - *13:03:28*

Was thinking about this but surely there are some problems with this method? If you are placing bet on receipt of the market inplay flag in 2 different locations there will be possible variations as to when that is received so you could already lag on the placement side. Also, are you using the placeOrders respnse or the stream to set the arrival time (I get inconsistency with which one of those arrives first)? Could you possibly run order stream in 2 locations and simply compare arrival times of each bet ID off the stream as that way you take the API-NG and interaction with the price stream inplay flag receipt differences?

*Tags: General Technical*

---

**PeterLe** - *13:24:21*

Thanks DC for your thoughts. So to answer the first part of that, Im probably 180 miles to Dublin, so there will probably be a slight variation when the inplay signal is received. If it was found that my home PC was consistently slower to receive the data that the instance in AWS, I wouldn't be concerned as Im not measuring the first part of the leg, but the whole trip, ie the ability to receive the data , act upon it; place an order and receive a Betid.

Re the second part, Im not as technical as you guys DC :grinning:, so I wouldn't know how to test that (I actually deleted the Lightsail instance last night anyway)

*Tags: Performance, Deployment*

---

**D C** - *13:58:49*

I see what you mean. I was just thinking about if you had 2 order stream subscriptions - one on your home machine where you are placing the bet and another on AWS, both order streams will push the bet ID when it is created and you can compare arrival times for latency because they will both have the same publish time. But as you say, that only tests a one way performance on the stream side, not the actual lag in the whole bet placement timeline. I don't know enough about this stuff other than working on real time embedded stuff where you can get more accurate timings on things. A bit different to comms between machines running full OS with multiple processes.

*Tags: Performance, Deployment*

---

**Mo** - *15:20:42*

Despite being latency sensitive, none of my strategies are triggered by the price stream so I've never looked personally. But I do these kinds of optimisations with the data feeds I do trigger off

*Tags: Performance*

---

**Amit Patel** - *16:04:47*

hey guys! is there any way to get the runner name inside a strategy when running live? I can see its there for backtests but can't figure out if its possible otherwise

*Tags: Deployment, Strategies*

---

## 2021-11-29

**KG** - *11:00:00*

All NSW thoroughbred and harness markets unfortunately, plus NRL, so safe to say 10% MBRs, though there is a track table that breaks down to a state level that I can share if that helps.

*Tags: General Technical*

---

## 2021-11-30

**PeterLe** - *22:06:40*

Hi [@U013K4VNB6D](@U013K4VNB6D) here is one of mine:

# My Betfair File Scraper



import wget

import traceback

from urllib.error import HTTPError

from os import path, mkdir, remove

from datetime import datetime, timedelta

import shutil

import os



# If set to True, it will re-download and overwrite existing files

OVERWRITE = False



DATE_INPUT_FORMAT = '%d/%m/%Y'



URL = '[https://promo.betfair.com/betfairsp/prices/](https://promo.betfair.com/betfairsp/prices/)'



SP_DATA_PATH = r'A:\Users\User-1\Data\BetfairSPData'



GREYHOUND_FOLDER = 'Greyhound'

HORSERACING_FOLDER = 'Horseracing'



PREFIX_LOCATION_MAP = {'dwbfgreyhoundplace': path.join(GREYHOUND_FOLDER, 'Greyhound_Place'),

'dwbfgreyhoundwin': path.join(GREYHOUND_FOLDER, 'Greyhound_Win'),

'dwbfpricesireplace': path.join(HORSERACING_FOLDER, 'IREHorseRacing_Place'),

'dwbfpricesirewin': path.join(HORSERACING_FOLDER, 'IREHorseRacing_Win'),

'dwbfpricesukwin': path.join(HORSERACING_FOLDER, 'UKHorseRacing_Win'),

'dwbfpricesukplace': path.join(HORSERACING_FOLDER, 'UKHorseRacing_Place')}



TEMP_FOLDER = 'Temp'



if __name__ == '__main__':



# Check if target folders exist and make sure temp folders are emptied



folders = list(PREFIX_LOCATION_MAP.values())

temp_folders = [path.join(x, TEMP_FOLDER) for x in folders]



for location in [GREYHOUND_FOLDER, HORSERACING_FOLDER] + folders + temp_folders:

folder_path = path.join(SP_DATA_PATH, location)

if location in temp_folders:

try:

shutil.rmtree(folder_path)

except FileNotFoundError:

pass

if not path.exists(folder_path):

mkdir(folder_path)



# Get date input



dt_start = dt_end = None



while dt_start is None:

try:

date_input = input('Please enter the start date of the Files you want to download: ').strip()

dt_start = datetime.strptime(date_input, DATE_INPUT_FORMAT)

except ValueError:

print('Could not parse the date.')



while dt_end is None:

try:

date_input = input('Please enter the end date of the Files you want to download: ').strip()

dt_end = datetime.strptime(date_input, DATE_INPUT_FORMAT)

except ValueError:

print('Could not parse the date.')



# Generate a range of dates



dates = [dt_start + timedelta(days=x) for x in range((dt_end - dt_start).days + 1)]



# Download files to temp folders



for date in dates:

date_str = datetime.strftime(date, '%d%m%Y')



for prefix in PREFIX_LOCATION_MAP:

try:

filename = '{}{}.csv'.format(prefix, date_str)

print(filename)

destination = path.join(SP_DATA_PATH, PREFIX_LOCATION_MAP[prefix], TEMP_FOLDER, filename)



if path.exists(destination):

if OVERWRITE:

remove(destination)

else:

continue



wget.download(URL + filename, destination, )

except HTTPError as http_error:

print('HTTP Error:', http_error.code)

except:

print(traceback.format_exc())



# Concatenate files



for prefix in PREFIX_LOCATION_MAP:

filename = '{}{}-{}.csv'.format(prefix,

datetime.strftime(dt_start, '%d%m%Y'),

datetime.strftime(dt_end, '%d%m%Y'))

file_path = path.join(SP_DATA_PATH, PREFIX_LOCATION_MAP[prefix], filename)



with open(file_path, 'w') as destination:

folder_path = os.path.join(SP_DATA_PATH, PREFIX_LOCATION_MAP[prefix], TEMP_FOLDER)

start = 0

for name in os.listdir(folder_path):

with open(os.path.join(folder_path, name)) as source:

lines = source.readlines()

# Column names should be added just once

destination.writelines(lines[start:])

start = 1

shutil.rmtree(folder_path)

*Tags: Errors Debugging*

---

## 2021-12-02

**ThomasJ** - *05:48:22*

[@U4H19D1D2](@U4H19D1D2) In `simulated.py &gt;  _def_ _calculate_process_traded` would you please explain the reason for the 'divide by 2' in the 1st line of the method.



`_def_ _calculate_process_traded(_self_, _publish_time_: int, _traded_size_: float) -&gt; None:`

`        _traded_size_ = _traded_size_ / 2`

        `if _self_._piq - _traded_size_ &lt; 0:`

            `size = _traded_size_ - _self_._piq`

            `size = round(min(_self_.size_remaining, size), 2)`

            `if size:`

`                _self_._update_matched(`

                    `[`

`                        _publish_time_,`

`                        _self_.order.order_type.price,`

                        `size,`

                    `]  # todo takes the worst price, i.e what was asked`

                `)`

`            _self_._piq = 0`

        `else:`

`            _self_._piq -= _traded_size_`

            `if logger.isEnabledFor(logging.DEBUG):`

                `logger.debug(`

                    `"Simulated order {0} PIQ: {1}".format(_self_.order.id, _self_._piq)`

                `)`

*Tags: Errors Debugging*

---

**ThomasJ** - *08:34:49*

hahaha well as a relative newcomer to Python it's been very difficult but very rewarding as it has taught me a hell of a lot.



In this instance I was trying to work out why a back order placed 3 mins before the off got matched but the exact same order placed 10 secs before the off did not. And the answer is of course position in queue which is approx. calculated in this method.

*Tags: General Technical*

---

**TT** - *21:20:18*

How do people deploy the market recorder alongside strategies? My initial thoughts are to try and keep them separate but I can see a few options:

 - Deploy two separate ec2 instances (one for the market recorder and one for running strategies)

 - Use one ec2 instance but run 2 separate python processes

 - Use one instance and just add both the market recorder strategy to the framework alongside the other strategies



 How do other people approach this? And are there any other things to consider i.e latency/connection limit issues etc?

*Tags: Data Quality, Performance, Deployment, Strategies*

---

**D** - *23:02:42*

1 ec2 instance, use tmux to run 2 (or more) separate python processes.

*Tags: Deployment*

---

## 2021-12-03

**Peter** - *06:25:03*

I'm a keep it simple kind of a guy, so even though the market recorder is very lightweight, I run it on it's own ec2 instance to remove any risk of interference. Or more accurately I run them (as I run five of them collect all the markets that interest me).

*Tags: Data Quality, Deployment*

---

**captainonionhead** - *07:06:07*

I've only recently got going and have 2 python processes (one recorder, one simple strategy to keep Betfair happy whilst I gather some data).  These are running on a VPS I already had that I've repurposed.  It's roughly equivalent to a nano and I'm nowhere near stressing it.

However, I'm expecting to move to something more like Peter's setup with many processes on different instances at least partly because it allows me to update different parts of my code/infrastructure without having to take everything down.  At the moment I have a lot of code churn as I implement new features and I expect that to continue as I develop strategies and refine ones that are already running.

*Tags: Getting Started, Feature Engineering, Strategies*

---

## 2021-12-04

**Jeff Waters** - *16:59:12*

Hi [@U016TGY3676](@U016TGY3676)



I'm actually trying to work this out too, so I can include runner names in my backtesting output.



I'd thought that maybe they would be stored in a dictionary with the selection_id's as the keys, but I've had no joy so far.



Any help would be much appreciated.



Cheers



Jeff

*Tags: General Technical*

---

**liam** - *17:13:37*

If it’s purchased data I think they are in the market_definition, if not record the catalogue (as per the market recorder example) and load them in with this [https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py|middleware](https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py|middleware) 

*Tags: Data Quality*

---

**Jeff Waters** - *20:40:07*

Hi [@U4H19D1D2](@U4H19D1D2)



I've added the middleware file that you posted.



I then added the following test print code to see if I understood how to use the functionality correctly:



```import marketcatalogue as mc





def output_results(framework):

    # [Deleted]

    for market in framework.markets:

        # [Deleted]

        catalogue = mc.MarketCatalogueMiddleware()

        catalogue.add_market(market)

        for runner in market.market_catalogue.runners:

            print(runner.runner_name)```

I'm not having any joy, however. Where am I going wrong, please?

*Tags: General Technical*

---

**Jeff Waters** - *21:08:52*

Random stats question.



Let's say a football team has a goal expectancy of 1 for a match. Assuming for the sake of simplicity that there is no added extra time and that they aren't more likely to score in one part of the match than another, does it follow that their goal expectancy per minute is 1/90 and their goal expectancy for the first half is 0.5?

*Tags: General Technical*

---

## 2021-12-05

**liam** - *09:33:00*

Use smart_open [https://liampauling.github.io/flumine/performance/#file-location|https://liampauling.github.io/flumine/performance/#file-location](https://liampauling.github.io/flumine/performance/#file-location|https://liampauling.github.io/flumine/performance/#file-location)

*Tags: Performance*

---

**Jeff Waters** - *11:37:39*

Thanks guys.



I must admit that soon after I posted my question, I was tempted to delete it, as it occurred to me that the answer to my question was self-evidently 'yes'.



A more interesting question is whether it's possible to profit from the fact that goals aren't distributed uniformly or if the market takes that into account, but I know that's not something that anyone is going to reveal! :slightly_smiling_face:

*Tags: General Technical*

---

**Mo** - *11:50:18*

It's one of the most basic properties of how to price up markets in running so no it's not possible and it's hardly secret sauce

*Tags: General Technical*

---

**Jeff Waters** - *22:05:51*

"I’ve learned a lot about using R, Python, Stan, JAGS, and other software platforms that can be used to create considerably more advanced machine learning sports models [than using Excel-based models]. In all seriousness that’s where the future of sports modelling lies.



Even Marco Blume, the head of trading at Pinnacle, has come out openly saying Pinny’s traders are using machine learning models deploying the Caret package in R and the

Scikit Learn package in Python. Someday soon, these platforms are likely to be the only path forward for attacking major sports betting markets."



Mack, Andrew (2019-07-09T23:58:59). Statistical Sports Models in Excel



Agree/disagree?

*Tags: Deployment, Strategies*

---

**Jeff Waters** - *22:13:17*

Hi [@U4H19D1D2](@U4H19D1D2)



I've tried adding it via process_market_book:



```def process_market_book(self, market, market_book) -&gt; None:

    catalogue = mc.MarketCatalogueMiddleware()

    catalogue.add_market(market)```

However, when I later try to reach the market catalog in my results output method like so, I get told: _AttributeError: 'NoneType' object has no attribute 'runners'_



```for market in framework.markets:

    print(len(market.market_catalogue.runners))```

Am I making any obvious mistakes that are jumping out at you, please?

*Tags: Errors Debugging*

---

**Jeff Waters** - *22:21:30*

Indeed. Incidentally, he does go on to write:



'For the meantime however, the Excel-based models in this book, particularly when applied to smaller markets and derivatives, can offer fairly regular flashes of positive expectancy.'

*Tags: Strategies*

---

**Mo** - *22:39:46*

R, Python, Stan, JAGS, all inoffensive

*Tags: General Technical*

---

**Mo** - *22:40:01*

The benefits of machine learning I'm much more sceptical about

*Tags: General Technical*

---

**thambie1** - *22:43:02*

Fundamental approaches are overwhelmingly machine learning based. But the simplest forms of machine learning pretty much look like simple statistics

*Tags: General Technical*

---

## 2021-12-06

**Javier Martín Pérez** - *08:32:31*

I have been using csv files to store data like bets placed, etc but I have had several problems with Excel when I have to edit them. Apparently most of the most annoying features of Excel (like the rounding up of 16+ digit numbers) are unavoidable and I was wondering if there is a better software that you are usually using. The notepad works relatively well but is generally difficult to work with. I remember that when I used Tableau in the past it was very slow, is it better nowadays?

*Tags: Feature Engineering, Performance*

---

**liam** - *08:33:01*

jupyter / pandas

*Tags: Feature Engineering*

---

**ThomasJ** - *09:42:01*

[@UBS7QANF3](@UBS7QANF3) May I please be allowed to be borderline rude (or is this Q quite offensive?) by asking...

 So you said "The benefits of machine learning I'm much more sceptical about"

So if not machine learning...whereupon do you cast your gaze?

*Tags: General Technical*

---

**Mo** - *09:53:24*

I don't think that's offensive at all. I think there's potentially a debate here about _what_ machine learning is.



For example, many people consider logistic regression to be machine learning but really they've just repackaged "statistics". If you think logistic regression is machine learning then is Poisson regression machine learning? Is Elo machine learning?



Tied up with machine learning is big data, and to really take advantage of many of the typical methods e.g. xgboost you're going to need a large data set. _But_ sports just doesn't generate the sorts of massive data sets you see in tech. You're always only going to get 380 matches in a Premier League season, not 380 million of them. You can't just go out and collect some more data.



I think you also miss the wood for the trees by thinking about applying machine learning to sports. Sports is not e.g. computer vision - _you can much more easily get at the data generation process in sports_. e.g. goals in a football match, running times in a horse race.



Machine learning also lets you more easily fall into engaging in intellectual masturbation rather than making money. There's a _hell of a lot_ more that goes into a successful sports betting strategy than just the model. It's really an engineering problem where the model is one small part and often not even that important a part.

*Tags: Strategies*

---

**ThomasJ** - *10:02:31*

What do you mean by an "engineering problem"?

*Tags: General Technical*

---

**ThomasJ** - *10:18:13*

Hi [@U026WRD7SHE](@U026WRD7SHE) You mentioned Excel and Tableau which are "high level wrappers" for handling data. But what most people do not realize is it that their "target market" is business users so only Bill Gates needs to concern himself with '$16+ digit numbers' :slightly_smiling_face:

In this space of Betfair data analysis, programming in Python is your best bet, and Pandas with it's gazillion features (methods) is unsurpassed due to the many many more code examples on the internet.

*Tags: Feature Engineering*

---

**Javier Martín Pérez** - *10:35:08*

Hi [@U01U24AG35W](@U01U24AG35W), I do all my programming on Python but when it comes to check bets P/L graphs etc I normally use Excel because I am used to it. Probably the best is to use Pandas but never used it to display data so will need to get my head around it for a while.

*Tags: Feature Engineering*

---

**Lee** - *10:56:21*

Add the Middleware to the framework and then the market_catalogue is available for all strategies.



```from flumine import FlumineBacktest, clients, BaseStrategy



from middlewares.marketcataloguemiddleware import MarketCatalogueMiddleware



client = clients.BacktestClient()



framework = FlumineBacktest(client=client)



markets = ["tests/resources/PRO-1.170258213"]





class ExampleStrategy(BaseStrategy):

    def check_market_book(self, market, market_book):

        if market_book.status not in ["CLOSED", "SUSPENDED"]:

            return True



    def process_market_book(self, market, market_book):

        for runner in market.market_catalogue.runners:

            print(runner.selection_id, runner.runner_name)





strategy = ExampleStrategy(

    market_filter={"markets": markets},

    max_order_exposure=1000,

    max_selection_exposure=105,

)

framework.add_strategy(strategy)



market_catalogue_middleware = MarketCatalogueMiddleware()

framework.add_market_middleware(market_catalogue_middleware)



framework.run()```

*Tags: Strategies*

---

**Ruben** - *11:19:24*

with flumine, is it possible to subscribe to events of a certain competition only? e.g. premier league soccer events only

*Tags: General Technical*

---

**Ruben** - *11:20:49*

sure, no problem

*Tags: General Technical*

---

**Mo** - *11:34:54*

To be clear this is a limitation of the *Betfair API*

*Tags: General Technical*

---

**ThomasJ** - *11:38:49*

Ah right...so do yourself a favour and get into Matplotlib for graphs.

Took me quite a while to come to grips with it but it's power is all you will ever need for the rest of your life. Big statement I know.

So csv to be read by Pandas and then Pandas to Matplotlib.

import pandas as pd

var1 = pd.read_csv("your_csv_file")

var1 is now a pandas dataframe and off you go.

*Tags: Feature Engineering*

---

**Mo** - *12:25:47*

I mean that you need to build i.e. _engineer_ a complete trading system in order to have an automated betting strategy not just fit a model. You are lucky these days that flumine exists but you still potentially need to integrate with various data feeds, work out how to implement the strategy logic, handle various edge cases, the lifetime of orders, decide how to apply your margin, decide on staking strategy etc etc.

*Tags: Strategies*

---

**Unknown** - *13:06:52*

For what it's worth, here's a small batch file that gets you the daily files.



NB: It uses python's -m wget

*Tags: General Technical*

---

**Ruben** - *16:28:56*

is the parameter `streaming_timeout`  what one should use to have process_market_book() executed even if no updates have been received?

*Tags: General Technical*

---

**Steve** - *23:33:44*

I'm trying to get my head around what flumine translate into Cython. I can see that all the flumine module are in pyc format - so I assume almost all the backend runs in Cython. But what happens if I manually make changes to the .py script in the flumine module? Will those changes be translated into Cython or does it just run in python? And does the file where my strategy logic sits get processed in cython or pyhton? Just want to get a better understanding as I rarely use Cython.

*Tags: Strategies*

---

## 2021-12-07

**liam** - *07:30:02*

No cython in flumine 

*Tags: General Technical*

---

**river_shah** - *07:34:36*

[https://stackoverflow.com/questions/2998215/if-python-is-interpreted-what-are-pyc-files/2998544](https://stackoverflow.com/questions/2998215/if-python-is-interpreted-what-are-pyc-files/2998544)

*Tags: General Technical*

---

**river_shah** - *07:37:46*

If you really are in interested in cython, [https://towardsdatascience.com/use-cython-to-get-more-than-30x-speedup-on-your-python-code-f6cb337919b6](https://towardsdatascience.com/use-cython-to-get-more-than-30x-speedup-on-your-python-code-f6cb337919b6) may be helpful.



If you use numpy based numerics, it is usually going to call into efficient c++ code anyways and will release global interpreter lock too (in case you have to parallelise certain sections)

*Tags: Performance*

---

**river_shah** - *07:46:25*

Haha, I have recently been working with `Julia` on a project so I do have a soft spot for people new to python wanting to get “compiled” language speeds

*Tags: Getting Started, Performance*

---

**river_shah** - *07:49:59*

Traversing the abstract syntax tree and dumping out llvm is probably the cleanest way to go full compiled speeds with python. I hope this kind of feature and  complete GIL removal makes it into python 3.11+

*Tags: Feature Engineering, Performance*

---

**Jeff Waters** - *15:10:42*

Hi [@UUCD6P13J](@UUCD6P13J)



I've amended my code - line 74 and 75 in [https://github.com/JeffW12345/runbacktests-python-file/blob/main/runbacktests.py](https://github.com/JeffW12345/runbacktests-python-file/blob/main/runbacktests.py).



However, when I run the test print in process_market_book, I get told:



 _AttributeError: 'NoneType' object has no attribute 'runners'"}_



What do you suggest, please?



It is possible that me using multi-processing is what's causing the problem?



Thanks,



Jeff

*Tags: Errors Debugging*

---

**Jeff Waters** - *15:20:12*

Thanks [@UUCD6P13J](@UUCD6P13J)



The middleware code should be fine, as I literally just copied and pasted the code that Liam provided in this thread:



```import json

import os



from betfairlightweight.resources import MarketCatalogue

from flumine.markets.middleware import Middleware



MARKET_CATALOGUE_PATH = "PRO"  # update to correct path





# Will read and parse the market_catalogue file and add to the market object when backtesting

# Usage framework.add_middleware(MarketCatalogueMiddleware())

class MarketCatalogueMiddleware(Middleware):

    def add_market(self, market) -&gt; None:

        catalogue_file_path = os.path.join(MARKET_CATALOGUE_PATH, market.market_id)

        if os.path.exists(catalogue_file_path):

            with open(catalogue_file_path, "r") as r:

                data = r.read()

                catalogue_json_data = json.loads(data)

                market.market_catalogue = MarketCatalogue(**catalogue_json_data)```



*Tags: General Technical*

---

**Jeff Waters** - *15:30:54*

Thanks [@UUCD6P13J](@UUCD6P13J).



I'm assuming it does, as line 48 in [https://github.com/JeffW12345/runbacktests-python-file/blob/main/runbacktests.py](https://github.com/JeffW12345/runbacktests-python-file/blob/main/runbacktests.py) uses the same reference, and works fine.



However, I've just tried creating an explicit reference to the folder - MARKET_CATALOGUE_PATH = "C:\\Users\\User\\Desktop\\bet-project\\PRO" - and I'm still having the same issue.

*Tags: General Technical*

---

**Jeff Waters** - *15:35:08*

Hi [@U4H19D1D2](@U4H19D1D2). I don't know. Do I need to import them, and if so how do I do that?



If you're referring to the MarketCatalogue class, I have that, as Pycharm is fine with this import:



from betfairlightweight.resources import MarketCatalogue

*Tags: General Technical*

---

**Jeff Waters** - *15:38:03*

I can get their selection id's no problem, but I haven't managed to find a way of getting selection names.

*Tags: General Technical*

---

**Jeff Waters** - *15:48:44*

I googled Betfair market definitions, and got this page: [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions),



selectionId and runnerName are listed next to one another.



If I write print(runner.selection_id), I get the id of the runner. However, when I write print(runner.runnerName), Pycharm gives me an unresolved attribute error message.



How do I get around this, please?

*Tags: Errors Debugging, Strategies*

---

**liam** - *15:51:58*

open up your debugger and have a look in `market_book.market_definition.runners`

*Tags: Errors Debugging*

---

**birchy** - *20:51:42*

In Flumine, if we wanted bets on multiple runners to be executed at the same time, does the following send each bet individually?

[https://github.com/liampauling/flumine/blob/bdd46e21b4015779bdccc30bbd5c0f1f86c90d79/examples/strategies/priceblockage.py#L73](https://github.com/liampauling/flumine/blob/bdd46e21b4015779bdccc30bbd5c0f1f86c90d79/examples/strategies/priceblockage.py#L73)

*Tags: General Technical*

---

**Beeblebrox** - *20:56:00*

[https://github.com/liampauling/flumine/blob/0737461323da409990d3108a158092487506aa7f/flumine/execution/transaction.py#L12](https://github.com/liampauling/flumine/blob/0737461323da409990d3108a158092487506aa7f/flumine/execution/transaction.py#L12)

*Tags: General Technical*

---

**birchy** - *21:06:50*

Bugger, I have a strategy leak. Was wondering why my bets were arriving at slightly different times. So something like this:

```for runner in market_book.runners:

    atb = get_price(runner.ex.available_to_back, 0)

    trade = Trade(

        market_book.market_id,

        runner.selection_id,

        runner.handicap,

        self,

    )

    # create order

    order = trade.create_order(

        side="BACK",

        order_type=LimitOrder(atb, self.context["stake"]),

    )

    # place order for execution

    market.place_order(order)```

Should be done like this instead....???

```with market.transaction() as batch:

    for runner in market_book.runners:

        atb = get_price(runner.ex.available_to_back, 0)

        trade = Trade(

            market_book.market_id,

            runner.selection_id,

            runner.handicap,

            self,

        )

        # create order &amp; add to batch

        order = trade.create_order(

            side="BACK",

            order_type=LimitOrder(atb, self.context["stake"]),

        )

        batch.place_order(order)

    batch.execute()```



*Tags: Errors Debugging, Strategies*

---

## 2021-12-08

**NC** - *17:59:21*

Hi guys, first post. Just soaking up as much knowledge as I can get at the mo, this looks like a great resource! Has anyone implemented an RL bot using python? Or know of a good resource or place to discuss this approach?

*Tags: General Technical*

---

**Steve** - *23:22:45*

Thanks for the replies. It seems like maybe I misunderstood. A better question would have been what are .pyc files for in the in flumine module?

*Tags: General Technical*

---

**Jonjonjon** - *23:35:31*

I've thought about it but it's probably better to have a non-losing baseline strategy to work from first.

*Tags: Strategies*

---

**Paul** - *23:44:22*

So I think knowing a lot about successful strategies can help close the avenues down, and then with a few hundred hours of building the gym and feature engineering you might get somewhere. I'm on that path myself, albeit slowly.

*Tags: Feature Engineering, Performance*

---

**Jonjonjon** - *23:45:31*

[@U012XF5CNPN](@U012XF5CNPN) I'm afraid I read them a while ago. And they tended to go above my head. From memory, nothing was worth mentioning. I think that following the David Silver course in YouTube, and then reading the free Sutton book should be enough. Then you just need to use your own imagination/experience. I spent a few months writing up a basic RL library in Python but neve found time to finish it.



[https://www.google.com/search?q=reinfo+cement+learning+market+making&amp;oq=reinfo+cement+learning+market+making&amp;aqs=chrome..69i57j33i22i29i30l2.10566j0j7&amp;client=ms-android-xiaomi-rvo3&amp;sourceid=chrome-mobile&amp;ie=UTF-8|https://www.google.com/search?q=reinfo+cement+learning+market+making&amp;oq=reinfo+cement+learning+market+making&amp;aqs=chrome..69i57j33i22i29i30l2.10566j0j7&amp;client=ms-android-xiaomi-rvo3&amp;sourceid=chrome-mobile&amp;ie=UTF-8](https://www.google.com/search?q=reinfo+cement+learning+market+making&amp;oq=reinfo+cement+learning+market+making&amp;aqs=chrome..69i57j33i22i29i30l2.10566j0j7&amp;client=ms-android-xiaomi-rvo3&amp;sourceid=chrome-mobile&amp;ie=UTF-8|https://www.google.com/search?q=reinfo+cement+learning+market+making&amp;oq=reinfo+cement+learning+market+making&amp;aqs=chrome..69i57j33i22i29i30l2.10566j0j7&amp;client=ms-android-xiaomi-rvo3&amp;sourceid=chrome-mobile&amp;ie=UTF-8)

*Tags: Performance*

---

**NC** - *23:51:22*

My main avenue at the moment is building a successful ML model on historical data, and using RL to develop a profitable long term strategy to place bets. I'm confident this will be profitable but at a fairly low %. My secondary avenue will be to create a bot to use RL and make money through trades. My hunch is this will be much more complex but potentially much more lucrative. I get the point about narrowing down some potential strategies though.

*Tags: Data Quality, Strategies*

---

## 2021-12-09

**Mr West** - *01:05:36*

Hey [@U02PKHL5W4F](@U02PKHL5W4F) your not the first and you won't be the last to think you can use ML &amp; historical data to make your fortune :moneybag: Good luck :crossed_fingers:

*Tags: Data Quality*

---

**NC** - *01:34:32*

Yes, completely agree. My expectations are quite low tbh, especially for the model based upon historical data, but I do think RL has a huge role to play to learn new ways to make money. From what I hear it's being used in the financial markets very successfully, and I would imagine something similar could be done on the sports exchange. Also the work that was done on chess, just shows how powerful it can be, outperforming all known engines to date. Anyway, needless to say it's a huge undertaking, but I hope either me or someone else here can make some big steps (if not already)

*Tags: Data Quality, Strategies*

---

**PeterLe** - *12:15:34*

I think that a lot wont renew after this first month. For me personally, ive had TPD a while but i find that the more triggers to add to a strategy the less effective it becomes. Im a firm believer that keeping things ultra simple is the best way forward. I know many others using TPD will disagree with this but that's my honest take on it. Appreciate that there maybe other strategies inpay that will benefit from it, but thats not the case for mine.

*Tags: Strategies*

---

**liam** - *12:26:23*

My most complicated model (lots of if statements) is for TPD :joy:

*Tags: Strategies*

---

**liam** - *12:48:14*

[https://stackoverflow.com/questions/2998215/if-python-is-interpreted-what-are-pyc-files/2998544](https://stackoverflow.com/questions/2998215/if-python-is-interpreted-what-are-pyc-files/2998544)

*Tags: General Technical*

---

**Aaron Smith** - *14:37:30*

when terminating flumine, i want all open bets to be cancelled. I assume this would be done in strategy.finish() ?

*Tags: Strategies*

---

**Lee** - *14:40:56*

probably depends on how you're terminating flumine, if you're using a worker to terminate i'd do it in there

*Tags: General Technical*

---

**Aaron Smith** - *15:39:28*

whats the best way to cancel those? I need to get all markets the strategy has placed an order in to cancel them. I see there is something called strategy._invested, however i cant find where this is being populated

*Tags: Strategies*

---

**Lee** - *17:19:46*

Not tested but something like this might get you started. If you have a lot of orders then wrap in a transaction to batch them.

```for market in flumine.markets:

    if market.market_book.status == "OPEN":

        for order in market.blotter:

            if order.status == OrderStatus.EXECUTABLE:

                market.cancel_order(order)```

*Tags: General Technical*

---

## 2021-12-10

**C0rnyFlak3s** - *18:28:23*

I am currently stuck at a nasty point. I want to extract goal times from O/U markets, and this all works great. However I now have the problem for goals which are scored in the second half time, that I don’t know when the second half time really started. Since games have different break lengths it is hard to extract the exact half time break from the data. What would be the most reliable market to extract the length of the half time break?

*Tags: General Technical*

---

**C0rnyFlak3s** - *18:30:10*

In general what would be the most reliable way to detect start/end of half times in both backtesting data but also live data?

*Tags: Deployment*

---

## 2021-12-11

**Martin Chambers** - *05:21:26*

Hi all. First post here. Just getting started and have plenty to learn. Have read the docs at betfairlightweight and flumine. Just after some guidance on a work flow here to create a simple bot/strategy to start with. Login through betfairlightweight and then create a strategy with flumine? I have the been able to log into my account, get the market list etc so far.

*Tags: Getting Started, Strategies*

---

**liam** - *06:28:28*

Recommend the examples, bflw first to get to grips with some of the endpoints and then flumine

*Tags: General Technical*

---

**Martin Chambers** - *08:15:13*

Thanks [@U4H19D1D2](@U4H19D1D2) have had more of a look.  Can log in ok with bflw, but not having much joy with the flumine example as per:

[https://github.com/liampauling/flumine/blob/master/examples/example.py](https://github.com/liampauling/flumine/blob/master/examples/example.py)

*Tags: General Technical*

---

**Martin Chambers** - *08:16:19*

Can you advise what I do for login in that script? Appreciate the help.

*Tags: General Technical*

---

**Mo** - *08:35:34*

Flumine will handle logging in for you

*Tags: General Technical*

---

**Martin Chambers** - *09:07:52*

Ok, thanks [@UBS7QANF3](@UBS7QANF3).  Do you have an example of logging in through Flumine?  I am not using certificates at the moment.

*Tags: General Technical*

---

**birchy** - *14:12:53*

[@U02P76UDUPK](@U02P76UDUPK) I'd recommend getting the Flumine marketrecorder example running first as it's a useful exercise to get login working and saving market data will benefit you for backtesting and analysis at a later date.

*Tags: General Technical*

---

## 2021-12-12

**Martin Chambers** - *00:42:04*

[@U016TGY3676](@U016TGY3676) OK, thanks for that Birchy.  Having some success, but looks like I need to have certs installed (was trying to do it without it to get rolling.  The error message is: Exception: [WinError 3] The system cannot find the path specified: '/certs/'

*Tags: Getting Started, Errors Debugging*

---

**Mo** - *06:58:48*

By default bflw will look in the /certs directory for your certificate and private key files. If you want to change this then edit the line where you construct the APIClient

*Tags: General Technical*

---

**captainonionhead** - *09:44:57*

Following up on the comment about getting the marketrecorder example going - if you're using a live key, you'll need a strategy to run with it to stop BetFair being upset that you're just scraping data.  I'd recommend posts like: [@U016TGY3676](@U016TGY3676)'s:

[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1623458534154600](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1623458534154600)

Which amongst many other things has some useful ways to get a strategy started at the bottom.

I also found a lot of useful discussions in [#CTPL3R3FU|strategies](#CTPL3R3FU|strategies) from around late-June/early-July this year.  It's well worth browsing through these channels as there's lots of super helpful people in here and this is a fabulous resource to get started!

*Tags: Deployment, Strategies*

---

**birchy** - *12:55:28*

[@U4H19D1D2](@U4H19D1D2) suggested setting interactive login = True, which should emulate the website style login and _doesn't_ need SSL certs.

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/apiclient.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/apiclient.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/apiclient.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/apiclient.py)

*Tags: General Technical*

---

**liam** - *20:35:23*

Anyone else getting a lot of connection errors?

*Tags: Errors Debugging*

---

## 2021-12-13

**Martin Chambers** - *00:48:40*

I can see some data being stored in my tmp file now, (and no errors in Visual Studio - so I must be underway!  Will take some time now and see if I can get a simple strategy working.  Appreciate your help guys, awesome!

*Tags: Errors Debugging, Strategies*

---

**Unknown** - *11:59:43*

Flumine smart matching, when backtesting multiple strategies what would be the expected behaviour for passive orders:

*Tags: General Technical*

---

**liam** - *12:01:18*

I am thinking option 2 becomes the default (more accurate as option 1 is what flumine currently does) and a flag for option 3 when you went to test potential impact on multiple strategies running at the same time etc. but welcome thoughts on this as its confusing

*Tags: General Technical*

---

**liam** - *14:53:14*

:thumbsup: POC PR [https://github.com/liampauling/flumine/pull/539|here](https://github.com/liampauling/flumine/pull/539|here), also added a sort method to better replicate the matching engine based on side/price/order

*Tags: General Technical*

---

**Aaron Smith** - *16:17:23*

can i use [https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

live (instead of backtest) and if so, is there anything i need to take care of except for getting rid of order.simulated.profit?

*Tags: Deployment*

---

## 2021-12-14

**Techno** - *10:28:07*

Hi guys. I'm Elliot. Thanks for having me. To be honest, I'm very green when it comes to programming and I'm having trouble installing Betfairlightweight. I want to use it to parse the historical data, and have a spreadsheet or even a graph if possible of all the traded prices for horses until the start of the race. Thanks.

*Tags: Getting Started, Data Quality*

---

**birchy** - *10:29:34*

[@U02P76UDUPK](@U02P76UDUPK) might be worth using the Lowestlayer example just to keep you ticking over until you have some other strategy running. Not sure if you are already using it, but PyCharm is a bloody good editor for python scripts.

*Tags: Strategies*

---

**Oliver Varney** - *10:30:01*

do you have any logs / error messages you can share ?

*Tags: Errors Debugging*

---

**Techno** - *10:31:00*

OK. So when run this in Python:

*Tags: General Technical*

---

**Techno** - *10:31:14*

```import betfairlightweight



trading = betfairlightweight.APIClient(

        "username", "password", app_key="app_key", certs="/certs"

    )

trading.login()

&lt;LoginResource&gt;```



*Tags: Strategies*

---

**Oliver Varney** - *10:36:46*

think there is also an example for the historical data which sounds like what your looking for also

*Tags: Data Quality*

---

**Techno** - *10:37:37*

I'm have very limited knowledge of Python.

*Tags: General Technical*

---

**Oliver Varney** - *10:38:28*

I think your fine on the install as its likely that you would of got an import error. As liam mentioned it probably just ran the code and exited but as it was doing nothing, nothing happened.

*Tags: Getting Started, Errors Debugging*

---

**Techno** - *10:39:51*

Sorry for the basic questions.

*Tags: General Technical*

---

**Oliver Varney** - *10:40:12*

May be worth just spending 1-2 hours watching a few tutorials on how to set up python, setting up an IDE and debugging, and python basics. Then try to get one of the examples going and come back with specific questions at that point.

*Tags: Errors Debugging*

---

**Techno** - *10:41:18*

I know the python basics, but that is all.

*Tags: General Technical*

---

**Techno** - *10:42:02*

So an example is a program which uses Betfairlightweight?

*Tags: General Technical*

---

**Oliver Varney** - *10:47:26*

So i think your original question, which is probably the best starting point is to try to get the historical file streaming working. To do this you need to have either previously recorded files or downloaded the files off betfair (free or paid versions). Once downloaded, modify/use as a basis the code in [https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaminghistorical.py) , changing the file path to your recorded file path. Id advise setting breakpoint in your IDE and looking what the resources (data) contains. Stuff like graphing, there will be hundreds of tutorials on youtube that will be a much better resource for learning.

*Tags: General Technical*

---

**Oliver Varney** - *10:52:38*

Someone will correct me if I am wrong but for the historical streaming, I dont think you need to have that set up (no account details, certs and stuff are required).

*Tags: General Technical*

---

**C0rnyFlak3s** - *14:26:14*

Has anyone deployed their backtesting engine to an AWS service? I am planning on porting my data and engine to a service that allows me to load files very fast and offers me enough performance to run parallel backtests on given data. For this I figured, I need 1.) enough fast read/write storage (more so the read speed should be maximized). 2.) enough CPUs to start multiple backtesting processes at once. For this I am looking for a suited AWS service, where I can run my IDE and have my data stored on fast memory. Can you guys point me in the right direction on this one? I am not so experience with cloud computing and finding the right configuration for a given purpose. Would gladly appreciate any pointers. :slightly_smiling_face:

*Tags: Performance, Deployment*

---

**Paul** - *17:29:46*

If you're used to Jupyter, search for Sagemaker in the console. You can start a notebook instance and start there. If you really mean you want an ide, look at AWS cloud 9

*Tags: Deployment*

---

## 2021-12-15

**Paul** - *08:37:36*

That works if you have the upfront capital and can keep it busy. One thing I would say on the AWS stack is see if you can use spot instances if you're doing a lot of this: reduces costs a lot, and obviously upfront costs disappear. I am biased as it's my employer, but I'd agree that if you can do the upfront cost and keep it busy and don't mind the occasional upgrade cycle, Mo’s suggestion can be cost effective.

*Tags: Deployment*

---

**Mo** - *08:43:17*

To be clear, didn't mean to suggest the local workstation was _better_ just an alternative to weigh up against the pros and cons of AWS

*Tags: Deployment*

---

**liam** - *08:54:40*

Found the [https://github.com/liampauling/flumine/pull/539/commits/9d41d1486bd22cb7d80317774a88efa0ecd279b8|bug](https://github.com/liampauling/flumine/pull/539/commits/9d41d1486bd22cb7d80317774a88efa0ecd279b8|bug), simulation code wasn't executing when a cancel/update/replace was in flight, looks like its been here since the beginning :man-facepalming:



Resorted to going through order by order in excel to find this..

*Tags: Errors Debugging*

---

**liam** - *08:58:12*

I have experimented with a few things on AWS lambda/spot instances/fargate/ecs and they all work well but there is always the upfront time cost in getting them setup and without a sophisticated framework on top it isn't exactly simple to quickly run something.



I agree with [@UBS7QANF3](@UBS7QANF3) as since purchasing a decent laptop (M1) the amount of backtests I carry out has skyrocketed as I can just press f5 and forgot about it

*Tags: Getting Started, Deployment*

---

**Peter C** - *09:15:00*

It's cool to see a bit behind the curtain and hear about the work you're doing on flumine

*Tags: General Technical*

---

**Oliver Varney** - *09:33:58*

I think they have remote debugging if thats what you mean

*Tags: Errors Debugging*

---

**Oliver Varney** - *09:35:03*

[https://www.jetbrains.com/help/pycharm/remote-debugging-with-product.html](https://www.jetbrains.com/help/pycharm/remote-debugging-with-product.html)

*Tags: Errors Debugging*

---

**liam** - *12:09:28*

Yep lots of errors 

*Tags: Errors Debugging*

---

**D C** - *12:10:16*

GPS was kicking up around 1130 but thought it was fixed. I suppose its been a while since the last cockup.

*Tags: Errors Debugging*

---

**Oliver Varney** - *14:45:12*

I find it hard that they should be able to market saying its a "trading platform". Obviously another plus for never greening as bankroll is set up for this vs "trading"

*Tags: Strategies*

---

**birchy** - *18:00:12*

Just read a news article saying that AWS went to shit again this morning. Coincidence?

*Tags: Deployment*

---

**birchy** - *18:51:21*

Probably, the article I read was from a typical rag newspaper that insinuated that ALL of AWS had crashed. :grinning:

*Tags: Deployment*

---

## 2021-12-16

**Paul** - *08:23:53*

Performance/price, and the architecture is quite novel. We should probably just flat out benchmark some of this stuff really, shouldn't we?

*Tags: Performance*

---

**Paul** - *08:26:10*

On another point last night I broke my remote code-server setup I used to write bot code on my iPad. Gave AWS Cloud9 (a cloud native IDE), a go, and was pleasantly surprised by it. Think over Christmas I can see myself getting a setup where I use sagemaker for jupyter, and cloud9 for coding. But then, I'm on the Kool Aid, so…

*Tags: Getting Started, Deployment*

---

**Peter** - *12:44:02*

I thought "that can't be too difficult" then took a look at the now very old script I have running as a daily cron job to do it. Hmmmm.



I'll stick it up here in case it helps. Ignore references to workspaces and the database stuff that uses my custom helper. Also my "trading" == your "client". Hope it helps ...



```import os

import sys

import json

import betfairlightweight

from datetime import datetime, timedelta

from helpers.DatabaseHelpers import database



WORKSPACE_PREFIX = os.getenv("WORKSPACE_PREFIX")

DATABASE_TABLE = "betfair_statement_lines"

SQL = """

    INSERT INTO {}

    (refId, amount, avgPrice, betSize, betType, betCategoryType, commissionRate, eventId, eventTypeId, fullMarketName, grossBetAmount, marketName, marketType, placedDate,

     selectionId, selectionName, handicap, startDate, transactionType, transactionId, winLose, deadHeatPriceDivisor, avgPriceRaw)

    VALUES ({})

""".format(DATABASE_TABLE, ('%s,' * 23)[:-1])



def processing_date():

    if len(sys.argv) &gt; 1:

        return sys.argv[1]

    else:

        return str(datetime.today() - timedelta(days=1))[:10] # returns date as a string without time



def download_account_statement(date_str):

    db = database()

    existing_lines = db.fetch_all('SELECT refId, winLose FROM {} where left(placedDate, 10) = "{}"'.format(DATABASE_TABLE, date_str))

    process = True

    from_record = 0

    while process:

        print(date_str, from_record, "-", from_record + 99)

        start_time = date_str + "T00:00:00Z"

        end_time = date_str + "T23:59:59Z"

        time_range = betfairlightweight.filters.time_range(from_=start_time, to=end_time)

        ast = trading.account.get_account_statement(

            from_record=from_record,

            item_date_range=time_range,

            include_item="ALL",

        )

        txns = []

        for a in ast.account_statement:

            line = json.loads(a.item_class_data["unknownStatementItem"])

            if (int(a.ref_id), line["winLose"]) not in existing_lines:

                txns.append(list({**{"refId": a.ref_id, "amount": a.amount}, **line}.values()))

        result = db.execute_many(SQL, txns)

        if result: print(result)

        from_record += 100

        process = ast.more_available

    db.close()



trading = betfairlightweight.APIClient(os.getenv("BETFAIR_USERNAME"), os.getenv("BETFAIR_PASSWORD"), app_key=os.getenv("BETFAIR_LIVE_KEY"), certs=WORKSPACE_PREFIX + "certs")

trading.login()



download_account_statement(processing_date())```

*Tags: Errors Debugging, Deployment, Strategies*

---

## 2021-12-17

**Beeblebrox** - *19:17:06*

Database questions... I currently have a single MongoDB database, running locally on the same machine as Flumine is running, to store all my orders with each strategy in a separate collection. I chose MongoDB as I know next to nothing about databases and the fact it could easily store JSON data seemed like the easiest solution just to get something working.



However...



1. Getting data from the db for analysis is pretty slow

2. It's crashed a couple of times and lost a load of orders

So I'm thinking there's probably a better choice of db I could chose, but I'm not really sure what and I'm not sure how to design it.



• Is an RDS db a better choice?

• Do people host their db's locally or do you use AWS to host it?

• Do you have one big db with all your orders (and separate tables for each strategy) or separate dbs to store orders for different strats?

I guess the general question is: what do people use and why?

*Tags: Performance, Deployment, Strategies*

---

**liam** - *20:02:40*

Yes AWS yes 

*Tags: Deployment*

---

**Peter** - *20:04:31*

There's no inherent slowness in Mongodb, so if your retrievals are slow, that's probably an indexing issue. Mongo (and indeed any schema-less database) makes more demands on the technical skills of the person setting it up and administering the data than a traditional SQL-based database.



Personally I use a MariaDB instance at RDS as, while a huge fan of Mongodb and Elastic, I haven't in my trading activities found a strong enough case to store data in documents, rather than well-designed rows, to warrant the added complexity.

*Tags: Performance, Strategies*

---

**Peter** - *20:07:47*

I use RDS rather than local databases, because the data is precious to me and I want it somewhere safe with redundancy and backups and not to have to organise that myself.



I do use one big database, but my tables don't generally follow individual strategies. So for example all my orders go in one table with the strategy name as an indexed column so that I can filter by strategy if I wish, but am also forced to store my data consistently, making comparisons between strategies feasible.

*Tags: Strategies*

---

**Beeblebrox** - *20:19:23*

Thanks. I'm definitely going to use aws to host my db - learnt my lesson having it local and crashing!



What's the reason you wouldn't use Mongodb over and RDS? To my mind it's simpler as you just have an object that you bung into the db, whereas with and RDS you have to design the tables and try and normalise the db.  Like I say though, my db knowledge is limited, so maybe it's not so hard.

*Tags: Deployment*

---

**Aaron Smith** - *20:27:09*

I am a tech noob in general. I chose RDS (well, i didnt think much about it, aws recommended it :smile: ) and i didnt struggle to much with it. I feel it will come easier to you than you may expect.

*Tags: Deployment*

---

**Mo** - *21:46:43*

Yeah I want to cut back on my database usage

*Tags: General Technical*

---

## 2021-12-18

**Peter** - *07:25:52*

First reason would be that while it is easy to dump objects into into mongo as documents, it's often way more difficult to get them back out again. The keys that you want to use for retrieval may be several layers down in the documents and absent from some of them. Setting up the indices for efficient retrieval can be complicated too unless all your documents are the same shape, which as you venture into more sports they won't be.



Another reason is that I do most of my analysis in Pandas which, like RDS, works with a column and row paradigm, so I can import data directly from RDS into Pandas with a single line of code. Or if like [@U016TGY3676](@U016TGY3676) you like CSVs, you can export directly to there too :wink:.

*Tags: Feature Engineering*

---

**D** - *08:09:04*

Just to add another viewpoint to all the good advice above: I used RDS (postgres) for quite a while but it started to seem expensive, especially for historical data I was hardly ever querying. My current approach is to upload csv, parquet and json data to S3 locations and use Athena to query them into pandas dataframes. Don't have any issues so far; performance is good and I can add new data to my json files without breaking existing queries.

*Tags: Data Quality, Feature Engineering, Performance*

---

## 2021-12-19

**Zach Godsell** - *03:07:40*

When using the streaming error handling that is provided in the examples. My stream only provides the following and never retrieves any market data. I feel like I am missing something simple.



`INFO:__main__:Starting MarketStreaming`

`INFO:__main__:Starting MarketStreaming`

`INFO:__main__:Starting MarketStreaming`

`INFO:__main__:Starting MarketStreaming`

`INFO:__main__:Starting MarketStreaming`

*Tags: Errors Debugging*

---

**Peter** - *07:11:24*

Depends. That's an option. Most of the time though it's just a connection string that includes the host address. E,g,



```import pymysql

import pandas as pd

from sqlalchemy import create_engine



engine = create_engine("mysql+pymysql://{user}:{password}@{host}/{database}".format(

    user=os.getenv("DB_USERNAME"),

    password=os.getenv("DB_PASSWORD"),

    host=os.getenv("DB_HOST"),

    database=os.getenv("DB_DATABASE")

))```

then



```sql = "some sql in here"

df = pd.read_sql(sql, engine)```



*Tags: Feature Engineering*

---

**liam** - *08:27:52*

Remove the retry decorator and the error should raise 

*Tags: Errors Debugging*

---

**Aaron Smith** - *16:42:28*

I am struggling to work with events in flumine. Where can i find what exactly hides behind each event and when exactly they are triggered? For example when `EVENT_TYPE = EventType.CLOSE_MARKET` , the event seemingly is a marketBook (but without manually checking it could ve been a market object aswell?). Also when are markets being cleared, triggering the cleared_markets event? Thanks for any pointer/help

*Tags: General Technical*

---

**liam** - *18:03:25*

Check out flumine/flumine.py for the logic, events/events.py for all events, best way to find its use is to right click in pycharm and find usages 

*Tags: General Technical*

---

**Aaron Smith** - *18:09:17*

i checked out both of those files, but i am still confused :smile: In events.event.py i cant really find what calling event.event would give me for each event and in flumine.py i can see what happens when an event occurs, but i dont see what causes the event to occur (for example when would a clearedMarketsEvent be put into the handler.que ? There is a decent chance my confusion is a result of me overlooking something obvious :smile:

*Tags: General Technical*

---

**Unknown** - *21:32:08*

This might help [@U01DVUAE2G1](@U01DVUAE2G1)

*Tags: General Technical*

---

## 2021-12-20

**Shashi Khaya** - *13:42:34*

Hey folks, this might be a silly question but could anyone please advise on what the options are for deploying flumine if I dont have a local machine available to run 24/7?

*Tags: Deployment*

---

**mandelbot** - *13:55:44*

an ec2 from aws, you can get one free for a year

*Tags: Deployment*

---

**Peter** - *20:12:59*

Just to note too, there is no "keep" persistence. It's a little confusing as Betfair use keep in the documentation and on the website, but the actual persistence type is "PERSIST". However if you use that you won't get the BSP, as it's purpose is to keep the bet running beyond the point at which the BSP auction is concluded.

*Tags: General Technical*

---

## 2021-12-21

**liam** - *12:24:32*

Juypter / pandas 

*Tags: Feature Engineering*

---

**Techno** - *12:25:08*

Thanks I'll look into it. Silly question, but do they handle CSV files?

*Tags: General Technical*

---

**Techno** - *12:27:55*

And, is Jupyter classic notebook the best one to use? I only have Python installed.

*Tags: Getting Started*

---

**Aaron Smith** - *12:34:53*

[@U02QK8FE1K7](@U02QK8FE1K7) yes, you can read csv files with pandas. Jupyter uses python and its better to use in this case than just a common IDE, as it allows you to run sections of your code seperatly so you dont have to run time intensive operations multiple times only to adjust a minor thing in the analysis

*Tags: Feature Engineering*

---

**Techno** - *12:37:02*

OK. Thanks Aaron. Pandas sounds like the best one. Is there a good tutorial for it somewhere on the net please?

*Tags: Feature Engineering*

---

**Aaron Smith** - *12:53:30*

pandas is a library in python, jupyter is a devloping enviroment (like an IDE) in which you write your python code. Its not a decision between pandas or jupyter, best would be to use the two in combination. I dont have a tutorial at hand, but i m certain there are thousands out there if you search for it :slightly_smiling_face:

*Tags: Feature Engineering*

---

**Peter** - *14:03:35*

If you install [https://github.com/jupyterlab/jupyterlab-desktop|Jupyter Desktop](https://github.com/jupyterlab/jupyterlab-desktop|Jupyter Desktop) it will create a whole working environment for you, including Pandas and its dependancies.

*Tags: Getting Started, Feature Engineering*

---

**Techno** - *16:22:26*

Hi Guys. I would like to download historical data only for Hcap races - but not novice or maiden handicaps. Is there a way to do that? Thanks.

*Tags: Data Quality*

---

## 2021-12-22

**Techno** - *01:58:26*

I'm  editing the examplestreaminghistorical.py file in BFLW to output a little more data but I can't find the last traded value. The last_price_traded for each runner is already there, but where do I find the actual amount traded? I think it corresponds to the TRD value in the BF historical data file.

*Tags: Data Quality*

---

**Peter** - *06:57:26*

Are you using the free data? If so TRD data isn't included. Betfair's [https://historicdata.betfair.com/Betfair-Historical-Data-Feed-Specification.pdf|Betfair's historical data specification](https://historicdata.betfair.com/Betfair-Historical-Data-Feed-Specification.pdf|Betfair's historical data specification) shows what's included in each package. Page 4 is especially helpful for its table showing the differences.

*Tags: Data Quality*

---

**Techno** - *09:33:55*

Sorry, I mean what is trd named in the program. I can see trd in the data I have downloaded from Betfair. It is advanced data. For example ltp in Betfair historical data is named "runner.last_price_traded" in the examplestreaminghistorical.py file. What is trd named in the software please?

*Tags: Data Quality*

---

**liam** - *09:46:07*

`runner.ex.traded_volume` [https://github.com/liampauling/betfair/blob/b202fe192b2a46c4d19ab50cf60e380f2a38cb4e/betfairlightweight/resources/bettingresources.py#L378|here](https://github.com/liampauling/betfair/blob/b202fe192b2a46c4d19ab50cf60e380f2a38cb4e/betfairlightweight/resources/bettingresources.py#L378|here)

*Tags: Strategies*

---

**Techno** - *09:47:36*

Thanks [@U4H19D1D2](@U4H19D1D2) Really helpful!

*Tags: General Technical*

---

**Techno** - *10:00:20*

[@U4H19D1D2](@U4H19D1D2) When I use runner.ex.traded_volume in the program to output trd for each runner the output looks like this for each price: [&lt;betfairlightweight.resources.bettingresources.PriceSize object at 0x0000014BEB606EF0&gt;]

*Tags: Strategies*

---

**liam** - *10:02:19*

It's a list of python objects, use your debugger  

*Tags: Errors Debugging*

---

## 2021-12-26

**S G** - *20:37:46*

Based on the below link,

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1635427333064000?thread_ts=1635415964.059500&amp;cid=C4HL6EZTQ](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1635427333064000?thread_ts=1635415964.059500&amp;cid=C4HL6EZTQ)



I tried to create the below strategy, but failed. Any ideas?

```strategy = MarketRecorder(

    name=recorder_name,

    market_filter=[

        streaming_market_filter(

            event_type_ids=[2],

            market_types=["SET_BETTING", "MATCH_ODDS"]

        ),

        streaming_market_filter(

            event_type_ids=[2],

            market_types=["SET_WINNER", "NUMBER_OF_SETS"]

        ),

    ],

    stream_class=DataStream,

    context={

        "local_dir": DATA_DIR_TENNIS,

        "force_update": False,

        "remove_file": True,

        "remove_gz_file": False,

    },

)```

I get the below error:

{"asctime": "2021-12-26 20:26:04,798", "levelname": "ERROR", "message": "DataStream 3001 run error", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\sg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flumine\\streams\\datastream.py\", line 143, in run\n    self._stream.start()\n  File \"C:\\Users\\sg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 60, in start\n    self._read_loop()\n  File \"C:\\Users\\sg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 212, in _read_loop\n    received_data_raw = self._receive_all()\n  File \"C:\\Users\\sg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 242, in _receive_all\n    raise SocketError(\nbetfairlightweight.exceptions.SocketError: [Connect: 3002]: Connection closed by server"}

{"asctime": "2021-12-26 20:26:06,813", "levelname": "INFO", "message": "Starting DataStream 3001", "stream_id": 3001, "market_filter": {"eventTypeIds": [2], "marketTypes": ["SET_WINNER", "NUMBER_OF_SETS"]}, "market_data_filter": {"fields": ["EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP", "EX_MARKET_DEF", "SP_TRADED", "SP_PROJECTED"]}, "conflate_ms": null}

*Tags: Errors Debugging, Deployment, Strategies*

---

**AP** - *21:19:19*

Try



```market_filter=streaming_market_filter(

            event_type_ids=[2],

            market_types=["SET_BETTING", "MATCH_ODDS", "SET_WINNER", "NUMBER_OF_SETS"]

        ),```

*Tags: Strategies*

---

## 2021-12-27

**VT** - *00:41:20*

I'm getting this error when I run the basic BFLW code for stream market, does anyone know why?

The error happens when I send an order to the market (by an external software).



import queue

import threading

import betfairlightweight

from betfairlightweight.filters import streaming_order_filter



trading = betfairlightweight.APIClient("username", "password", app_key="appKey")

trading.login()



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener)



# create filters

order_filter = streaming_order_filter()



# subscribe

streaming_unique_id = stream.subscribe_to_orders(

    order_filter=order_filter,

    conflate_ms=1

)



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()



# check for updates in output queue

while True:

    current_orders = output_queue.get()

    print(current_orders)



===================================================



Exception in thread Thread-10:

Traceback (most recent call last):

  File "C:\anaconda3\lib\threading.py", line 932, in _bootstrap_inner

    self.run()

  File "C:\anaconda3\lib\threading.py", line 870, in run

    self._target(*self._args, **self._kwargs)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 60, in start

    self._read_loop()

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 219, in _read_loop

    self._data(received_data)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 258, in _data

    if self.listener.on_data(received_data) is False:

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\listener.py", line 151, in on_data

    self._on_change_message(data, unique_id)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\listener.py", line 194, in _on_change_message

    self.stream.on_update(data)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\stream.py", line 73, in on_update

    img = self._process(data[self._lookup], publish_time)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\stream.py", line 209, in _process

    order_book_cache.update_cache(order_book, publish_time)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\cache.py", line 587, in update_cache

    runner.matched_backs.update(order_changes["mb"])

TypeError: update() missing 1 required positional argument: 'active'

[&lt;CurrentOrders&gt;]

*Tags: Errors Debugging, Deployment, Strategies*

---

**liam** - *07:43:04*

What version of bflw? I think we fixed this

*Tags: Errors Debugging*

---

**VT** - *23:34:14*

Hi everyone, the only way to send orders on bflw is through trading.betting.place_orders, even for those who use the stream? I'm doing my first tests, I want to send an order and as soon as it is matched send a counter order to close the position. The quickest way is to monitor the order stream and once the order is matched I send another trading.betting.place_orders?

*Tags: Strategies*

---

## 2021-12-28

**VT** - *00:02:58*

I searched the Betfair API documentation and it seems to me that there is really only one way to send orders (via placeOrders) but here in Python it takes almost 1 second to place bets (on pre-live market), a little slow for me.

*Tags: Performance, Deployment*

---

**Aaron Smith** - *00:45:18*

That doesnt sound like it has much to do with placeOrders or Python or the run time of your code (at least only very little), but with the time it takes the order to reach betfair, then for betfair to process it and then for betfair to send back a response to you.

*Tags: General Technical*

---

**VT** - *01:36:55*

Aeron, I used as reference a software for Betfair, the GeeksToy, it shows the market in a ladder format. When I send the order through Python, I can see it in the software. The point is that clicking directly on the software ladder would say it's almost 1 second faster. The problem is that in my little application every millisecond matters.

*Tags: General Technical*

---

**Mo** - *08:23:39*

How is logging configured? Most likely you're getting an error on subscription which is not being reported to you

*Tags: Errors Debugging*

---

**S G** - *09:01:02*

Thanks for your reply Mo, i noticed that some of debug sessions were holding connections. After killing them, it worked OK

*Tags: Errors Debugging*

---

**S G** - *13:51:52*

Hi All i tried using multiprocess backtest from flumine docs with jupyter json logging control and the output file orders.json doesnt seem to have all orders.  I see that jupyterloggingcontrol.py is opening file with mode set to "w". Does this mean multiprocess backtest doesnt work with juputer control?

*Tags: General Technical*

---

**Mo** - *15:21:51*

Use a session. Having to do an SSL handshake on every request is going to kill performance

*Tags: Performance*

---

**VT** - *15:41:06*

I've been asking about the subject out there, some recommended changing the language to C but I discarded it, I intend to do my studies in Python, I'm a beginner.



Others have suggested Cython, Cpython or Pypy, I'm still studying about that. My application is simple, just send orders and close the matched orders and speed is the most important.

*Tags: Getting Started, Performance*

---

**Mo** - *16:24:11*

Your choice of language or Python implementation is largely irrelevant. It's going to be dominated by networking 

*Tags: General Technical*

---

**Shashi Khaya** - *17:28:39*

Hi folks, would a single strategy have the logic included to open and close positions on a market. If so has anyone got an example of how to efficiently trigger the closing part of the strategy once a position is open (e.g. there is some liability for a particular market id) ?

*Tags: Strategies*

---

**Shashi Khaya** - *17:38:23*

Haha is this in response to my question? If you have a strategy that predicts particular price movements rather than final results (E.g. back under X goals market before match and lay X mins in for a small profit/loss) how does one implement that in Flumine?

*Tags: Strategies*

---

**Shashi Khaya** - *18:03:06*

Thanks all for your input! I am still very new to the idea of sports trading and just have a few ideas that I thought could work - What is the reason for never greening?! And [@U4H19D1D2](@U4H19D1D2) where you say - we only take value - what do you mean by that? Sorry for all the questions but this seems like such a great thing to get involved in and I am eager to learn!

*Tags: Getting Started, Strategies*

---

**Aaron Smith** - *18:24:03*

By value he means the expected value(win/loss) of a bet. Generally, when placing a bet you d want the expected value to be &gt;0, otherwise you wont be making any money in the long run obviously. When greening (closing exposure), you place a bet independent of value, which generally means negative expected value and thats also why many dont green up. Still, you ll find other (also on here) who do green up. Greening up does have some advantages. Lower variance is the obvious one and may be important when starting out. Also greening up reduces paid commission and maybe (if you dont track orders yet) allows you to more quickly judge if a strategy is working or not.

*Tags: Strategies*

---

**VT** - *20:51:47*

[@U024A7ACBA7](@U024A7ACBA7) The basic principle in betting, value, is simple to understand, the question here is whether it's worth investing time in a crowded and difficult area. If you are looking for money, the answer will likely be no, if you're just looking for extra money I recommend that you study the entire tradematesports website, they do a serious job. Betfair, programming and math seeking to beat the markets is possibly the most complex alternative.

*Tags: Strategies*

---

## 2021-12-29

**birchy** - *11:21:44*

One point worth mentioning is that "unintended"  greening is possible when you can both back and lay at value prices. e.g. you have a model (which you are confident is correct) that tells you the "true" price is ~2.0. You initially get a back bet matched at 2.5 and then, a short time later, you have an opportunity to lay at 1.5. Both bets are _individually_ +EV (if your model's predicted price is correct), however you have also "greened" your position and reduced your exposure.

*Tags: Strategies*

---

**river_shah** - *11:26:52*

Easily testable. Write a strategy that backs at BSP systematically and see ROI profile for staked amount. Keep stakes small to set aside adverse selection impact

*Tags: Strategies*

---

**birchy** - *11:30:59*

This raises a question...so we all generally agree that comparing our bet prices vs BSP is a good measure of model accuracy. Given that the BSP book is rarely 100%, presumably it makes sense to remove the overround/underround before comparing?

*Tags: Strategies*

---

**nthypes** - *11:32:43*

If I understand correctly, the "model" at this moment shows the "true" price as 1.50 as well. Is that correct [@U016TGY3676](@U016TGY3676)?

*Tags: Strategies*

---

**Oliver Varney** - *11:43:41*

the topic of benchmarks is quite interesting though, especially from a modelling perspective

*Tags: Strategies*

---

## 2021-12-30

**Oliver Varney** - *19:24:46*

does anyone know the reason why betfair send subsequent market book with the same publish time down to the millisecond (looks like there is a different streaming update)?

*Tags: General Technical*

---

## 2022-01-03

**VT** - *13:22:07*

Hi everyone, quick question, does anyone have a script to convert back to lay odds in 2 runner markets (over/under)?

*Tags: General Technical*

---

## 2022-01-07

**Ke** - *13:44:54*

Can someone explain the concept of trade in flumine? I found it's very confusing. Is that just supposed to manage all the order for one runner in one market with one strategy?

*Tags: Strategies*

---

**Ke** - *14:24:44*

Everything seems to be fine. No warning or error

*Tags: Errors Debugging*

---

**liam** - *14:27:19*

if you can dm the full logs I can have a look but can't debug without tbh

*Tags: Errors Debugging*

---

**VT** - *17:09:29*

Hi everyone, when I send an order through trading.betting.place_orders I get the response:



{'status': 'SUCCESS',

  'marketId': '1.193107086',

  'instructionReports': [{'status': 'SUCCESS',

    'instruction': {'selectionId': 2542448,

     'limitOrder': {'size': 5.0, 'price': 1.01, 'persistenceType': 'LAPSE'},

     'orderType': 'LIMIT',

     'side': 'LAY'},

    'orderStatus': 'PENDING'}]}



However I don't get the order ID. I noticed there is a PlaceOrderInstructionReports class but I don't know how to use it.

*Tags: Strategies*

---

**VT** - *18:08:35*

I send a bet using the code:



limit_order_filter = filters.limit_order(size = 5, price = 1.01, persistence_type = 'LAPSE')



order = trading.betting.place_orders(

         lightweight = True,

         async_ = True,

         market_id = market_id,

         instructions=[instructions_filter],

         session=session)



I can't get the order ID but I can see the ID through streaming_order_filter.

*Tags: Strategies*

---

## 2022-01-08

**Jono** - *15:44:27*

i have a simple middleware script set up in one of my strategies to add EXECUTION_COMPLETE orders into the blotter upon restarting the flumine instance. Ive noticed the order.date_time_created field doesnt actually reflect the placedDate field from the betfair response that looks something like this normally in the response json   'placedDate': '2021-01-08T12:38:46.000Z' instead its just the utc datetime from when the object was created.  Is this palcedDate information accessible from order objects as once i restart a strategy i cant seem to actually determine how long a go it was placed from the process_orders(self, market, orders) function. I believe i could gather this placedDate field in from a separate listCurrentOrders call but this seems inefficient. Am i missing an obvious attribute that stores this placedDate info in the order object? Thank you!

*Tags: Strategies*

---

## 2022-01-09

**S G** - *17:25:18*

Hi All, I have got a market recorder setup with log rotation set to 1 day. However the logs arent written until midnight, this could be due to no activity or no info logs. Can I tell fulmine to write a log message every minute? something like a heart beat?

*Tags: Getting Started, Data Quality*

---

**S G** - *18:28:57*

I can see logs getting generated. However there are gaps. For ex: log rotation happens on 00:00AM. But the last log line is an hour earlier or something like that. I just want to make sure that recorder is ok. May be i could add a log line in the market recorder to log every 1 min.

*Tags: Data Quality*

---

**Jimmy** - *19:11:37*

Are there any docs on tying the data/any live score data into Flumine?

*Tags: Deployment*

---

**Mo** - *19:18:31*

Look at the flumine tennisexample.py which uses a BackgroundWorker to access the inplayservice. Same principle applies whether using that or some other feed

*Tags: General Technical*

---

## 2022-01-10

**liam** - *09:17:18*

Fixed in [https://github.com/liampauling/flumine/pull/550|1.21.1](https://github.com/liampauling/flumine/pull/550|1.21.1) let me know if you have any other issues

*Tags: Errors Debugging*

---

**Peter** - *09:48:14*

I use Sport Monks and have found them to be very reliable. Compared to other (shall we say budget) services their data is less error prone, and when I've found errors in their historic data, they've been super responsive.



I haven't tried to use the Betfair endpoint recently, I gave up a while ago as it wasn't pretty inaccurate, often putting out the wrong score on the website itself.

*Tags: Errors Debugging*

---

**Aaron Smith** - *19:48:40*

does someone have a function at hand that reliably extracts basic runner_data (market_id, selection_id, bsp, result) from a streaming_data file (the ones we get from the market_recorder / pro history data)? Somehow mine ends up only giving me 80% of runners and i thought before debugging that thing i may check if someone is so kind to free me from that work :smile:

*Tags: Errors Debugging*

---

## 2022-01-12

**Finn** - *11:04:03*

Thank you for checking. I’m just trying to join data from external sources (for race card info etc) and they often put a different time on the race. Not sure why. Might be an Australian problem? Happens a couple times a week/

*Tags: General Technical*

---

**Stefan** - *11:04:14*

[@UUUAJQZ5J](@UUUAJQZ5J) works for betfair, so she should know better, I do not think there are any problems in market data entries, they do it semi automatically.

*Tags: General Technical*

---

**Finn** - *12:05:26*

yeah stefan I totally get you, but the problem is not all data sources agree. Some are lazy at changing times, even the offical horse racing organisations.

*Tags: General Technical*

---

**Unknown** - *16:41:10*

[@U02FXEUUMRN](@U02FXEUUMRN) It depends on what is your execution flow when running your bot strategies. Using streaming api you have got updates to market start time as well, in rest api betfair did not think about that, there is no info about market start time change when querying listMarketBook on already loaded markets.



My code implantation has this method:  GetAssociatedMarkets and as original market id is returned as well, I can synchronize time of already open market, so my code has no such problems even when after initially loaded markets for instance 10 hours ago, had been rescheduled to other start times.

*Tags: General Technical*

---

**Stefan** - *16:42:34*

Of course if you work with historical data from different sources, you could have problems, but your question was about live data, right?

*Tags: Data Quality, Deployment*

---

**Dario Scardina** - *21:47:57*

Hi everyone, I'm wondering to setup a python script that everyday update me (i.e. by email) about my account fund stats.

I mean something like that:

• last week: +13.20€ 

• last month: -2.13€

• ...

So I need a way to extract my account balance in a specific moment of the day (for example at 23:59:59 of seven days ago, a month ago, ...)



How I can get this info? Do you have some suggestions? I'm trying get_account_statement method but I have some issues with that (and anyway I need to parse the result eventually to find out what I want).

*Tags: Getting Started*

---

## 2022-01-13

**mandelbot** - *08:57:56*

[@U9JHLMZB4](@U9JHLMZB4) shared a script to help with that [https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1639658642449300?thread_ts=1639652085.449000&amp;cid=C4HL6EZTQ](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1639658642449300?thread_ts=1639652085.449000&amp;cid=C4HL6EZTQ)

*Tags: General Technical*

---

**liam** - *11:08:16*

[https://github.com/liampauling/flumine/blob/770fcf967dab590cba63350dd2ee46839363f8d1/flumine/order/order.py#L457](https://github.com/liampauling/flumine/blob/770fcf967dab590cba63350dd2ee46839363f8d1/flumine/order/order.py#L457)

*Tags: General Technical*

---

**rob smith** - *11:14:07*

Hi guys, I am trying to put together a simple script to print current prices for in play matches. The problem I am having is that my script is not getting the current prices, it just keeps printing the same prices over and over. Can anyone see what I've done wrong? Thanks

*Tags: General Technical*

---

**rob smith** - *11:14:21*

`import betfairlightweight`

`import datetime`

`import pandas as pd`

`from betfairlightweight import filters`

`from time import sleep`

`from datetime import datetime`

`from datetime import timedelta`



`username = ""`

`password = ""`

`api_key = ""`

`pd.set_option('display.max_rows', 1000)`



`def current_time(message):`

    `current_time = datetime.now()`

    `current_time = current_time.strftime('%H:%M:%S:%f')`

    `print(f'{current_time}: {message}')`



`def get_todays_games():`



    `trading = betfairlightweight.APIClient(`

            `username, password, app_key=api_key`

        `)`

    `trading.login_interactive()`

    `print("Logged in")`



    `trading.keep_alive()`



    `t20_filter = betfairlightweight.filters.market_filter(`

        `event_type_ids=[4],`

        `market_type_codes=["MATCH_ODDS"],`

        `market_start_time={`

            `'to': (datetime.utcnow() + timedelta(days=1)).strftime("%Y-%m-%dT%TZ")`

        `}`

    `)`

    `t20_events = trading.betting.list_events(`

        `filter=t20_filter`

    `)`



    `t20_events_today = pd.DataFrame({`

        `'Event Name': [event_object.event.name for event_object in t20_events],`

        `'Event ID': [event_object.event.id for event_object in t20_events],`

    `})`



    `print(t20_events_today)`



    `print()`

    `event_id = input("Input Event ID:  ")`

    `print()`



    `market_catalogue_filter = betfairlightweight.filters.market_filter(event_ids=[event_id])`



    `market_catalogues = trading.betting.list_market_catalogue(`

        `filter=market_catalogue_filter,`

        `max_results='100',`

        `sort='FIRST_TO_START'`

    `)`



    `market_types_mooney_valley = pd.DataFrame({`

        `'Market Name': [market_cat_object.market_name for market_cat_object in market_catalogues],`

        `'Market ID': [market_cat_object.market_id for market_cat_object in market_catalogues],`

        `'Total Matched': [market_cat_object.total_matched for market_cat_object in market_catalogues],`

    `})`

    `print(market_types_mooney_valley)`



    `print()`

    `market_id = input("Input Market ID:  ")`

    `print()`



    `market_books = trading.betting.list_market_book(`

            `market_ids=[market_id],`

            `price_projection=filters.price_projection(`

                `price_data=filters.price_data(ex_all_offers=True)`

            `),`

        `)`



    `while True:`

        `for market_book in market_books:`

            `print(` 

                `market_book.market_id,`

                `market_book.inplay,`

                `market_book.status,`

                `market_book.total_matched,`

            `)`

        `sleep(1)`



        `if market_book.inplay == True:`



            `while True:`

                `for runner in market_book.runners:`

                    `print(`  

                        `runner.selection_id,`

                        `runner.status,`

                        `runner.last_price_traded,`

                        `runner.total_matched,`

                    `)`

                `sleep(1)`



`get_todays_games()`

*Tags: Feature Engineering, Deployment, Strategies*

---

**liam** - *11:15:09*

Better still use streaming instead

*Tags: General Technical*

---

**rob smith** - *11:17:58*

Faster than flumine that response Liam!

*Tags: General Technical*

---

## 2022-01-14

**Dario Scardina** - *16:31:18*

hello everyone :slightly_smiling_face:



Has somebody ever faced this exception



`ERROR Invalid response received: {'accountStatement': bla bla bla...`



with `trading.account.get_account_statement` method?



It seems that json response is not in a valid format...

*Tags: Errors Debugging, Strategies*

---

## 2022-01-16

**Colin** - *05:45:03*

Does anyone have any experience with developing their own bot and using it with a stake that is below the default minimum amount in your region/country for a prolonged period of time and for betting types that were not classed as closing out? If so, did you receive a warning from Betfair quite swiftly?

I note the 4th bullet point here, [https://developer.betfair.com/en/exchange-api/faq/](https://developer.betfair.com/en/exchange-api/faq/)

I'm keen to production-ise my bot soon, but I also want to start slow given it is not feasible for me to use Betfair's historical data to back test the algorithm I am using.

*Tags: Data Quality, Performance, Deployment, Strategies*

---

**liam** - *07:23:57*

They will charge if you go over the transaction limit, they won’t take issues with you using small stakes providing you aren’t taking advantage of rounding and doing it on low odds (recent update on the API gives a cryptic error message on this now anyway) 

*Tags: Errors Debugging*

---

**NC** - *14:17:26*

Can someone help or direct me to some python code that will read the .bz2 or .tar files from the betfair historical data and put it into a nice tabular format, ready for a simple csv? I have all the bz2/tar files on my hard drive aleady. thanks in advance

*Tags: Data Quality*

---

**NC** - *18:16:05*

```UnicodeDecodeError: 'charmap' codec can't decode byte 0x81 in position 581: character maps to &lt;undefined&gt;```

I'm encountering this error when trying to stream the historical data from a file on disk. I've copied the code from examplestreaminghistorical.py and changed the file_path for the historical stream, to that of my .bz2 file (also tried for .tar with similar result). I guess I've misunderstood something. Any ideas?

*Tags: Data Quality, Errors Debugging*

---

## 2022-01-17

**Unknown** - *10:19:15*

hopefully these help [@U010GM77S4W](@U010GM77S4W); convert to BSP bets on greyhound markets lapse if there is a scratching.

*Tags: General Technical*

---

**VT** - *20:51:03*

Does anyone know what this error is?



Traceback (most recent call last):

  File "C:...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 230, in _receive_all

    part = self._socket.recv(self.buffer_size)

*Tags: Errors Debugging*

---

**VT** - *22:33:08*

Betfair Stream:

Traceback (most recent call last):

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 230, in _receive_all

    part = self._socket.recv(self.buffer_size)



  File "C:\anaconda3\lib\ssl.py", line 1226, in recv

    return self.read(buflen)

  File "C:\anaconda3\lib\ssl.py", line 1101, in read

    return self._sslobj.read(len)

ConnectionResetError: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto



During handling of the above exception, another exception occurred:

*Tags: Errors Debugging*

---

## 2022-01-18

**liam** - *07:46:55*

Looks to be network or server side, quite common, are you using the error handling example?

*Tags: Errors Debugging, Deployment*

---

**VT** - *19:39:53*

No, I'm now analyzing the error handling code, thanks Liam.

*Tags: Errors Debugging*

---

**birchy** - *22:48:38*

[@U4H19D1D2](@U4H19D1D2) just to confirm... do Flumine PR's #538 and #539 now allow us to "safely" run backtests without double counting the matchable volumes? I have some icky historical code in my strategies that essentially checks `current_bet != last_bet` to avoid the double betting issue but of course that means I miss quite a few opportunities where the available size is &gt; my stake/payout.

*Tags: Strategies*

---

## 2022-01-19

**VT** - *14:57:32*

Hi guys, quick question, I'm not a programmer, of course it's something noticeable, I'm a gambler and trader (manual), but I have the feeling that this code is looking too ugly even by my standards.



a = [3, 3.05, 3.10, 3.15, 3.20, 3.25, 3.30]

b = 3.19



def find_tick(a, b):

     le = a[bisect_right(a, b)-1]   # less or equal

     ge = a[bisect_left(a, b)]        # greater or equal

     if abs(round(b-le, 3)) &lt; abs(round(b-ge, 3)):

         tick = le

     else:

         tick = ge

     return tick



The function returns the closest value of Betfair ticks, if the distance is equal to both sides it returns the highest tick. Is there a better way to do it?

*Tags: General Technical*

---

**Mo** - *14:59:55*

[https://github.com/liampauling/flumine/blob/770fcf967dab590cba63350dd2ee46839363f8d1/flumine/utils.py#L113-L123](https://github.com/liampauling/flumine/blob/770fcf967dab590cba63350dd2ee46839363f8d1/flumine/utils.py#L113-L123)

*Tags: General Technical*

---

## 2022-01-20

**PeterLe** - *08:50:04*

Morning, A quick question please (and sorry I should know the answer to this already).

In the following scenario	:



Place a £0.01 lay at 600

Place a £2.00 lay at 600



Would both bets be executed in a single transaction, and in approx the same amount of time? (Ie betids would be close)?



Thanks

*Tags: General Technical*

---

**James T** - *13:06:10*

Well 1p at 600 is under the min size limit of £10 min payout isn't it? 2p would be fine?



If you're using BetAngel then it would split the 1p bet into 3 transactions. 



I don't know if Flumine/BFLW has built-in functionality for placing lower than min size bets.



I might be missing something though…

*Tags: General Technical*

---

**PeterLe** - *13:53:57*

So this is what I received back:

20/01/2022 13:45:38: [G_Auto 1] :  £ 2.00 Lay bet placed on Geordie Des Champs at 1.7. Entirely unmatched when it initially reached the market.   Ref: 256340606189          ( Fill or kill bet with 5 seconds delay.   )

20/01/2022 13:45:40: [G_Auto 1] :  £ 0.40 Lay bet placed on Geordie Des Champs at 1.7. Entirely unmatched when it initially reached the market.   Ref: 256340607494          ( Fill or kill bet with 5 seconds delay.   )



Given the difference in the Betid's you're probably right James.

So to close the loop on this...The reason i was asking this originally is because I was reviewing some data last night against a particular strategy and the sub £2 bets seemed to be doing much better overall (irrespective of the odds). i was wondering if it was because these bets were hitting the market slightly behind the £2 bets...now I know :grinning:

*Tags: Strategies*

---

## 2022-01-21

**VT** - *20:16:07*

Hi guys does anyone have a simple script to measure our latency with betfair?

*Tags: Performance*

---

## 2022-01-22

**J** - *11:22:59*

Hi, just wondering how we get horse names in python using betfairlightweight? Sorry for no doubt what's a silly question, but the only place I can find online pointed me here

*Tags: General Technical*

---

## 2022-01-24

**Techno** - *22:36:47*

Hi guys. I have noticed on the historical data, there are prices (odds) taken that aren't on the betfair exchange, Like 3.47 and 4.68 etc.  How are these figures calculated? Are they average odds or something? Thanks.

*Tags: Data Quality*

---

## 2022-01-25

**Jorge** - *08:07:54*

No, I mean market_book.status while streaming

*Tags: General Technical*

---

**Nacho Uve** - *08:42:07*

Hello,



I can not download any historical data from [http://historicdata.betfair.com|historicdata.betfair.com](http://historicdata.betfair.com|historicdata.betfair.com). Unfortunately this service is not available to customers in some countries.

I'm interesting in historical soccer data to start to try flumine strategies.



Could someone who already has these files share them?

Are there any alternative link with historical files to download?



Thank you very much!

*Tags: Data Quality*

---

## 2022-01-27

**D C** - *13:28:35*

Is anyone having issues with the GPS stream today? 2 races in and no bets placed and also don't seem to have any data. I have made a new change so probably my fault but thought I would ask the question

*Tags: General Technical*

---

**Peter** - *17:32:01*

[@U013ZS16QJZ](@U013ZS16QJZ) you're trying to solve a problem, but asking how to implement part of the solution you've come up with. If you share the problem, the experience in this forum may lead you to a better/easier solution.

*Tags: General Technical*

---

**VT** - *17:49:11*

I only want to receive updates from one market at a time, so suppose I make the request for market 1.193143509, just back and lay odds, as fast as possible, that's all.



But suppose I want to trade the market, now I will receive back and lay value from market Y, so I would like this stream to send me the values of the new market only and stop searching for the previous values.



The question was how to stop this thread that your code creates, which I copied and pasted above.

*Tags: General Technical*

---

**Dario Scardina** - *17:55:52*

Did you find some solution? :slightly_smiling_face: I'm also interested in it... but I've not been able to solve  this problem :disappointed:

*Tags: General Technical*

---

## 2022-01-28

**VT** - *17:33:01*

yes, I've known this for some time, I'm using python to create my strategies, commands, shortcuts and after everything is tested I intend to invest in someone to reprogram in C.



But only after everything is tested because I use other APIs and database to 'predict' the price of goals live in football.

*Tags: Deployment*

---

**VT** - *17:39:17*

I would also like to say that I am very grateful for everyone's help, even with my primitive knowledge *I can send and close orders (market stream hosted in the UK) faster than any program I've ever used*. Having access to the stream also opens up many possibilities for semi-automated trading in live football.

*Tags: Deployment, Strategies*

---

**birchy** - *19:35:17*

This may help: [https://github.com/captain-igloo/greentop|https://github.com/captain-igloo/greentop](https://github.com/captain-igloo/greentop|https://github.com/captain-igloo/greentop)

*Tags: General Technical*

---

## 2022-01-29

**birchy** - *09:13:42*

[@U013ZS16QJZ](@U013ZS16QJZ) well if you're familiar with python, you also have the option to rewrite specific functions in C and then import them into python. But obviously you'd need to profile the code first to find the bottlenecks.

*Tags: General Technical*

---

## 2022-01-30

**Techno** - *23:00:33*

So, how do you all handle price reductions in horse races when a horse is a non-runner? Those aren't prices people have taken, they're prices reduced by Betfair. I am looking at the historical data and it seems like a lot of the races have price reductions.

*Tags: Data Quality*

---

## 2022-01-31

**Rob** - *12:42:06*

Hi Guys (I’m a lurker from the Aus Quants channel), Im doing some work with [@UUUAJQZ5J](@UUUAJQZ5J) writing some content for the BF Aus Automation hub around using the historic data files. The main problem I’m tasked with addressing is the speed and performance of how long it takes to parse through the files and the ease and developer experience when doing so. 

I’ve written a new python library whose aim is solely to read/parse these files and I’ll be open sourcing it along with my article and tutorials. The library is written entirely in rust and uses some tricky things like in place deserialisation and offloading decompression to threads to significantly speed up the parsing performance.



Im now onto writing the accompanying article and I want to make sure I cast betfair_lightweight in the best possible light. Im not a python dev at all, I don’t understand its concurrency patterns or its async capabilities and Im worried my benchmark code (stolen from the hub tutorials) is missing some simple optimisations that might dramatically increase the performance. In particular the bz2 decompression seems to happen in the same thread blocking everything else, so seems like a good candidate to offload to a thread/async?



Any help people could share here would be much appreciated!

*Tags: Performance*

---

**Rob** - *12:42:23*

```from typing import Sequence 



import logging

import unittest.mock

import os

import tarfile

import zipfile

import bz2

import glob

import betfairlightweight



market_paths = [

    "data/2021_10_OctRacingProAu.tar",

]



# loading from tar and extracting files

def load_markets(file_paths: Sequence[str]):

    for file_path in file_paths:

        if os.path.isdir(file_path):

            for path in glob.iglob(file_path + '**/**/*.bz2', recursive=True):

                f = bz2.BZ2File(path, 'rb')

                yield f

                f.close()

        elif os.path.isfile(file_path):

            ext = os.path.splitext(file_path)[1]

            # iterate through a tar archive

            if ext == '.tar':

                with tarfile.TarFile(file_path) as archive:

                    for file in archive:

                        yield bz2.open(archive.extractfile(file))

            # or a zip archive

            elif ext == '.zip':

                with zipfile.ZipFile(file_path) as archive:

                    for file in archive.namelist():

                        yield bz2.open(archive.open(file))

    return None





logging.basicConfig(level=logging.WARN)

trading = betfairlightweight.APIClient("username", "password", "appkey")

listener = betfairlightweight.StreamListener(max_latency=None)



count = 0

for i, file_obj in enumerate(load_markets(market_paths)):

    with unittest.mock.patch("builtins.open", lambda f, _: f):  

        stream = trading.streaming.create_historical_generator_stream(

            file_path=file_obj,

            listener=listener,

        )

        gen = stream.get_generator()

    

        for market_books in gen():

            for market_book in market_books:

                count += 1



        print("&gt; Market {} Update {}".format(i, count), end='\r')

                ```

*Tags: Performance, Strategies*

---

**liam** - *12:46:18*

Real shame you haven't looked to improve / speed up bflw instead

*Tags: Performance*

---

**Rob** - *12:46:56*

Im just not a python dev, I doubt I would be able to contribute much of value

*Tags: General Technical*

---

**Rob** - *12:48:08*

If you were interested in upstreaming some of my rust parsing code into bflw tho, I would be happy to work with you

*Tags: General Technical*

---

**liam** - *13:00:47*

Is this rust or python? My understanding is that to make rust bindings you need to have a pretty good knowledge of cpython? Is the code open source?



Regarding your example you would need to set `lightweight=True` and `update_clk=False` on the listener for a fairer comparison. Ideally the `[speed]` install is used as well, see [https://liampauling.github.io/betfair/advanced/#performance|here](https://liampauling.github.io/betfair/advanced/#performance|here) for the performance docs

*Tags: Getting Started, Performance*

---

**Rob** - *13:06:44*

The [http://pyo3.rs|pyo3.rs](http://pyo3.rs|pyo3.rs) rust library is pretty top notch with great documentation and build tools which is what I used to create the library, the code will be open source as soon as happy with what the python api should look like, but I hope to get it finished this week

I didnt mean to tread on anybodies toes here.

*Tags: General Technical*

---

**liam** - *13:08:05*

just seems a shame as we could have worked together and created a rust binding that plugged straight into bflw

*Tags: General Technical*

---

**liam** - *13:17:56*

Its CPU bound so threads won't help, are you assuming the decompression is slow or have you profiled? Can you share the profile if so?

*Tags: Performance*

---

**liam** - *13:33:02*

relative to the streaming logic I doubt it would even register

*Tags: General Technical*

---

**thambie1** - *13:39:37*

Can't speak to bflw, but in my java code base decompression was taking ~20% CPU, so I just started storing the files uncompressed

*Tags: General Technical*

---

**liam** - *13:46:31*

Locally I get around 5 markets/second (AU win markets) using bflw compared to your ~1.6, very intrigued to see your Rust code if you are getting ~40+.



Tbh I would say file decompression would be out of scope for the library and this is up to the user on a cost/ease of use case

*Tags: General Technical*

---

**Rob** - *13:58:16*

im having a quick read on how to publish to pypi and I’ll happily share an early version

*Tags: General Technical*

---

**thambie1** - *17:12:20*

[https://betfairlightweight.slack.com/archives/C4H05KKMY/p1642176260006700](https://betfairlightweight.slack.com/archives/C4H05KKMY/p1642176260006700)

*Tags: General Technical*

---

**VT** - *19:14:13*

Hi everyone, I received this error about 10 times today, I had received it before but it was not common, now I can't stay connected to the stream for 5 minutes and the connection is interrupted. I connect to one market only.



Exception in thread Betfair Stream:

Traceback (most recent call last):

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 230, in _receive_all

    part = self._socket.recv(self.buffer_size)

  File "C:\Users...\anaconda3\lib\ssl.py", line 1226, in recv

    return self.read(buflen)

  File "C:\Users...\anaconda3\lib\ssl.py", line 1101, in read

    return self._sslobj.read(len)

ConnectionResetError: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "C:\Users...\anaconda3\lib\threading.py", line 932, in _bootstrap_inner

    self.run()

  File "C:\Users...\anaconda3\lib\threading.py", line 870, in run

    self._target(*self._args, **self._kwargs)

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 60, in start

    self._read_loop()

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 212, in _read_loop

    received_data_raw = self._receive_all()

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 234, in _receive_all

    raise SocketError("[Connect: %s]: Socket %s" % (self._unique_id, e))

betfairlightweight.exceptions.SocketError: [Connect: 2]: Socket [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto

*Tags: Errors Debugging*

---

## 2022-02-01

**liam** - *13:50:42*

Got some errors when trying to install from PyPi, expected?



```  error: could not compile `bzip2-rs` due to 5 previous errors

  warning: build failed, waiting for other jobs to finish...

  error: build failed

  💥 maturin failed

    Caused by: Failed to build a native library through cargo

    Caused by: Cargo build finished with "exit status: 101": `cargo rustc --message-format json --manifest-path Cargo.toml --release --lib -- -C link-arg=-undefined -C link-arg=dynamic_lookup -C link-args=-Wl,-install_name,@rpath/betfair_data.cpython-39-darwin.so`

  🔗 Found pyo3 bindings

  🐍 Found CPython 3.9 at /Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9

  error[E0432]: unresolved import `std::io::ReadBuf`

   --&gt; /Users/liampauling/.cargo/git/checkouts/bzip2-rs-5185c758a5d48e65/748b36f/src/decoder/parallel/reader.rs:2:5

    |

  2 | use std::io::ReadBuf;

    |     ^^^^^^^^^^^^^^^^ no `ReadBuf` in `io`





  error[E0432]: unresolved import `std::io::ReadBuf`

   --&gt; /Users/liampauling/.cargo/git/checkouts/bzip2-rs-5185c758a5d48e65/748b36f/src/decoder/reader.rs:2:5

    |

  2 | use std::io::ReadBuf;

    |     ^^^^^^^^^^^^^^^^ no `ReadBuf` in `io`





  error[E0599]: no method named `read_buf` found for type parameter `R` in the current scope

     --&gt; /Users/liampauling/.cargo/git/checkouts/bzip2-rs-5185c758a5d48e65/748b36f/src/decoder/parallel/reader.rs:110:37

      |

  110 |                         self.reader.read_buf(&amp;mut read_buf)?;

      |                                     ^^^^^^^^ method not found in `R`





  error[E0599]: no method named `read_buf` found for type parameter `R` in the current scope

    --&gt; /Users/liampauling/.cargo/git/checkouts/bzip2-rs-5185c758a5d48e65/748b36f/src/decoder/reader.rs:68:37

     |

  68 |                         self.reader.read_buf(&amp;mut read_buf)?;

     |                                     ^^^^^^^^ method not found in `R`





  error: aborting due to 4 previous errors





  Some errors have detailed explanations: E0432, E0599.



  For more information about an error, try `rustc --explain E0432`.



  Error: command ['maturin', 'pep517', 'build-wheel', '-i', '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9'] returned non-zero exit status 1

  ----------------------------------------

  ERROR: Failed building wheel for betfair-data

Failed to build betfair-data

ERROR: Could not build wheels for betfair-data which use PEP 517 and cannot be installed directly

WARNING: You are using pip version 20.3.1; however, version 22.0.2 is available.

You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.```

*Tags: Getting Started, Errors Debugging*

---

**Rob** - *14:11:43*

(for anybody else that encounters the problem)

*Tags: General Technical*

---

**Rob** - *14:51:25*

hey [@U4H19D1D2](@U4H19D1D2), get it all working ok? and have any feed back or performance results?

*Tags: Performance*

---

**Rob** - *14:53:24*

happy to answer any questions. I imagine the big speed up is from the in place deserialisation

*Tags: Performance*

---

**liam** - *14:54:20*

the python bottleneck is all in the logic required to update and store the order book for `ex`

*Tags: General Technical*

---

**Rob** - *14:58:50*

Yea I dont think that it would cost me much performance, would be the most expensive when lots of new prices get added and the vec needs to allocate a new buffer. But that wouldnt happen on many updates.

I imagine my biggest slow down is in the json and number parsing

*Tags: Performance*

---

**Rob** - *15:00:22*

if betfair slightly changed the json to make sure ids were always the first field I could probably eek out a decent chunk more performance using a simdjson processor. But I dont see that ever actually happening

*Tags: Performance*

---

**VT** - *22:45:22*

Guys, if anyone can help, I'm not able to proceed with my tests, every 5 minutes the error stream.



I ran the example code for examplestreamingerrhandling.py errors and the answer is below.



I even thought it could be due to data consumption, but the account I currently use must have already paid more than 20 thousand pounds just in commissions. I use a VPS on AWS and I leave the stream market running there, I believe it has nothing to do with it, I connect to the server and my computer at the same time.



I'm going to stop testing because I'm afraid my account will be blocked.



+++++++++++++++++++++++++++++++++++++++++++++++++++++++



The latency error appears 100% of the time, but it used to be between 0.6 and 0.9, this value above 1 had never seen. I'm in Brazil and this is the best we can get, 200mb download and upload, fiber optics.



WARNING:betfairlightweight.streaming.stream:[MarketStream: 1003]: Latency high: 1.6549324989318848



+++++++++++++++++++++++++++++++++++++++++++++++++++++++



ERROR:__main__:MarketStreaming run error



Traceback (most recent call last):

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 230, in _receive_all

    part = self._socket.recv(self.buffer_size)

  File "C:\Users...\anaconda3\lib\ssl.py", line 1226, in recv

    return self.read(buflen)

  File "C:\Users...\anaconda3\lib\ssl.py", line 1101, in read

    return self._sslobj.read(len)

ConnectionResetError: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "C:\Users...\AppData\Local\Temp/ipykernel_6984/859485830.py", line 61, in run

    self.stream.start()

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 60, in start

    self._read_loop()

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 212, in _read_loop

    received_data_raw = self._receive_all()

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 234, in _receive_all

    raise SocketError("[Connect: %s]: Socket %s" % (self._unique_id, e))

betfairlightweight.exceptions.SocketError: [Connect: 1003]: Socket [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto

INFO:__main__:Starting MarketStreaming

*Tags: Errors Debugging, Performance, Deployment*

---

**Alex Alex** - *23:08:43*

Never encountered this kind of error before but my guess would be that it takes some time to get a response for your request reaching the server. `Latency high` sounds like it. What is the CPU load on your VM when you encounter this error?

*Tags: Errors Debugging, Performance, Deployment*

---

**VT** - *23:12:06*

Hi [@U030L8PPSHM](@U030L8PPSHM), VM stands for Virtual Machine? If so, I have no problems with the VPS, I never had an error in the market stream hosted on AWS, the error is on my local machine and has nothing to do with CPU or memory.

*Tags: Errors Debugging, Performance, Deployment*

---

## 2022-02-02

**Mo** - *05:47:16*

Firstly, you are being paranoid, this has nothing to do with you being blocked



It's a simple network error and the fact you have 200 mb fibre is irrelevant. Your bandwidth has nothing to do with latency and the packets have a very long way to travel to you from Dublin



I don't speak Brazilian Portugese but I believe it's saying the remote host (i.e. Betfair) has closed the connection. Possibly at the TCP level your ACK packets are not getting back to them so they believe the connection has been lost. You might be able to keep the connection from dropping by tweaking some of your network settings but I don't use Windows so can't offer any specific advice there



The simple solution is what you're already doing which is to use a server (much) closer to Betfair

*Tags: Errors Debugging, Performance, Deployment*

---

**Mo** - *07:23:48*

Will it be possible to leverage this speed improvement in bflw or is constructing the Python objects a bottleneck that would dominate any improvement?

*Tags: Performance*

---

**Dave** - *07:53:20*

Free tier AWS should be sufficient to validate the above.

*Tags: Deployment*

---

**Unknown** - *07:55:14*

Spits out python objects so see no reason why this can't be used in bflw some way

*Tags: General Technical*

---

**Rob** - *08:13:20*

There is some different behavior tho, bflw creates new values on each update, whereas I update the value in place, and have a copy() method if you want to save an  image (copy) of the market at that time.

This is a subtle but significant end user difference that could trip people up. You could just call copy on each update, but there would be much faster ways of doing this if you wanted to keep the immutable behavior

*Tags: General Technical*

---

**Rob** - *08:28:59*

```runner.total_volume should be total_matched

no marketDefinition?

decimal/float rounding (e.g. 6429.689999999999)

pt / publishTime not present?```

1. yeap happy to rename this

2. all the market definition / runner definition fields are mapped onto the market / runner

3. dont know what I can do about this, I just expose the floats to python - could maybe try and tidy them but probably not worth the computation

4. should be there on market.publish_time



*Tags: General Technical*

---

**Rob** - *08:29:57*

another limitation is that all my dates are just timestamps, as I cant link to the python datetime object and be abi3 stable.

*Tags: General Technical*

---

**Mo** - *08:39:50*

[@U4H19D1D2](@U4H19D1D2) Returning MarketBook objects is less of a problem if this library solves the overhead. It would be even less of a problem if you added a `__getitem__` method to `BaseResource`. Is that something you've considered?

*Tags: General Technical*

---

**liam** - *08:54:33*

Yeah I imagine it would still be a lot faster than python. Personally I would prefer to keep the streaming data structure as provided with the definition.



[@UBS7QANF3](@UBS7QANF3) no but happy to add

*Tags: General Technical*

---

**Rob** - *08:58:18*

realistically the answer is probably making a new bflw specific library, that can match the data structure and immutability, Then you can just feed bytes in and it spits out market objects

*Tags: General Technical*

---

**Dan Q** - *23:57:55*

Yes, there's a time_in_force argument when setting a limit order which can be expressed in seconds or with 'FILL_OR_KILL', which will match the order against currently available funds to the extent that it's possible and immediately dump the rest. There's documentation about it on Betfair.

*Tags: General Technical*

---

**Dan Q** - *23:59:17*

I could be wrong about the specifics, I just looked at the docstring and I don't think you can pass in seconds to time_in_force, but I know there's a way to do it in BF/BFLW

*Tags: General Technical*

---

## 2022-02-03

**ShaunW** - *20:43:01*

I dunno [@U8ZGPN5H9](@U8ZGPN5H9), I think if people have got a tenner to spend they'll spend it whatever the minimum is.  And for new people considering automation then £1 might be less daunting if they're new to letting things run all day.  Possibly also a small nod to the authorities too that they're trying to consider the cost/impact of gambling? Can't change it anyway and vs real problems it doesn't seem too bad.

*Tags: Getting Started*

---

**Jonjonjon** - *21:23:12*

Will it encourage more people to attempt rounding error exploits?

*Tags: Errors Debugging*

---

## 2022-02-04

**Chris H** - *10:42:47*

Question, does anyone have any data for betfair which contains mappings of football event id's to competition ids for past events they would be willing to share?



I have historical data since Dec 2019 and there does not seem to be a way to get the competition id from the api, and trying to join everything up via the event/runner names sounds... not fun :sweat_smile:

*Tags: Data Quality*

---

**Lee** - *10:46:01*

Oh sorry just re read the question

*Tags: General Technical*

---

**Nacho Uve** - *16:45:04*

Hello,



I am starting with bf exchange bots for soccer. At this initial point with a very limited budget, I have to work with BF BASIC PLAN and streams that I have recorded (a ridiculous amount, :/  ).



So, I would like to do backtesting with historical data of the BASIC PLAN, but it does not include "atb/atl", just "ltp".

I think it could be a good idea to transform "ltp" of each runner into a some kind of "atb" and "atl" (like it was the best price). I think it can match the needs of my simple test strategies.



Do you think it makes any sense? What would be the best approach to do it(a middleware, a custom FlumineBacktest class, a custom FlumineMarketStream?

*Tags: Data Quality*

---

**Alex Alex** - *21:42:06*

You want to model the spread as a function of traded prices. In a liquid market, if you see an uptick compared to the ltp this is likely hitting the offer price (lay price in betfair terms). Not really accurate if you don't have many trades though.

*Tags: Strategies*

---

**VT** - *22:06:35*

Guys I found the reason for the error I posted a few days ago, at least I believe I found it. As I only seek the stream of 1 market at a time, it is necessary to end this connection because Betfair, somehow, even if the market is closed, takes a long time to close this connection. Now, always before opening a new request to stream I finalize the last one. Since I started doing this I haven't had that error anymore.



streaming.stop()

INFO:__main__:Stopped MarketStreaming 1001

*Tags: Errors Debugging*

---

## 2022-02-08

**Nacho Uve** - *11:02:40*

How can I filter a historical data stream by a specific marketType?

*Tags: Data Quality*

---

**liam** - *11:05:10*

[https://liampauling.github.io/flumine/quickstart/#market-filter](https://liampauling.github.io/flumine/quickstart/#market-filter)

*Tags: General Technical*

---

**liam** - *11:14:59*

no, not without reading the files, flumine does it using this code



```def get_file_md(file_dir: Union[str, tuple], value: str) -&gt; Optional[str]:

    # get value from raw streaming file marketDefinition

    if isinstance(file_dir, tuple):

        file_dir = file_dir[0]

    with open(file_dir, "r") as f:

        first_line = f.readline()

        update = json.loads(first_line)

    if "mc" not in update or not isinstance(update["mc"], list) or not update["mc"]:

        return None

    md = update["mc"][0].get("marketDefinition", {})

    return md.get(value)```

*Tags: General Technical*

---

**Nacho Uve** - *11:38:49*

[@UBS7QANF3](@UBS7QANF3) Yes, It was my first idea. Is it resolved that filtering in betfairutil?

*Tags: General Technical*

---

**VT** - *16:04:47*

Hi guys, I'm trying to send a Fill or Kill order but I can't see the error.



    limit_order_filter = filters.limit_order(

        size=3,

        price=2,

        persistence_type='LAPSE',

        time_in_force='FillOrKill'

        )



    instructions_filter = filters.place_instruction(

        selection_id=id_leader,

        order_type="LIMIT",

        side='BACK',

        limit_order=limit_order_filter,

        )



    order = trading.betting.place_orders(

        lightweight=True,

        async_=True,

        market_id=id_mercado,

        instructions=[instructions_filter],

        session=session

        )



--------------------------------------



Error: {'code': -32602, 'message': 'DSC-0018'}

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32602, 'message': 'DSC-0018'}, 'id': 1}



I searched for DSC-0018 error in API



DSC-0018 MandatoryNotDefined Client 400 A parameter marked as mandatory was not provided

*Tags: Errors Debugging, Strategies*

---

**Unknown** - *16:14:19*

Thanks [@UUCD6P13J](@UUCD6P13J) was the problem, in the commented code this FillOrKill.

*Tags: General Technical*

---

**Dan Q** - *20:17:35*

Hey, just wanted to share my process and do a quick sanity check since I'm not super familliar with BFLW/Flumine. This is my process right now:

• Container on AWS ECS recording Betfair odds data every 5 seconds - using the API via BFLW - for the markets I'm interested in (NBA) and writing to a DB

• Model trained on historical data from Betfair pro files (lots of cleaning/processing as there are very weird behaviors in the historicals that don't reflect the live environment)

    ◦ All historical reading and treatment done with code I wrote myself, no BFLW here

• External data read from other sources and also fed into a DB. Historicals are used along with BF historicals to train the model.

• Live, constantly running betting bot reads the most recent data from the DB for live games joined with external data, manipulates it to the format the model expects, and makes a decision based on model output and strategy parameters

    ◦ Again a custom class/process, BFLW used only to connect to the API to collect bankroll info and place orders

• If bet decision is made, sends a fill or kill limit order through BFLW client

• Strategy parameters are determined through intensive backtesting/bootstrapping on the live data collected so far

    ◦ Also a custom backtesting suite I wrote to calculate expected profitability and other stats over n games in the season based on resampling previous games

This is working well and is thankfully profitable so far, but I feel like I may have reinvented the wheel at least five times during the whole thing considering BFLW/Flumine exists. I've seen a lot of mention of backtesting on Flumine, would it also allow more complex operations like I described with using external data and output from a model? What is the benefit of doing so vs. the solution I've described? One thing I'm not doing which I know I could be is streaming and recording the data instead of the 5-second snapshots I'm using right now, but so far it's served me well and I haven't seen a reason to switch.

*Tags: Data Quality, Deployment, Strategies*

---

**liam** - *20:40:41*

Certainly recommend streaming, much lighter on CPU and reduces the complication. 



What issues are you seeing the historical data? Thought the pro stuff was good.



flumine can do whatever you want, the selling point is the switch to backtest / paper / live with no changes to your code.  Switching wouldn’t make sense depending on how advanced your current setup is and/or you want some of the features. 

*Tags: Getting Started, Data Quality, Feature Engineering, Deployment*

---

**Dan Q** - *21:51:13*

So there's the issue where atb and atl are non-virtualized and thus not very useful comparisons to the live pulled atb/atl data. The LTP is usually pretty close to both though, when there's decent liquidity (which there usually is), so it's fine. However, there are intense volatility spikes in LTP that I can't explain and had to take great pains to treat. Stuff like a team being comfortably in the lead at the very end of the game with 1.01 odds and then suddenly updates will come in at much higher odds, like 2.4 in the middle of the more sensible 1.01 prices. Stuff like that. It got to the point where for the historicals I need to calculate local volatility in the LTP time series and take the rolling median as the actual LTP and the rolling min as the best_atb just to be safe, otherwise my models perform way worse because there's so much noise.



It could be an issue of me making some incorrect assumption early on when reading in the data as I had to kind of blindly feel my way through rather than using something like BFLW to read them in properly, but I don't think so, I'm pretty sure I did it right as I've reviewed it a lot.

*Tags: Deployment, Strategies*

---

**Dan Q** - *22:08:23*

[@U4H19D1D2](@U4H19D1D2) Do either Flumine or BFLW have a way to get processed data (ie timestamped best ATB/LTP) out of historical files? I could only find the bits relating to pulling the historical files via the API endpoint

*Tags: General Technical*

---

## 2022-02-09

**Guy Incognito** - *06:37:19*

Hey Guys, trying to follow the flumine quick start guide and just place bets on any greyhound market but doesn't seem to be working. Am I missing something?



```from flumine import BaseStrategy

from flumine.order.trade import Trade

from flumine.order.order import LimitOrder





class ExampleStrategy(BaseStrategy):

    def start(self):

        # subscribe to streams

        print("starting strategy 'ExampleStrategy'")

        

    def check_market_book(self, market, market_book):

        # process_market_book only executed if this returns True

        if market_book.status != "CLOSED":

            return True



    def process_market_book(self, market, market_book):

        for runner in market_book.runners:

            if runner.selection_id == runner.selection_id:

                trade = Trade(

                    market_id=market_book.market_id, 

                    selection_id=runner.selection_id,

                    handicap=runner.handicap,

                    strategy=self

                )

                order = trade.create_order(

                    side="BACK", 

                    order_type=LimitOrder(price=1000, size=5.00)

                )

                market.place_order(order)```

```from betfairlightweight.filters import (

    streaming_market_filter, 

    streaming_market_data_filter,

)



strategy = ExampleStrategy(

    market_filter=streaming_market_filter(

        event_type_ids=["4339"],

        country_codes=["AU"],

        market_types=["WIN"],

    ),

    market_data_filter=streaming_market_data_filter(fields=["EX_ALL_OFFERS"])

)



framework.add_strategy(strategy)```

```framework.run()```



*Tags: Strategies*

---

**liam** - *08:41:04*

Bflw will output historical data as per a normal marketBook request / stream

*Tags: Data Quality*

---

**Guy Incognito** - *14:49:50*

The only output I get is 

```starting strategy 'ExampleStrategy'

```

From my understanding should this code just place bets on all selections on all greyhound races in Australia over and over again?

*Tags: Strategies*

---

## 2022-02-10

**Nacho Uve** - *10:55:13*

Cool! It works!



Just a minor comment: it raises the exception:



&gt;  ValueError: No objects to concatenate

if the `market_type_filter` contains a market_type that is not in the file.



I don't know if it is better to catch that with a better message or it is ok like that.



Thank you very much!

*Tags: Errors Debugging*

---

**Laybot McBackbot** - *14:15:00*

Does anyone know what's the cheapest GPU instance on AWS and how much it costs per hour?

*Tags: Deployment*

---

**Laybot McBackbot** - *16:54:16*

Talking about ML model. I need gpu for inference for this particular model.  Usually I train with GPU on my workstation then use CPU for inference. But it doesn't work for this particular model.

*Tags: Strategies*

---

**Laybot McBackbot** - *19:09:17*

I have too many different values for one hot encoding. Also, the algorithm it uses for categorical data somehow is working great for me. So I would rather stick with it because between screwing around with what is working I prefer to choose the easier solution -- either get a gpu machine on AWS or continue running from home for now.



My return from this model should be able to cover the AWS cost but I would still like to save money wherever possible.

*Tags: Deployment, Strategies*

---

**Laybot McBackbot** - *19:29:52*

So yeah, the problem is I have too many  categories for OHE on CPU and other methods like label encoding are not as good so that's why I need a GPU.

*Tags: General Technical*

---

## 2022-02-11

**J** - *05:03:30*

Hi, just wondering if the marketrecorder in flumine doesn't record runner names? Great app btw!

*Tags: General Technical*

---

**J** - *05:05:58*

I can see the runner names ('name')  in the runners: part of the jso in the historical info but not the flumine recorded data

*Tags: General Technical*

---

**J** - *05:44:33*

Did you find a good solution [@U0266A8579B](@U0266A8579B)? I'll be in the same situation soon, moving from workstation with a 3070 on CUDA for ML to AWS

*Tags: Deployment*

---

**liam** - *08:01:23*

Extra [https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py|middleware](https://github.com/liampauling/flumine/blob/master/examples/middleware/marketcatalogue.py|middleware) for this so you can store the catalogue

*Tags: General Technical*

---

**anomaly** - *23:52:40*

Hi all, got some confusion about reconnects and market subscriptions.



My main question is If you want to continually get updates for newly added marketbooks, should you re-init the stream.subscribe_to_markets() _without_ including *initial_clk* and *clk*? For example, say the new day has ticked over and new markets exist in the marketcatalogue, how do you ensure that these new markets will be picked up in the stream? My current understanding is that using the *clk args are for only a pre-defined stream, and not for a complete refreshed re-subscription.

Right now I am 1. initialising the stream once in __init__ using self.stream.start in a new thread with daemon=True (as in the examples), then 2. periodically calling stream.subscribe_to_markets() in my runtime loop without initial_cl and clk when the market_catalogue polling code detects new markets. I never .start() that previously created thread ever again.  Is this the best way to go about this?



cheers!

*Tags: General Technical*

---

## 2022-02-12

**liam** - *06:58:20*

Are you filtering by marketIds? Idea with streaming is to use a filter which doesn’t require a resubscribe. It isn’t designed for your current use, the clk’s are for resubscribe when you lose connection and want to reconnect 

*Tags: General Technical*

---

**anomaly** - *23:04:01*

Not filtering with market ids right now, however doesn't the same problem occur even if you were to use market ids in the filter: new day ticks over, new markets appear in the catalogue, the old market filter is not relevant anymore. I guess the question boils down to how do ensure a continually updated set of streams for the "current" set of markets. Would a reasonable approach simply be manage pool of streaming threads that periodically get killed and repopulated with new stream threads as new markets appear in the market catalogue?  Right now I can get away with a single thread per (betting_type, country) and periodically kill existing and create a new streaming thread when new markets appears in the market catalogue. It seems to be working but not sure if it's best practice

*Tags: Strategies*

---

## 2022-02-14

**Beeblebrox** - *11:30:29*

[@UBS7QANF3](@UBS7QANF3), I'm using Betfairviz with recorded data from Flumine and it's displaying the selections with their ids rather than their names.  Is there an option to get it to read the market catalogue as well as the price data so it can display them?

*Tags: General Technical*

---

**Mo** - *12:01:06*

CSVs are the first step, I'm aiming to build these into an sqlite database with a Python wrapper so you could do something along the lines of



```from betfairmappings import betfair_selection_id_to_runner_name_map



betfair_selection_id_to_runner_name_map.get(42791012)```

In the ultimate solution described above, `betfairutil` will import `betfairmappings` (if it is installed, will be a big package so definitely don't want it to be a required dependency) then use it to add runner names to self recorded data completely automatically

*Tags: Getting Started*

---

## 2022-02-16

**Oliver Varney** - *06:47:11*

[@UUX1L88MC](@UUX1L88MC) this particular file for total_matched sum was a betfair file (from their site). Also setting cumulative_runner_tv=False on the file still seems to throw up some weird numbers. Im assuming like bflw with this setting set to false it doesnt recalc the value and just uses the raw betfair number?

*Tags: General Technical*

---

**Oliver Varney** - *08:01:25*

okay cool cheers mate, appreciate the help :slightly_smiling_face: My understanding and maybe [@U4H19D1D2](@U4H19D1D2) will correct me if im wrong, is that the bflw settings relate to recalculating the total_matched in historical files as betfair send through duff/unreliable data in there own files (i.e. the sum of tvs does not equal runner.total_matched, thus this can be set to recalc in the cache). Although i maybe wrong and what you describe above is the rationale specifically why bflw provides an option

*Tags: General Technical*

---

**Rob** - *08:04:50*

hmm, that could be the problem, I do use the tv value they provide for the runner.

Should I just sum the volume in the tv cache? There shouldnt be any performance worries so its just a matter of which is more accurate.

*Tags: Performance*

---

**Oliver Varney** - *08:07:38*

[https://github.com/liampauling/betfair/blob/0dd9c151d58ef1cfa065fa12dccd69e52104bc74/betfairlightweight/streaming/cache.py#L292](https://github.com/liampauling/betfair/blob/0dd9c151d58ef1cfa065fa12dccd69e52104bc74/betfairlightweight/streaming/cache.py#L292)

*Tags: General Technical*

---

## 2022-02-17

**Mo** - *05:43:47*

[@UUX1L88MC](@UUX1L88MC) - rust beginner question, I am trying to build the package from source. I have cloned it and am running `maturin develop` and get this error:



```💥 maturin failed

  Caused by: Cargo metadata failed. Does your crate compile with `cargo build`?

  Caused by: `cargo metadata` exited with an error: error: failed to parse manifest at `xxx`



Caused by:

  feature `edition2021` is required



  consider adding `cargo-features = ["edition2021"]` to the manifest```

*Tags: Getting Started, Errors Debugging, Feature Engineering*

---

**Mo** - *06:39:51*

Thanks, I've managed to get it to build. A couple of questions:



1. Are the files used in the benchmark freely available?

2. I'd like to be able to read gzip files containing single markets - i.e. the typical file format that is self recorded. I think you're already aware there is demand for this but wanted to know your timeline on implementing and whether it's worth me having a go as a way to get to grips with the code?

*Tags: General Technical*

---

**KG** - *07:13:47*

They historic data files have been shared with Rob to help with the parser creation, [@UBS7QANF3](@UBS7QANF3) - they're the same files that you can get from the historic data site hosted by the UK. 

*Tags: General Technical*

---

**Rob** - *07:50:30*

My plan after this is to work on some adapters, so we can have a mutable version, immutable version and a bflw compat version.



Theres good arguments for all of them, with differing pro’s con’s

*Tags: General Technical*

---

**Mo** - *08:18:22*

No pressure but what's your timeline for being able to generate bflw objects?

*Tags: General Technical*

---

**Oliver Varney** - *10:14:21*

has anyone had/thought about somhow storing a serialised full market book at check points and streaming from there, like somewhat of an index. i.e. if you had the full market book at 5 mins from the off, could you then combine that with the deltas and pick up from that point and skip the 20k messages prior?

*Tags: General Technical*

---

**Unknown** - *15:41:31*

[@UBS7QANF3](@UBS7QANF3) hooked up the dashboard to my database so I can now quickly download a market I've recorded and view it:

*Tags: General Technical*

---

**Andrew** - *16:36:45*

It seems that Betfair will close your _entire_ cricket score feed connection if you subscribe to a single bad eventId?

e.g.

```[

  ;; outgoing subscription request

  {

    "eventIds": [

      "31238985", ;; Islamabad United v Peshawar Zalmi, in progress right now

      "31238538"  ;; Australia v Sri Lanka in 17 hours from now

    ],

    "heartbeatMs": 10000,

    "id": 2,

    "op": "cricketSubscription"

  },



  ;; incoming subscription response

  {

    "connectionClosed": true, ;; !!!!

    "connectionId": "xxx",

    "errorCode": "INVALID_REQUEST",

    "errorMessage": "Not all provided eventIds are available to subscribe to. EventIds: [31238538]",

    "id": 2,

    "op": "status",

    "statusCode": "FAILURE"

  }

  ;; entire connection was killed!

]```

worse, you have to parse out which eventId failed from the `errorMessage` string. Is anyone else trying to subscribe to individual eventIds? Am I better off just leaving it blank and subscribing to the entire firehose?

*Tags: Errors Debugging*

---

## 2022-02-18

**Gooat** - *13:18:31*

How is latency versus the market looking?

*Tags: Performance*

---

**Alex Alex** - *14:10:54*

Thinking of moving my live model code into the flumine framework. I quite like that it seems to have the option to simulate execution and run backtest/live interchangeably. I want to 1) get the current score, and 2) do some calculations on all matches within an event to generate a return forecast. From the `tennsiexample.py` I gather that the correct way to do this within flumine is to add a background worker that generates a custom event once it is done doing all its calcs. Then move the return forecast into market context when processing custom event. Or is there a more straight forward way to do this within flumine?

*Tags: Deployment, Strategies*

---

**liam** - *15:38:46*

Depends how slow the calculation is, you can use a worker to update the context and do it in the strategy itself or like you describe in a worker

*Tags: Performance, Strategies*

---

**Alex Alex** - *16:05:33*

By doing it in the strategy itself I assume you mean run the calc for each market? The calculation is not super slow, but slow enough that I’d rather run it only once for all markets.

*Tags: Performance, Strategies*

---

**river_shah** - *18:41:09*

Ok…problem solved. All bets that I thought occurred over material event have been voided. That being said, if anyone can guide me to the flag to check managed / not managed markets would be very helpful.

*Tags: General Technical*

---

## 2022-02-19

**VT** - *03:23:02*

Hi [@U0128E7BEHW](@U0128E7BEHW), out of curiosity which league did you have problems with the timing of suspension?

*Tags: General Technical*

---

**liam** - *06:33:46*

if you are a flumine user and assuming the market is available in your filter:



```correct_score_market = market.event["CORRECT_SCORE"]```

*Tags: General Technical*

---

**rob smith** - *08:39:37*

Does anyone know of any examples of how to get the new cricket scores? I'm new to python and am struggling. Cheers

*Tags: Getting Started*

---

**Mo** - *10:37:41*

```import logging

import queue

import threading



import betfairlightweight



# setup logging

logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))  # change to DEBUG to see log all updates



# create trading instance (app key must be activated for cricket stream)

trading = betfairlightweight.APIClient("username", "password", app_key="appKey")



# login

trading.login()



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener, host="sports_data")



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()



# subscribe

streaming_unique_id = stream.subscribe_to_cricket_matches()



# check for updates in output queue

while True:

    updates = output_queue.get()

    for update in updates:

        print(update.json())```

*Tags: Getting Started, Errors Debugging, Deployment, Strategies*

---

## 2022-02-20

**river_shah** - *08:51:24*

[@U4H19D1D2](@U4H19D1D2) could we please get an example using flumine on recommended way to integrate this 

*Tags: General Technical*

---

**PeterLe** - *13:26:33*

Afternoon :grinning:

I was just looking at the account statement in my master account and i can see that I incurred Transaction Charges for &gt; 5000/Hour. However, it doesnt detail which of the sub accounts that incurred the charges. BDP had previously told me that the 5000 limit was per account. I just seemed to have clocked up more than I thought. I've no problem paying them, but it does dent the P&amp;L. I just wondered if anyone else had encountered a similar scenario and whether BDP would provide a breakdown if I were to ask them. Thanks

*Tags: General Technical*

---

## 2022-02-21

**Rob** - *16:06:46*

just finished a (very early) version of a bflw compatible version, which mirrors the same object layout, fields and immutability characteristics

*Tags: General Technical*

---

**Rob** - *16:07:49*

All the extra allocations and copying of data has taken a big hit on the performance - but im still getting a touch above 50 markets/s || 375,000 update/s which isn’t a bad place to start. I expect to be able to raise that a bit when I dig into a profiler

*Tags: Performance*

---

## 2022-02-22

**thambie1** - *11:53:06*

My bank is asking for proof of funds to make sure I'm not doing anything shady... I remember a few people had said they were able to request some documentation from Betfair. Was this by contacting bdp, or their account manager?

*Tags: General Technical*

---

**thambie1** - *11:55:26*

Just in case things don't get resolved quickly, have a suggestion of a friendly bank?

*Tags: General Technical*

---

**Mo** - *11:57:14*

All I can say is Natwest have never once questioned my Betfair withdrawals

*Tags: General Technical*

---

**thambie1** - *12:31:15*

Thanks guys! I'm using Revolut, looks like every other bank is good :joy:. I just got an automatic questionnaire about source of funds. Not a big deal yet, I'm just worried if it'll get escalated.

*Tags: General Technical*

---

**James T** - *14:25:39*

Yeah, I similarly know people who have had serious problems with Revolut. 

*Tags: General Technical*

---

## 2022-02-23

**Peter C** - *10:36:22*

In flumine, is it possible to subscribe to events across all available countries by leaving out the `country_code` filter in the `streaming_market_filter`, or do I need to supply a list of all possible countries?

*Tags: General Technical*

---

**Leon** - *11:19:05*

Hi [@UEA14GBRR](@UEA14GBRR) in answer to your question, your 2% comm rate under "My Betfair Rewards" only apply to .com markets. Betfair points are still accrued to offset the market base rate for ANZ markets

*Tags: General Technical*

---

## 2022-02-24

**Unknown** - *01:24:24*

the feedback I’ve received is that there are no changes on your end, just that the MBR will change on any winnings on relevant markets, as suspected. hope that helps [@UEA14GBRR](@UEA14GBRR)

*Tags: General Technical*

---

**ShaunW** - *01:31:00*

That's really good news. Thanks for helping, much appreciated.

*Tags: General Technical*

---

**Rob** - *10:44:22*

Just “finished” the betfairlightweight compat part of my data parsing library ([https://github.com/tarb/betfair_data](https://github.com/tarb/betfair_data)) and I would love if some people would like to try it out.

Even better if you want to donate some cpu cycles and run my test script with some data, to help me find some remaining differences.

*Tags: General Technical*

---

**Rob** - *10:54:00*

heres some sample code as I havent added this part to the readme

```import tarfile

import bz2

from typing import Sequence

import betfair_data

import logging

from betfair_data import bflw



logging.basicConfig(level=logging.WARN, format='%(levelname)s %(name)s %(message)s')



paths = [ 

    "data/2021_10_OctRacingAUPro.tar",

    "data/2021_11_NovRacingAUPro.tar",

    "data/2021_12_DecRacingAUPro.tar"

]



def run_with_py_loading():

    market_count = 0

    update_count = 0



    def load_tar(file_paths: Sequence[str]):

        for file_path in file_paths:

            with tarfile.TarFile(file_path) as archive:

                for file in archive:

                    f: bz2.BZ2File = bz2.open(archive.extractfile(file))

                    name = file.name

                    bytes = f.read()



                    yield (name, bytes)

        return None



    for name, bytes in load_tar(paths):

        file = bflw.BflwIter(name, bytes, cumulative_runner_tv=True)



        market_count += 1

        for market_books in file:

            for market_book in market_books:

                update_count += 1



        print(f"Market {market_count} Update {update_count}", end='\r')

    print(f"Market {market_count} Update {update_count}")





def run_with_rust_loading():

    for file in betfair_data.TarBz2(paths, cumulative_runner_tv=True).bflw():

        market_count += 1



        for market_books in file:

            for market_book in market_books:

                update_count += 1



        print(f"Market {market_count} Update {update_count}", end='\r')

    print(f"Market {market_count} Update {update_count}")





run_with_rust_loading()```

*Tags: General Technical*

---

**Rob** - *10:54:53*

performance wise, its quite a lot slower the mutable version in the benchmark, finishing in 7:15sec for the same data (22k markets)

*Tags: Performance*

---

## 2022-02-25

**liam** - *09:44:04*

Very easily [https://github.com/liampauling/flumine/blob/bc4d18a941778f41087b055c5c8d3f00ccd63201/flumine/clients/baseclient.py#L27](https://github.com/liampauling/flumine/blob/bc4d18a941778f41087b055c5c8d3f00ccd63201/flumine/clients/baseclient.py#L27)

*Tags: General Technical*

---

## 2022-02-26

**Steve** - *09:39:40*

Revolut is the worst. HSBC and TransferWise never gave me problems.

*Tags: General Technical*

---

## 2022-02-27

**LM** - *10:01:53*

Hey All,

Have thought about trying to build a bot a few times now and am going to try and commit some time to it this year. I'm sure it'll be terrible and I'll lose some cash, but primary motivation is to upskill. Glad I stumble onto this slack channel! I'll be sure to ask plenty of stupid questions along the way, feel free to tell me to shut up, but you seem like a nice enough bunch :slightly_smiling_face:

Looking forward to getting to know a few of you.

Cheers

*Tags: General Technical*

---

**Peter** - *11:31:07*

[@U033HM6Q8BU](@U033HM6Q8BU) You don't have to lose money while you're learning. Checkout the paper trading option in the Flumine documentation. You can build a model that works with live data but doesn't place live bets, just measures what would have happened if you had. Make sure to place some real bets though, so that Betfair doesn't think you're just stealing their data.

*Tags: Deployment, Strategies*

---

**Aaron Smith** - *12:27:57*

I did it like a real man, no paper-trading no backtest - just raw negative EV :smile: Anyways, i think you need to trade at least a little when streaming the data, otherwise betfair may dislike your use of their api. Shouldnt cost a lot though, with minimal stakes and only a few bets. You may aswell win some, EV and reality converge towards each other slowly

*Tags: Performance, Strategies*

---

**VT** - *18:32:20*

Welcome, I'm part of this group of stupid questions and it's not that bad. If you need bets with a tendency to null EV an option can be the over 2.5 goals in Premiere League games betting less than 1 hour to the start of the game.

*Tags: Strategies*

---

## 2022-02-28

**Jorge** - *08:58:26*

Hey, is it true that if I restart the [https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py|marketrecorder.py](https://github.com/liampauling/flumine/blob/master/examples/strategies/marketrecorder.py|marketrecorder.py) with the same `local_dir` and `recorder_id` it will continue seamlessly?

*Tags: General Technical*

---

**liam** - *09:22:23*

[https://liampauling.github.io/flumine/sportsdata/|sports-data](https://liampauling.github.io/flumine/sportsdata/|sports-data) stream now integrated into flumine, any issues or improvements let me know

*Tags: General Technical*

---

**Newbie99** - *10:04:04*

What data does the Race Subscription show, I appear to have completely missed that (is it just a streaming version of the satus API)?

*Tags: General Technical*

---

**Tony** - *11:52:06*

Hi all :wave:, i am new to the channel. I am going to try create a  tennis trading bot as i already trade it daily but would like to try automate certain scenarios. I have a delayed API key but from reading around it doesn’t feel like it would allow me to get an accurate picture and also i would like to stream some data for backtest purposes but the betfair free files don’t seem to have what i would like either. What is the best place way to get the live key activated?

*Tags: Getting Started, Deployment, Strategies*

---

**liam** - *20:16:52*

From memory you have to apply and tell them what you plan on doing / turnover but once in they are keen to help, well they where for me up until I got banned from XSP

*Tags: Performance*

---

**Jonjonjon** - *21:52:05*

I think you need £200 in your account. Otherwise it wasn't hard for me to get access. But they didn't like my strategy of placing orders far away from the current market, so I didn't get anywhere with it.

*Tags: Strategies*

---

## 2022-03-01

**liam** - *08:11:21*

They basically seed it themselves and its loss making so any strategy that involves exploiting its inefficiency results in banning, its for the 'greeners' / to compete with betfair BSP

*Tags: Strategies*

---

**Matthieu Labour** - *12:26:46*

Hi, is `clk` a unique id on the market level snapshot? What could be a use case for propagating it through the API? I am looking at this PR from Mo [https://github.com/liampauling/flumine/issues/522](https://github.com/liampauling/flumine/issues/522). Thank you for your help.

*Tags: General Technical*

---

**Nacho Uve** - *13:10:43*

Some newbie questions:



1. Is it possible to combine the "market_recording_mode=True" with trading strategies?

2. Are the orders placed manually in the BF web interface noted on the market.blotter?

*Tags: Strategies*

---

## 2022-03-02

**liam** - *08:22:34*

to answer your second question flumine uses the marketVersion to trigger a catalogue request 

*Tags: General Technical*

---

**Jorge** - *08:39:10*

Ah great! I see [https://github.com/liampauling/flumine/blob/0737461323da409990d3108a158092487506aa7f/flumine/markets/market.py#L144|market_start_datetime](https://github.com/liampauling/flumine/blob/0737461323da409990d3108a158092487506aa7f/flumine/markets/market.py#L144|market_start_datetime) property. But how do I access this through a MarketBook object?

*Tags: General Technical*

---

**liam** - *08:40:52*

but its also in the definition as per line [https://github.com/liampauling/flumine/blob/0737461323da409990d3108a158092487506aa7f/flumine/markets/market.py#L148|148](https://github.com/liampauling/flumine/blob/0737461323da409990d3108a158092487506aa7f/flumine/markets/market.py#L148|148)

*Tags: General Technical*

---

**Jorge** - *08:41:13*

I use `streaming.output_queue.get()`  which returns a list of all the MarketBooks

*Tags: General Technical*

---

**Jorge** - *09:03:30*

Mmm, how can I access the MarketCatalogue with flumine?

*Tags: General Technical*

---

**liam** - *09:03:54*

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1646210350923579](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1646210350923579)

*Tags: General Technical*

---

**Jorge** - *09:07:06*

Ah I think the problem is I'm using `betfairlightweight`  and not flumine

*Tags: General Technical*

---

**liam** - *09:07:35*

:man-facepalming: why the first link to the flumine codebase?

*Tags: General Technical*

---

**Jorge** - *09:07:49*

I create a `betfairlightweight.Streaming` object and I dequeue using `streaming.output_queue.get()`

*Tags: General Technical*

---

**Jorge** - *09:09:31*

Sorry, now makes sense:sweat_smile:. I wrote this long time ago, I need to update from betfairlightweight to flumine then

*Tags: General Technical*

---

**liam** - *09:11:31*

its probably going to solve a lot of your problems, however as it stands you need to poll the catalogue endpoint yourself (probably in a thread) and store the data

*Tags: General Technical*

---

**Paul** - *13:10:14*

I don't know why I was using `price_size` - just getting on with it I guess, didn't read the docs properly - but it explains a _lot_ of strange behaviour that I thought was a transient error in my code. I guess it was. PEBKAC on this one though, definitely PEBKAC.

*Tags: Errors Debugging*

---

## 2022-03-03

**J** - *06:29:06*

Is anyone else hitting subscription limit exceeded errors since Betfair came back up? I'm hitting it just on the Australian horse racing market by itself (using the same code as before, and the code from this morning even went across multiple countries)

*Tags: Errors Debugging*

---

**J** - *08:20:36*

Will try that, and have asked them to increase the limit. Just weird that the exact code worked fine before it went down today, thought the issue might be more wisedpread. Also trying to work out how to filter it a bit more when I actually do want all Aussie races, but will work it out

*Tags: General Technical*

---

**Mo** - *08:59:27*

Market start time is not a streaming filter

*Tags: General Technical*

---

**J** - *09:13:55*

That's good to know, thanks. I'm fairly new to this so I'm probably doing it wrong, but I switched to a non streaming filter to use the start date but no cigar. I'm trying to find a list of venues or something to further restrict the list (I essentially just want all 'win' type 7 (horse racing) races for Australia for today but the venue list is also hard to find. Not sure if there's a better way.

*Tags: Getting Started*

---

**EJono** - *11:43:10*

Whats the best way to get a print out of/access the settled orders report at the end of a market stream life cycle in a flumine strat? im currently using process_closed_market for some final tidy up operations at the end of a market but wondering if the final outcomes of orders placed on the market are passed to this function in some way or in process_orders perhaps. Essentially I just want to be able to access the info normally retruned from listClearedOrders call but want to go about it the most flumine way possible for each market. Thank you!

*Tags: General Technical*

---

**EJono** - *12:25:26*

Thank you very much for the code example. A few things to help m,e understand how it should be used:



Should this class be utilised like any other strategy ie i would initialise by

strategy = ClosedMarketLogging(...)

framework.add_strategy(strategy)

framework.run()   ?



Could i call "def _process_cleared_orders_meta(self, event): " in my strategy that i have set up and running currently and expect it to correctly run at the end of a market



And is the order information you are accessing via

orders = event.event

Available in one of the standard methods that run in a flumine strat like process_market_book, process_orders, process_closed_market obviously once the market outcome has been determined?



Apologies if the above questions are nonsensical, my undertanding of flumine is still quite surface level at this stage. cheers

*Tags: Strategies*

---

**Newbie99** - *12:34:39*

I just adapted [@U4H19D1D2](@U4H19D1D2)'s example tbh, so this you use in the same way as your current ClosedMarketLogging:



```framework.add_logging_control(ClosedMarketLogging())

framework.run()```

Here is the example from the Flumine github:



[https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: General Technical*

---

**Newbie99** - *12:38:56*

The event type differs according to the control, but a list of the different types is available here:



[https://github.com/liampauling/flumine/blob/master/flumine/events/events.py](https://github.com/liampauling/flumine/blob/master/flumine/events/events.py)



So in this instance you need to extract the orders from the Cleared Order object (which is a BFLW class).



I think I've explained that correctly, its something along those lines, but I might be confusing myself as I haven't looked at it in a while!

*Tags: General Technical*

---

## 2022-03-04

**J** - *19:04:45*

Been playing with Flumine pretty hardcore for the the last week. Amazing software, thanks for releasing it [@U4H19D1D2](@U4H19D1D2)!

*Tags: General Technical*

---

**J** - *20:38:36*

That multi processor backtesting works like a dream too... whipped up a strategy brute forcer today with it that's humming along at 100% on all 16 cores perfectly

*Tags: Strategies*

---

## 2022-03-05

**J** - *09:31:41*

Does Flumine work out commissions for profit? How does it work out profit? Just wondering if I'm (for example) in Australia how accurate it will be?

*Tags: General Technical*

---

## 2022-03-06

**Alex Alex** - *12:43:10*

~Is it correct that there is no way to filter *only* inplay markets when streaming, and I have to ask betfair to increase my connection limit beyond the 200?~ Never mind it seems to be covered just a few messages up.

*Tags: General Technical*

---

## 2022-03-07

**Nacho Uve** - *10:09:40*

If I want to save in live every placed order and its result (matched, cancelled, etc without wait to be cleared the market, how would be the best approach?



I think it could be a good idea to do that to avoid any problem with the connection, a unwanted restart of the machine, etc.

*Tags: Deployment*

---

**J** - *11:52:18*

This is probably a stupid question, but is there a place in flumine where the winner is stored? Like the winner of a horse race. I can code it up (and have before with BFLW) but want to fit into the framework

*Tags: General Technical*

---

**liam** - *11:53:43*

once close you can get it from the market_book



[https://github.com/liampauling/flumine/blob/0f12a0f9d200b4a4c36c8bd26c9f4fbe82f8cce1/examples/controls/jupyterloggingcontrol.py#L81](https://github.com/liampauling/flumine/blob/0f12a0f9d200b4a4c36c8bd26c9f4fbe82f8cce1/examples/controls/jupyterloggingcontrol.py#L81)

*Tags: General Technical*

---

**liam** - *12:01:20*

if you check in the `strategy.process_closed_market` there should be the correct status

*Tags: Strategies*

---

**J** - *12:49:11*

Oh, think I may have screwed something up in trying to get around a codec error and inadvertently filtered out completed files, which would explain it. I'll dig deeper into that and confirm before I waste more of your time. Thanks for the help mate

*Tags: Errors Debugging*

---

**Peter** - *18:09:53*

I have seen similar. A strategy that was placing several hundred bets a day, suddenly just stopped. I anticipated an error in my code, some sort of untrapped condition that I'd failed to allow for and catch.



However when I dug into it, it turned out to be a large bets coming arriving in a way that trampled all over my signal, to the point when very few of my trades were still were still in range, and those that were still triggered, were failing a 2nd signal. Lasted about three days, then went back to normal.



I suspect that somebody was running a new strategy that didn't work out for them - maybe because they pushed me and others out and didn't like what they were left with. But it did warn me that my strategy was more sensitive than I'd realised.

*Tags: Errors Debugging, Strategies*

---

**Jonjonjon** - *20:44:48*

Good observation Peter. I saw some large bets smash up my signal late last year, but again it lasted 3 days. The last couple of days have been tough too. A strategy that hadn't had a losing day this year took a couple or small losses. But I'd increased the size on some other strategies. Hmmm... Maybe my bot is doing the trampling? :thinking_face:

*Tags: Strategies*

---

## 2022-03-08

**Michael** - *08:07:17*

Seems like you have occasional unusually good days as well [@UPMUFSGCR](@UPMUFSGCR)? Looked at that way your latest blip maybe just looks like one unusually bad day and a couple flat-ish? I don't think I'd take any immediate action based on the plot above.



[@U01DVUAE2G1](@U01DVUAE2G1) Going from thousands of bets to very few over-night seems unusual, but even that I'd give time. Whenever I try something new I start it small and ramp up gently, a "big bang" type switch on seems likely to be a bad idea. Whilst I could be wrong I wouldn't normally expect a sophisticated player to operate that way, which might leave you with someone more naive, if that's the case they won't be around for long. Great big structural errors in your approach kill you really quickly.



As a general rule overnight changes are usually not the concern as they tend to be more random although they often don't feel like it. It's the long term trends that get you, the gradual decline of margin or tailing off of opportunities. That's what you look out for.

*Tags: Errors Debugging*

---

## 2022-03-09

**Michael** - *21:32:03*

Ah ok. Maybe it's time to re-structure your code.... Flumine?

*Tags: General Technical*

---

## 2022-03-10

**J** - *09:24:10*

Yeah the commission rates are a pain. Although I can keep playing with it with $1 bets until I get the model right. I was wondering who had played with machine learning models in the inflight data. I've been adding them to Flumine and they're working pretty well

*Tags: Strategies*

---

**Mo** - *09:33:33*

I have a prediction model but it's not ML and that seems overkill for in running horse racing IMHO

*Tags: Strategies*

---

**J** - *09:54:18*

What I'm missing is how to accurately predict the value, bot in in-flight data and using data before the race. I need to keep working on those but it's hard to find how to go about it

*Tags: General Technical*

---

## 2022-03-11

**jhaa** - *12:53:33*

I am looking for examples on how to convert what the flumine marketrecorder saves to a pandas dataframe or a dict for betfairviz.  Any suggestions?

*Tags: Feature Engineering*

---

**Laybot McBackbot** - *21:04:03*

Noob question. When backtesting, how do I make flumine shows profit/loss after processing each market instead of waiting until all markets have been processed?

*Tags: General Technical*

---

## 2022-03-12

**Peter** - *11:38:09*

You could build something into your strategy to output the P&amp;L of each market as the backtesting closes it, possibly by adding something into the strategy's process_closed_market() method. But I wouldn't recommend it.



Better would be to understand that if you're going to be doing this seriously, you'll be doing a lot of backtesting, and sitting watching the results in real-time is going to get old really quick. So rather than solving this problem, maybe have a think about how you could use the backtesting run time more productively.

*Tags: Strategies*

---

**Laybot McBackbot** - *11:49:05*

No, I am not watching this real-time. I am running the script overnight and I want to be able to come to back to it in the morning and decide whether I want to continue running or just abort if I find it halfway that it is not a strategy I want to pursue.  I am also running a lot of other stuff/simulations on my machine and there is always a chance something goes wrong and it doesn't finish.

*Tags: Strategies*

---

**Peter** - *15:25:33*

I do similar but in parallel. I chunk the markets by day and then use multithreading to run each day in a separate thread. Each day's results are persisted to a csv file and it's then trivial to load all available days into Pandas and summarise the results. It's also easy to spot any days that fail and either restart from there or just hit any holes.

*Tags: Feature Engineering*

---

**Newbie99** - *18:03:36*

[https://github.com/liampauling/flumine/blob/master/examples/backtest.py](https://github.com/liampauling/flumine/blob/master/examples/backtest.py)

*Tags: General Technical*

---

## 2022-03-13

**LM** - *01:58:12*

When backtesting in flumine does the market.blotter.simulated.profit take into account market base rate fees (I'm assuming no)?

*Tags: General Technical*

---

**Guy Incognito** - *05:23:44*

What is the best way to calculate the volume weighted average price of all the matched bets including other peoples for a selection using flumine?

*Tags: General Technical*

---

**Oliver Varney** - *14:26:50*

in reviewing this you just help me find an assumption about some data that doesnt hold so thank :slightly_smiling_face: if im computing it right though these look a tad fast to me

*Tags: General Technical*

---

**VT** - *20:08:44*

Guys, noob statistics question.



Assuming the probability of 1 more goal in a live event is 50% (@2.00), how do bookies calculate upcoming markets? Is it just a Poisson formula (or something like that)?  It seems too simple to use 50% as the event probability and just make the progression.

*Tags: Deployment*

---

## 2022-03-14

**Oliver Varney** - *06:38:03*

ah gotcha :+1: surface would also help im guessing as there is AW that would fall under the banner of flat

*Tags: General Technical*

---

**Mo** - *06:44:13*

A key question is how much this actually matters as I have no idea how sensitive the success of my strategy is to the prediction of the duration

*Tags: Strategies*

---

**Oliver Varney** - *06:47:38*

yup :+1: I just find the modelling part the only enjoyable part tbh

*Tags: Strategies*

---

**bensasl2** - *22:59:57*

Betfair is indicating this sample code for order book reconstruction : [https://github.com/betfair/stream-api-sample-code/tree/master/java](https://github.com/betfair/stream-api-sample-code/tree/master/java) available in Java, c# and node.js but no python ...

*Tags: General Technical*

---

**bensasl2** - *23:00:51*

I use betfairlightweight , can anyone help please ? many thanks

*Tags: General Technical*

---

**Mo** - *23:19:47*

I doubt the code you linked does anything that betfairlightweight can't do

*Tags: General Technical*

---

## 2022-03-15

**Steve** - *02:47:18*

I'm looking to drop some bets into the BSP market. I want to do it as close to the jump as possible (ideally 1-2 second pre-jump). I mainly bet on Australian horse racing. Does anyone know any fields within the betfair API or an independent API service which could provide a signal that the race is immeniently about to start. I have trailed using the schedule start time, but there is way too much noise because some races start minutes after the scheduled time. Any help/advice would be appreciated.

*Tags: General Technical*

---

**Unknown** - *03:45:19*

Does anyone how to get the metadata in betfairlightweight? I keep getting None for this specific horse race

*Tags: General Technical*

---

**Jorge** - *08:09:46*

Hey, I am betting pre-game and do not want to bet in_play because my bots don't use action data. But I would like to bet as close as possible to the market start time. So far, I check if the market is in_play using betfairlightweight streaming MarketBook.inplay. Is there a better way of knowing when the event will start?

*Tags: Strategies*

---

**bensasl2** - *08:36:21*

[https://github.com/betfair/stream-api-sample-code/tree/master/java](https://github.com/betfair/stream-api-sample-code/tree/master/java) available in Java, c# and node.js but no python ...

order book reconstruction is receiving BATL and BATB stream data and update the cache order book in real time . The cache order book, is an image of the real order book that you keep updating in your program. so above code allows you to do exactly that. can betfairlighweight or any other Python project do order book reconstruction ?

*Tags: General Technical*

---

**Mo** - *08:36:48*

Yes betfairlightweight does that

*Tags: General Technical*

---

**Mo** - *08:38:54*

[https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py)

*Tags: General Technical*

---

**bensasl2** - *08:39:11*

I did look at the examples , it only shows how you can subscribe to market data , not how to use that data to rebuild a cached order book

*Tags: General Technical*

---

**Mo** - *08:39:29*

You're not getting it. betfairlightweight does this for you automatically

*Tags: General Technical*

---

**bensasl2** - *08:44:42*

let me double check this. again your help is much appreciated

*Tags: General Technical*

---

**Mo** - *10:07:00*

Not sure how common it is but obviously your strategy will need to be able to handle it

*Tags: Strategies*

---

**JFP** - *12:38:29*

Just wondering if there is a Flumine function to get traded volume at specific price? Currently using below code, just wondering if there is a better way:

```#get traded volume at x price. Returns zero if no volume traded at price.

price_tv = next((item for item in runner.ex.traded_volume if item['price'] == x), 0)

if price_tv != 0:

    price_tv = price_tv['size']```

*Tags: General Technical*

---

## 2022-03-16

**anomaly** - *02:23:18*

Hi all,



Newbie to flumine/bflw here! Got a few question about best practice placing single ex / bsp bets with flumine. In the examples, it seems you need to wrap BaseStrategy and internally define the logic in process_market_book() which seems straightforward enough. Peeking inside the Flumine class the .run() executes a while True loop suggesting that the process_market_book() will run forever when triggered.

In my case, I simply want to execute a single bet on demand and that's it. What's the recommended practice for placing single market-selection_id bets like this? Is it fine to continuously initialise new Flumine() instances per market and include a select_id as an additional class attribute (e.g self.selection_id)? In hindsight, I have a feeling flumine might be overkill for what I need and that a simple bflw order placement is enough... but don't know enough to know if I should be taking advantage of what flumine has to offer (order tracking, error handling, etc) - the impression is that it seems more trading-focused which is not what I need right now.



The last question is, what's the best way to uniquely identify a strategy in bflw or flumine so that it's easy to track specific strategies and the bets they place? From what I understand it's ideal to use the listClearedOrders api and also customerOrderRef / customerStrategyRef. How do you specify these with bflw / flumine?

*Tags: Errors Debugging, Strategies*

---

**liam** - *06:00:25*

Nothing wrong with using flumine for this (we push a never green policy) however you would only need a single instance/strategy and take advantage of the strategy.trade_count to limit the number of bets per selection.



If you provide a rough idea of what you are doing (market type / bet type etc) I can provide a simple example.



Flumine uses the refs internally and instead designed to record all bets / strategy info through logging control. 

*Tags: Strategies*

---

**anomaly** - *09:32:38*

Can do! So essentially a typical situation is this:



1. you have a strategy name. say it's called "back_strat_42". it's betting on UK horseracing.

2. you have a single market_id and a single selection_id that you want to place a bet on

3. you have the liability to stake (back or lay doesn't matter).

4. you want to place just a single bet on the selection at anytime and leave it in the market. so for simplicity say a OrderType=MARKET_ON_CLOSE with PersistenceType=MARKET_ON_CLOSE

5. you want a log/trace of the result of the bet that you placed for post-race analysis (typically with a database dump)



This is to be repeated multiple times from a few different strategies but the logic is essentially as above and the current system design is to place these bets from individual threads or processes.

A follow up question is if you have dozens of these happening in parallel will it be an issue from a betfair api perspective? I'm connecting with a live api key with bot login (certs) and it seems you're only allowed 9 connections. Would this be an issue with the above setup?

*Tags: Getting Started, Deployment, Strategies*

---

**liam** - *10:26:21*

Have a look at [https://github.com/liampauling/flumine/pull/568/files|this](https://github.com/liampauling/flumine/pull/568/files|this)



This is the flumine/event driven way of using the stream so that you use a single market stream and make decisions based off the updates. Note that this is just laying at 1.01 but you just need to change this to a MarketOnCloseOrder and add your selections to the context for it to do what you want.



Haven't added your part 5, but this would involve a variation of [https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py|this](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py|this) example.



Moving forward you would probably want to pull these selections from a csv, api or database rather than hardcoding into the context.

*Tags: General Technical*

---

**Laybot McBackbot** - *18:15:49*

Question - how do I change the default commission when backtesting? I assume it's already calculated and is 5%?

*Tags: General Technical*

---

**NOT Damien** - *18:30:44*

hello [!everyone](!everyone) i am new to betfairlightweight have been looking for documention but have had no luck online does anybody have a link or file that i can get my hands on thanks in advance

*Tags: Getting Started*

---

**Laybot McBackbot** - *18:34:23*

[https://liampauling.github.io/flumine/](https://liampauling.github.io/flumine/)

*Tags: General Technical*

---

## 2022-03-17

**anomaly** - *05:25:03*

[@U4H19D1D2](@U4H19D1D2) I was able to run the code successfully! is there a way to use same framework/betfair client to push new strategies?



.run() seems to be blocking but I would like to call framework.add_strategy(strategy) on demand. i.e. a framework process is reading a queue and it constructs aribtrarily many new Strategies on demand, and pushes them to the framework.



Additionally, assuming it's possible to seamelessly add strategies, how would you safely "remove" them?

*Tags: Strategies*

---

**Jonjonjon** - *08:46:01*

What would be the recommended way of running Flumine backtests on AWS?

*Tags: Deployment*

---

**Paul** - *11:37:48*

That's a very broad and open question. Do you mean EC2 vs Containers? How to get your code in there? How to move data around?

*Tags: Deployment*

---

**anomaly** - *11:54:21*

Hmm I think i've gotten myself in a tangle because of the way the modelling has been done. I'm currently using bflw to manage and then spawn a model that makes a selection. Given that selection, I want to place a bet.

Each model communicates through a queue to the main thread which I originally planned to have place the bets through bflw... but now it appears flumine is much better suited for this purpose.

In summary, the current setup is: bflw process loop (monitor market stream) -&gt; spawn models according to some trigger logic and each model makes a single prediction -&gt; send the (market_id, selection_id, order_type, stake) through the queue -&gt; bflw process reads the queue and places bet -&gt; bflw closes and cleans up the model process

Given what you said and having another look at the code it seems I need to instead encode the model spawning logic _inside_ the BaseStrategy (process_market_book, check_market_book etc). Is this correct?

*Tags: Getting Started, Strategies*

---

**liam** - *12:04:44*

Yes ideally, how slow is the modelling process? 

*Tags: Performance, Strategies*

---

**Jonjonjon** - *20:28:35*

I'm undecided how to get code there. I was thinking I'd have recorded Betfair files on some sort of storage there. I'm considering renting something with 128 cores. Vs using lambda. I've never used lambda before, so don't know if it'll be easy/hard, or whether or not it can use shared storage. I'm concerned about the cost of a 128 core machine that I will only need for a few minutes at a time

*Tags: General Technical*

---

**Paul** - *20:30:35*

Get your files onto s3 (flumine helps), then go down container or ec2 route. Lambda not a good route IMO. YMMV

*Tags: Deployment*

---

**Paul** - *20:34:07*

Flumine will do the s3 lift for you. What you then want is a startup script or script in an image on boot up that. Lines your repo and day and then takes some params (perhaps) for your backtest.

*Tags: General Technical*

---

**liam** - *20:44:45*

I have tried a few ways of doing it on AWS but always fallen back to backtesting locally, although slower it is worth the cost in being able to very quickly analyse the results / make changes / rerun etc 

*Tags: Performance, Deployment*

---

**Paul** - *20:49:08*

Agreed. I suggest using a subset of markets for quick local testing and do AWS for bigger validation

*Tags: Deployment*

---

**Jonjonjon** - *20:50:08*

Is transferring the results from AWS to local machine a pain? I usually use a loggingcontrol to write my backtest orders to disk. And then use Jupyter or Spyder to load them into a dataframe for analysis

*Tags: Feature Engineering, Deployment*

---

## 2022-03-18

**VT** - *21:42:36*

Hi guys, noob question. After I placed the bet and it was matched I get an ID. What is the simplest way to take profit from this bet after markets are closed?

*Tags: General Technical*

---

## 2022-03-19

**anomaly** - *00:28:16*

Pretty slow but not really an issue for runtime. I'm understanding how to work with the framework better, cheers for the help. It's going to require a major rewrite to merge with flumine so i'll stick with the low level bflw ops for now and slowly rewrite the codebase into flumine.

*Tags: Performance*

---

**Peter** - *08:12:27*

Hi [@U013ZS16QJZ](@U013ZS16QJZ). Not really sure what you mean here. Once the bet is matched you can't change it. Once the market is closed you can't place other bets that would affect your overall profit. So I suspect you're asking how you would use the bet ID find out what what your profit (or loss) was.



If you're using Flumine to place the bet, then allowing streaming to continue until the market is closed would allow you to capture the market result in the process_closed_market() method and use that you calculated your financial result. If you're using betfairlightweight then you would use the get_account_statement() method on an account object and use your bet ID to find the bet that interests you.



Note that using either method will only give you the gross profit. In the happy event that you bet yielded a profit, commission may be levied at the market level and this won't show on individual bets.

*Tags: General Technical*

---

**Jonjonjon** - *08:26:58*

[@U9JHLMZB4](@U9JHLMZB4)  do you use EC2, containers or lambdas to run the backtests?

*Tags: Deployment*

---

**birchy** - *10:08:06*

When using Flumine backtest to process market data into csv's and I only need to call `process_market_book()` once per market, is it possible to skip straight to market closure? I currently use:

`def check_market_book(self, market, market_book):

    if 'signals' in market.context: return`

...but am wondering if it's possible to speed up/skip the unwanted updates? It seems like premature optimisation at first glance, but when processing 50K+ markets, it would make a difference.

*Tags: Performance*

---

**birchy** - *11:16:12*

Possibly. Flumine seems like the best approach as it already handles the deltas, etc which will have benefits when I expand the signals/stats. Will have to think about the pros/cons. :thinking_face:

*Tags: General Technical*

---

**Dave** - *11:35:49*

Scan the file directly, use bflw to construct market books when needed from the underlying JSON

*Tags: General Technical*

---

**Newbie99** - *14:15:36*

So you can run both from a logging control and then bundle up as you see fit (I create a .json for example at each market close, but equally you can just insert directly into a db if easier...my db is local not on AWS which is the only reason I don't do it that way)

*Tags: Deployment*

---

**Peter** - *16:12:57*

Like [@U4H19D1D2](@U4H19D1D2) I run most (by number) of my backtests locally. But when I have a good prospect and want to run a lot more markets, I use either a cheap dedicated VPS if I can wait, or a powerful EC2 instance if I want quick results.



My local backtests run in a container, but I've never built the pipeline to deploy them remotely in containers. I did look at running on Lambda a while back but retreated rapidly. Too many gotchas.

*Tags: Deployment*

---

**thambie1** - *16:38:25*

Anyone else getting occasional DUPLICATE_TRANSACTION errors when placing a bet, despite not actually placing duplicate orders? Just started happening in the last week, no code change on my end, and logs aren't showing duplicate customer refs.

*Tags: Errors Debugging*

---

## 2022-03-21

**mandelbot** - *09:45:59*

Is it possible to use recorded TPD data in backtests using Flumine?

*Tags: General Technical*

---

**liam** - *11:24:16*

Morning [!here](!here), very excited to announce the rename/rebrand of bflw to *betcode*!



This incorporates the new [https://github.com/betcode-org|GitHub organisation](https://github.com/betcode-org|GitHub organisation) with this slack and will open up the community to more users.



Currently betfairlightweight and flumine have been moved across (likely to involve updating any remotes if you pull the code) but hope to add more repo’s under our wing as well as inviting other members to become part of the org. Invite is open to anyone who has contributed or wants to contribute to the code :slightly_smiling_face:



Big thanks to the betconnect team ([@U02KNTX2Z7X](@U02KNTX2Z7X) and [@U02HPTBLCJY](@U02HPTBLCJY)) for sponsoring the logo and brand :sunglasses:

*Tags: General Technical*

---

**Joel Beasley** - *14:52:26*

Hi all, as some of you will already know, [@U4H19D1D2](@U4H19D1D2) has been working on integrating BetConnect via Flumine, which is coming in this PR [https://github.com/betcode-org/flumine/pull/560](https://github.com/betcode-org/flumine/pull/560) building on initial work by [@U9JHLMZB4](@U9JHLMZB4) and [@ULDAVFDRP](@ULDAVFDRP).



In readiness for this we’re now looking to talk to people who bet in pre race Horse Racing and pre match Football, as that is where we have our largest amounts of liquidity currently.



To kick things off we are hosting an event in London on the 31st March which we’d also like to invite those above to, to briefly present our commercial offering and USPs, and where our roadmap plans take us to, and how that benefits your P&amp;L :slightly_smiling_face:



Feel free to message me directly if you have any questions or would like to sign up for the event :thumbsup:

*Tags: General Technical*

---

**Aaron Smith** - *19:42:00*

Whats the idea behind betconnect? I see it offers bookmaker prices, but also claims to be an exchange and seemingly bets need to be filled, which would support that. Right now that sounds like taking the bad aspects from both worlds (bad odds from bookmakers combined with lower liquidity from exchanges). Is there some page that helps me understand it better? The "How it works"/FAQ on the website didnt help me much.

*Tags: General Technical*

---

## 2022-03-22

**Eamonn** - *09:47:18*

BetConnect offers the chance to access soft bookmaker prices rather than playing against other sharps on the traditional exchanges.



Back bets are typically layed by matched bettors who have no opinion on your back selection and will place the bet at a bookmaker on your behalf. They then have a flat position and try to exploit bookmaker promotions which are not passed on to the backers on BetConnect.



BetConnect offers backers the chance to bet earlier in the market cycle when liquidity on traditional exchanges is poor. The big advantage in this spot is that the roi is much higher for skilled bettors and negates the over round you are betting into.



Yes I work at BetConnect and we welcome all questions. We would love to get more users from betcode taking advantage of BetConnect

*Tags: Strategies*

---

**D C** - *10:26:54*

How do the soft books regard the BetConnect approach? Allowing sharps to make use of loser accounts to place their bets must surely not be a good thing for the books? How does this affect the bettors whose accounts are being made use of? What if bookies hold up withdrawals etc? Sorry for the multiple questions but I feel I am missing something in how things work here. For example, if I were a long term losing punter, what advantage would there be for me to allow my bookie accounts to be used in this manner?

*Tags: General Technical*

---

**Eamonn** - *10:35:52*

[@UUE6E1LA1](@UUE6E1LA1) Im happy to have a call to answer all your questions if you would like rather than clogging up the thread. Feel free to DM me.



Yes Oliver you are quite right there

*Tags: Deployment*

---

**VT** - *16:53:02*

Hi guys, I'm having problems with the order_stream connection, after some time I get disconnected by Betfair. Is there a way for me to automatically reconnect when this happens? I use the simplest code.



----------------------------------------------------------------------------

Exception in thread Thread-13:

Traceback (most recent call last):

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 230, in _receive_all

    part = self._socket.recv(self.buffer_size)

  File "C:\anaconda3\lib\ssl.py", line 1226, in recv

    return self.read(buflen)

  File "C:\anaconda3\lib\ssl.py", line 1101, in read

    return self._sslobj.read(len)

ConnectionResetError: [WinError 10054] Forced cancellation of an existing connection by the remote host



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "C:\anaconda3\lib\threading.py", line 932, in _bootstrap_inner

    self.run()

  File "C:\anaconda3\lib\threading.py", line 870, in run

    self._target(*self._args, **self._kwargs)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 60, in start

    self._read_loop()

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 212, in _read_loop

    received_data_raw = self._receive_all()

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 234, in _receive_all

    raise SocketError("[Connect: %s]: Socket %s" % (self._unique_id, e))

betfairlightweight.exceptions.SocketError: [Connect: 2]: Socket [WinError 10054] Forced cancellation of an existing connection by the remote host

----------------------------------------------------------------------------



Code:



output_queue = queue.Queue()

listener = betfairlightweight.StreamListener(output_queue=output_queue, lightweight=True)

stream = trading.streaming.create_stream(listener=listener)

order_filter = streaming_order_filter()

streaming_unique_id = stream.subscribe_to_orders(order_filter=order_filter, conflate_ms=0)

t = threading.Thread(target=stream.start, daemon=True)

t.start()

*Tags: Errors Debugging, Strategies*

---

**liam** - *19:07:59*

Have you seen the error handling [https://github.com/betcode-org/betfair/blob/master/examples/examplestreamingerrhandling.py|example](https://github.com/betcode-org/betfair/blob/master/examples/examplestreamingerrhandling.py|example)?

*Tags: Errors Debugging*

---

**VT** - *20:00:56*

Thank you [@U4H19D1D2](@U4H19D1D2)  I don't use this example that it's configured for a market stream and I don't know how to 'adapt' to an order stream that I'm having problems with.

*Tags: General Technical*

---

## 2022-03-23

**liam** - *08:38:57*

Example in [https://github.com/betcode-org/flumine/blob/07447d78df137ad5e3a34e6298c7e329618cefc0/flumine/streams/orderstream.py#L18|flumine](https://github.com/betcode-org/flumine/blob/07447d78df137ad5e3a34e6298c7e329618cefc0/flumine/streams/orderstream.py#L18|flumine)

*Tags: General Technical*

---

**Nacho Uve** - *10:35:41*

So, I understand that flumine does the re-subcribe by default, doesn't it?

*Tags: General Technical*

---

**Alex Alex** - *11:43:27*

So without having spent much time on this: Is there a way for me to see the orderbook? I.e. I want to see the available back bets that I could lay. I’d be quite interested in this. I have a model that could be adopted for pre game.

*Tags: Strategies*

---

**VT** - *13:39:23*

Thanks for the help [@U4H19D1D2](@U4H19D1D2) but I couldn't change it to order_stream, the code is too dense for a programmer who is limited to working with data analysis, scraping and statistical tests.

*Tags: General Technical*

---

## 2022-03-25

**liam** - *09:59:02*

flumine v2.0.0 now released, this has a few breaking changes 99% around naming (Backtest-&gt;Simulated) hence the major version bump (over 2yrs since v1!) but now allows multi clients/exchanges, see [https://betcode-org.github.io/flumine/clients/|docs](https://betcode-org.github.io/flumine/clients/|docs) on how to use. Also added a rough [https://github.com/betcode-org/flumine/blob/master/examples/example-betconnect.py|example](https://github.com/betcode-org/flumine/blob/master/examples/example-betconnect.py|example) on using the BetConnect client.



I have been testing for a few weeks now and I believe it to be bug free, any issues are likely to be around simulation rather than live due to the changes but let me know if you spot anything.



Future work will involve opening up the framework to be more exchange/API agnostic in terms of execution/market/orders etc.

*Tags: Errors Debugging, Deployment, Multi Client*

---

**mandelbot** - *15:26:13*

Are marketids on betconnect the same as betfair? [https://github.com/betcode-org/flumine/blob/9307dd5a0084c3583207921a521a1c39e430220f/examples/example-betconnect.py#L130](https://github.com/betcode-org/flumine/blob/9307dd5a0084c3583207921a521a1c39e430220f/examples/example-betconnect.py#L130)

*Tags: General Technical*

---

**mandelbot** - *15:57:48*

Not sure what im missing here

```Traceback (most recent call last):

  File "C:/FlumineNew/Lib/site-packages/flumine/examples/strategies/BSPtest.py", line 210, in &lt;module&gt;

    framework.run()

  File "C:\FlumineNew\lib\site-packages\flumine\flumine.py", line 15, in run

    with self:

  File "C:\FlumineNew\lib\site-packages\flumine\baseflumine.py", line 415, in __enter__

    self._add_default_workers()

  File "C:\FlumineNew\lib\site-packages\flumine\flumine.py", line 54, in _add_default_workers

    client_timeouts = [

  File "C:\FlumineNew\lib\site-packages\flumine\flumine.py", line 55, in &lt;listcomp&gt;

    client.betting_client.session_timeout for client in self.clients

AttributeError: 'NoneType' object has no attribute 'session_timeout'```

*Tags: Errors Debugging, Strategies*

---

**mandelbot** - *16:03:13*

Fixed it. Was using `Flumine()` instead of `FlumineSimulation()`

*Tags: Errors Debugging*

---

**mandelbot** - *16:03:58*

In the docs its `framework = Flumine()`

*Tags: General Technical*

---

**mandelbot** - *17:32:08*

[https://betcode-org.github.io/flumine/clients/](https://betcode-org.github.io/flumine/clients/)

*Tags: General Technical*

---

**liam** - *19:15:54*

ah that first example is very confusing, will fix it

*Tags: Errors Debugging*

---

## 2022-03-28

**Unknown** - *06:00:23*

Hey, You might be interested in my library I made with the bf aus automation guys - I posted it here a few months ago.

[https://github.com/tarb/betfair_data](https://github.com/tarb/betfair_data) its a rust based library for reading/parsing stream files, that supports official betfair data, and flumine recorded files. It has multiple formats including a drop in replacement for betfairlightweight.



If youre generating csv’s you can see our article about how we dropped it in as a replacement, and cut our scripts time from over 2 and a half hours to less then 5 minutes, and generated the exact same csv output.

[https://betfair-datascientists.github.io/historicData/jsonToCsvRevisited/](https://betfair-datascientists.github.io/historicData/jsonToCsvRevisited/)

*Tags: General Technical*

---

**VT** - *19:54:35*

[@UUX1L88MC](@UUX1L88MC) BF AUS articles are well explained, detailed, it is of great help for beginner programmers. I've added it to my study list. Thank you.

*Tags: Getting Started*

---

## 2022-04-04

**VT** - *17:16:07*

Is it possible from a market ID to check the profit and loss (final balance containing all bets on this market)? I tried to download the account_statement from bflw but it only returns 100 results even changing record_count.



start_time = str(datetime.today() - timedelta(days=3))[:10] + "T00:00:00Z"

end_time = str(datetime.today())[:10] + "T00:00:00Z"



time_range = betfairlightweight.filters.time_range(from_=start_time, to=end_time)



bets= trading.account.get_account_statement(

     item_date_range=time_range,

     record_count = 10000

)

*Tags: Strategies*

---

## 2022-04-06

**Alessio** - *17:35:51*

Hey folks, there's no way in Flumine to reproduce the cancellations due to VAR in backtesting, right?

*Tags: General Technical*

---

## 2022-04-08

**liam** - *13:59:47*

Just pushed a [https://github.com/betcode-org/flumine/pull/577|branch](https://github.com/betcode-org/flumine/pull/577|branch) to flumine which integrates [@UUX1L88MC](@UUX1L88MC) awesome [https://github.com/tarb/betfair_data|betfair-data](https://github.com/tarb/betfair_data|betfair-data) library, I am seeing a rough 1.5-2x speed increase with no code changes other than the price/size change (now bflw objects)



Few things to cleanup and fairly sure it can be sped up more, going forward I want to see how this can be the default for bflw and fall back to pure python if the library isn't installed but welcome any thoughts on this.

*Tags: Getting Started, Performance*

---

**Oliver Varney** - *15:21:56*

Ive been playing around with it and the performance is very impressive. Maybe be worth noting that you cant change market_book attributes (unless rob has changed anything in the recent pushes or my settings are wrong), i.e. adjusting old market_book etc

*Tags: Performance*

---

**Rob** - *15:45:06*

currently no, both to the mutability, and construction. Adding mutability is actually pretty hard, if you change a ladders values, then you will break future updates that depend on that ladder, and any other change (times/strings/etc) will be ignored and replaced next time the parser spots a difference anyway. The only real way to do it would be to copy the object and mutate the copy, and once you’re paying the price of a deep copy you’re probably best just copying into a new python class that you can control.



As for constructors I’ll need to do research on the benefits. If using my ‘native’ objects is way faster, then I’ll probably add them, but I suspect it wont be any faster then just making your own python class - which would also be a lot more flexible.  I honestly dont know tho, so will need to do some benchmarking. If my objects are slower/(or just not substantially faster) and are also less flexible, then I dont know if theres really much point adding constructors.

*Tags: Performance*

---

**Rob** - *16:01:00*

heres an example of how I think I would go about it (baring in mind, Im totally not a python dev)

*Tags: General Technical*

---

**Rob** - *16:01:07*

```from betfair_data import bflw



class BiglyMarketBook:

    def __init__(self, book: bflw.MarketBook):

        self.total_matched = book.total_matched * 10

        self.runners = [ BiglyRunnerBook(r) for r in book.runners ]



        self.streaming_unique_id = book.streaming_unique_id

        self.bet_delay = book.bet_delay

        self.bsp_reconciled = book.bsp_reconciled

        self.complete = book.complete

        self.cross_matching = book.cross_matching

        self.inplay = book.inplay

        self.is_market_data_delayed = book.is_market_data_delayed

        self.key_line_description = book.key_line_description

        self.last_match_time = book.last_match_time

        self.market_definition = book.market_definition

        self.market_id = book.market_id

        self.number_of_active_runners = book.number_of_active_runners

        self.number_of_runners = book.number_of_runners

        self.number_of_winners = book.number_of_winners

        self.price_ladder_definition = book.price_ladder_definition

        self.publish_time_epoch = book.publish_time_epoch

        self.publish_time = book.publish_time

        self.runners_voidable = book.runners_voidable

        self.status = book.status

        self.total_available = book.total_available

        self.version = book.version



class BiglyRunnerBook:

    def __init__(self, book: bflw.RunnerBook):

        self.total_matched = book.total_matched * 10



        self.adjustment_factor = book.adjustment_factor

        self.ex = book.ex

        self.handicap = book.handicap

        self.last_price_traded = book.last_price_traded

        self.removal_date = book.removal_date

        self.selection_id = book.selection_id

        self.sp = book.sp

        self.status = book.status

        self.matches = book.matches

        self.orders = book.orders```

*Tags: General Technical*

---

**Rob** - *16:02:01*

its just a silly example, but because of pythons duck typing, you should be able to sub that version of a MarketBook, in anywhere that a normal marketbook would go

*Tags: General Technical*

---

## 2022-04-09

**JFP** - *16:24:23*

Hi all, I'm trying to filter out Novice races for a live strategy. For back testing the below code works:



```if "Nov" in market_book.market_definition.name:```

However I get NoneType error when running live. I have tried using the catalogue:



```if "Nov" in market.market_catalogue.market_name:```

This does not work either (I am trying to use this in the process market_book function). Would really appreciate it if someone could point me in the right direction. Thanks

*Tags: Errors Debugging, Deployment, Strategies*

---

## 2022-04-10

**JFP** - *03:21:31*

Thanks [@U01C12ZEADQ](@U01C12ZEADQ) [@U9JHLMZB4](@U9JHLMZB4) [@UFTBRB3F1](@UFTBRB3F1)



So if I add market.market_catalogue is not None check, it will skip until catalogue becomes available?



I am not using middleware and I'm not calling the market_catalogue anywhere else, I thought it is automatically loaded in the market class? Or do I need to explicitly load it?



This error does arise as soon as market_book starts processing, sounds like adding the catalogue check should do the trick.

*Tags: Errors Debugging*

---

## 2022-04-12

**AP** - *03:11:04*

What would be the best way to extract the market p&amp;l within the Strategy class with flumine? Something within process_closed_market?

*Tags: Strategies*

---

## 2022-04-14

**DFL** - *11:21:34*

Hi all, I hope this posting this here is considered kosher!



We have a sports trading team with background in (financial) algo trading industry that is looking to add developers. We've had initial success in a few different sports and really looking to scale up.



Python Developer: Looking for someone to help us collect new data, build new tools and automate our systems.



Core Developer: Also will be primarily python, prefer someone with development within Financial, Crypto or Sportsbetting. Need to have proficiency in building low-latency network applications and experience with low-latency trading applications is a huge plus.



We may also have a role for someone with a JavaScript background.



Our team is based in Dublin, but we may be open to other locations or remote for the right candidates. Let me know if you're interested in hearing more!

*Tags: Getting Started, Performance, Strategies*

---

**ShaunW** - *14:16:18*

One missing thing [@UPCC7TU8L](@UPCC7TU8L), wages?   As a guide my brother-in-law is in low latency financial trading development and makes £750 a day.

*Tags: Performance, Strategies*

---

**D C** - *14:34:39*

Do people really use Python for low latency trading?

*Tags: Performance, Strategies*

---

**foxwood** - *15:07:28*

£2k pd remote working for this sort of stuff imho and if you really want minimum latency it needs to be C/C++ with asm as well. Oh, and a piece of the action :grin:

*Tags: Performance*

---

## 2022-04-15

**Jonjonjon** - *08:39:52*

Managers often don't know how to use decent developers either. I've seen people give devs a vague requirement and then telling them to just get the job done quickly. That didn't lead to good results.

*Tags: General Technical*

---

**Oliver Varney** - *19:41:27*

How many people here don't run their own money? Or would rather run someone else's money? This is a question I really find interesting

*Tags: General Technical*

---

**Oliver Varney** - *20:47:08*

For example the job above, anyone decent wouldn't give up a strategy for the wage they are offering?

*Tags: Strategies*

---

## 2022-04-16

**Graham** - *10:56:42*

Ultimately, unless the rev share is significant then it’s unworkable. Even from the perspective of the fund. Market cap doesn’t allow for a significant enough amount to warrant someone simply handing their model over for the sake of their stakes increasing. 

*Tags: Strategies*

---

**Michael** - *12:58:56*

Quite a different proposition to be fair - this poster seems to want to hire people to do a job. I don't have a problem with that.

*Tags: General Technical*

---

## 2022-04-17

**D** - *09:56:40*

I always assumed with these kinds of deals that they don't want you to share your secret model, or to share their model with you - what they're looking for is people competent/experienced at all the surrounding systems such as data capture, reporting, trade execution etc. All of that stuff is quite time consuming and treated as a boring distraction by some people. I agree it seems unworkable if there's any intention of sharing algorithms/strategies without profit share.

*Tags: Strategies*

---

**Krz Heph** - *22:25:04*

Most likely C++ or Rust.. Python maybe to prototype or train models?

*Tags: Strategies*

---

## 2022-04-18

**AndyL** - *20:36:45*

Does anyone know how to identify if your account has incurred a transaction charge? Ie.where to look, and what to look for please?

*Tags: General Technical*

---

**Aaron Smith** - *20:51:04*

are you using flumine? If i recall correctly it should throw some warning when hitting the transaction limit, so maybe check your logs.

*Tags: General Technical*

---

**AndyL** - *20:52:00*

I have several separate strategies, ie.more than one Flumine instance

*Tags: General Technical*

---

## 2022-04-19

**liam** - *09:53:55*

what do you mean? Is it missing from betfair_data or flumine or both?

*Tags: General Technical*

---

**AndyL** - *15:35:33*

When running multiple live strategies, what's the best practice of single vs multiple Flumine instances/processes ?

*Tags: Deployment*

---

**liam** - *16:01:37*

strategy/market/instance dependant, just make sure you have some decent logging on what is important (to you)

*Tags: Strategies*

---

**AndyL** - *16:04:31*

I have a separate Flumine python process for each strategy at the moment. Surprisingly cpu usage is quite low, memory good, so mainly network bound

*Tags: Performance, Strategies*

---

**AP** - *17:49:37*

What's the best way to alter/append to the customer order ref within a strategy?

*Tags: Strategies*

---

**AndyL** - *18:10:07*

Trying to improve execution hence question above...

*Tags: General Technical*

---

**AndyL** - *18:13:14*

Hence transaction questions

*Tags: General Technical*

---

**Michael** - *18:20:20*

Ok. Personally I would focus on that. You don't want to be carpet bombing the exchange if you can help it.



One thing that you've done well is played a long game and hung in there - don't let that go. Stay patient and work out where your winnings are coming from.



You might find yourself tempted to try to jack up your betting in the hope that you'll win more that way - you won't.



Take your time, build slowly and drive up the average profit of your transactions. Go for the robust long term operation, not the short term win.

*Tags: Performance, Strategies*

---

## 2022-04-20

**thambie1** - *10:46:35*

A bit of a long shot, but figured this is worth a try. Willing to pay £3k to anyone who can tell me the goal distribution model being used by the primary market maker of the football correct score market. Hint it's not poisson, or anything I've been able to find in academic papers.

*Tags: Strategies*

---

**Mo** - *10:59:52*

I speak from personal experience of trying to reverse engineer the parameters of a competitor's model and then finding out that the model was blended with their position making that essentially impossible

*Tags: Strategies*

---

**river_shah** - *13:24:58*

put the poisson model in the trash bin for sure though

*Tags: Strategies*

---

**river_shah** - *13:26:38*

my intuition as why parametric models have such trouble fitting CS markets is that the goal transition probabilities change drastically conditional on game state / time

*Tags: Strategies*

---

**river_shah** - *13:30:13*

a full state space model ends up being underdertmined (in the system of equations sense) so you do end up plugging in simulation driven estimates of critical parameters

*Tags: Strategies*

---

**Dave** - *16:28:20*

Trust me, modelling this is really hard, you don't want to try

*Tags: Strategies*

---

## 2022-04-21

**Jon K** - *11:16:09*

Hi all, I'm following some Flumine examples and have created a Python script that accepts a marketId as a parameter.

The strategy then uses that MarketId as part of the market filter, using this example

market_filter=streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB"],

        market_types=["WIN"],

        market_ids=[marketToCheck]

    )



This works fine but I notice more and more python processes remain open (using Windows Task Manager) as each new call is made to the script despite the market(s) in question being closed.

Should I be adding something specific to stop the framework running, maybe at this point?



framework.add_strategy(strategy)

framework.run()

*Tags: Strategies*

---

**liam** - *11:18:26*

flumine isn't designed for this as it will keep running (as you have found out) Best practice is to take advantage of the stream and have a nice wide filter on your `market_filter` ie eventType+marketType and then filter in the strategy via `strategy.check_market_book`



Resulting in a single running process

*Tags: Strategies*

---

## 2022-04-23

**Mo** - *07:50:29*

Version 0.3.0 of [https://github.com/mberk/betfairutil|betfairutil](https://github.com/mberk/betfairutil|betfairutil) released:



```### Added



* Unit tests #14

* Various price related functions:

    * get_spread

    * calculate_price_difference

    * get_inside_best_price

    * get_outside_best_price

    * get_best_price

    * increment_price

    * decrement_price

    * is_price_the_same_or_better

    * is_price_worse

* Various horse racing related functions:

    * get_race_id_from_string

    * get_win_market_id_from_race_card

    * read_race_file



### Changed



* Updated typing of all price related functions to accept Path objects



### Fixed



* Various fixes when using MarketBook objects instead of dictionaries```



*Tags: Errors Debugging*

---

## 2022-04-25

**Guy Incognito** - *10:01:24*

Hey guys what is the best way to use betfairlightweight in conjunction with flumine? I want to call betfair light weight once per market around 20 mins before the start of the race to get the names of horses to merge with the rest of my data

*Tags: General Technical*

---

## 2022-04-29

**Kunal Maneck** - *07:48:05*

Any recommendations on books or reading material on strategy? 

*Tags: Strategies*

---

**liam** - *08:34:42*

Yeah, was talking to Rob about adding a getattr to allow `market_book.market_id` and `market_book["marketId"]`



There is then the question of how its added, does it become the default? Or used when installed? Or do we drop down into bflw as the default for processing historical data?

*Tags: Getting Started, Data Quality*

---

**Mo** - *09:04:30*

Yeah [@U4H19D1D2](@U4H19D1D2) is exactly right, the only reason for the type checks in `betfairutil` is to work out whether to use key or attribute syntax. I've been discussing with [@U4H19D1D2](@U4H19D1D2) and [@UUX1L88MC](@UUX1L88MC) about having both bflw and betfair_data MarketBook objects support `__getitem__`. If that's done then the dictionary lookup syntax will work regardless of type

*Tags: General Technical*

---

**Tom** - *23:12:20*

```from matchbook.apiclient import APIClient

from matchbook.exceptions import AuthError, PasswordError, ApiError, MBError



import logging

import os

logger = logging.getLogger(__name__)

MATCHBOOK_PW = os.environ.get('MATCHBOOK_PW')

MATCHBOOK_USR = os.environ.get('MATCHBOOK_PW')

api = APIClient(MATCHBOOK_USR, MATCHBOOK_PW)



def get_client():



    try:

        if not api.session_token or api.session_token is None:

            api.login()

            logger.warn(f'this is the {api.session_token}')

    except (AuthError,ApiError,MBError):

            api.login()

    return api



def get_orders():

    api = get_client()

    r = api.betting.get_orders(runner_ids=None, market_ids=None, offer_id=None, offset=0, per_page=500,

                    interval=None, side=Side.Default, status=Status.All, session=None)

    print(r)

    print(api.session_token)```

Hi there, I was wondering if anyone is using this repo for matchbook [https://github.com/rozzac90/](https://github.com/rozzac90/)

I cannot log in again after 6 hours to refresh the token. For some reason I can't run the except block to create a new token when ApiError occurs. Does anyone know how best to handle logging in to refresh the session? My get orders function polls indefinitely so I need to be able to handle the logging in again when the token expires. Thanks

*Tags: Getting Started, Errors Debugging, Strategies*

---

## 2022-05-01

**Newbie99** - *09:59:04*

Are you checking right after startup?



Market catalogue isn't included in the stream, Flumine makes a REST API call behind the scenes, so it can take a few seconds when starting up before the object is populated.

*Tags: General Technical*

---

**Dave** - *19:11:08*

Question: is Betfair's order book truly based on time-priority at the same price? I cannot find any documentation on it - just curious if it has been officially established that the order book is a traditional order book with price-time priority and nothing funky/unexpectwd happens during the matching cycle where orders later in the queue we think there is might be matched first?

*Tags: General Technical*

---

## 2022-05-02

**Marco** - *17:12:16*

Hello! I am trying out this wonderful project :slightly_smiling_face: Is there a way to record market data without using streaming? So classic polling. I've searched a bit but it doesn't seem possible out of the box

*Tags: General Technical*

---

## 2022-05-03

**Mr West** - *00:32:02*

Streaming is the best way to get the data.

*Tags: General Technical*

---

**Peter** - *08:30:40*

Setting up polling on a loop is pretty straightforward, but Betfair won't like you doing it and may even cut you off (personal experience early in my journey), and as your sophistication grows streaming is a much, much better option anyway.

*Tags: General Technical*

---

**LM** - *10:02:02*

Is there any way to gauge the actual latency of bet placement to the exchange? Also is there significant improvement using lightweight mode in Flumine streaming to improve latency (obviously is going to be code dependent but I'm only running super simple logic)?

*Tags: Performance*

---

**Mo** - *11:08:53*

If you want to compete on latency you shouldn't be using Python 

*Tags: Performance*

---

**liam** - *21:05:37*

Lightweight mode won’t work in flumine as it’s designed to use resources however a few things are patched to improve speed (check out the patch file)

*Tags: Performance*

---

## 2022-05-04

**Rudeger Jamison** - *01:16:11*

Hey - is there a general strategy limit that is known to slow down Flumine?



eg. Is there any advice to run one instance of Flumine with say 5 strategies or is it resilient enough to  use with say 30 strategies +?



I can test this out myself, just thought you guys would have experience with this already. Cheers

*Tags: Performance, Strategies*

---

**Steve Roach** - *04:11:06*

Hi Guys,



I've implemented the datacollectwom.py strategy from GitHub: flumine-strategy-development and I've run a few PRO files through it. The resulting datapoints are about 5 seconds apart. Is this about right? Is this what I would expect running Flumine against a live market?



Cheers

*Tags: Deployment, Strategies*

---

**liam** - *07:22:48*

Very strategy dependant, if you are getting latency warnings then you have gone way too far 

*Tags: Performance, Strategies*

---

**liam** - *07:25:08*

Have a read of the strategy code in check_market_book 

*Tags: Strategies*

---

**Steve Roach** - *07:28:34*

hmm, my bad - brain fart.



As to the second part of my question; what would the expected data gathering rate be in a live market? Or, I suppose, what are the limiting factors?

*Tags: Deployment*

---

**Jonjonjon** - *08:02:29*

I run over 50 on a single python instance. I haven't tracked latency, but it (along with 3 other Flumine scripts) uses 30-80% CPU utilisation on a dual CPU cloud machine

*Tags: Performance*

---

## 2022-05-05

**Ivan Zhou** - *15:47:24*

Whats the best way to grab a log of your bets at the end of the day? Having issues using the blotter to collect results on a live strategy the same way as you would for a simulation

```# Collect results

results = []

for market in framework.markets:

    print("Profit: {0:.2f}".format(sum([o.simulated.profit for o in market.blotter])))

    for order in market.blotter:

        results.append(

            [

                order.market_id,

                order.selection_id,

                order.responses.date_time_placed,

                order.status,

                order.order_type.price,

                order.average_price_matched,

                order.size_matched,

                order.simulated.profit,

            ]

        )```

*Tags: Deployment, Strategies*

---

**Newbie99** - *15:49:38*

[https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: General Technical*

---

**Newbie99** - *15:50:12*

That is probably the best starting point, tweak that example to suit your exact needs (e.g. can write to a JSON or database if preferable to a .txt file)

*Tags: General Technical*

---

**Peter** - *15:55:52*

My preference is to use logging control to write each order to a database when it's placed and then update it when the market is closed.

*Tags: General Technical*

---

**Rudeger Jamison** - *17:37:41*

Hey, when adding a Custom Streams for something like dynamically updated model odds that are written to a Db by another process. (is this a good use case?)



How do I then access that data in my strategy after adding it to the franweork.

*Tags: Strategies*

---

**liam** - *17:38:41*

Yeah that is the correct use case, use a custom event with a callback to update a market or strategy context 

*Tags: Strategies*

---

**Rudeger Jamison** - *17:43:22*

[https://betcode-org.github.io/flumine/advanced/|https://betcode-org.github.io/flumine/advanced/](https://betcode-org.github.io/flumine/advanced/|https://betcode-org.github.io/flumine/advanced/) - instead of process my event you mean to just actually add to the strategy.context.



Got ya. Thanks.

*Tags: Strategies*

---

## 2022-05-06

**Mo** - *08:36:13*

How do you think betfairlightweight knows what server to connect to? Or you could just read the API documentation 

*Tags: Deployment*

---

**Jorge** - *09:08:07*

But my problem is how to convert that to an IP address

*Tags: General Technical*

---

**Peter** - *13:24:27*

I use them. Data quality is pretty good and they're super-responsive to corrections in their historic data.



I don't know what the delay is, but I would expect it to be somewhat variable depending on the speed of the feed the data analysts receive, how far ahead of the "live" broadcast they are receiving it, how much of the encoding is automated and how quickly specific analyst is able to encode the non-automated events. Overall however, I would expect it to be dwarfed by the impact of polling for new data.



I'm not aware of a mapping between Betfair team names and Sporting Monks. I've done that myself but found a fair number of inconsistencies in team names on the Betfair making, even in major competitions, rendering a mapping a bit problematic. My preference instead is to use fuzzy logic on the team names to find best matches. I've found that more reliable (i.e. less brittle).

*Tags: Data Quality, Performance, Deployment*

---

**Graham** - *13:32:14*

I wouldn’t be surprised to hear they are reselling official feeds with latency which renders it useless for bet placement 

*Tags: Performance*

---

**Alessio** - *14:06:25*

I think the main problem of sportmonks is that it's not a streaming API, you need to poll. You can poll, say, twice a second, but it's still going to be a pull interface rather than push.

*Tags: General Technical*

---

**Kunal Maneck** - *18:20:17*

How is the latency of sportmonks feed? 

*Tags: Performance*

---

**Fab** - *22:33:20*

[http://www.betsapi.com|www.betsapi.com](http://www.betsapi.com|www.betsapi.com)

[http://www.totalcorner.com|www.totalcorner.com](http://www.totalcorner.com|www.totalcorner.com)



The former is generally good. The latter is prone to blackouts for a few days every now and then.



Neither do streaming as far as I know.

*Tags: General Technical*

---

## 2022-05-08

**Peter** - *13:56:18*

It's a general upgrade of the API. Including improvements to the Football API to provide new endpoints and better reliability (not that I've perceived that to be a problem) and extra filtering options.



It also converts the APIto a multi-sport data model to make it easier for them to add more sports in future and for us users to give us a more consistent structure for ingesting different sports.

*Tags: Strategies*

---

## 2022-05-09

**John Foley** - *18:59:36*

Does anyone know of any APIs that have xG in their historical data offering? (aside from statsbomb or the likes targeting the big fish)

*Tags: Data Quality*

---

## 2022-05-11

**H** - *08:15:25*

Hi, anyone else having issues with the Betfair historic data API? I'm trying to download some files I've purchased and I'm getting something that looks like a redirect error from cloudflare:

```JSONDecodeError: Expecting value: line 2 column 1 (char 1)



During handling of the above exception, another exception occurred:



...



InvalidResponse: Invalid response received: 

[!DOCTYPE html](!DOCTYPE html)

&lt;html&gt;

&lt;head&gt;

&lt;meta name="viewport" content="width=device-width" /&gt;

&lt;title&gt;ngErrorRedirect&lt;/title&gt;

&lt;/head&gt;

&lt;body&gt;

&lt;div&gt;

Error

&lt;/div&gt;

&lt;script defer src="[https://static.cloudflareinsights.com/beacon.min.js/](https://static.cloudflareinsights.com/beacon.min.js/)...&lt;/script&gt;

&lt;/body&gt;

&lt;/html&gt;```

I tried downloading through the GUI/web page as well and that also didn't work.

*Tags: Errors Debugging*

---

**H** - *08:35:33*

Yeah, even if I restrict to one day I still get that error

*Tags: Errors Debugging*

---

**Lukas Trišauskas** - *20:01:38*

hi there, I have a newbie question. I am trying to get match odds for several events with betfairlightweight python package. I am able to login and get market catalogue for match_odds, but when I try to get market book for a specific market id, I get an error that I do not provide selection id. Maybe someone could explain me, which method returns this data? By the way, I am using delayed API key

*Tags: Errors Debugging*

---

**Newbie99** - *22:57:30*

The delayed key won't help much, other than testing your code (despite the name its not simply a case of using it to view delayed data, it does weird stuff to it to make the prices unusable).



Are you able to share your code and someone might be able to spot the issue?

*Tags: General Technical*

---

## 2022-05-12

**Rudeger Jamison** - *03:23:53*

Hey [@U4H19D1D2](@U4H19D1D2) - finally getting around to this.



If I have some code eg.



```import requests

from flumine.events.events import CustomEvent

from flumine.streams.basestream import BaseStream





class CustomStream(BaseStream):

    def run(self) -&gt; None:

        # connect to stream / make API requests etc.

        response = self._get_random_data()



        # callback func

        def callback(framework, event):

            for strategy in framework.strategies:

                strategy.process_my_event(event)



        # push results through using custom event

        event = CustomEvent(response, callback)



        # put in main queue

        self.flumine.handler_queue.put(event)



    def _get_random_data(self):

        response = requests.get("[https://random-data-api.com/api/cannabis/random_cannabis?size=2](https://random-data-api.com/api/cannabis/random_cannabis?size=2)")        

        return response.json()```

I then handle this in my strategy, now how would I go about making sure this is updated for example every 1 hour?



Is that something I can do within the framework?

*Tags: Strategies*

---

**Rudeger Jamison** - *03:35:07*

I thought perhaps I could set the `streaming_timeout` arg and that would trigger it, but having no luck.

*Tags: General Technical*

---

**liam** - *09:06:31*

You are just missing a while loop, notice the use of `is_alive` so it can end cleanly and a sleep which I have used the streaming_timeout



```import time

import requests

from flumine.events.events import CustomEvent

from flumine.streams.basestream import BaseStream





class CustomStream(BaseStream):

    def run(self) -&gt; None:

        while self.is_alive():

            # connect to stream / make API requests etc.

            response = self._get_random_data()



            # callback func

            def callback(framework, event):

                for strategy in framework.strategies:

                    strategy.process_my_event(event)



            # push results through using custom event

            event = CustomEvent(response, callback)



            # put in main queue

            self.flumine.handler_queue.put(event)



            time.sleep(self.streaming_timeout)



    def _get_random_data(self):

        response = requests.get("[https://random-data-api.com/api/cannabis/random_cannabis?size=2](https://random-data-api.com/api/cannabis/random_cannabis?size=2)")

        return response.json()```

*Tags: Deployment, Strategies*

---

**Nicholas vizard** - *11:59:35*

Hey guys, bit of a left-field question here. I was wondering if anyone knew were to find aggregate stats on betfair's recent general performance, particularly relating to their importance in the betting landscape.



For example, things like what was the total traded volume through betfair in 2021? How many sports did betfair cover? How many markets were run in 2021? etc. Just looking to do a brief right up of the importance of betfair and any help would be much appreciated.



Cheers,

Nick

*Tags: Performance, Strategies*

---

**H** - *13:04:27*

Yesterday they said they'd let me know once it's fixed

*Tags: Errors Debugging*

---

**H** - *16:17:35*

They claim it's fixed now but I'm still seeing the same error

*Tags: Errors Debugging*

---

**H** - *19:53:05*

I managed to download through the GUI, though I keep getting the same error when I try the API. Let me know if the API works for you!

*Tags: Errors Debugging*

---

## 2022-05-13

**Rudeger Jamison** - *06:20:21*

Hey - how do I explicitly set my `customer_strategy_ref` so that it isn't my host name? if I want a different strategy name to the class name of my actual strategy ?

*Tags: Strategies*

---

**Rudeger Jamison** - *06:27:23*

I can see it is set in the `config.py` but do I actually need to go in to the flumine code and change this ?



Cant see much info on the docs explaining it

*Tags: General Technical*

---

**liam** - *06:36:21*

```from flumine import config



config.whatever = whatever ```

*Tags: General Technical*

---

**liam** - *06:37:02*

```Strategy(name=“whatever”, market_filter..)```

*Tags: Strategies*

---

**Rudeger Jamison** - *06:37:10*

Can i set it per strategy though?

*Tags: Strategies*

---

**liam** - *06:37:46*

No flumine uses it internally, if you are trying to record your bets use loggingcontrol 

*Tags: General Technical*

---

**liam** - *06:39:05*

Why do you want to change the strategy ref?

*Tags: Strategies*

---

**Rudeger Jamison** - *06:40:08*

because I have loads of different strategy names and I use that to differentiate how they perform in my logging.



I send all the logs to athena in aws, and that is the way I can differentiate an order from one strategy to the next

*Tags: Deployment, Strategies*

---

**liam** - *06:41:03*

Just log/store the strategy name as well?

*Tags: Strategies*

---

**Rudeger Jamison** - *06:41:10*

I would assume that by Flumine accepting multiple strategies that you can set multiple customer_strategy_refs

*Tags: Strategies*

---

**Rudeger Jamison** - *06:42:07*

because if I am setting it globally how will it be different per strategy ?

*Tags: Strategies*

---

**liam** - *06:46:42*

I think you are after `order.trade.strategy.name`

*Tags: Strategies*

---

**Rudeger Jamison** - *07:10:39*

is there anyway to get it from *`_process_cleared_orders` because the `ClearedOrder` object doesnt know about the strategy or the trade by the looks of things* :disappointed:

*Tags: Strategies*

---

**liam** - *07:28:06*

Is that a problem? For me I have various tables; market, order, trade etc. Market data gets stored on creation along with order and trade which also gets updated on closure (profit/matched price)

*Tags: General Technical*

---

**Mo** - *16:47:13*

Same range. It's been helped by a current month long drawdown

*Tags: General Technical*

---

**liam** - *19:35:42*

[@UEA14GBRR](@UEA14GBRR) + the backhanders help soften the blow :wink:

*Tags: General Technical*

---

## 2022-05-14

**EJono** - *14:25:24*

Getting to grips with the fromRecord indexing that can be used for retrieving current orders when over 1000 active trades exist. Do other filters work as normal inside the list_current_orders call when doing the following:



```p_orders = []

record_count = 0



while more_available:

    orders = trading.betting.list_current_orders(

        from_record=record_count,

        lightweight=True,

    )

    p_orders += orders.get('currentOrders', [])

    more_available = orders.get('moreAvailable', False)

    record_count += len(orders)```

for instance when using fromRecord for order retrieval the docs are ambiguous how other parametes like the following can affect the response or if they will at all:

```date_range={

    "from": startTime,

    "to": endTime

},

customer_strategy_refs=["strategy_ref"]```

if i added these to list_current_orders request will i now index through bets that meet the other parameters passed?

*Tags: Strategies*

---

**liam** - *14:55:45*

Probably, quicker to test than write the question? 

*Tags: General Technical*

---

**EJono** - *15:21:13*

thank you for the heads up that will save me some debugging :slightly_smiling_face:

*Tags: Errors Debugging*

---

## 2022-05-16

**Nicholas vizard** - *09:00:27*

Hey guys just a quick one and hope everyone is keeping well!

*Is there a reason why (other than a coding error) a ladder midpoint  between the best back and lay price at an event start may be dramatically different to BSP for the same selection?*

I have been conducting some analyses on Australian Thoroughbred markets, and in this analyses need to compare the ladder midpoint at the off to the BSP for all selections.

On whole, these two numbers reconcile. However, there are some instances were they are dramatically different. e.g. (A ladder mid point of 4.9 and BSP of 1.11).

I probably only have a couple hundred of these instances in a sample of 600,000 so am not overly concerned but would like to identify if it is likely i have made a coding error or this can be expected based on the way in which BSP is constructed.



Many thanks for any help!

Nick

*Tags: Errors Debugging*

---

## 2022-05-17

**Ralegh** - *11:33:27*

Has anyone had restrictions on betfair api use from being too read heavy?

*Tags: General Technical*

---

## 2022-05-18

**Unknown** - *05:16:41*

hey guys just reading through the flumine documentation looking at the simulation section, I was wandering what data/data format is stored in the marketdata folder i.e /tmp/marketdata/1.170212754? Is it just the raw marketbook? Thanks

*Tags: General Technical*

---

**Peter** - *06:16:16*

Close. It's a flat file containing the raw updates received in the stream in json format. One update per line. It's the output from the marketrecorder strategy.

*Tags: Strategies*

---

**Peter** - *06:18:40*

When you feed this into the simulator, Flumine will process each update to construct a market object, which includes the market_book, and an orders object based on the actions that you strategy would have taken in response to market changes.

*Tags: Strategies*

---

**Stefan** - *10:15:51*

I am using Orange app to run my ML models using different ML algorithm, it is actually visual designing ML. So have no idea how python will utilize GPU. Orange uses *numpy, scipy and scikit-learn, so my question is: are these libraries able to detect what your system offers, and use GPU of graphic card automatically?*

*Tags: Strategies*

---

## 2022-05-20

**mandelbot** - *11:09:03*

Can I expect a latency bump from running on a linux box vs windows? Just switched :nerd_face:

*Tags: Performance*

---

**liam** - *14:43:27*

My only recommendation is to make sure your dev box is the same, wasted way too much time trying to debug os issues 

*Tags: Errors Debugging*

---

**Aaron Smith** - *16:37:26*

in a runner_book, is runner_book.sp.actual_sp always a float or None? I saw in the direct streaming updates things like "Infinity" were also possible, would this just convert to None in a runner_book?

*Tags: General Technical*

---

**liam** - *16:40:02*

it can be a float, string, none and a list just to keep you on your [https://github.com/betcode-org/flumine/blob/b33d82b75175106b2de0d0c4f9851599b085e389/flumine/utils.py#L148|toes](https://github.com/betcode-org/flumine/blob/b33d82b75175106b2de0d0c4f9851599b085e389/flumine/utils.py#L148|toes)

*Tags: General Technical*

---

**liam** - *16:45:01*

that logic is in flumine and not bflw

*Tags: General Technical*

---

**Gooat** - *17:23:42*

Friday afternoon pub question...if you wanted "something" running alongside the streaming would you just create a worker that didn't die off...while X, keep working.

*Tags: General Technical*

---

**Gooat** - *17:35:32*

Ha feedback. 



Ok I have a function that sits on a socket and blocks until new. What a correct way to get that playing along with Flumine doing the other stuff

*Tags: General Technical*

---

**Dave** - *17:38:30*

use a Process from multiprocessing, assuming you're running on a multi-core machine you'll be able to run something in parallel without impacing flumine

*Tags: General Technical*

---

**Gooat** - *19:29:33*

Is there an ELIF of middleware v worker, in the Flumine context

*Tags: General Technical*

---

**Johnny** - *19:47:24*

Do you use Docker to help with this?

*Tags: General Technical*

---

## 2022-05-21

**Alessio** - *09:57:03*

To come back to your original, though, a GPU is useful only if you either have (1) very deep NNs or (2) special operations like convolutions. Unless you are processing images, you probably don't need either. You will understand you need a GPU the moment it takes days and days to train your model

*Tags: Strategies*

---

**Jonjonjon** - *11:17:44*

My first setup was a web page. The web page was Python. The Python placed the bets. To activate it I had to visit the web page. Can you beat that?

*Tags: Getting Started*

---

## 2022-05-25

**Jonjonjon** - *21:49:30*

What git command do I need in order to pull Flumine 1.22.2 (2022-03-24) ?

*Tags: General Technical*

---

## 2022-05-26

**liam** - *05:30:25*

Find the [https://github.com/betcode-org/flumine/pull/570|commit](https://github.com/betcode-org/flumine/pull/570|commit) and pull it, need to start tagging in release to make this easier 

*Tags: General Technical*

---

## 2022-05-27

**Rudeger Jamison** - *00:54:40*

Hey [@U4H19D1D2](@U4H19D1D2) - looking to revisit this conversation.



Is there anyway to actually set this `customer_strategy_ref` per strategy with multiple strategies operating within a framework.



The reasoning is quite simple - currently I have some strategies that run on the polling api and now some on Flumine.



I collect the data through the `LoggingControls` and that is well and good, but if for any reason the stream is down on my end I am obviously going to lose data and as a result, I am not able to allocate my orders to a specific `customer_strategy_ref.`



In the polling api - when I place an order in the polling api - I can do so like:



```trading.betting.place_orders(

                market_id=market_id,

                customer_strategy_ref=strategy_name,

                instructions=instructions_list,

            )```

Which allows me to `list_cleared_orders` and identify my strategy by the `customer_strategy_ref`



Is this a shortcoming of the stream api that I am unable to get around?

*Tags: Strategies*

---

## 2022-05-29

**Rudeger Jamison** - *23:48:00*

Hey [@U4H19D1D2](@U4H19D1D2) - any chance we can discuss this?



I see you hash the strategy name into the customer order ref - that would be good enough for me, but I can't work out how to decode the hash because you limit the length of it in the `create_cheap_hash` function. Any ideas?

*Tags: Strategies*

---

## 2022-05-30

**Rudeger Jamison** - *09:35:49*

What doesn't make sense that I can help clear up?



The long and short is that I want to be able to split P&amp;L by strategies and retrieve that from the polling api list_cleared_orders.

*Tags: General Technical*

---

**Unknown** - *11:27:38*

I've purchased some pro data, it looks like the market catalogue data is in there. How do I load the market catalogue into the middle wear,  it's in the same file as all the pricing streaming updates instead of a separate file?

*Tags: General Technical*

---

## 2022-05-31

**Michael** - *21:02:40*

For historical data in the form 1.xxxx.bz2. Does the xxx mean anything i.e. date or market?

*Tags: Data Quality*

---

## 2022-06-06

**Unknown** - *09:04:22*

Any flumine users get super high CPU usage from the downtime yesterday?

*Tags: General Technical*

---

**liam** - *09:10:12*

Any ideas? My logs all looked fine, showing the 503 errors etc, I can't see anything obvious that was causing it, instinct is telling me maybe it was one of the workers stuck in a loop

*Tags: Errors Debugging*

---

## 2022-06-08

**nthypes** - *17:19:58*

nope. they are matched bets executed from flumine

*Tags: General Technical*

---

**nthypes** - *17:20:33*

they are matched during the flumine execution

*Tags: General Technical*

---

## 2022-06-10

**foxwood** - *21:09:50*

The proform data is in SQL server (last time I subscribed some years ago) so if you are on Windows then no need to export - just run queries to suit. Install SSMS (free) and explore with that. Use pyodbc for real time queries if you are a Python user. You can also run SQL queries from Excel.

*Tags: Getting Started, Deployment*

---

## 2022-06-11

**LM** - *02:25:11*

Does flumine do any coversion of volume to local currency (or is it GBP)?

*Tags: General Technical*

---

**Peter** - *07:35:13*

Flumine works with Betfair streams. Those streams are available only in pounds. There will usually be some conversion from local currency happening, but it will be at Betfair, not within Flumine.

*Tags: General Technical*

---

## 2022-06-13

**Unknown** - *03:41:25*

Anyone been getting really bad latency issues recently? Yesterday my latency was at 1 second today when I checked its hitting 4s :disappointed:

*Tags: Performance*

---

**Barclay** - *09:10:41*

ok a restart fixed the issue, not sure what the exact problem was then

*Tags: Errors Debugging*

---

## 2022-06-16

**Rudeger Jamison** - *02:59:37*

Hey - I cant work it out - does doing :point_down: work out your runner exposure in your specific strategy or across all strategies running on Flumine?



```            runner_context = self.get_runner_context(

                market_id=market.market_id, selection_id=runner.selection_id, handicap=runner.handicap

            )```

*Tags: Strategies*

---

**liam** - *06:11:57*

That is context per strategy

*Tags: Strategies*

---

**liam** - *06:12:37*

however exposure calculations are best done in the [https://github.com/betcode-org/flumine/blob/b33d82b75175106b2de0d0c4f9851599b085e389/flumine/markets/blotter.py#L149|blotter](https://github.com/betcode-org/flumine/blob/b33d82b75175106b2de0d0c4f9851599b085e389/flumine/markets/blotter.py#L149|blotter)

*Tags: General Technical*

---

## 2022-06-18

**Halil Erdoğan** - *16:46:44*

Hello guys :wave: I have a question regarding the exchange API of betfair and i am using [https://github.com/betcode-org/betfair](https://github.com/betcode-org/betfair). Currently I am working on upcoming F1 Race and would like to get all traded data. When I am getting _streaming update,_ i can just see 3 _ladder_ in the `"trd"` OTOH, if you check [https://graphs.betfair.com/#/1.200149997/10997893/0](https://graphs.betfair.com/#/1.200149997/10997893/0) you can see that there are many more ladders which are already _traded_. Are there any way to get all of those ladders?

```output_queue = queue.Queue()



listener = betfairlightweight.StreamListener(output_queue=output_queue)

stream = trading.streaming.create_stream(listener=listener)

market_filter = streaming_market_filter(market_ids=[1.200149997])



market_data_filter = streaming_market_data_filter(

    fields=["EX_BEST_OFFERS_DISP", "EX_BEST_OFFERS", "EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", 

                "EX_LTP", "EX_MARKET_DEF", "SP_TRADED", "SP_PROJECTED"],

    ladder_levels=10)





streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=2000)





t = threading.Thread(target=stream.start, daemon=True)

t.start()

while True:

    updates = output_queue.get()

    for update in updates:

        print(update.json())

    break # just for testing purpose, will be removed later on!



# id of leclerc -  10997893



for update in updates:

    for rc in update.streaming_update["rc"]:

        if rc["id"] == 10997893:

            print (rc["trd"])```

*Tags: Strategies*

---

**Unknown** - *17:32:29*

*Update*: Looks like that is related with this [https://github.com/betcode-org/betfair/blob/e3d36f67d5291faa8e7ac85c061631b0dd722091/betfairlightweight/streaming/cache.py#L262|code-line](https://github.com/betcode-org/betfair/blob/e3d36f67d5291faa8e7ac85c061631b0dd722091/betfairlightweight/streaming/cache.py#L262|code-line). Looks like that is intended to make it faster and using `Available` class for that. Because when there is market change and we print that in [https://github.com/betcode-org/betfair/blob/e3d36f67d5291faa8e7ac85c061631b0dd722091/betfairlightweight/streaming/cache.py#L247|here](https://github.com/betcode-org/betfair/blob/e3d36f67d5291faa8e7ac85c061631b0dd722091/betfairlightweight/streaming/cache.py#L247|here), we can see all "trd" information. So that is available but for speed reason, that is disabled. Are there any way to enable full log via _streaming module_ or is it not possible? Also I can create a github issue if that is preferable.

*Tags: Performance*

---

**Mo** - *18:26:10*

I think you are confused. With streaming you only receive the deltas in each update. So when you access the streaming_update field you are only seeing those traded volumes which have just changed. If you want to see the full traded ladder then you need to iterate over update.runners and for each runner access runner.ex.traded_volume

*Tags: General Technical*

---

## 2022-06-19

**Halil Erdoğan** - *11:54:34*

Thanks [@UBS7QANF3](@UBS7QANF3) That is what I was looking for actually :pray: Apparently I was confused with some of changes but now everything is more clear :muscle:



Just one more question -- That _streaming update_ just send what happened recently right? So by just looking to that _update_, we can understand what has been changed in the _exchange_ :thinking_face:

*Tags: General Technical*

---

**Unknown** - *16:42:01*

I also guess that is not _possible_ to create _trade volume/last price traded_ in different time-step without keeping datas from beginning. :thinking_face: For instance, i am not sure can we get that _line_ without subscribing streaming from the beginning of event.

*Tags: General Technical*

---

## 2022-06-20

**mandelbot** - *19:15:39*

```from flumine.utils import get_price, get_size

get_price(runner.ex.available_to_back, 0)

get_size(runner.ex.available_to_back, 0)```

*Tags: General Technical*

---

## 2022-06-22

**EJono** - *13:42:47*

Im struggling to find any documentation on the inplayservice end point. Is there a resource or page i can reference to find out requests limits for this service and other important parameters to be aware of. Is there a problem hitting the endpoint as often as possible doing something like:



`while True:`

    `response = [http://trading.in|trading.in](http://trading.in|trading.in)_play_service.get_scores(event_ids=[EVENT_IDS])`

    `queue.put(response)`



where EVENT_IDS can be be anywhere from 50 to 1000 elements in length. Cheers

*Tags: Strategies*

---

**EJono** - *14:52:26*

hmmm unfortunate there arent any numbers to go by, from personal experience is there any indication of best practices? I see in the flumine example for "poll_in_play_service" that individual requests are used for each event id in a loop as opposed to bulk requests with many ids. Is there any reason for this as i was under the impression minimising overall reqs was the safest method. Is it worth getting in contact with neil to clarify any of this information on my end?

*Tags: General Technical*

---

## 2022-06-26

**Barclay** - *03:47:29*

Hey, basic question but how do I get the results of a market after it has closed?

*Tags: General Technical*

---

**Halil Erdoğan** - *12:30:07*

Thanks for clarification. My question was *can we get _runner name_ from _`market_book`*._ But I understand that is not possible. Will change my work-flow a bit to get that from _`market_catalogue`_.

*Tags: General Technical*

---

**Ke** - *20:07:54*

A question about how to use Trade object. If i created a trade object in the process_market_book call to place an order. Is this trade object persistent and can be reused when process_market_book is called next time? If yes, am I supposed to reuse it as much as possible or I should create a new trade object to place new order for the same horse?

*Tags: General Technical*

---

## 2022-06-27

**James** - *14:19:14*

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32602, 'message': 'DSC-0018'}, 'id': 1}

*Tags: Errors Debugging*

---

## 2022-06-29

**EJono** - *09:11:32*

is there a reliable way of determining if a football event will have score data through the inplayservice api pre kick off? Secondly is anyone aware of the reliability of the score feed? For instance does it seem like the scores have a latency of ~1-2 minutes from moment of the goal (or other score feeds), are incorrect scores to be expected every 100 or so games, are small games given less attention than those with high traded volumes? etc - just peoples general experience of how useful the score feed from the inplay service actually is or if they try and avoid it in favour of other sources

*Tags: Performance*

---

**Wayne** - *12:54:11*

Hey all, I hope this posting is best placed in the general channel.



I am helping to scale a trading syndicate, so far they have a team of 17 quants and quant devs and focus on football and horse racing - it is not all Betfair focussed. Looking to hire developers with experience and interest trading sports – that led me here.



The client uses a mix of low latency and value-based approaches, latency is quite key to some strategies they run so C++ experience is a big plus. Compensation will consist of base salary, guaranteed bonus for the first year (team profit share after) and a sign on bonus to compensate for any strategy you have running currently. As a guide, dependent on experience base salary can range from £110-200k (huge range I know this is not helpful).



Let me know if you're interested in hearing more! Or let me know if this is not the right place for this message and I will not reoffend!

*Tags: Performance, Strategies*

---

**Wayne** - *13:45:00*

Thank you, and I do appreciate the feedback - not that my word counts for anything on here but I promise this is real. I consult for most of the established trading firms globally. The salaries on the low latency focussed dev roles are not much different to those in finance nowadays

*Tags: Performance, Strategies*

---

## 2022-06-30

**foxwood** - *09:13:47*

I took [@UBS7QANF3](@UBS7QANF3)'s post as a pretty solid indicator that it wasn't a scam post. Once upon a time in my youth I would have been interested in this lol. If they want hft then, for the critical bits, you need to come down to inline C, thus losing the class overhead, and maybe down to assembler where it really matters imho. I know hardware is so fast these days but the competition has the same kit and locations so your execution USP has to be software. Does help if you also make value selections !

*Tags: General Technical*

---

**EJono** - *09:52:40*

Anyone using thesports data feed for football comfortable sharing how much they are billed for access to their services? and what specs of their plan is with regards to request allowance per hour/minute etc. [@U01C12ZEADQ](@U01C12ZEADQ) pointed me towards this as strong candidate for a robust score feed since bf inplay service is often unreliable. Pricing plans are not immediately available from their site or api documentation. Cheers!

*Tags: General Technical*

---

**PeterLe** - *13:14:12*

Afternoon, I noticed I had some exposure message yesterday. I have the exposure setting at account level (ie via my betfair account) and also in tradingcontrols. Is there anywhere else I would need to set this value in Flumine? Thanks. (by the way; if you breach the exposure value in tradingcontrols, is the message you receive the same as if you exceed the exposure level set at account level? (sorry for simple question :grinning:) Thanks

*Tags: Strategies*

---

**PeterLe** - *18:01:51*

Hi Liam, This is hte message (which i can now see is coming from flumine) : "Violation", "violation_msg": "Order has violated: STRATEGY_EXPOSURE Error: Potential selection exposure (32.00) is greater than strategy.max_selection_exposure (30)"

*Tags: Errors Debugging, Strategies*

---

**liam** - *18:07:05*

How are you initiating the strategy, looks like you have max_selection_exposure set to 30

*Tags: Strategies*

---

## 2022-07-03

**Trex44** - *09:15:05*

Hey all. Is there anywhere where I can get historical data on Runners that would link to Betfair id's.  I have a lot of race data recorded from Betfair but didn't record the runners actual names or finishing positions at the same time.

*Tags: Data Quality*

---

## 2022-07-04

**Mo** - *10:26:51*

Just to be a bit less mysterious about it, [@U03MQ71U2AG](@U03MQ71U2AG) has recruited me for roles before and I've used him to recruit members for my team. He knows pretty much everyone in the industry and can help not just with recruitment but introductions to data providers, facilitating joint ventures, etc. etc. It's fair to say he's played a significant role in the success of my career

*Tags: General Technical*

---

## 2022-07-05

**Phil Anderson** - *11:38:12*

Hi all. I'm having problems with market_start_time filter. I've used this same filter with football and it works fine:



```comp_filter = filters.market_filter(

                                    event_type_ids=[7],

                                    venues=[race_edit],

                                    market_countries=['GB', 'IE'],

                                    market_start_time={

                                        "from": current_time2.format(datetime.date.today()),

                                        "to": "{0}T14:45:00Z".format(datetime.date.today())

                                       }```

But for horse racing it's showing races after 14:45. I was actually looking to find the exact time (i.e. market_start_time=["14:45"] but it didnt like that so was just going to put "from2 and "to" as race start time, but realised the cutoff time didn't seem to have any effect.



In essence my questions are:

1. Is there something different with horse racing why my cutoff time doesnt work?

2. How do I find a specific race?

I can cycle through the races and match the time but I'm sure there must be a simpler way to do it.



EDIT:



OK so I've actually got it working. After the original filter i've put:



```results = trading.betting.list_events(

    filter=comp_filter

)



for i in results:

    print(i.event.id, i.event.name)

    market_types_filter = betfairlightweight.filters.market_filter(event_ids=[i.event.id],

                                                                   market_type_codes=["WIN"],                                                                           

                                                                   market_start_time={

                                        "from": "{0}T14:45:00Z".format(datetime.date.today()),

                                        "to": "{0}T14:45:01Z".format(datetime.date.today())

                                       }

                                                                   )```

Now this works fine, but I'd still like a way of putting in the start time if possible. something like:



```market_start_time = "{0}T14:45:00Z".format(datetime.date.today())```

Whatever way I try it I get highlighted as an error. Does the market_start_time ave to have a "from" or a "to". And if so is there another way to do this?



Thanks in advance for any help.

*Tags: Errors Debugging, Strategies*

---

**D** - *12:01:50*

If you can see past all my formatting errors :grinning:

*Tags: Errors Debugging*

---

**Phil Anderson** - *12:21:04*

[@URMM9463X](@URMM9463X) Thanks. I've got something similar now and works fine. was just wondering if there was a way I could specify the particular race e.g. 14:45, without having to do "from" and "to". Not a big problem, just anted it a bit neater I guess.



Actually stuck on runner_name now. Trying to find a way to search for specific horse (e.g. using filter) without having to cycle through all runners.

*Tags: General Technical*

---

**Unknown** - *14:07:56*

Afternoon, Vey basic question inbound :grinning:.

I usually start/restart my stuff each..Does Flumine automatically add new markets as and when the become avail (or do you have to specify t somewhere)?

The reason i ask; is it realistic to spin up a micro instance and run Flumin for weeks and months at a time?

It would be useful for times when you dont have internet access for days at a time (ie Cruises etc)

Thanks

*Tags: General Technical*

---

**PeterLe** - *14:09:10*

How many instances of Flumine could you run on a micro instance? Thanks Liam

*Tags: General Technical*

---

**liam** - *14:09:44*

Strategy/market/count dependant, personally I want a CPU per instance 

*Tags: Strategies*

---

**mandelbot** - *14:13:31*

FYI you can put more than one strategy into an instance

*Tags: Strategies*

---

**Jonjonjon** - *14:30:34*

I run over 150 Flumine instances on 2 cores, split across 5 Python scripts. It's OK. Uses 20% to 70% CPU most of the time

*Tags: General Technical*

---

**Aaron Smith** - *14:35:23*

150 instances? I usually put a few strategies in each instance, but you d be having close to 1k strats running at a time if that was the case? Do you guys make 1 instance per strategy? Also how is so many strategies even possible in regards to subscription limits??

*Tags: Strategies*

---

**Jonjonjon** - *14:40:37*

Within a Python instance, they use the same connection for market data. Mostly, they use the same Strategy subclass, but with different parameters.

*Tags: Strategies*

---

**PeterLe** - *14:59:51*

[@UPMUFSGCR](@UPMUFSGCR) on AWS? Micro?

*Tags: Deployment*

---

**Aaron Smith** - *15:01:16*

[@UPMUFSGCR](@UPMUFSGCR) why make a new flumine-instance for each instead of chunking all those who use the same strat subclass in one instance?

*Tags: General Technical*

---

**PeterLe** - *15:04:46*

Thanks [@U010GM77S4W](@U010GM77S4W) still learning Python but will do in the future :+1:

*Tags: General Technical*

---

**Alessio** - *15:09:55*

exceptions, I'd say :wink:

*Tags: Errors Debugging*

---

**mandelbot** - *15:20:54*

[@UQL0QDEKA](@UQL0QDEKA) just use `framework.add_strategy(strategy)` to add more strategies to a framework

*Tags: Strategies*

---

**mandelbot** - *15:28:26*

for example like this



```import logging

import betfairlightweight

from pythonjsonlogger import jsonlogger



from flumine import Flumine, clients

from flumine.streams.datastream import DataStream

from Strategy1 import Strategy1

from Strategy2 import Strategy2





logger = logging.getLogger()



custom_format = "%(asctime) %(levelname) %(message)"

log_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_handler.setFormatter(formatter)

logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



trading = betfairlightweight.APIClient('username', 'password', app_key='app_key', 

					certs='/path/to/certs')

trading.login()



client = clients.BetfairClient(trading)



framework = Flumine(client=client)



markets1 = betfairlightweight.filters.streaming_market_filter(

           event_type_ids=["7"],

           country_codes=["GB", "IE"],

           market_types=["WIN"]

)



markets2 = betfairlightweight.filters.streaming_market_filter(

           event_type_ids=["4339"],

           country_codes=["GB"],

           market_types=["WIN"]

)





strategy1 = Strategy1(

    name="Strategy1",

    market_filter=markets1,

    max_order_exposure=50000000,

    max_selection_exposure=6000000,

    max_trade_count=200,

    max_live_trade_count=1,

    context={"stake": 2},

)



strategy2 = Strategy2(

    name="Strategy2",

    market_filter=markets2,

    max_order_exposure=50000000,

    max_selection_exposure=6000000,

    max_trade_count=2000,

    max_live_trade_count=2,

    context={"stake": 2},

)



framework.add_strategy(strategy)

framework.add_strategy(strategy2)



framework.run()```

*Tags: Deployment, Strategies*

---

**Jonjonjon** - *15:33:03*

They just use different parameters. I use it to help work out what is working and what isn't.

*Tags: General Technical*

---

**Jonjonjon** - *15:34:07*

I'm on Digital Ocean as AWS confuses me.

*Tags: Deployment*

---

**Jonjonjon** - *15:36:05*

Good point. I probably confused myself when writing that. I think each Python script has 1 instance of a Flumine framework. Then I have a spreadsheet of parameters, and the script uses the spreadsheet to construct the strategy instances that it adds to the framework. I don't think it's elegant but I'm too old to keep up with best practices.

*Tags: Strategies*

---

**D C** - *15:43:59*

I used to do a similar thing but nowhere near on that scale. I gave it up as I found that I just couldn't separate strategies from the point of view of mutual interference. How do you get around that or are you spread across so many sports that you don't have to worry about inter strategy competition?

*Tags: Strategies*

---

## 2022-07-08

**Trex44** - *17:23:26*

As we are doing AWS questions - I currently copy market files from S3 on to my EC2 instance where I back test them. I wondered is there a way to stream direct from S3 without copying the file over? I don't have any problems with the way I currently do it but just wondered if there was a better practice/way it should be done.

*Tags: Deployment*

---

**Mo** - *17:26:12*

With `smart_open` package in Python you can read directly from an S3 URL

*Tags: General Technical*

---

## 2022-07-09

**D C** - *11:15:31*

I'm pretty new to AWS and I just SCP my files to a local store at the end of each day. I looked as S3 but I just saw it as extra costs. What do the people who use S3 consider to be the main upside?

*Tags: Getting Started, Deployment*

---

**D** - *12:39:05*

S3 is very cheap and as Mo says, you don't have to worry about hard drive failure. I also like that if I store csv/parquet/json files on S3 I can query them directly with SQL using Athena - no need to copy locally or into a database.

*Tags: General Technical*

---

## 2022-07-11

**liam** - *10:59:57*

code [https://github.com/betcode-org/flumine/blob/b33d82b75175106b2de0d0c4f9851599b085e389/flumine/streams/orderstream.py#L65|here](https://github.com/betcode-org/flumine/blob/b33d82b75175106b2de0d0c4f9851599b085e389/flumine/streams/orderstream.py#L65|here)

*Tags: General Technical*

---

**Fab** - *19:48:55*

```greyhound_strategy = GreyhoundStrategy(

    market_filter = streaming_market_filter(

        event_type_ids = ["4339"],

        market_types = ["WIN"],

    )

)



football_strategy = FootballStrategy(

    market_filter = streaming_market_filter(

        event_type_ids = ["1"],

        market_types = ["MATCH_ODDS"],

    )

)



cricket_strategy = CricketStrategy(

    market_filter = streaming_market_filter(

        event_type_ids = ["4"],

        market_types = ["MATCH_ODDS"],

    )

)



framework = Flumine(client=client)

framework.add_strategy(greyhound_strategy)

framework.add_strategy(football_strategy)

framework.add_strategy(cricket_strategy)

framework.run()```

May I ask for some clarification regarding the above Flumine scenario.



• A) Betfair adds new greyhound markets every 24H. My understanding is that the above code would receive automatically start receiving streaming messages for new markets. Correct?

• B) Would running simple strategies for 3 different sports be too tasking on a cloud machine with 1 CPU and 2GB RAM?

• C) Is there a `streaming_market_filter` combo to auto-subscribes to only football matches starting in the next 2 hours? (instead of all matches like in the example above)

*Tags: Strategies*

---

**Fab** - *20:08:07*

Thanks Liam. One more question: would the above code risk running into trouble with Betfair about subscribing to too many markets?

*Tags: General Technical*

---

**liam** - *20:17:31*

Potentially, run it and find out, you will get errors 

*Tags: Errors Debugging*

---

## 2022-07-12

**WaftyCrancker** - *14:10:46*

Hi, I am currently learning python etc. Ideally I would like to start collecting data as soon as I am able, as obviously you need a fare amount so it would be good if I can collect data whilst continuing to learn so it's available when I'm ready. I've looked at Github but unfortunately it means very little to me. Could someone point me in the right direction please? I see a lot of files and folder but I'm unsure what to do with them or even what to look into to start using them! Thank you.

*Tags: General Technical*

---

**Mo** - *14:30:00*

This is what you need to run to record data: [https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py)



However if it looks like Greek to you then you probably need to progress your Python learning before doing anything

*Tags: General Technical*

---

**PeterLe** - *17:23:48*

Just to add...

This is a great course for a beginner:

[https://www.udemy.com/course/the-modern-python3-bootcamp/](https://www.udemy.com/course/the-modern-python3-bootcamp/)



DONT pay more than £20 for the course, search for a coupon..

*Tags: Getting Started*

---

**WaftyCrancker** - *17:57:32*

I actually have a Udemy subscription, it's what I took my previous python course on, it wasn't the one you have posted, it was by Giles McMullen-Klein, so I may take a look at you suggestion as well. I also have PyCharm already installed.

*Tags: Getting Started*

---

**PeterLe** - *18:33:11*

Ah OK, Well if you have covered the basics then just have a go. There are some very helpful folk on here who will help when you get stuck as long as you have had a good go before asking. Good luck

*Tags: General Technical*

---

**WaftyCrancker** - *18:43:28*

Yeah I'm taking another look, going to see if I can figure it out with the help of Google and Udemy, so that recommended course may come in handy. Thank you.

*Tags: General Technical*

---

## 2022-07-13

**liam** - *11:13:16*

Anyone got an example market and corresponding scores data from the cricket stream? Required for an example in simulating in flumine :sunglasses:

*Tags: General Technical*

---

## 2022-07-14

**Rudeger Jamison** - *03:20:35*

Is there any smart way to trigger a stream from a strategy?



Eg I have some `model odds`  that I want updated every 5 seconds when a market is within 5 minutes of jump, but otherwise I dont care ( so dont want it to be pinging away when the markets are closed overnight )

*Tags: Strategies*

---

**Rudeger Jamison** - *06:47:39*

but you only have access to the stuff [https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py|here](https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py|here) in the strategy?



I would have to work out a way to find `time_to_next_market` or something like that?



That would have to be done with bflw and the market catalogue, yeah ?

*Tags: Strategies*

---

**Rudeger Jamison** - *06:55:37*

now thinking about it, calling bflw isnt a very nice solution after you run into api limit issues

*Tags: General Technical*

---

**liam** - *08:58:13*

What is wrong with something like this:



```class MyStream(BaseStream):



    @retry(wait=RETRY_WAIT)

    def run(self) -&gt; None:

        while True:

            markets_to_poll = []

            for market in self.flumine.markets:

                if market.seconds_to_start &lt; 600:

                    markets_to_poll.append(market.market_id)

            

            # make API request with marketIds

            

            time.sleep(5)





framework.streams.add_custom_stream(

    MyStream(framework, custom=True)

)```

*Tags: General Technical*

---

**Rudeger Jamison** - *09:02:05*

ahh ok, I am doing things differently.



My current implementation is



```class DatabaseStream(BaseStream):

    """

    This stream reads from a database and updates the context with the data required

    """



    def __init__(self, sql_path: str, context_key: str, table_name: str, strategy_name_filter: str, *args, **kwargs):

        BaseStream.__init__(self, *args, **kwargs)

        self.sql_path = sql_path

        self.context_key = context_key

        self.table_name = table_name

        self.strategy_name_filter = strategy_name_filter



    def run(self) -&gt; None:

        logger.warning(

            "Starting Stream: {0}".format(self.__class__.__name__),

            extra={"update_interval": self.streaming_timeout, "context_key": self.context_key, "sql_path": self.sql_path},

        )



        while self.is_alive():

            response = self.get_sql(sql_path=self.sql_path, table_name=self.table_name)



            def callback(framework, event):

                for strategy in framework.strategies:

                    if self.strategy_name_filter in strategy._name:

                        if strategy.context:

                            strategy.context.update({self.context_key: event.event})

                        else:

                            strategy.context = {self.context_key: event.event}



            event = CustomEvent(response, callback)

            self.flumine.handler_queue.put(event)

            time.sleep(self.streaming_timeout)  # this is the time to check again```

*Tags: Deployment, Strategies*

---

**Rudeger Jamison** - *09:02:52*

I guess I could loop over the markets in `self.flumine.markets` to determine if I need to make the api call first?

*Tags: General Technical*

---

**Rudeger Jamison** - *09:14:53*

```            next_race_start = min([market.seconds_to_start for market in self.flumine.markets] or [900])```

*Tags: General Technical*

---

**liam** - *09:36:53*

[https://github.com/betcode-org/flumine/blob/master/HISTORY.rst#221-2022-07-14|flumine 2.2.1](https://github.com/betcode-org/flumine/blob/master/HISTORY.rst#221-2022-07-14|flumine 2.2.1) released, few bug fixes but also added `SimulatedSportsDataMiddleware` this allows the ability to process betfair sports data (cricket or race) chronologically through the simulation engine with a single line:



```framework.add_market_middleware(

    SimulatedSportsDataMiddleware("cricketSubscription", "res/sportsdata")

)```

The [https://github.com/betcode-org/flumine/blob/master/flumine/markets/middleware.py#L287|middleware](https://github.com/betcode-org/flumine/blob/master/flumine/markets/middleware.py#L287|middleware) will pick up the file on the marketId and create a generator that is then called based on the publishTime of the market, this could easily be modified to simulate other historic events football/tennis/scores/custom events etc



Full example strategy [https://github.com/betcode-org/flumine/blob/master/examples/simulate-sportsdata.py#L22|here](https://github.com/betcode-org/flumine/blob/master/examples/simulate-sportsdata.py#L22|here) that places a back bet on the bowling team on a wicket :moneybag:

*Tags: Errors Debugging, Strategies*

---

**George** - *16:50:51*

I've noticed that the flumine simulator doesn't work if you try to feed in a zipped market file. This appears to be because the function `get_file_md` , in flumine.utils, contains an `open` function.

Is this deliberate?

*Tags: General Technical*

---

## 2022-07-16

**birchy** - *20:58:42*

I've been using python for years but have discovered a strange (to me) issue with copying lists in a Flumine strategy. So all of these create a REFERENCE to the same object, however I was expecting a new list/dereferenced copy:



`atbs = runner.ex.available_to_back[:]

atbs = runner.ex.available_to_back.copy()

atbs = [d for d in runner.ex.available_to_back]`



If I make changes to elements in `atbs`, they are also changed in the Flumine object  `runner.ex.available_to_back`. The only way to make a real copy is:



`atbs = deepcopy(runner.ex.available_to_back)`



I'm going to hazard a guess that this is due to the list being dict elements, so the shallow copy methods copy the dict references rather than making new ones? Python has probably always done this and I've just not noticed it before...

Just to clarify, I don't think this is an issue with Flumine.

*Tags: Strategies*

---

## 2022-07-17

**Dave** - *12:04:21*

your guess is correct - just check the src

• .copy() does a slice under the hood: [https://github.com/python/cpython/blob/main/Objects/listobject.c#L840](https://github.com/python/cpython/blob/main/Objects/listobject.c#L840)

• slice operator doesn't make copies of the objects when creating the new list ([https://github.com/python/cpython/blob/main/Objects/listobject.c#L481](https://github.com/python/cpython/blob/main/Objects/listobject.c#L481))

generally it is the reason why shallow/deepcopy principle exists (i.e. shallow copying tends to be very common)

*Tags: General Technical*

---

**birchy** - *12:35:32*

Now I need to check my boilerplate functions to make sure I'm not accidentally changing anything in Flumine. :grinning: 

FFR this also works as expected:

`atbs = [d.copy() for d in runner.ex.available_to_back]`

*Tags: General Technical*

---

**Dave** - *13:07:32*

This reminds me, I wonder how much of a boost BFLW would get if it used SortedKeyList from [https://grantjenks.com/docs/sortedcontainers/sortedlist.html](https://grantjenks.com/docs/sortedcontainers/sortedlist.html) rather than a raw list of dicts in the Available class, which currently involves re-sorting when a new level enters the book.

*Tags: General Technical*

---

**Dave** - *13:09:04*

Maybe not much when the bottlenecks really are in network latency/in play delay/matching cycle itself.

*Tags: Performance*

---

**thambie1** - *20:10:21*

That specific change (though in a different language) is on my todo list to speed up my backtest. Doesn't matter much in production though

*Tags: Performance, Deployment*

---

## 2022-07-20

**Rudeger Jamison** - *08:23:48*

cant say enough how awesome you guys are and really appreciate the help

*Tags: General Technical*

---

**liam** - *08:40:45*

:+1: np, keep the questions coming, I am sure they are helping others as well

*Tags: General Technical*

---

**liam** - *11:04:09*

flumine will never 'kill' / 'cancel' a bet on your behalf

*Tags: General Technical*

---

**liam** - *11:07:23*

`place_reset_seconds` is seconds to wait after an order on a market/strategy/selection has reset, ie execution complete



`reset_seconds` is seconds to wait after an order has already been placed on a market/strategy/selection

*Tags: Strategies*

---

**PeterLe** - *11:11:07*

Yep I understand that flumine wont fill/kill.

I think this code takes care of that doesnt it?:

```def process_orders(self, market, orders):

    for order in orders:

        if order.status == OrderStatus.EXECUTABLE:

            if order.elapsed_seconds and order.elapsed_seconds &gt; 0.2:

                market.cancel_order(order)```

Either way, that explanation makes sense now Liam. Thanks

*Tags: General Technical*

---

**Fab** - *11:31:52*

Hello. Is it possible to stop the Flumine framework when it’s running, in a way that it gracefully closes all stream connections to Betfair?

*Tags: General Technical*

---

**Peter** - *15:16:56*

full example [https://github.com/betcode-org/flumine/blob/master/examples/workers/terminate.py|here](https://github.com/betcode-org/flumine/blob/master/examples/workers/terminate.py|here)

*Tags: General Technical*

---

**birchy** - *22:43:07*

I know this has been asked before, but a slack search didn't reveal anything, so I guess it's been lost to the archives. As a rough ballpark figure, using Flumine simulation with a basic horses inplay strategy that places ~100 bets per market, how long should it take to run per 1000 markets on a modern laptop? I currently have a somewhat dated ProBook with 8gb ram, SSD and (I think) an Intel i3 CPU and it takes around 10-15 minutes per 1000 markets. It's sufficient for general programming and development work but lacks when running more intensive stuff like simulations, so I'm wondering if it's time to upgrade?

*Tags: Strategies*

---

## 2022-07-21

**Jonjonjon** - *00:06:32*

It's hard to give exact figures as we all run different code. But when I upgraded my PC a couple of years ago, relative passmark scores (between my old and newer PC) were a reasonable indication of the relative performance for Flumine backtesting.



[https://www.cpubenchmark.net/high_end_cpus.html](https://www.cpubenchmark.net/high_end_cpus.html)

*Tags: Performance*

---

**Rudeger Jamison** - *11:30:31*

Shout out to the underrated fact that using the Flumine simulation is an unbelievably good way to iron out any possible bugs you have in your code. :white_check_mark:

*Tags: Errors Debugging*

---

**PeterLe** - *15:01:16*

Just on simulation; Although its early days for me using it properly. The results I am getting seem to be accurate in terms of confirming, whether of not a strategy is profitable overall, (and it performs inline with my understanding of the markets, eg I know from experience that if I increase X Variable too much, then my match rates and profits fall). The actual profits appear to be quite conservative. ie the actual profits seem less than what I find in reality.

Mainly I'm been using it to fine tune some existing strategies. I just wondered how others use it.

How instrumental has it been in finding new stuff and do you use it more of a verification tool and gauge rather than expect to find new strategies ?

*Tags: Strategies*

---

**EJono** - *15:57:30*

Question related to flumine. Im wanting to log market and if possible individual runner returns from "def process_closed_market(self, market, market_book):".

 Im currently looping through the market_book.runners and checking order.runner_status to determine if back or lay returns profits or losses, then finally summing the results. Im now looking at using the blotter to get runner exposures, but before progressing i thought i'd ask if there is something obvious i could be doing at this stage in this function. Something like market.profits or runner.profits

Thank you

*Tags: General Technical*

---

**liam** - *16:37:38*

You want to look at `order.profit` this [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|example](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|example) looks like it does what you want :wink:

*Tags: General Technical*

---

**liam** - *16:45:29*

No but only because I think it would probably create more problems, it's possible, not sure on its accuracy though 

*Tags: General Technical*

---

**birchy** - *16:52:10*

Yeah, I've thought many times about how it would work but it would end up as a messy available_to_... delta for every runner and strategy.

*Tags: Strategies*

---

**birchy** - *18:00:04*

[@U4H19D1D2](@U4H19D1D2) while on the subject of simulation, I recall someone posting on here months ago a full implementation in Rust (IIRC) and there was some discussion about pulling it into Betcode and/or Flumine as it was significantly faster? Did/is anything happening with that?

*Tags: General Technical*

---

## 2022-07-23

**PeterLe** - *10:25:09*

One thing I've found extremely useful is gaining a deeper understanding of the relationship of the key components and execution;  ie matching rates, stake, placement (amongst others) and the very delicate nature of those interactions.

the biggest eye opener for me is that one of the key ingrdients in a very long time running strategy (15 years) 'maybe' isnt that important as i thought it was. In fact Ive stripped it out and started to run a live account from yesterday for comparison.

It will be really useful to see how this perform live.

One of the things that stood out for me when reading Daniel Khanerman's work many years ago is Conjunction Fallacy. (Linda the bank teller). It really struck a chord with me in the realms of what we do. (Check it out of you're not aware). Seems obvious when you read it.

I've always believed that keeping things as simple as possible (occams razor) is a good basis for most things

...anyway on with the testing too much chatting  :grinning:

*Tags: Deployment, Strategies*

---

**Michael** - *20:38:35*

What cloud services do you guys use for running your bots and storing data? Was just thnking of a EC2 instance

*Tags: Deployment*

---

**D** - *22:15:47*

EC2 

*Tags: Deployment*

---

## 2022-07-24

**Johnny Boston** - *03:41:01*

Howdy, is there any way to access the Market class from an Order?



My use case is that I'm wanting to implement my own trading controls and variable exposure limits for a cross different markets. Eg. A small Monday race I'll have smaller exposures to a big weekend race.



I see that exposures are set at the strategy level, so I was hoping to control that in the trading controls, but I'm not having any luck obtaining the Market class from the Order object passes to those controls.



Thank you in advance for any help.

*Tags: Strategies*

---

**Mo** - *10:35:58*

I use EC2 but I've seen Lightsail mentioned a few times on here

*Tags: Deployment*

---

**PeterLe** - *10:54:00*

I use Lightsail, windows and Ubuntu. I didn't realise till Birchy mentioned it, that Lightsail is actually based on a EC2 (T Class?) but branded as Lightsail.  Lightsail is a fixed monthly cost and is much cheaper than If you choose an ‘EC2’

*Tags: Errors Debugging, Deployment*

---

**liam** - *11:04:44*

Have you had a look at the example controls? Fairly sure there is one that does a lookup for the market, `StrategyExposure` I think 

*Tags: Strategies*

---

**birchy** - *18:15:38*

I've read somewhere that Lightsail is based on EC2 T2 but that was on S.O. or somewhere like that. I chose it because it's a much simpler preconfigured VPS and that suited me when I first started using AWS.

*Tags: Deployment*

---

**InvestInHorses** - *23:22:06*

Question for `CORRECT_SCORE` markets on football, specifically Premier League:



The "runner" in the correct score market has `SelectionId` and `runnerName`:



`{RunnerDescription : SelectionId=1 : runnerName=0 - 0 : Handicap=0 : SortPriority=1 : Metadata=}`



Does anyone know if the `SelectionId` and `runnerName` always remain constant? i.e. if I know the selection the `SelectionId=1` will that always be `0 - 0` for every Premier League game?



This seems like a reasonable assumption to make, but I haven't parsed through the Historical Data, but if someone could confirm this for me would save me a bit of time. Or would you recommend parsing the `runnerName`? Also, does this data ever have  deviations e.g. instead of `0 - 0`  we can have `0 - 0.` i.e. notice the period, for example

*Tags: Data Quality*

---

## 2022-07-25

**Peter** - *15:19:18*

EC2

*Tags: Deployment*

---

**George** - *21:39:08*

A quick question about recording racing inplay/TPD market data using Flumine.

The Flumine docs say that the inplay recording should be done like this:

```strategy = MarketRecorder(market_filter=None, stream_class=RaceDataStream, context={...})```

and that's fair enough. But it means you need two strategies; one to capture the exchange market data and one for the inplay/TPD data.

*Tags: Strategies*

---

**George** - *21:40:06*

My question is: can it all be done using a single strategy? Simply by adding a `sports_data_filter = ['raceSubscription']`  option into the main MarketRecorder example which captures the exchange market data, and then using the `process_sports_data`  method to write to a file, in exactly the same way that `process_raw_data` does?

*Tags: Strategies*

---

## 2022-07-26

**Johnny Boston** - *07:31:49*

Hey - is there a way to find out the last traded price ( pre in play ) for a runner after the race without recording the entire race in Flumine?

*Tags: General Technical*

---

**Michael** - *10:53:46*

ATR data - strategy won't be that profitable but its moreso so I can bet and then stream data to build other models

*Tags: Strategies*

---

## 2022-07-27

**Michael** - *13:32:56*

I'm quite new to coding so struggling to read the docs for Flumine. If I'm creating the strategy outside of Flumine (i.e. I have marketID and I have runnerID and amount I'd like to bet) do I need to have a strategy class or can I just use Baseorder?

*Tags: Getting Started, Strategies*

---

**Newbie99** - *13:49:54*

You don't have to use Flumine, if you prefer a non-streaming approach, you can just use BFLW:



[https://github.com/betcode-org/betfair/blob/master/examples/exampletwo.py](https://github.com/betcode-org/betfair/blob/master/examples/exampletwo.py)



But you may find (as many of us here did), that actually streaming is a more viable way to go long term and then its worth using Flumine as a lot of the hard work is already done (not to say there isn't plenty more hard work in developing a strategy of course). This example may help:



[https://github.com/betcode-org/flumine/blob/master/examples/example-single.py](https://github.com/betcode-org/flumine/blob/master/examples/example-single.py)



You can also use BFLW for streaming, without using Flumine, but unless you have a very specific use case, on the assumption you're starting from scratch it probably makes sense to use Flumine, here is the BFLW streaming example anyway:



[https://github.com/betcode-org/betfair/blob/master/examples/examplestreaming.py](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaming.py)

*Tags: Strategies*

---

## 2022-07-28

**Michael** - *21:54:42*

Just thinking of a way to do this, I guess I can't do this in the strategy class, so would need to make a stream class and then take a input of the horse name and market name (venue / time) and then pull the ID from the stream and then send to the strategy class?

*Tags: Strategies*

---

## 2022-07-29

**WaftyCrancker** - *12:50:59*

Hi can anyone tell me how I go about getting a live key? for streaming. Thank you.

*Tags: Deployment*

---

## 2022-08-02

**Peter** - *16:24:50*

I think that [@UPMUFSGCR](@UPMUFSGCR) was thinking for blotter.market_exposure() which I wrote. But that won't give you what you're looking for. The blotter.get_exposures() method may help though as that will return what you could win and what your could lose for a given strategy/selection combination, but you'd need to interpret the results yourself and you would probably want to only sparingly it in process_market_book as it's a tad CPU-intensive.

*Tags: Strategies*

---

## 2022-08-04

**Mo** - *07:30:36*

Does anyone have any tips for organising backtest results? The output of my backtests is one file per event so each backtest occupies a single directory. I'm currently distinguishing between the different parameters used in each backtest in the directory name but this is quickly becoming unwieldy as I add more parameters and it's hardly the best way to encode this information. For example:



```.../backtests/results-new-min-roi-0.0-max-roi-120.0-max-price-1000-bankroll-10000-bet-latency-100ms-cooldown-20000ms-entire-race-with-tags```

I'm thinking of having some kind of "meta file" in each backtest directory that could contain these parameters and other useful information for example:



```{

    "git_commit": "00e8402a7395de0b91d24a2fa15953a9f3271dca",

    "number_of_events": 19214,

    "event_status_breakdown": {

        "SUCCESS": 12007,

        "ERROR": 43,

        "MISSING_DATA": 7167

    },

    "parameters": {

        "min_roi": 0.0,

        "max_roi": 120.0,

        ...

    },

    "profit": -143762.12,

    ...

}```

and/or possibly have a top-level index of the backtests, either in a flat file or SQLite database and then the directories can have uninformative names like "00000043" or "cc097b8048064a3acd969de313e52f9b3fd896c7"

*Tags: Errors Debugging, Performance*

---

**Alessio** - *09:34:56*

since i also do local backtesting, i went like this:

• add a json file with the git commit and the exact command line i used, plus the parameters in abstract form.

• add a line in a psql database which contains exactly the json file contents +  directory

• group backtests by "idea"", which are named by the directory

• still name the directory of each iteration of the tuning grid with a reasonablel fantasy name (edit: prefix), this is to avoid duplication and still have a way to remember which thing is which at a glance (so something like, big-horses-roi4)

psql or sqlite up to you, but psql was easy because i already had it and i can add json querying. the json file is because i may want to process things with the json file on another machine without having to port the psql over.

*Tags: Errors Debugging*

---

**Alessio** - *09:45:34*

i think it's a bit of short term vs long term: if i am working on something my mind is fresh, so directory names are cheap and easy enough and very fast to glance over. I don't expect it to be fully descriptive though, that's why the params file. however, i know myself and in 2 months i won't remember which other parameters i. changed or i kept fixed, that's why i save the full command line in the file

*Tags: Errors Debugging*

---

**D** - *10:16:24*

[@UBS7QANF3](@UBS7QANF3) I have a bias towards cloud/serverless over local for most things and find dynamo just really convenient. It might be the lazy option. SQL databases didn't work well for me, was difficult to keep adjusting the structure to match the direction of experiments and variety of parameters as things evolved. [@U01C12ZEADQ](@U01C12ZEADQ)’s point of using psql to store the json makes sense but then I'd have to run a postgres database. Interesting topic!

*Tags: Deployment*

---

**D** - *10:34:21*

[@U01C12ZEADQ](@U01C12ZEADQ) - agreed, and sqlite is great. Access from everywhere can be useful. For my similar but different use case, I needed to read and update params from one or more ec2.

*Tags: Deployment*

---

**Mo** - *11:20:12*

Minor point but I should add that I output all of this stuff to a directory in my Dropbox so it is technically available from anywhere although perhaps not quite as frictionless as AWS

*Tags: Deployment*

---

**John Foley** - *13:44:06*

Been thinking about this as well recently - a half-baked idea I had was to try using [https://github.com/mlflow/mlflow|mlflow](https://github.com/mlflow/mlflow|mlflow). It's designed for a very similar situation where you’re training hundreds or thousands of ML models, all with different parameters, and need a flexible way to attach performance metrics to each run. Definitely might be overkill though.. 

*Tags: Performance, Strategies*

---

## 2022-08-05

**Jonjonjon** - *21:40:41*

Following on from [@UBS7QANF3](@UBS7QANF3)’s question, does anyone use any Python libraries for generating reports from their backtest results?

*Tags: General Technical*

---

**thambie1** - *22:45:32*

That's the one place I use python. I use jupyter-lab, sqlite, and matplotlib

*Tags: General Technical*

---

## 2022-08-06

**Jonjonjon** - *21:22:23*

Do you think they know the general gist of your strategy?

*Tags: Strategies*

---

**Trex44** - *21:35:16*

AWS/computing question if any one has a sec - If I am data mining  using 3000 files on a Volume attached to my EC2 does anyone know why switching from an 8 cpu instance to a 32 cpu instance doesn't result in an decrease in the time taken to process the files? Its the same time per file. I am a bit perplexed, thought increased CPUs would result in faster processing time.

*Tags: Deployment*

---

**Trex44** - *22:25:24*

I don't know unfortunately. How would I check?



 Its my own python code not Flumine. It does what I want it to so that's great but its taking me 8 hours on a 8cpu machine to work through 3000 files. Code basically performs a bunch of calculations and extracts a few time stamps and relevant data for each runner in each race. I actually have no frame of refence to know if 8 hours is  good or bad. Only been coding on and off for two years and AWS for about a year on and off.

*Tags: Deployment*

---

## 2022-08-07

**liam** - *06:40:19*

Or just use flumine :wink:

*Tags: General Technical*

---

**Mo** - *07:25:17*

It sounds like you are only using 1 core so it doesn't matter how many vCPUs your instance has. You need to be using some form of multiprocessing. Ideally in conjunction with flumine

*Tags: General Technical*

---

**foxwood** - *08:45:50*

There's a nice multiproc example in the flumine performance docs but if you want to get in deep then [https://docs.python.org/3/library/multiprocessing.html](https://docs.python.org/3/library/multiprocessing.html)

*Tags: Performance*

---

**birchy** - *08:49:36*

Also, make sure your files are local rather than on S3 as pulling from S3 will add network latency.

*Tags: Performance*

---

**PeterLe** - *13:07:13*

Im using an MP600 PCIe, eitherway Im happy with the backtest speed, 6 mins to test that many is superfast , In the time I can make a brew, i can discover another losing strategy :grinning:

*Tags: Performance, Strategies*

---

**Peter** - *20:42:33*

Question for users of [@UBS7QANF3](@UBS7QANF3)’s betfairviz. Is there a way of getting runner names into the a dashboard when the file being examined is gzipped file generated by the marketrecorder. I have the corresponding market catalogue, but can't see a way to inject that data into betfairviz.

*Tags: General Technical*

---

## 2022-08-08

**Unknown** - *00:22:53*

Well I got it down to 25mins for 3000 files on an 8 vcpu instance. Did it with some code edits and use of the multiprocessing package which, as I understand it, is making use of all the cores by processing each file asynchronously. Couldn't reduce the time using the threading package. All in all a good learning experience! For anyone else new to multiprocessing using python I would recommend [https://www.youtube.com/watch?v=fKl2JW_qrso&amp;t=1215s|this video](https://www.youtube.com/watch?v=fKl2JW_qrso&amp;t=1215s|this video).

*Tags: Getting Started*

---

**Peter** - *10:39:55*

Threading wouldn't help you much, if at all, in this situation as it's still a single CPU process - it just works around any IO that would leave that CPU inactive. So you did right to go multiprocessing.

*Tags: General Technical*

---

**Alessio** - *11:21:24*

Threads could be scheduled on other cores also in python, no?

*Tags: General Technical*

---

**Jonjonjon** - *11:40:11*

Not really. Python threads are different to threads in other languages and don't generally use more than one CPU core or process.

*Tags: General Technical*

---

**Alessio** - *11:52:48*

but what I meant is that they actually get scheduled on other cores, the real problem is GIL, no?

*Tags: General Technical*

---

**Mo** - *12:12:56*

Very happy to help - any feedback/requests for new features appreciated

*Tags: Feature Engineering*

---

## 2022-08-09

**LM** - *08:05:45*

Does using the flumine backtest functionality remove simulate liquidity being removed by each strategy being run. The reason i ask is that backttesting in flumine generally takes a while (even when multiprocessing is utilized) and i generally use it to help estimate liquidity in the market for a particular strategy (I know this isn't perfect but is indicative). My approach is usually to run multiple backtests with the same strategy adjusting the stake. I'm wondering if instead I can run a backtest once using multiple different staking strategies and get the same results or if these strategies would compete for the available liquidity?

*Tags: Strategies*

---

**liam** - *08:13:46*

What is a while? Have you processed your code?



Flumine will not remove liquidity when taking prices but will you can prevent double counting of passive with this config [https://betcode-org.github.io/flumine/advanced/#simulated_strategy_isolation|setting](https://betcode-org.github.io/flumine/advanced/#simulated_strategy_isolation|setting). Default is True which is what you probably want.

*Tags: Strategies*

---

**liam** - *08:14:54*

But to answer your question yes to the latter, run one simulation with multiple strategies as its the processing the data that takes a while

*Tags: General Technical*

---

## 2022-08-11

**liam** - *07:50:15*

set `streaming_timeout` to a value on the `Strategy` for example this would send a marketBook through to `process_market_book` every update and every 10s regardless of an update or not



```strategy = ExampleStrategy(

    market_filter=streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB"],

        market_types=["WIN"],

    ),

    streaming_timeout=10

)```

*Tags: Strategies*

---

**James** - *10:17:49*

I am trying to create a dataframe containing Teams/Event Names, Event IDs, Competition Names and Competition IDs. I've managed to create one dataframe containing Teams/Event Names and Event IDs and another containing Competition Names and Competition IDs. How can I match up Competition Names/IDs with the Event IDs? Any help would be appreciated!

Here's the code:

```    # Create competition filter

    competition_filter = betfairlightweight.filters.market_filter(

        event_type_ids=[1],

        market_start_time={

            'to': (datetime.utcnow() + timedelta(days=7)).strftime("%Y-%m-%dT%TZ")

        })



    # Get a list of competitions

    competitions = trading.betting.list_competitions(

        filter=competition_filter

    )



    # Create competition dataframe

    competition_df = pd.DataFrame({

        'Competition': [competition_object.competition.name for competition_object in competitions],

        'ID': [competition_object.competition.id for competition_object in competitions]

    })

    print(competition_df)



    comp_id_list = competition_df['ID'].tolist()

    print(comp_id_list)



    # Create event filter

    event_filter = betfairlightweight.filters.market_filter(

        event_type_ids=[1],

        competition_ids=comp_id_list,

        market_start_time={

            'to': (datetime.utcnow() + timedelta(days=7)).strftime("%Y-%m-%dT%TZ")

        }

    )



    upcoming_events = trading.betting.list_events(

        filter=event_filter

    )



    # Create event dataframe

    events_df = pd.DataFrame({

        'Teams': [event_object.event.name for event_object in upcoming_events],

        'Event ID': [event_object.event.id for event_object in upcoming_events],

    })

    print(events_df)```

*Tags: Feature Engineering, Strategies*

---

**Dario Scardina** - *17:30:52*

Hello, how can I get the runner name of a selection in flumine? Let's say I want to place orders only to Under selections

```strategy = ExampleStrategy(

    name="BackUnderSH",

    market_filter=streaming_market_filter(

        event_type_ids=["1"],

        event_ids=["31653002"],

        market_types=["OVER_UNDER_05", "OVER_UNDER_15", "OVER_UNDER_25", "OVER_UNDER_35", "OVER_UNDER_45",

                           "OVER_UNDER_55", "OVER_UNDER_65"]

    )

)```

I was trying then to skip Over selections in

```def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:

    # process marketBook object

    for runner in market_book.runners:

        skip_runner = False

        for runner_info in market.market_catalogue.runners:

            logger.debug("process_cleared_markets: %s" % runner_info.runner_name)

            if runner.selection_id == runner_info.selection_id:

                if "Ov" in runner_info.runner_name:

                    skip_runner = True

...```

But I've found out that market.market_catalogue is None.



Did somebody faced this problem?

*Tags: Errors Debugging, Strategies*

---

**Mo** - *17:36:25*

I'll let someone else address your underlying problem but FYI:



```selection_id,runner_name

47972,Under 2.5 Goals

47973,Over 2.5 Goals

1221385,Under 1.5 Goals

1221386,Over 1.5 Goals

1222344,Under 3.5 Goals

1222345,Over 3.5 Goals

1222346,Over 4.5 Goals

1222347,Under 4.5 Goals

1485567,Under 5.5 Goals

1485568,Over 5.5 Goals

1485572,Under 7.5 Goals

1485573,Over 7.5 Goals

1485596,Under 9.5 Goals

1485597,Over 9.5 Goals

2407528,Under 8.5 Goals

2407529,Over 8.5 Goals

2542448,Under 6.5 Goals

2542449,Over 6.5 Goals

5851482,Under 0.5 Goals

5851483,Over 0.5 Goals```



*Tags: General Technical*

---

## 2022-08-13

**Dennis** - *09:50:24*

Hi, sorry for the dumb question but how exactly do I log my orders using the LoggingControl class? I am calling the `_process_cleared_orders` function but how exactly do I access the data in the `event` object to write out to csv?

*Tags: General Technical*

---

## 2022-08-15

**Andy Bason** - *08:48:46*

Hi guys, I've been trying things out with betfairlightweight for a while. I can get prices, place bets, etc but getting prices/lines for Asian Handicap markets has stumped me. I've been searching round for guidance and found [https://support.developer.betfair.com/hc/en-us/articles/360016170431-How-do-I-place-bets-on-handicap-markets-|this](https://support.developer.betfair.com/hc/en-us/articles/360016170431-How-do-I-place-bets-on-handicap-markets-|this) but I'm struggling to implement it with BFLW. Does anyone know of any examples or can someone point me in the right direction? Thanks

*Tags: General Technical*

---

**Andy Bason** - *08:58:30*

Hi Mo, I can filter events to find matches with AH markets using `matFilter = betfairlightweight.filters.market_filter(event_type_ids=[1], market_start_time={'to': (datetime.utcnow() + timedelta(hours=1)).strftime("%Y-%m-%dT%TZ")}, market_betting_types=["ASIAN_HANDICAP_DOUBLE_LINE"])` but then I'm not able to get the prices/lines for a specific event.

*Tags: Strategies*

---

**Andy Bason** - *09:13:28*

I select the event ID then filter using

`markCatFilt = betfairlightweight.filters.market_filter(event_ids=[event_id])`

`markCats = trading.betting.list_market_catalogue(filter=markCatFilt, max_results='100', sort='FIRST_TO_START')`

And this is where I get stuck

*Tags: Strategies*

---

**foxwood** - *09:55:39*

"should" be ok since simulation would be using zipped files and recording raw files. But ! - if you are continually scanning the directory for new files you might hit a sharing problem at the instant the recorder transforms the raw file to a zipped one depending on how file attributes/properties are being handled. One scan to find all zipped files and then backtest should be ok though imho.

*Tags: General Technical*

---

**PeterLe** - *10:06:50*

Thanks Foxwood. Ive been dragging the files down from the virtual server for backtesting locally but I may as well just add some extra disk space on AWS. its not that expensive in the grand scheme of things.

Thanks for the reply :+1:

*Tags: Deployment*

---

**Guy Incognito** - *10:34:51*

Hi, I'm trying to get the match_date in Flumine when looking at previously matched bets, but it's returning an empty list to me even though runner.traded_volume does return a dictionary of values. I'm looking to get the match date for some of the trades, does anyone know how I can subscribe to this data?

*Tags: General Technical*

---

**Unknown** - *15:55:45*

Hi, When recording with Flumine, I also notice the small JSON files like in the image; are these of any use or can they be deleted? Thanks

*Tags: General Technical*

---

**PeterLe** - *16:31:33*

OK, first few lines look like the below text..So i can see that it contains all the market information...but is something I dont need to run backtests on..if Flumine is recording it, I guess that it should be retained then :grinning:



{"marketId": "1.201121593", "marketName": "6f Hcap", "marketStartTime": "2022-07-18T15:10:00.000Z", "description": {"persistenceEnabled": true, "bspMarket": true, "marketTime": "2022-07-18T15:10:00.000Z", "suspendTime": "2022-07-18T15:10:00.000Z", "bettingType": "ODDS", "turnInPlayEnabled": true, "marketType": "WIN", "regulator": "GIBRALTAR REGULATOR", "marketBaseRate": 5.0, "discountAllowed": true, "wallet": "UK wallet", "rules": "&lt;br&gt;&lt;a href=\"[https://www.timeform.com/horse-racing/](https://www.timeform.com/horse-racing/)\" target=\"_blank\"&gt;&lt;img src=\" [http://content-cache.betfair.com/images/en_GB/mr_fr.gif](http://content-cache.betfair.com/images/en_GB/mr_fr.gif)\" title=\u201dForm/ Results\u201d border=\"0\"&gt;&lt;/a&gt;\n&lt;br&gt;&lt;br&gt;&lt;b&gt;MARKET INFORMATION&lt;/b&gt;&lt;br&gt;&lt;br&gt;This market information applies to Exchange Singles bets only. Please

*Tags: Strategies*

---

## 2022-08-17

**Peter** - *15:09:30*

Anybody else having problems accessing Betfair historic data? I've tried multiple routes without success, but their customer service seem to think that it's fine (or at least they haven't had reports of any problems - though I though that was what i was giving them :face_with_raised_eyebrow:).

*Tags: General Technical*

---

**Peter** - *15:17:57*

Can't access it at all. The webpage is a Cloudflare 522 error page saying that it although it can connect to the [http://historicdata.betfair.com|historicdata.betfair.com](http://historicdata.betfair.com|historicdata.betfair.com) server, requests aren't being completed i.e. they're timing out.

*Tags: Errors Debugging, Deployment*

---

**thambie1** - *15:20:57*

Yeah... that's definitely not normal. I'm getting the same error page. I'm guessing you'll have more luck reporting it to Betfair Developer Support

*Tags: Errors Debugging*

---

**birchy** - *23:23:11*

I'm finally going to get my hands on a MacBook M1 in the near future (daughter has one that she rarely uses and has agreed to add an account for me) but as a Linux and Android user, I'm a complete noob to all things Apple. Have just had a quick Google for "MacBook M1 python development" and it looks to be a minefield of recommendations for homebrew, emulators and native installations.

This thing will basically be used exclusively for Flumine development and simulations, so my basic requirements would be:

• Python 3.10+

• PyCharm

• Git/GitHub

• Jupyter

• SSH

Any advice or links on how to achieve this setup in an uncomplicated way, things to avoid/watch out for, recommendations, etc?

I'm assuming that once python is installed, all other dependencies and libraries can be installed using `pip` in the usual way?

*Tags: Getting Started*

---

**river_shah** - *23:30:41*

That sounds like no sweat. I use an M1. There can be some hiccups for aarch64 support (especially if you are messing around with cython) but overall for python only development, you'll have no trouble.

*Tags: General Technical*

---

**river_shah** - *23:31:54*

The typical numpy, pandas, sklearn stack works flawlessly 

*Tags: Feature Engineering*

---

**river_shah** - *23:34:15*

There is a system python but I think it defaults to 3.7. You can just download 3.10 and it does have an M1 optimised installer (you won't need to deal with building it etc)

*Tags: Getting Started*

---

## 2022-08-18

**Akwera Junior** - *08:26:06*

I have a project that I would like anyone to help me complete building.

*Tags: General Technical*

---

## 2022-08-21

**Trex44** - *13:30:00*

Does flumine apply reduction factors automatically when running back tests?

*Tags: General Technical*

---

**birchy** - *19:59:36*

Yeah, so each runner has a predetermined RF. If a runner is removed then it's RF is applied to all the OTHER runners in order to correct the book to ~100%. A new market is then formed with the remaining runners, so the RF is only applied to all bets that were matched on the "old" market. There is a market version number in the API response that will indicate that the market has changed. In Flumine it is `market_book.version`.

*Tags: General Technical*

---

## 2022-08-22

**mandelbot** - *09:21:31*

Is this still relevant?

```Installing betfairlightweight[speed] will have a big impact on processing speed due to the inclusion of C and Rust libraries for datetime and json decoding.```

*Tags: Getting Started, Performance*

---

**Jonjonjon** - *09:56:00*

Hi Liam, please could you take another look at my PR regarding the NR adjustments?



[https://github.com/betcode-org/flumine/pull/591|https://github.com/betcode-org/flumine/pull/591](https://github.com/betcode-org/flumine/pull/591|https://github.com/betcode-org/flumine/pull/591)

*Tags: General Technical*

---

**birchy** - *10:18:11*

[@U4H19D1D2](@U4H19D1D2) Should we use that with Flumine?

*Tags: General Technical*

---

**Mr West** - *13:19:50*

Quick question; Horse Racing, did there used to be a separate place market. Is there now just 2 place, 3 place &amp; 4 place …?

*Tags: General Technical*

---

**liam** - *13:24:21*

Not sure I understand your question but assuming you are looking at the site I think you are just seeing them display the "PLACE" market as per the number of places (i.e. its still just 'PLACE' in marketType)

*Tags: General Technical*

---

## 2022-08-23

**birchy** - *13:14:37*

So presumably, the `[speed]` install has issues on Windows? I've always used the default Flumine install as I (wrongly) thought that `[speed]` was not compatible as it's not the default version. I prefer Linux but occasionally develop on a Windows machine as it has work related software that is not available on other OS's. Is it worth installing `[speed]` on Windows, or is not incompatible/too much hassle?

*Tags: Getting Started, Performance*

---

## 2022-08-24

**Unknown** - *11:43:01*

:beach_with_umbrella::beach_with_umbrella::beach_with_umbrella::beach_with_umbrella::beach_with_umbrella::beach_with_umbrella:  [!here](!here) Hi guys, to celebrate our Matched Betting Pro soft launch, we're giving away these lovely pink beach balls!!



We want to share our balls with the BetCode community, so just send me a DM with your address and we'll send some balls your way!! :incoming_envelope:



Also, if you want to test the Matched Betting platform yourself to see what the fuss is about, make sure to join our waiting list here --&gt; [https://www.betconnect.com/matchedbettingpro](https://www.betconnect.com/matchedbettingpro)



Drop us any questions and comments you might have about Matched Betting Pro here in this chat, we'll be happy to answer all of them! :rocket:

*Tags: Strategies*

---

## 2022-08-25

**Aaron Smith** - *17:29:39*

what is the best way to get all markets a strategy has subscribed to in flumine? context: when starting flumine, i want the strategy to access the market.context for all its markets.

*Tags: Strategies*

---

**Aaron Smith** - *19:38:20*

havent used middleware before and trying to wrap my head around it - the way i understand it right now:

• flumine receivs its first market_book from a certain market

• BaseFlumine calls `_add_market` 

• for each middleware we added, `add_market`  is called.

so once on the initial market_book i receive, my middleware runs `add_market` , am i seeing this right?

*Tags: General Technical*

---

**Trex44** - *20:57:14*

Is there a way to speed up/optomise the downloading of files from S3 to EC2 does anyone know? A python library perhaps or some hardware tweak.

*Tags: Performance, Deployment*

---

**Peter** - *21:18:38*

Not really. The constraint is the size of the internal network pipe at AWS, which no external software can influence. Best package is almost certainly boto3, which I'm sure AWS will have optimised for speed in this area as it's in their interest too to keep these pipes clear.



So the only practical way to optimise the speed is to ensure that the S3 bucket and EC2 instance are in the same region.



However, I've never found the streaming speed from S3 to be a constraint. Profiling and streamlining my code is usually a much more productive use of my time.

*Tags: Performance, Deployment*

---

## 2022-08-26

**liam** - *09:14:50*

Its a bit complicated, a strategy is subscribed to streams, you can see the ids with `Strategy.stream_ids` or the streams themselves with `Strategy.streams`



We add the `stream_id` to the `MarketBook` which you can see with `MarketBook.streaming_unique_id` however there isn't anything in the Market object telling you this as it can be shared with multiple streams. This relates to a current bug where two streams could be updating a single Market due to a crossover in the filters

*Tags: Errors Debugging, Strategies*

---

**liam** - *10:52:19*

Any flumine users want to see what simulation is like with 100% matching on placement this code patches the place code:



```def _create_place_response(

    self,

    bet_id: Optional[int],

    status: str = "SUCCESS",

    order_status: str = None,

    error_code: str = None,

) -&gt; SimulatedPlaceResponse:

    if status == "SUCCESS" and self.size_remaining:

        self.matched.append([123, self.order.order_type.price, self.size_remaining])

        self.size_matched, self.average_price_matched = wap(self.matched)

    if order_status is None:

        if self.size_remaining == 0:

            order_status = "EXECUTION_COMPLETE"

        else:

            order_status = "EXECUTABLE"

    return SimulatedPlaceResponse(

        status=status,

        order_status=order_status,

        bet_id=str(bet_id),

        average_price_matched=self.average_price_matched,

        size_matched=self.size_matched,

        placed_date=datetime.datetime.utcnow(),

        error_code=error_code,

    )



SimulatedOrder._create_place_response = _create_place_response```

*Tags: Errors Debugging*

---

**liam** - *12:27:58*

Helps alleviate depression whilst simulating 

*Tags: General Technical*

---

**birchy** - *12:32:21*

I'm having problems with numpy in PyCharm on a MacBook. If I import numpy in a python console, it's fine, however in PyCharm it complains something about architecture being wrong. This on an M1. Does anyone have any advice on PyCharm/numpy installation? I tried uninstalling and installing via PyCharm but still no joy. Baffled TBH.

*Tags: Getting Started*

---

**Peter** - *13:04:58*

Don't use PyCharm, but it could be that when running in the console you're using the version of numpy that's native to the M1 architecture, but when running through PyCharm, it's expecting a version that's Intel-compatible.



I ran into a not dissimilar problem and documented it on [https://stackoverflow.com/questions/65942641/docker-image-built-on-mac-osx-wont-run-on-aws-ec2-instance/65952339#65952339|Stackoverflow](https://stackoverflow.com/questions/65942641/docker-image-built-on-mac-osx-wont-run-on-aws-ec2-instance/65952339#65952339|Stackoverflow).

*Tags: Deployment*

---

**liam** - *14:07:22*

I remember having similar issues, no idea how I fixed it tbh

*Tags: Errors Debugging*

---

**LF** - *14:42:34*

Anyway I think if you can actually install it and use it directly on your machine from the terminal  Pycharm is just pointing to the wrong python, my suggestion would be to have a fresh python installation either conda/mamba or use virtualenv and Pycharm should work

*Tags: Getting Started*

---

**LF** - *14:54:11*

conda/mamba are usually much painless when you want to install python scientific libraries

*Tags: Getting Started*

---

**liam** - *14:58:31*

this work? [https://github.com/betcode-org/flumine/pull/608/files](https://github.com/betcode-org/flumine/pull/608/files)

*Tags: General Technical*

---

## 2022-08-27

**birchy** - *10:13:57*

Numpy is now working in PyCharm, but not really sure what the fix was. So I'd installed python 3.10 apple silicone version and was using that for PyCharm and running my code in the terminal as it became the default for `python3`. I then removed 3.10 which left me with the default 3.8 version and now PyCharm &amp; numpy work as expected. Haven't looked into the details but I suspect the default mac version may be Intel compatible.

*Tags: Getting Started, Errors Debugging*

---

**Johnny Boston** - *21:03:05*

Further to this discussion, could I skip a market being added from the add_market func?



Say I wanted to exclude a specific set of market ids? Instead of checking in the strategies I want to avoid it being added to flumine.



How would I do that?

*Tags: General Technical*

---

**liam** - *21:10:01*

Don’t ask for it from betfair? No easier way than filtering at the strategy tbh, what’s the need?

*Tags: Strategies*

---

**Johnny Boston** - *21:12:12*

I own a couple of horses lol and if I lay them on betfair with my bots I will get in trouble. So I  filter them out at the strategy level, but would be good to just not have them at the strategy level. In case I forget to add the filter on a new strategy one day.

*Tags: Strategies*

---

## 2022-08-29

**Trex44** - *15:06:37*

Guys can any AWS users direct me to a video/doc that will show me how to connect to EC2 remotely and receive data from scripts executed on the EC2 environment back to an application running on  my local PC? keep running into issues trying to do this via boto3 and I figure its not the most secure way to go about it. Wondered if the sockets package was the correct route to go down?

*Tags: Deployment*

---

**Unknown** - *19:21:18*

It depends a lot on what you are doing and how you are doing it...



There are a lot of ways to connect to EC2.



One solution is to have the EC2 script write the data to an S3 bucket; you can then access the S3 bucket from your local PC.



For one-off downloads I use FileZilla to grab data from EC2.

*Tags: Deployment*

---

**Trex44** - *19:40:52*

I am building a GUI to sit on top of a script I have built for analysing data. Getting fed up of changing the filters in the code manually each time. However there is no way an EC2 environment can display a GUI. I want to adapt the script (which sits on my EC2 instance) to accept an input sent from the GUI on my local PC (the client), do the compute on the EC2 environment then return the output data back to my local PC where the GUI can make use of it.   I used WinSCP for one off downloads but that's too slow. I want to be able to move faster.

*Tags: Performance, Deployment*

---

**Trex44** - *19:42:41*

Still haven't figured out how to get a local python script to connect to my EC2 environment though even though I can easily get my IDE and WinSCP to do so.

*Tags: Deployment*

---

**Newbie99** - *19:50:32*

That is a simple script I use to SSH in via the Paramiko Python library. I'm not 100% sure if that will help you, but its potentially one way to consider

*Tags: General Technical*

---

**Trex44** - *20:14:45*

Thanks Newbie99. It might. I am currently exploring Paramiko having spent the day exploring boto3 and sockets but with no luck.



 I am essentially just building an app and deploying it to cloud then having a client log in. Thought there would be loads of demos of how to do this. If there is anyone here who has actually deployed an app to the cloud for commercial purposes before then would be great to hear how this is done.

*Tags: Deployment*

---

**Newbie99** - *21:10:33*

That's all you need to transfer files (tbh I haven't tried too much complex stuff, I just use this to grab files from AWS and copy them to my local machine)

*Tags: Deployment*

---

**Trex44** - *21:13:21*

I don't want to transfer files as such. Just data. So the idea is the GUI sends a JSON/DICT to EC2. EC2 executes a command based on the JSON/DICT then returns the ouput to the local host. I can make it work via file transfers if needed though.

*Tags: Deployment*

---

**Unknown** - *22:36:29*

[@U03N4QBJ0TV](@U03N4QBJ0TV) I think there are lots of demos of this kind of application (calculation on the server, view results on the local PC). The 2 example patterns that come to mind are:

1. Publish the output as a web page or web app on the EC2 server and view the results/graphics with a browser;

2. Publish the output to a database on AWS (or to S3) and have the local PC GUI query that.



*Tags: Deployment*

---

## 2022-08-30

**liam** - *08:11:06*

Sounds like you are trying to overcomplicate what should be an API backed by a database

*Tags: General Technical*

---

**Unknown** - *09:43:33*

Thought Id check a current strategy (although stakes a bit higher)..Shows the importance of matching rates.

I never really concentrated on matching rates much before the Slack group, but it has my attention now (red line profit) :grinning:

Thanks for pointing this feature out by the way, got me thinking...

*Tags: Feature Engineering, Strategies*

---

**Newbie99** - *09:48:42*

I second that, having played around with it this week, its great as it allows you to highlight where its execution that's letting you down, rather than the underlying strategy logic.

*Tags: Strategies*

---

**Guy Adini** - *09:57:43*

Hi guys, and apologies if this has been asked a million times before: I'm new and don't know where to find the FAQ :slightly_smiling_face:

Could someone please tell me where are the Betfair servers physically located, and where to put a server in order to get the lowest possible latency to them?

*Tags: Performance, Deployment*

---

**Mo** - *10:15:01*

Dublin. AWS Dublin

*Tags: Deployment*

---

**Peter** - *11:20:30*

Also, since when was an EC2 instance unable to display a GUI? Do you mean maybe that you can't run desktop apps on it?

*Tags: Deployment*

---

**Trex44** - *18:39:17*

Almost certainly am overcomplicating it. The files are currently CSV files that are the results from a load of computation done on different races. I am concatenating them into a single pandas data frame then I am basically just using the GUI to filter the results and display various Seaborn graphs.  I am not looking to keep the results permanently though so haven't considered Amazon RDS or anything like that. The CSV's are deleted eventually.



Just wanted the filtering to be done server side then a dictionary to be sent back to the client. Trying to teach my self how apps are built/function whilst doing this. That's half the fun.

*Tags: Feature Engineering, Deployment*

---

**Trex44** - *18:42:49*

I can't get Tkinter to work on EC2 [@U9JHLMZB4](@U9JHLMZB4). I read that Tkinter requires a display so wont work on EC2 as there is no display. That's what prompted me to consider building the display on the client side (where I have a screen) and having the host handle everything else.

*Tags: Deployment*

---

**liam** - *20:17:14*

Database -&gt; API -&gt; SPA 



This is the current go to on app development, no one understands tkinter 

*Tags: General Technical*

---

## 2022-08-31

**Jon K** - *09:45:10*

Hi all, I'm trying to place an SP bet but am getting Full Response: {'jsonrpc': '2.0', 'error': {'code': -32602, 'message': 'DSC-0018'}, 'id': 1} which is a mandatory param isn't being supplied yet I'm scratchign my head with what it could be.

This is what is being sent through

betfairlightweight.exceptions.APIError: SportsAPING/v1.0/placeOrders

Params: {'marketId': 11166579, 'instructions': [{'orderType': 'MARKET_ON_CLOSE', 'selectionId': '1.202730586', 'side': 'BACK', 'handicap': 0, 'marketOnCloseOrder': {'liability': 1.5}}]}

*Tags: Errors Debugging*

---

## 2022-09-01

**LM** - *12:12:45*

In the tennis example on Flumine ([https://github.com/betcode-org/flumine/blob/master/examples/tennisexample.py](https://github.com/betcode-org/flumine/blob/master/examples/tennisexample.py)) I can see that it refernces a score from the market context. Does anyone know if this score data is available in the historical pro data files from betfair?

*Tags: General Technical*

---

## 2022-09-03

**Jeff Waters** - *10:20:29*

Hi



I've copied my back testing code onto a second computer, and added the required libraries to the environment I created.



My code includes the following:



```from flumine import FlumineBacktest, clients```

Pycharm is complaining that it can't find reference FlumineBacktest in init.py.



Would I be best reverting to an earlier version of Flumine? Alternatively, can I change 'FlumineBacktest' to something else to get it to work?



Thanks



Jeff

*Tags: General Technical*

---

**mandelbot** - *10:32:06*

`from flumine import FlumineSimulation, clients`

*Tags: General Technical*

---

**Jeff Waters** - *20:35:43*

Just a follow up question.



If I were to place the price_log dictionary in a standalone module, because it's a resource that's shared by all the processes, would the data rendered by all of the processes end up there?

*Tags: General Technical*

---

**Mo** - *20:48:31*

Nope. Like I said it depends what you're trying to do. The concurrent package might help or it might not

*Tags: General Technical*

---

## 2022-09-05

**Mick** - *07:01:27*

From an earlier Q&amp;A I learned that the "BASIC" betfair historical data ([https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home)) could not be used to reconstruct the state of the back/lay ladders but you can do it with the "PRO" data. I was just wondering if you can reconstruct the ladder state with the "ADVANCED" data (albeit less frequently than with the pro).

*Tags: Data Quality*

---

**Mo** - *07:57:14*

I doubt it's only the single best. I would say it's either the top 3 because that is the default for listMarketBook/what is displayed on the website or the top 10 because that is the maximum for listMarketBook/streaming

*Tags: General Technical*

---

## 2022-09-06

**liam** - *08:32:00*

AUS automation team have just released these, should be helpful for those that are just starting out and want to learn how to use flumine, I would have killed to have something like this when I first started (note that I haven't reviewed)



[https://betfair-datascientists.github.io/api/How_to_Automate_1/|How to Automate I: Understanding Flumine](https://betfair-datascientists.github.io/api/How_to_Automate_1/|How to Automate I: Understanding Flumine)



[https://betfair-datascientists.github.io/api/How_to_Automate_2/|How to Automate II: Backing or laying the 1st/2nd/.../nth favourite shot](https://betfair-datascientists.github.io/api/How_to_Automate_2/|How to Automate II: Backing or laying the 1st/2nd/.../nth favourite shot)



[https://betfair-datascientists.github.io/api/How_to_Automate_3/|How to Automate III: Betfair Data Scientists Models](https://betfair-datascientists.github.io/api/How_to_Automate_3/|How to Automate III: Betfair Data Scientists Models)



[https://betfair-datascientists.github.io/api/How_to_Automate_4/|How to Automate IV: Automate your own Model](https://betfair-datascientists.github.io/api/How_to_Automate_4/|How to Automate IV: Automate your own Model)



[https://betfair-datascientists.github.io/api/How_to_Automate_5/|How to Automate 5](https://betfair-datascientists.github.io/api/How_to_Automate_5/|How to Automate 5)

*Tags: Strategies*

---

**Jimmy** - *11:41:09*

Hello! sorry in advance for what is I suspect a daft question. I’m trying to backtest a football strategy which has the criteria of waiting for a market to reform (60 seconds or X) after a suspension. However, trapping that time in a backtest is proving tricky - it never seems to go above a 1s or so. What am I missing?

*Tags: Strategies*

---

**Jimmy** - *11:44:40*

the backtest file is the one out of the project — and the code simply records the last suspension time and then checks it against the current time.



you can see the hacking going on here - I’d tidy this up once I’d resolved, honest :rolling_on_the_floor_laughing:



`suspend_time = stored_data["last_suspend_time"]`

`current_time = datetime.now()`

`diff = current_time - suspend_time`

`and then using: diff.total_seconds()`

*Tags: General Technical*

---

**Newbie99** - *12:15:59*

There was an issue with the way datetime is patched for flumine backtesting, so you may have to import differently.



So instead of:



from datetime import datetime (etc.)



you need to do:



import datetime



then



datetime.datetime.utcnow()



(and the same for any other datetime references)

*Tags: General Technical*

---

**Jimmy** - *12:18:00*

will try - question - given this is in the strategy, does it matter for live?

*Tags: Deployment, Strategies*

---

**Newbie99** - *12:28:04*

Shouldn't matter for live, but just for ease of use I keep everything consistent (the beauty of flumine for backtesting is that it allows you to do that)

*Tags: Deployment*

---

**Trex44** - *20:05:33*

Guys, can anyone who is familiar with the [https://historicdata.betfair.com/Betfair-Historical-Data-Feed-Specification.pdf|betfair historical data-feed-specs](https://historicdata.betfair.com/Betfair-Historical-Data-Feed-Specification.pdf|betfair historical data-feed-specs) explain to me what _*0 vol is remove*_ means. For `trd` for instance you would never reduce the trd value or remove it as I understand it as this number represents the value traded at a given price so should only ever increase.

*Tags: Data Quality*

---

## 2022-09-07

**George** - *11:35:36*

Hi guys, I'm looking at some PRO data and I would like to get a snapshot of the market at a particular point in time. Currently my approach is to load the data into a BFLW historical generator stream, and then check the timestamp of every single market book in the generator until I reach the one I'm interested in.

This is proving to be very slow (like 2 hours to run a month of data). I'm wondering whether there is a much smarter way to achieve what I am trying to do?

*Tags: Performance*

---

**George** - *11:42:18*

Not currently using lightweight mode I think. Would it help?

Just looking for standard book info like top price and size etc along with runner name and event name

*Tags: General Technical*

---

**George** - *11:48:05*

Will do.

Is there a way in python to subsample a generator? That would also work. But I'm guessing not.

*Tags: General Technical*

---

**liam** - *12:13:55*

Flumine does this with listener_kwargs 

*Tags: General Technical*

---

**Newbie99** - *12:28:42*

[https://betcode-org.github.io/flumine/quickstart/#listener-kwargs](https://betcode-org.github.io/flumine/quickstart/#listener-kwargs)

*Tags: General Technical*

---

**Newbie99** - *12:29:03*

(its not quite what you're after, but it does help)

*Tags: General Technical*

---

**foxwood** - *13:45:19*

I'm meaning that the strategy PL for the market is not known until you get to the final packet and process_closed_market is called - then the runner book has a status of winner/loser. So if you only put bets in say the morning market it's a lot of data to chunder through to get the final outcome. Having made me think about that though, the matching has got to be simulated and MOC catered for etc so I'm not sure the suggestion had any merit now :joy:

*Tags: Strategies*

---

**EJono** - *15:04:35*

qq Im just about to submit a rquest to have an account raised from the standard 200 market streaming limit to 1000 (or higher if possible). Is the correct way to move forward with this by contacting betfair through the developer support page, and submit a request at [https://support.developer.betfair.com/hc/en-us/requests/new](https://support.developer.betfair.com/hc/en-us/requests/new) for said increase? Thanks

*Tags: General Technical*

---

**foxwood** - *15:28:05*

[@U4H19D1D2](@U4H19D1D2) my base strategy has params like these which are checked for each packet in check_market_book() so can dip in quickly (eg) 5 hours before, place any bets and then ignore all data until market complete. But, since simulation has to check matching the only chance to skip to end would be once all bets were matched - all a bit too complex to bother with I now think.

```startAtSeconds = 18000    # seconds before official start to begin strategy

stopAtSeconds = 17995     # when to stop strategy ie start time - this : set -ve to carry on beyond official start```



*Tags: Strategies*

---

**Jimmy** - *19:48:45*

Hello. What's the most flumine-y way of checking across another market? Subscribed to market X and when something happens, check market Y for status/price/whatever. 



Couldn't see an example in the docs/search so thought I'd ask :)

*Tags: General Technical*

---

**liam** - *21:05:26*

[https://betcode-org.github.io/flumine/quickstart/#event-processing|docs](https://betcode-org.github.io/flumine/quickstart/#event-processing|docs)

*Tags: General Technical*

---

**Jimmy** - *21:10:15*

are they just added to the same list with the different path and then flumine works it out?

*Tags: General Technical*

---

**Jimmy** - *21:11:53*

wow, that’s pretty straightforward then. thanks for putting up with the questions :+1:

*Tags: General Technical*

---

## 2022-09-09

**Peter** - *20:14:17*

Threading probably won't help you with this. It allows the CPU to continue processing while the IO is taking place. But in this case you need the file to complete downloading before you can process it.



If you have multiple CPUs however, multi-processing might give you some improvement allowing you to be processing on other CPUs while one process is downloading.

*Tags: General Technical*

---

## 2022-09-12

**Jimmy** - *15:47:45*

Code structure question — I’d like to collect data on several football markets (Match, 05, 15, 25, CS) but if I do it in one instance, I hit a subscription limit and if I do it individually, I eventually hit connection issues. Any top tips?

*Tags: General Technical*

---

**Aaron Smith** - *16:01:04*

when backtesting with flumine, does runtime of your code effect the (simulated) latency?

*Tags: Performance*

---

**Aaron Smith** - *16:32:15*

not sure if i understand. The way i imagine it is

total_latency = signal_from_betfair_to_me + code_runetime + signal_from_me_to_betfair.

In a backtest, the time a signal between me and betfair takes can only be assumed, so you set some fixed value. But runtime of your code would be the same. Basically, if i added a time.sleep(1) in my process_market_book, this would not effect the result of the simulation ?

*Tags: Errors Debugging, Performance*

---

**liam** - *21:16:02*

The processing cost of an under25 market is so small you would struggle to register it. If you want the market available either ignore it or just create a dummy strategy that does nothing but subscribes 

*Tags: Strategies*

---

## 2022-09-13

**Peter** - *11:47:29*

Is there a way to access the raw order stream updates in Flumine? Market books have the streaming_update attribute. Is there an equivalent or alternative for orders?

*Tags: General Technical*

---

## 2022-09-14

**Dario Scardina** - *14:23:08*

I found answer in documentation:



`*Please note:* listMarketCatalogue does not return markets that are CLOSED.`

*Tags: General Technical*

---

**Dario Scardina** - *14:38:42*

Although not explicitly said in documentation... but also listCompetitions does give anything back for a closed market...

*Tags: General Technical*

---

**Mo** - *14:40:52*

My project [https://github.com/mberk/betfairmappings](https://github.com/mberk/betfairmappings) can help with this. There isn't currently a mapping from Betfair event ID to competition ID but I can add one

*Tags: General Technical*

---

**Mo** - *14:42:48*

To provide a bit more detail, I have an interactive mapping tool that shows me upcoming Betfair markets and third party events and then I hit some buttons to match things up. Then the new mappings get pushed to my database/Redis

*Tags: General Technical*

---

**Alessio** - *21:08:11*

40k means you have some historical data for both right? Then I would use something that is not names, like which days the played and against whom. That way you kinda get a graph, and you can do a first batch.

*Tags: Data Quality*

---

## 2022-09-15

**VT** - *04:20:43*

I use thefuzz library, it's simple to generate matches, totals and partials. As mentioned, using the date, and also the country, should help with correspondence.

*Tags: General Technical*

---

**Andy Bason** - *15:59:12*

Hi guys, I want to search for events using market_start_time. I thought the below would work but It throws this error: `Full Response: {'jsonrpc': '2.0', 'error': {'code': -32602, 'message': 'DSC-0018'}, 'id': 1}`

```        

game_time = '2022-09-15 14:49:41.209270'

game_filter = betfairlightweight.filters.market_filter(

            event_type_ids=[1],

            market_start_time={

                'from': (game_time - timedelta(hours=2)).strftime("%Y-%m-%dT%TZ"),

                'to': (game_time + timedelta(hours=2)).strftime("%Y-%m-%dT%TZ")

            },

        )```

Can anyone see what the problem is? Thanks

*Tags: Errors Debugging*

---

**Peter** - *17:31:47*

Is that the actual code you're running? Asking because it shouldn't get as far as the API call. Instead it should throw an error when you try to perform arithmetic on the game_time string and the timedelta object.

*Tags: Errors Debugging*

---

**Peter** - *19:49:11*

Putting those snippets together works, i.e. this works:

```start_time = "17 September 2022 11:00"

game_time = datetime.strptime(start_time, "%d %B %Y %H:%M")

game_filter = betfairlightweight.filters.market_filter(

    event_type_ids=[1],

    market_start_time={

        'from': (game_time - timedelta(hours=2)).strftime("%Y-%m-%dT%TZ"),

        'to': (game_time + timedelta(hours=2)).strftime("%Y-%m-%dT%TZ")

    },

)

games = trading.betting.list_events(

    filter=game_filter

)```

returning (currently) a list of 57 EventResult objects.



Looking at the specific error you're getting though, it's telling you that your request is missing a mandatory parameter. Since listEvents only has one mandatory parameter, the filter. This means that you're either not actually attaching it, or that the market filter method is returning None, so it's still feeling like we're not quite seeing the actual code that you're using.

*Tags: Errors Debugging, Strategies*

---

## 2022-09-16

**EJono** - *15:29:27*

Is there a problem with calling "trading.keep_alive()" too frequently? Would doing so too many times in a given minute cause betfair to suspend or restrict an account?

*Tags: Deployment, Strategies*

---

## 2022-09-19

**Tom** - *07:48:05*

Hi guys, I am new to coding- I've done about 120 hours of training in Python in a Udemy course, which was ok but decided to get into a project to maintaining the trajectory of interest and have a background in financial markets and thought I would test an idea I've had in Horse Racing pricing, so am building a Betfair bot.



Although I don't know enough to really understand it at depth, Liams work is pretty impressive and appreciate the sharing (and no doubt others in here contributed, it's pretty cool/impressive what some people can do with this stuff)



```Anyway, the question I had is the code below - for the part that says if runner_book.ex.available_to_back - is that returning a boolean that will progress to runner_book.ex.available_to_back[0].price (to avoid an error) and will instead give us the extreme result automatically (1.01, or 1000 for the lay options)?```

```best_back_prices = [runner_book.ex.available_to_back[0].price

    if runner_book.ex.available_to_back

    else 1.01

    for runner_book

    in runner_books]```

*Tags: Getting Started, Errors Debugging*

---

**Peter** - *11:02:03*

However, the code could be dangerous depending on the context in which it's executed.



If it's being used as part of a Flumine strategy you should be OK as IIRK Flumine will ensure that you have an available_to_back attribute on your runner_book.ex object.



But as your code refers to "runner_books", rather than just "runners", this suggests to me that you might be using Betfairlightweight, which doesn't offer the same safety, so your runner_book.ex object could be None. In which case your code would fall over with a missing attribute error.



So for safety, you might want to make that condition:

 `if runner_book.ex and runner_book.ex.available_to_back`

*Tags: Errors Debugging, Strategies*

---

**Dario Scardina** - *18:07:26*

Hi [@UBS7QANF3](@UBS7QANF3) I have a question and a request  for you :smile:

*Tags: General Technical*

---

**Mo** - *20:57:38*

1. We've had a lively debate about this before but in my very humble opinion it is not correct to store the market ID as a string. The "1." prefix is a throwback to when there was a separate Australian exchange where the markets had a "2." prefix. As this is no longer the case, it is significantly better to encode market IDs as the integer after dropping the prefix

2. Yes will update it when I have a chance. Poke me if I forget

*Tags: Errors Debugging, Deployment*

---

## 2022-09-20

**Tom** - *02:22:51*

```market_types_filter = [betfairlightweight.filters.market_filter(event_ids=[event_ids]) for event_ids in aus_thoroughbred_events_today['Event ID']]

market_types = [trading.betting.list_market_types(filter=market_types_filter) for market_types_filter in market_types_filter]

aus_thoroughbred_events_today['Market Types'] = market_types```

I've got another one there - a lot of this code is just copied from the intro tutorial - I am going to run other peoples codes (with some adjustments) that perform similar functions to what I eventually need to build to just get things working and getting used to stepping outside of that structured learning environment I have been in without going all the way to the deep end.



I have a DataFrame called aus_thoroughbred_events_today, and I wanted to add market_types to that DataFrame (but not market_types objects, the actual information of which market types are available) I've only managed to attach the objects to the aus_thoroughbred_events_today DataFrame.



Besides this, I need to test whether the market-types I am calling match up with the existing sequence of events (that they actually represent the event they are supposed to) - and I will do the same thing with market_catalogues (I think the actual code should be similar enough).



I think that a lot of this is learning how to think about the problem rather than solving the specific problem and I'll be in learning phase for some time, but I am planning on doing 4+ hours a day on this 5 days a week and hope to have a very basic first iteration of a functional bot by 2023 - happy to take any advice on any of this process!

*Tags: Feature Engineering, Strategies*

---

**Tom** - *03:06:57*

In terms of using the testAPI on betfairlightweight - should my test order sent to market come back as ( status : FAILURE )? Or if it's coded correctly, should that be SUCCESS?

*Tags: General Technical*

---

**liam** - *06:57:42*

SUCCESS, error code should tell you the problem (sometimes)



2023? Start using flumine and you can be running by the end of the day 

*Tags: Errors Debugging*

---

**Tom** - *07:17:13*

Cheers. Roger that, I have a BET_ACTION_ERROR and INVALID_RUNNER which I'll chase down - did flumine get built on top of betfairlightweight and you use it instead, or do they work together?



Ultimately I am trying to create a bot, although I also want to get good at coding, python and understand all this stuff so in no hurry.



I'm just going through the tutorials and getting them to work on my end, make a few simple changes and just do the work to learn it at depth.



But I will get onto flumine and sounds like I will be up and running quicker than I think!

*Tags: Errors Debugging*

---

## 2022-09-23

**PeterLe** - *10:17:31*

I was just checking my P&amp;L's this morning and noticed that one particular flumine strat on AWS Ubuntu had done well over the last 30 days or so...so i thought Id log into to check all looked ok disk space etc...

Everything looked good except I noticed that i had run that particular program via Pycharm rather than standalone...:grinning::man-facepalming:

So now im thinking, whether to just leave it running or stop it and run standalone... (I appreciate that the former is not good practice and maybe slower, but what would you do?) Thanks

*Tags: Performance, Deployment*

---

## 2022-09-26

**Mick** - *09:46:31*

I only need to see the first three rungs on the price ladder but I've just noticed that my request for prices is returning many more than that. The code I'm using to scoop the prices for a single race is as follows



```	price_filter = betfairlightweight.filters.price_projection(price_data=['EX_ALL_OFFERS','SP_AVAILABLE'])

	market_books = bfex_trading.betting.list_market_book(market_ids=[mk_id],price_projection=price_filter)

	market_book = market_books[0]```

I read somewhere that if you didn't specify how many rungs you wanted then the default returned would be three. What have I misunderstood?

*Tags: Strategies*

---

## 2022-09-28

**Mick** - *17:35:08*

I just want to confirm something betfair's "cash out" button in relation to horses. I had assumed that the cashout process should equalise what happens if your your horse wins or loses. But now AFAICT what it does is it equalises what happens while totally ignoring the effects of rule4 deductions. So if a good horse withdraws at some point between the original back bet and the subsequent cashout, you can be left with a position which is very lopsided with regard winning vs losing. Can anyone confirm this is so?

*Tags: Deployment*

---

## 2022-10-01

**Jeff Waters** - *22:16:34*

I was just reading an article about Pyspark, which said it can process data at vastly faster speeds than traditional Python. [https://sparkbyexamples.com/pyspark/what-is-pyspark-and-who-uses-it/|https://sparkbyexamples.com/pyspark/what-is-pyspark-and-who-uses-it/](https://sparkbyexamples.com/pyspark/what-is-pyspark-and-who-uses-it/|https://sparkbyexamples.com/pyspark/what-is-pyspark-and-who-uses-it/)



Could it be used to achieve faster back testing with Flumine?



Thanks



Jeff

*Tags: Performance*

---

**thambie1** - *22:26:41*

Apache Spark is a distributed data processing framework. If you want to run flumine backtest parallelized on many machines at once, it is useful. Other than that, it is not useful. It's not going to speed anything up on your local machine.

*Tags: Performance*

---

**thambie1** - *22:33:12*

That being said, running on many machines at once ain't such a bad idea :wink:. Would take some effort and money (assuming paying for AWS/GCP)

*Tags: Deployment*

---

## 2022-10-02

**Jonjonjon** - *09:11:11*

I sometimes wonder how hard it would be to refactor Flumine to run on a GPU. I often run the several backtests in parallel, just varying the parameters. So it should be possible, but I have a feeling it would be a lot of hard work.

*Tags: General Technical*

---

**Alessio** - *09:55:36*

GPUs are good for floating point streaming stuff. A lot of the work in Flumine is decompressing, reading files, and updating the internal state for your code to run. Don't think you will get much.

*Tags: General Technical*

---

**Alessio** - *09:56:16*

If you want to speed it up i think aws batch/lambda and equivalents are a reasonable route for "pay-as-you-go"

*Tags: Performance, Deployment*

---

## 2022-10-03

**Jonjonjon** - *13:01:33*

Rust is beyond my skills. Are there any areas we might be able to improve performance if we use Numba? If you aren't already familiar, it will compile and make purely numerical stuff a lot faster. But if we need to use classes, attributes and dictionaries then things will be trickier.

*Tags: Performance*

---

## 2022-10-07

**Faye** - *20:09:37*

Could anyone tell me how to calculate / save the final price, please? I want it for greyhounds to help me see if I'm getting value. Its not live, it's back testing.

*Tags: Deployment*

---

**birchy** - *23:13:52*

[@U042PP6NAHM](@U042PP6NAHM) I'm going to assume you're using Flumine...what I would do is use the `market.context` dict to save the price of each runner on each update while inplay=False, then retrieve those prices in `process_closed_market()`



`class ClosingPriceStrategy(BaseStrategy):



    def check_market_book(self, market, market_book):

        if market_book.inplay: return

        return True



    def process_market_book(self, market, market_book):

        market.context['closing_prices'] = {

            r.selection_id: r.last_price_traded

            for r in market_book.runners

            if r.last_price_traded

        }



    def process_closed_market(self, market, market_book):

        # save to csv or whatever...

        for k, v in market.context['closing_prices'].items():

            print(k, v)`



This can be used for either backtesting or live

*Tags: Deployment, Strategies*

---

## 2022-10-08

**Unknown** - *20:22:32*

Just discovered retool for creating web apps ([https://retool.com/](https://retool.com/)). As someone with no experience of web development it makes it really easy to create an app to get data from your database and display it on a web page.

*Tags: General Technical*

---

## 2022-10-09

**Pos** - *13:32:16*

[@UBS7QANF3](@UBS7QANF3) Not sure, maybe because i feel like there's less of a barrier to getting started quickly. Its kind of confusing getting started on the api and it seems like everyone is using python while i'm using node js

*Tags: Getting Started*

---

## 2022-10-10

**Pos** - *01:45:01*

[@UBS7QANF3](@UBS7QANF3) why is the website incorrect? noob question

*Tags: General Technical*

---

**Peter** - *16:29:58*

Betfair updates prices 20 times per second, which is 20 times faster than the minimum refresh rate allowed for the website, and that's assuming that your scraper logs in and you have your account set to the fastest refresh rate. Then you have to add the latency and processing time that your process adds on top.



If your script doesn't log in then you're seeing delayed prices, which can often bear very little resemblance to the current prices (we get queries here about that fairly regularly).



[@UBS7QANF3](@UBS7QANF3) is right. If you're serious about this, it's way better to take the time to learn how to do it properly.

*Tags: Performance*

---

**Pos** - *16:39:39*

Yes it logs in. Ok yes I am serious about it. I'll try get setup on the api. Question for you [@U9JHLMZB4](@U9JHLMZB4), I'm running node for my application but everyone here is using python and they say its a better choice as there's more resources here. Do you think its worth going the python route and then using sockets to communicate the information to my node app?

*Tags: Getting Started*

---

## 2022-10-11

**Tom** - *05:29:45*

Hey guys, there is some information that would be pretty handy to start with - and it's not the end of the world to figure out how to do it myself because I want to develop those skills anyway - but those skills can also be developed on other steps - so has anybody done work on which odds to use as the 'real' odds for a race? Is it on Jump? a minute out? time-weighted? value-weighted? Is it possible to model this, or does it change significantly in the reality?

*Tags: Strategies*

---

**Tom** - *06:02:25*

Also, some of the code in the Flumine Tutorial 4 refers to fasttrack as Python package; but I'm unable to install or use it. I'm fairly new to this and it's probably some obvious problem.

*Tags: Getting Started*

---

**Ivan Zhou** - *06:07:56*

you can't pip install the fasttrack library. You can get it from here [https://github.com/betfair-down-under/autoHubTutorials/tree/master/FastTrack](https://github.com/betfair-down-under/autoHubTutorials/tree/master/FastTrack).



There is a tutorial on how to use fasttrack here [https://betfair-datascientists.github.io/modelling/fasttrackTutorial/](https://betfair-datascientists.github.io/modelling/fasttrackTutorial/)

*Tags: Getting Started, Strategies*

---

**liam** - *07:55:34*

It is common but its a trap, unless you are trying to optimise your bankroll greening is simply giving profit away

*Tags: General Technical*

---

**Peter** - *09:20:07*

[@U03TJKFLE8K](@U03TJKFLE8K) My path was comparable to yours. I wrote the (still) most widely used PHP package for consuming the Betfair API. But it became clear that PHP was a severely limiting factor. All the best tooling for analysis, testing and trading in this space is in Python (i.e. Betfairlightweight, Flumine, Pandas and Jupyter Notebooks).



So I went all in, learnt the language and now work almost exclusively in it (with a bit of Rust thrown in when I need compiled execution speeds). So my recommendation would be the familiarise with the tools that best fit the job rather than trying to bend your process to fit what you currently know. Though that is said without knowing what your processes are, e.g. if you are placing your bets manually at the bookmakers from data summarised via web pages you’ve constructed with Node, there may be a case for not re-inventing the wheel, just the engine :slightly_smiling_face:

*Tags: Feature Engineering, Performance, Strategies*

---

**Mo** - *11:52:50*

Needs a "It's OK to do so if you fully understand the implications and have (significant) bankroll constraints. If you think you understand the implications you probably don't" option

*Tags: General Technical*

---

**Tom** - *12:47:21*

Roger that thanks Ivan. It won't come up for me to use and says "ModuleNotFoundError: No module named 'fasttrack'" So I thought I needed to install a package. I'll check those out.

*Tags: Getting Started, Errors Debugging*

---

**Tom** - *15:13:11*

Cheers - is the scheduled start then considered the best possible odds as a model for the real probabilities?



I know it's a false abstraction and why we can bet successfully e.t.c. e.t.c.

*Tags: Strategies*

---

**btb** - *22:49:33*

hi everyone, a question



is it possible to call the api without setting up a *logged in* apiclient?



i.e. would like to obtain some market data without logging in n out all the time

*Tags: General Technical*

---

## 2022-10-12

**liam** - *13:15:45*

you can get stale data by calling the private website endpoints with the site appKey but I would question what problem you are trying to solve here...

*Tags: General Technical*

---

**Mo** - *13:32:11*

Surely it can be solved with better session management. There's no need for "logging in n out all the time"

*Tags: General Technical*

---

## 2022-10-13

**btb** - *13:55:48*

want to show a US person who doesn't have access to a betfair account some basics of betfairlightweight

*Tags: General Technical*

---

**liam** - *15:07:04*

its just an API client/wrapper, if it can't access the API then you are just going to get errors :man-shrugging:

*Tags: Errors Debugging*

---

## 2022-10-14

**FT** - *12:10:25*

Hi, I just found bflightweight a few minutes ago. I got the following question:

I purchased historical data in 50ms resolution and I wanted to recreate the stream of orders (place as well as cancel) that went into the markets. I'm not yet sure if this is something where bflightweight can help. I found the _create_historical_generator_stream_ function. Can I subscribe to the order stream generated by it?

I thought I'd just ask before wasting a lot of time exploring.

Thanks in advance

*Tags: Data Quality*

---

**liam** - *12:13:46*

[https://github.com/betcode-org/betfair#historic-data|yep](https://github.com/betcode-org/betfair#historic-data|yep)

*Tags: General Technical*

---

## 2022-10-19

**GaryK** - *16:19:45*

Yep [https://www.api-football.com/documentation-v3](https://www.api-football.com/documentation-v3)

*Tags: General Technical*

---

## 2022-10-22

**Ralegh** - *23:05:20*

(Flumine)

*Tags: General Technical*

---

## 2022-10-23

**Unknown** - *14:55:43*

Hello all,



Please, does anyone suggest a way to get a historical database for the Corner`s event in a timeline view (minute of the event) ? Thanks!

*Tags: Data Quality*

---

**Mo** - *15:57:56*

You want to scrape the inplayservice endpoint [https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py|https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py](https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py|https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py)

*Tags: General Technical*

---

**Mo** - *15:58:18*

They had some problems with that endpoint a couple of days ago. Should be fixed now

*Tags: Errors Debugging*

---

## 2022-10-26

**Jorge** - *15:59:43*

Hi all. I'm trying to separate results from 2 strategies that bet in the same markets. How do you deal with commission?



I find that when calling `list_cleared_orders` with different `customer_strategy_refs`, the `profit` column is correct but the `commission` column considers commission taking into account bets placed by other strategy_refs.

*Tags: Strategies*

---

**Jorge** - *16:20:06*

For example, in the same market:

• Calling `list_cleared_orders`  with strategy A gives profit=32.35, commission=0.06

• Calling `list_cleared_orders`  with strategy B gives profit=-29.46, commission=0.06. But here commission should be equal to 0?

Is there any way of separating them or do I have to calculate the commission myself?

*Tags: Strategies*

---

**Peter** - *17:20:35*

You will have to calculate it yourself, as Betfair calculates it only at market level and has no interest in the strategies you used under the hood.



From earlier conversations here though I suspect that most people would calculate the commission for strategy A as 0.65 and record the other 0.59 as bonus profit from running the two strategies together rather than implicitly adding it into the profit for strategy A.

*Tags: Strategies*

---

## 2022-10-30

**Meow** - *08:21:30*

cat_filter = betfairlightweight.filters.market_filter(

    market_ids = [‘1.205704199’])



# Request market cat

market_cat = trading.betting.list_market_catalogue(filter = cat_filter)



[x.runner_name for x in market_cat[0].runners]

*Tags: Strategies*

---

**Newbie99** - *08:34:16*

Have a look at this example: [https://github.com/betcode-org/betfair/blob/master/examples/exampleone.py](https://github.com/betcode-org/betfair/blob/master/examples/exampleone.py)



I see runners correctly showing for that market, so I suspect the problem is an incorrect filter.

*Tags: General Technical*

---

## 2022-11-02

**liam** - *11:49:09*

[https://github.com/betcode-org/betfair/blob/71c8fe2acdd20c9d65663e5a5af7692267489e81/betfairlightweight/streaming/cache.py#L74|both](https://github.com/betcode-org/betfair/blob/71c8fe2acdd20c9d65663e5a5af7692267489e81/betfairlightweight/streaming/cache.py#L74|both) i think

*Tags: General Technical*

---

**Jorge** - *13:14:42*

Hi, is it possible to use flumine to simulate a strategy that uses data which is not recorded in the market files?

*Tags: Strategies*

---

**liam** - *13:17:03*

Yep, example [https://github.com/betcode-org/flumine/blob/master/examples/simulate-sportsdata.py|here](https://github.com/betcode-org/flumine/blob/master/examples/simulate-sportsdata.py|here) that uses the cricket stream and the [https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/markets/middleware.py#L287|middleware](https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/markets/middleware.py#L287|middleware) that does the heavy lifting

*Tags: General Technical*

---

**liam** - *13:22:28*

just an example but the idea would be the same, use middleware that gets called on each `marketBook` update to pull in the data that you need and then store in `market.context` for strategy processing

*Tags: Strategies*

---

**Jorge** - *13:31:21*

Actually, the [https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/markets/middleware.py#L287|middleware](https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/markets/middleware.py#L287|middleware) would work if I have a file with the marketId even if it is not coming from a cricketSubscription, right?

*Tags: General Technical*

---

## 2022-11-03

**Guy Incognito** - *03:21:01*

Is there anyway to run the loop at the end of [https://github.com/betcode-org/betfair/blob/master/examples/examplehistoricdata.py|https://github.com/betcode-org/betfair/blob/master/examples/examplehistoricdata.py](https://github.com/betcode-org/betfair/blob/master/examples/examplehistoricdata.py|https://github.com/betcode-org/betfair/blob/master/examples/examplehistoricdata.py) in parallel to speed up download. I tried using joblib but there isn't much difference.



The original loop looks like this:

```for file in file_list:

    print(file)

    download = trading.historic.download_file(file_path=path)

    print(download)```

*Tags: Performance, Strategies*

---

**Mo** - *07:18:25*

Not sure if this will work as each process may need its own `trading` object but this is the basic pattern I follow for any parallelisation job in Python:



```import os

from concurrent import futures

from tqdm import tqdm



with futures.ProcessPoolExecutor(max_workers=os.cpu_count()) as pool:

    all_futures = [pool.submit(trading.historic.download_file, file_path=file) for file in file_list]

    for future in tqdm(futures.as_completed(all_futures), total=len(all_futures)):

        future.result()```

*Tags: Strategies*

---

**Jorge** - *14:52:35*

Getting back to my [https://betcode-org.slack.com/archives/C4HL6EZTQ/p1667395275679079|question](https://betcode-org.slack.com/archives/C4HL6EZTQ/p1667395275679079|question) from yesterday. I made it work but I am facing a problem using external data time series to trigger the bet...

*Tags: General Technical*

---

**Jorge** - *14:52:39*

An example:

• At t0, the market available_to_back is 2.0 and my fair odds are 2.1, so no bet.

• At t1, the market has not changed, so we don't have a new MarketBook, but my fair odds are now 1.9, so here I would trigger a bet.

• At t2, the market available_to_back is 1.8 and my fair odds are 1.9, so no bet.

How can I simulate the bet at t1 with flumine?

*Tags: General Technical*

---

**Jorge** - *16:43:34*

Exactly, so this is not possible with flumine, right? My use-case is an in-play football model that receives stats

*Tags: Strategies*

---

**liam** - *16:44:06*

without seeing some code this is very tricky to debug...

*Tags: Errors Debugging*

---

**Jorge** - *16:48:40*

This is my strategy class:

```class ExampleStrategy(BaseStrategy):

    def check_sports_data(self, market, sports_data) -&gt; bool:

        return True



    def process_sports_data(self, market, sports_data) -&gt; None:

        # called on each update from sports-data-stream

        fair_price = sports_data.runners[0].fair_price

        available_to_back = market.market_book.runners[0].ex.available_to_back[0]['price']

        if available_to_back &gt; fair_price:

            selection_id = market.market_book.runners[0].selection_id

            if self._invested and self._invested[(market.market_id, selection_id, 0)].executable_orders == True:

                return



            trade = Trade(

                market.market_id,

                selection_id,

                0,

                self,

            )

            order = trade.create_order(

                side="BACK",

                order_type=LimitOrder(available_to_back, 2, time_in_force="FILL_OR_KILL"),

            )

            market.place_order(order)```



*Tags: Strategies*

---

**Jorge** - *16:49:36*

And I'm using this [https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/markets/middleware.py#L287|middleware](https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/markets/middleware.py#L287|middleware)

*Tags: General Technical*

---

**Jorge** - *16:54:32*

I don't get what function would I need to refactor... Or is this something that doesn't make sense considering flumine's scope?

*Tags: General Technical*

---

**liam** - *17:07:58*

But that won’t satisfy your problem you described 

*Tags: General Technical*

---

**Jorge** - *17:33:44*

But I don't know how to do the above:sweat_smile:

*Tags: General Technical*

---

## 2022-11-04

**Unknown** - *08:15:35*

Hi [@U4H19D1D2](@U4H19D1D2). This is the market and sports_data I'm using, both in marketSubscription format. And also the python code where I have the Strategy and Middleware

*Tags: Strategies*

---

**Jorge** - *08:45:00*

Yes, this strategy relies solely in the sports_data. But I could place a bet if the marketBook changes in my favor, even if sports_data doesn't change

*Tags: Strategies*

---

**Jorge** - *11:45:04*

Related question: Is it possible to modify the example `marketrecorder.py` to record external data within the marketBook update so that I can access it when Simulating under `process_market_book` ?

*Tags: General Technical*

---

**liam** - *12:07:17*

Probably, just chuck it in and then access via `MarketBook._streaming_update`

*Tags: General Technical*

---

## 2022-11-05

**Unknown** - *01:56:45*

You can get this historical data from [http://betsapi.com|betsapi.com](http://betsapi.com|betsapi.com) a cheap (and slow) API.

*Tags: Data Quality, Performance*

---

**Nicholas vizard** - *16:04:56*

Hi everyone,

I hope you are all keeping well!

I just had a quick question related to understanding the MarketBook and MarketDefinition class in the betfairlightweight module. I am using the betfairlightweight module to parse the Historical Stream Data into a csv file. I am just starting out with object orientated programming, so any advice would be very much appreciated :blush::

*Why does the attribute _.market_definition_, when applied to a MarketBook object, return a MarketDefinition object, rather than kwargs.pop("market_definition", None)?*  

I suspect that:

 Applying the attribute market_definition to a marketbook object should return: kwargs.pop("market_definition", None) as that is wait is contained in the sourcecode for the MarketBook class (see: _betfairlightweight.resources.bettingresources.MarketBook')_

 However when I run: print(type(marketbookobject.market_definition),

I get: _*&lt; class, betfairlightweight.resources.streamingresources.MarketDefinition&gt;*_, which suggesting that I have generated a class by calling the market_definition attribute on a marketbook object.

Any help in clarifying why calling the market_definition attribute on a marketbook object generates a class would be much appreciated.



Thanks,

Nick

*Tags: Strategies*

---

## 2022-11-06

**Mo** - *08:33:03*

[https://github.com/betcode-org/flumine/blob/9307dd5a0084c3583207921a521a1c39e430220f/docs/quickstart.md#listener-kwargs](https://github.com/betcode-org/flumine/blob/9307dd5a0084c3583207921a521a1c39e430220f/docs/quickstart.md#listener-kwargs)



```:param bool cumulative_runner_tv: Cumulative runner traded volume (should be True if using betfair purchased data)```



*Tags: General Technical*

---

**Mo** - *08:37:24*

Then that will be the problem

*Tags: General Technical*

---

**Jeff Waters** - *09:13:45*

That seems to have fixed the issue. Thank you, Mo.

*Tags: Errors Debugging*

---

**Peter** - *13:38:09*

Not sure what you mean by automatically, but most of what you need is already there and easy to implement. Take a look at this [https://github.com/betcode-org/flumine/blob/master/examples/example.py|example strategy](https://github.com/betcode-org/flumine/blob/master/examples/example.py|example strategy). In particular the process_orders method and the the pieces of code that are commented out.

*Tags: Strategies*

---

**liam** - *14:04:03*

Is this the place response? currentOrders? streaming?

*Tags: General Technical*

---

**liam** - *14:31:27*

Tbh I don’t understand, what are you using to develop, just open up the debugger 

*Tags: Errors Debugging*

---

**AI Trader** - *15:47:06*

Hi guys,

I placed some orders using a Client/Strategy, and then restarted the program. After that, I cannot get anymore the matched orders using

• `makret.blotter.client_orders(client)` or

• `makret.blotter.client_orders(strategy_name)`

This worries me as if the code restarts for some reason (like crashing), I will basically loose all my matched orders and get a wrong `market_exposure`. Could someone provide some help to better understand if this is indeed an issue?



Is there any built-in mechanism to fetch the existing orders from the rest API when the program starts?

*Tags: Strategies*

---

**liam** - *17:55:46*

Not sure Guy is a flumine convert 

*Tags: General Technical*

---

**foxwood** - *19:36:11*

Assumed so since `process_market_book`  was mentioned but maybe that is also in bflw

*Tags: General Technical*

---

**Michael** - *21:41:08*

Betfair API question - how do I put dates into date_range?

*Tags: General Technical*

---

**AI Trader** - *21:51:31*

*INCLUSION OF MANUAL ORDERS (UI) IN BLOTTER.CLIENT_ORDERS()*

Hi guys,

Currently, I have a single strategy, and when using blotter functionalities such as market_exposure or selection_exposure I need to specify a strategy. The problem with that is that it does not include manual orders from the UI (such as manual cashout orders). Is there a way (without rewriting these functions) to get the market/selection exposure for all client orders (not only from a particular strategy), *including orders placed from the UI (cashout orders included*)?

*Tags: Strategies*

---

**AI Trader** - *21:52:27*

Additional info: I've checked, and orders placed through manual UI cashout have *customer_strategy_ref = None*. The problem is that after fetching past orders through the middleware available in the [https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|examples](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|examples), the cashout orders don't appear even when *not* filtering orders by strategy, ie, *market.blotter.client_orders(client)* or in Blotter.strategy_orders(None).



I have also tried to modify the middleware example and remove line [https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/examples/middleware/orders.py#L24|24](https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/examples/middleware/orders.py#L24|24) :`(customer_strategy_refs=[config.customer_strategy_ref])`, but this causes an issue as in lines [https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/examples/middleware/orders.py#L49|46-49](https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/examples/middleware/orders.py#L49|46-49) we try to access *customer_order_ref* and *customer_order_ref,* which are both `null` for orders placed by cashout (UI).



lines 46-49:

```strategy_name_hash = current_order.customer_order_ref[            :STRATEGY_NAME_HASH_LENGTH]

order_id = current_order.customer_order_ref[STRATEGY_NAME_HASH_LENGTH + 1 :]```

*Tags: Strategies*

---

## 2022-11-07

**Newbie99** - *08:50:58*

For this to work, you essentially need to create a hash (import create_cheap_hash from flumine.utils) of your strategy name. Then you can assign the orders with a null customer_order_ref to that strategy and they will appear in the blotter.



```            if current_order.market_id in market_id_list:

                if current_order.side == 'BACK':

                    strategy_name_hash = create_cheap_hash(STRATEGY_NAME, STRATEGY_NAME_HASH_LENGTH)

                else:

                    strategy_name_hash = create_cheap_hash(STRATEGY_NAME, STRATEGY_NAME_HASH_LENGTH)

                order_id = current_order.bet_id

            else:

                strategy_name_hash = current_order.customer_order_ref[:STRATEGY_NAME_HASH_LENGTH]

                order_id = current_order.customer_order_ref[STRATEGY_NAME_HASH_LENGTH + 1:]```

*Tags: Strategies*

---

## 2022-11-08

**Andy Bason** - *09:39:46*

Hi guys, I'm using the historical streaming example ([https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py)). It works well but I would like to add team names to the output. Is this possible? I can see them in the market definition but can't work out how to parse them out. Thanks

*Tags: General Technical*

---

**Andy Bason** - *09:55:20*

Thanks and sorry for the basic question. I'm learning by doing. Thanks again

*Tags: General Technical*

---

**Jorge** - *13:34:17*

Hi guys. Is there any way to get the current exposure from a market/strategy in flumine? Otherwise, I'd need to calculate it manually. For ex: If I placed a 3$ back order at 2.0 and a 2$ lay order at 2.0, the exposure would be 1$

*Tags: Strategies*

---

**Jorge** - *13:54:04*

Ah, I just discovered the [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L26|blotter](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L26|blotter) class, nice!

*Tags: General Technical*

---

**Jorge** - *14:33:12*

I know about the #nogreen but is there any function in flumine to send a cash out order for a market/selection?

*Tags: General Technical*

---

**Jorge** - *14:57:28*

I can create it if I solve it for my strategy first

*Tags: Strategies*

---

**Mo** - *15:16:23*

Not that I'm a flumine user but it should handle it all for you

*Tags: General Technical*

---

**R** - *16:24:59*

New face in the slack with a potentially dumb question: does flumine have a method to grab related markets in betfair?  e.g. from the win market I can navigate to the place market, forecast market, etc?

*Tags: General Technical*

---

## 2022-11-09

**AI Trader** - *03:07:14*

Thank you so much [@UFTBRB3F1](@UFTBRB3F1), that's really helpful! I will try to implement during the weekend

*Tags: General Technical*

---

**Jorge** - *08:00:36*

Hi guys, any way of getting the total profit (bonus for profit net of commission) per market with flumine? Currently I'm calculating it manually using `sum([o.profit for o in market.blotter])`

*Tags: General Technical*

---

**foxwood** - *12:08:06*

One for any ML users. Any recommendations for an easy ML framework other than tensorflow ? I'm revisiting some 3 year old tensorflow Estimator experiments and tf.v2 deprecates all that work and has switched to Keras - the upgrade path is turning into a time wasting mare. The old v1 is still supported for a while but don't want to risk relying on legacy stuff. Played with [http://ML.NET|ML.NET](http://ML.NET|ML.NET) but while that looks promising it will require some sort of IPC to work with flumine in real time which will be too laggy. So preferably something that works natively with Python.

*Tags: General Technical*

---

**James** - *12:33:07*

Hi all, I'm trying to build a simple dataframe containing basic details for the day's races. For some reason I can't get the off times for each race. This line `'Start Time': [market_cat_object.market_start_time for market_cat_object in market_catalogues]` just returns None. Can anyone see what the issue is? Thanks



```# Create event filter

event_filter = betfairlightweight.filters.market_filter(

    event_type_ids=[7],

    market_type_codes=["WIN"],

    market_countries=["GB", "IE"], 

    market_start_time={

        'to': (datetime.utcnow() + timedelta(hours=time_window)).strftime("%Y-%m-%dT%TZ")

    }

)



todays_races = trading.betting.list_events(

    filter=event_filter

)



races_today = pd.DataFrame({

    'Event Name': [event_object.event.name for event_object in todays_races],

    'Event ID': [event_object.event.id for event_object in todays_races],

})



print(races_today)



races_list = races_today['Event ID'].to_list()

print(races_list)



market_catalogue_filter = betfairlightweight.filters.market_filter(event_ids=races_list, market_type_codes=["WIN"])



market_catalogues = trading.betting.list_market_catalogue(

    filter=market_catalogue_filter,

    max_results='100',

    sort='FIRST_TO_START',

)



# Create a DataFrame for each market catalogue

races_df = pd.DataFrame({

    'Start Time': [market_cat_object.market_start_time for market_cat_object in market_catalogues],

    'Market Name': [market_cat_object.market_name for market_cat_object in market_catalogues],

    'Market ID': [market_cat_object.market_id for market_cat_object in market_catalogues],

    'Total Matched': [market_cat_object.total_matched for market_cat_object in market_catalogues],

})```



*Tags: Feature Engineering, Strategies*

---

**liam** - *12:35:47*

You haven't asked for it in the `list_market_catalogue` request, example [https://github.com/betcode-org/betfair/blob/71c8fe2acdd20c9d65663e5a5af7692267489e81/examples/exampleone.py#L25|here](https://github.com/betcode-org/betfair/blob/71c8fe2acdd20c9d65663e5a5af7692267489e81/examples/exampleone.py#L25|here) that does

*Tags: General Technical*

---

**Newbie99** - *12:36:54*

You can get this on closure using this example:



[https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: General Technical*

---

**FT** - *12:57:35*

Hi guys, I've got a flumine specific question. I don't know if I misunderstand something in the way the blotter works. I'm simulating a simple strategy with historical data. In this particular case, at the end all the orders in the markets blotter show up as PENDING, which is very unlikely since there are a lot of them and I take very bad prices just to see if they ever get the status EXECUTION_COMPLETE.

I collect some of the orders information into a dataframe. This is one example row. Why do I have a `avg_price_matched` and `size_matched` and `profit`, when the order is still PENDING? By the way the placed_at and completed_at are seconds to start time.

```selection_id  placed_at  completed_at  status                price  side  avg_price_matched  size_matched  profit

28396755      -348.287   -348.287      OrderStatus.PENDING   1.81   BACK  1.91               2.0           1.82```

*Tags: Data Quality, Feature Engineering, Strategies*

---

**liam** - *13:11:50*

without some code to replicate very hard to debug this

*Tags: Errors Debugging*

---

**FT** - *13:27:19*

Problem solved: I had a small but catastrophic typo in the `process_orders` function. An exception lead to the orders beeing stuck in PENDING until the end.

*Tags: Errors Debugging*

---

**R** - *14:12:33*

Not sure on compatibility with flumine, but in general you have:



pytorch or JAX that are pretty efficient and python-friendly.



Using ML in practice often means converting your model to some lightweight format, check out ONNX for that.  I would be more than happy to discuss further!

*Tags: Strategies*

---

**foxwood** - *17:56:06*

Got autogluton going fairly quickly so will explore the models on that to start with. Not sure about the trend to try everything then just pick simple best accuracy - the quality of the ones it gets right is more important. I got better results with tf by sticking to one model at a time and playing with the data in terms of changing the feature set, nodes, depth etc. ONNX looks interesting and MS give it some support for Windows so will have a look at that as well. Thanks for suggestions.

*Tags: Feature Engineering, Strategies*

---

## 2022-11-10

**Guy Incognito** - *13:09:31*

Hi guys, I'm trying to run a strategy where after an order is matched u would like to update a value e.g. if my back bet gets matched then update my probability. What's the best way to do this? I will have multiple limit orders at once so I just want a code of chunk to execute when one of the orders get matched I've been looking at

 `_process_market_orders`

But not sure if it's the right way to go

*Tags: Strategies*

---

**liam** - *13:13:56*

in your case `process_orders` is the most logical and store any data in `Strategy.context` or `Market.context`

*Tags: Strategies*

---

**Andy Bason** - *14:42:42*

Hi Liam, another question if I may: is it possible to get the countryCode too? Thanks

*Tags: General Technical*

---

## 2022-11-11

**AI Trader** - *02:43:07*

*AVOID DUPLICATED ORDERS*

How does flumine deals with avoiding duplicated orders? Say that my strategy always keeps a single bid and lay on the best levels of the orderbook. If I implement this logic (target orders generation and placement) in `process_market_book` , what will happen if I get a new market_update before betfair sends a message with my open orders to the websocket acknowledging that the orders have been placed? Example: I place both back and lays at the top of the orderbook at time T0, I receive another book update at time T1 and at time T2 my open orders are updated once the websocket receives my orders from betfair. Is is possible that in between T1 and T2 I place duplicated orders? As in this interval (T1-T2) I will not have acknowledged the open orders that I submitted at T0 just yet? Any help is enormously appreciated.

*Tags: Strategies*

---

**Jorge** - *13:14:00*

Hi guys, what is [https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py#L46|conflate_ms](https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py#L46|conflate_ms) used for when creating a flumine Strategy?

*Tags: Strategies*

---

**Jorge** - *13:53:47*

I was just confused because in the [https://github.com/betcode-org/flumine/blob/master/examples/example-single.py#L83|example](https://github.com/betcode-org/flumine/blob/master/examples/example-single.py#L83|example) it is set to 1000 ms

*Tags: General Technical*

---

## 2022-11-12

**Carlos M** - *15:20:46*

Hi people, I am new here. Im currently using the framework I've developed myself some time ago to execute my strategies in an event-driven format. I have started migrating to Flumine. I have some questions. It would be great if someone could help me :slightly_smiling_face:

1. It seems to me that the framework runs all markets using a single process/ single thread and manages events through a FIFO queue (self.handler_queue). Is my understanding correct? I see two potential disadvantages with that:

    a. By having all events going to the same FIFO queue, if one gets 100 book updates in a very short time interval (from the same market), it will trigger the `process_market_book` 100 times, even though one would want to react only to the latest market book update (what would be the logic of reacting to older updates?) In that case, wouldn't it make sense to only react to the latest market_update? 

    b. It seems to be that all market updates (from different markets) go to the same queue. Wouldn't it be better to treat every market in a separate thread, especially for non-intensive CPU strategies? How do you guys go about making it multi-threading?

2. I am not sure I fully understand what should middlewares and workers be used for, that might be the reason for my previous question. Would someone be generous enough to explain to me?

Thank you so much!

*Tags: General Technical*

---

**Carlos M** - *16:02:36*

Fixed

*Tags: Errors Debugging*

---

**liam** - *16:43:45*

Assumption are:



• you would never want to miss an update

• make sure your strategies can process quicker than a new book update 

• If you find this is a limitation then flumine isn’t for you (although I would fix the slow strategies)

• Workers are for polling type situations (

ie. external data update)

• Middleware gets called on every marketBook update (similar to django request/response cycle / middleware)

*Tags: Errors Debugging, Performance*

---

**Carlos M** - *17:19:28*

Thank you so much [@U4H19D1D2](@U4H19D1D2)!



Could you explain to me what is the benefit of processing every single book update (other than reconstructing the orderbook through book differences, which is not the case of a strategy) ? An example would be extremely appreciated

*Tags: Strategies*

---

**liam** - *17:22:34*

Let’s assume you are running a strategy inplay, things move fast so should your strategy, you wouldn’t want to miss an opportunity that presents itself (market based) 



If you don’t want all updates you can use the conflate variable 

*Tags: Strategies*

---

**Carlos M** - *17:33:09*

I entirely agree! What I am trying to understand is the following: Assume that my strategy is fast, but there is a situation where there are so many market updates (assume 100 in 1ms, as an extreme case). Why would I want to react on the oldest update? Isn't that even dangerous to place orders based on old prices (the price might have changed between the 0th and the 100th market updates in the queue). I am by no means suggesting this is a wrong approach, just trying to understand the risks

*Tags: Strategies*

---

**Carlos M** - *17:34:59*

Thanks Mo. So this means that if my strategy cannot react as fast as the market updates, there is a risk that I will place orders based on old (and wrong) prices on the orderbook?

*Tags: Strategies*

---

**liam** - *17:43:09*

You would want to monitor this but it will be number of orders that tends to cause problems (&gt;1k per market) Simple fix is to split up markets and run a flumine instance per split but you are probably over complicating things 

*Tags: Errors Debugging*

---

**Mo** - *17:43:56*

Personally for computationally heavy strategies I essentially do conflation (although I'm not using flumine). You won't be acting on stale data, you just might miss an opportunity in between conflated updates

*Tags: General Technical*

---

**Carlos M** - *21:45:47*

Thank you so much [@U4H19D1D2](@U4H19D1D2) and [@UBS7QANF3](@UBS7QANF3), that was genuinely helpful! I am still curious though to understand the advantage of approach 1 vs 2, where:

1: A FIFO storing every single book update

2. Keeping only the most recent book update per market



In the best case scenario, you are fast enough to react to every single update in both of the approaches (they will behave in the exact same way).

In the worst case scenario where you are late, you miss the opportunity in both approaches, but in approach 1 you place orders based on old data, which can be harmful. You also further increase the delay by processing old markets data.

On the other hand, in approach 2, you don't place harmful orders or further increase the delay

*Tags: General Technical*

---

## 2022-11-13

**liam** - *06:11:07*

The best case scenario you describe is the only acceptable scenario, if it isn't you fix it

*Tags: Errors Debugging*

---

## 2022-11-14

**Jorge** - *14:15:40*

Hey, any clean way of changing the customer_strategy_ref of a flumine Strategy? I'm currently overwriting the config variable, like:



```import flumine.config

flumine.config.customer_strategy_ref = "my_strategy_ref"```

*Tags: Strategies*

---

**Jorge** - *15:22:17*

Hi, I am debugging an inplay market where I run flumine. The live log indicates an order was placed but I cannot see this order anywhere in the betfair website or quering `trading.betting.list_cleared_orders`



```2022-11-13 14:53:25,981 | INFO | a | a | Order status update: Pending

2022-11-13 14:53:25,982 | INFO | a | a | Deleting requests.Session

2022-11-13 14:53:25,982 | INFO | a | a | New requests.Session created

2022-11-13 14:53:25,987 | INFO | a | a | Thread pool submit

2022-11-13 14:53:25,987 | INFO | a | a | 1 order packages executed in transaction

2022-11-13 14:53:31,266 | INFO | a | a | execute_place

2022-11-13 14:53:31,267 | INFO | a | a | Trade status update: Pending

2022-11-13 14:53:31,267 | INFO | a | a | Order Place: SUCCESS

2022-11-13 14:53:31,267 | INFO | a | a | Order status update: Executable

2022-11-13 14:53:31,267 | INFO | a | a | Trade status update: Live

2022-11-13 14:53:31,505 | INFO | a | a | Order status update: Execution complete

2022-11-13 14:53:31,505 | INFO | a | a | Trade status update: Complete```

Any idea what could have happened?

*Tags: Errors Debugging, Deployment, Strategies*

---

## 2022-11-15

**liam** - *06:21:03*

Set `streaming_timeout` to a value in the strategy, this causes snap to be called on the cache every x seconds, ie



```strategy = ExampleStrategy(

    market_filter=streaming_market_filter(market_ids=["1.205602407"]),

    streaming_timeout=1

)```



*Tags: Strategies*

---

**Jorge** - *08:14:02*

that's awesome, it's a easy solution to my problem of triggering a `process_market_book` whenever my probabilities of fair_price change. at least I can simulate if it improves my strategy

*Tags: Strategies*

---

**Jorge** - *08:26:16*

When running a FlumineSimulation I see that orders placed have [https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L287|order.publish_time](https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L287|order.publish_time)  == [https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L286|order.date_time_created](https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L286|order.date_time_created). But this is not 100% realistic, is it possible to add a processing delay? Let's say my strategy takes 100 ms to decide if it places an order.

*Tags: Strategies*

---

**Jorge** - *08:52:48*

Other question:sweat_smile:, does the "FILL_OR_KILL" attribute work in Simulation? In live trading I find "Order status update: Execution complete" but I don't receive this in Simulation...

*Tags: Deployment, Strategies*

---

**liam** - *08:53:56*

[@UPMUFSGCR](@UPMUFSGCR) had a [https://github.com/betcode-org/flumine/pull/296|PR](https://github.com/betcode-org/flumine/pull/296|PR) but he closed it?

*Tags: General Technical*

---

**Jorge** - *09:11:47*

[@U4H19D1D2](@U4H19D1D2) This doesn't seem to affect orders being matched or not. Shouldn't I modify this instead?

```import flumine.config

flumine.config.place_latency += 0.1```

*Tags: Performance*

---

**liam** - *09:13:51*

```from flumine import config



config.place_latency += 0.1```

*Tags: Performance*

---

**Jorge** - *14:36:25*

I added this to my strategy to simulate FILL_OR_KILL orders, it is not perfect but works most of the time:

```def process_orders(self, market, orders):

    for order in orders:

        if order.status == OrderStatus.EXECUTABLE:

            market.cancel_order(order)```



*Tags: Strategies*

---

## 2022-11-16

**Mo** - *11:34:44*

Don't you scrape market catalogues for your sports of interest? Then you can just do a local database query

*Tags: General Technical*

---

**George** - *15:52:52*

When a race has finished, the Betfair App displays a nice table with the finishing position, BSP and ISP. *Is there any way of getting this table through the API?*

I know about the [https://promo.betfair.com/betfairsp/SP_history.html](https://promo.betfair.com/betfairsp/SP_history.html) page but you have to wait around 12 hours for the latest file to upload.

I also know that it's possible to get the BSP etc by using streaming in Flumine but it seems like a lot of work just to get the BSP - I'd only need one line of the 100k+ line file that is created.

*Tags: General Technical*

---

**D C** - *16:03:47*

Fairly certain there is nothing in API-NG or streaming that will give you ISP or finish positions (other than winner). I suppose you could deduce from "winners" in the associated place markets but would be messy and depend on how many place markets are offered. BSP should be available in a listMarketBook call once the market is inplay (I've inferred here you have an aversion to using the stream based on your post).

*Tags: General Technical*

---

**George** - *16:09:50*

thanks for your answer. ISP would be interesting and clearly the information is "there" because it's in the App. But yes, it looks like it's not available through the API.

You are right, I could just do a listMarketBook call once the market is inplay, though I guess if I wasn't using streaming then I could accidentally miss it.

thanks again

*Tags: General Technical*

---

**D C** - *16:11:48*

I'd go for streaming if I were you. Makes life a lot easier.

*Tags: General Technical*

---

**Mo** - *16:39:09*

Undocumented but trivial to access with bflw

*Tags: General Technical*

---

**D C** - *16:40:13*

OK I'll work it out from the bflw. Cheers

*Tags: General Technical*

---

**Mo** - *16:41:13*

[https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py](https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py)

*Tags: General Technical*

---

## 2022-11-17

**Jorge** - *09:13:30*

Does the backtest (simulation) match live trading for you guys? I'm testing and want to feel confident they match before running the Strategy live with higher stake and in more markets. But maybe this is too much to ask from the backtest?

*Tags: Deployment, Strategies*

---

**Jorge** - *09:19:44*

Exactly, that's my plan. But does the simulation match live in flumine? Or are there known limitations?

*Tags: Deployment*

---

**Aaron Smith** - *09:19:50*

i think that would raise some error, it needs to be executable. Dont worry about the milliseconds the bet is up. I m doing the same as you are right now and all is good on my end.

*Tags: Errors Debugging*

---

**liam** - *09:20:37*

lmgtfy [https://betcode-org.github.io/flumine/quickstart/#simulation_1](https://betcode-org.github.io/flumine/quickstart/#simulation_1)

*Tags: General Technical*

---

**Aaron Smith** - *09:23:23*

depends heavily on the strategy. From my experience, strategies where i am offering bets underperform in backtest compared to live and strategies that take bets are very comparable. i think i m actually still doing slightly better in live than in backtesting, unless i backtest a lot to get the perfect parameters, in which case you can assume that the parameter set that was performing best was probably also a little lucky in that particular run and live will be performing slightly worse.

*Tags: Deployment, Strategies*

---

**Jorge** - *09:32:04*

Makes sense, that was my other idea, just run it with smaller stakes in many different markets and wait to compare results. thanks mate, this is very helpful

*Tags: General Technical*

---

**D C** - *11:35:46*

I hacked something together in nodejs last night using the bflw example as a template. It works but it is VERY flakey and I don't always get a response. Is this common with these endpoints (find raceResults is WAY less reliable) or possibly an issue with my implementation?

*Tags: General Technical*

---

**Mo** - *11:51:25*

You definitely need to be robust to random HTTP errors

*Tags: Errors Debugging*

---

## 2022-11-19

**river_shah** - *11:02:12*

What would cause this issue please?

```2022-11-19 10:59:46.180 WARNING[flumine.order.process:process_current_orders:45] Order xxx not present in blotter

2022-11-19 10:59:46.291 WARNING[flumine.order.process:process_current_orders:45] Order yyy not present in blotter

2022-11-19 10:59:46.291 WARNING[flumine.order.process:process_current_orders:45] Order zzz not present in blotter```



*Tags: General Technical*

---

## 2022-11-20

**Unknown** - *05:15:55*

Hi guys,

Any guesses why I am getting a strategy_name equal to None? The order has been submitted by a strategy with a name different from None, and when trying to fetch the executed orders through the middleware available in the flumine examples, I am getting None as a strategy name, which implies that the middleware is not adding any of the executed orders when it restarts. Are there any requirements for the strategy name? Eg. a max length ?

*Tags: Strategies*

---

**Jeff Waters** - *11:29:54*

Hi



I'd like to make is so that, when I run back tests using the multiprocessor code at [https://betcode-org.github.io/flumine/performance/](https://betcode-org.github.io/flumine/performance/), the orders in each process's blotter are written to a spreadsheet when that process has completed running.



Would that be possible, and if so where would I put the a call to the spreadsheet-generating function?



Thanks



Jeff

*Tags: Performance*

---

**Jonjonjon** - *13:29:31*

Yes that is what I do too. Though there can be annoying difficulties with concatenating too much as Pandas is not good at it..someone here probably has a good way to do it.

*Tags: Feature Engineering*

---

**liam** - *14:14:27*

I just read/write to a big file then process through pandas

*Tags: Feature Engineering*

---

**AI Trader** - *15:28:36*

[@U4H19D1D2](@U4H19D1D2) Sorry, let me make it more clear:



When using the `_create_order_from_current()` (see in the middleware example [https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/examples/middleware/orders.py#L45](https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/examples/middleware/orders.py#L45)), I get None in `strategy = self.flumine.strategies.hashes.get(strategy_name_hash)`

This entails that none of my previous orders gets created when the program restarts, as I get the error :

{"asctime": "2022-11-20 15:23:35,051", "levelname": "WARNING", "message": "OrdersMiddleware: Strategy not available to create order 138876445504398400", "bet_id": "287696652099", "market_id": "1.197093506", "customer_strategy_ref": "menoci", "customer_order_ref": "5458cb00631fc-138876445504398400", "strategy_name": "None", "client_username": "[mailto:xxxxx@gmail.com|xxxxx@gmail.com](mailto:xxxxx@gmail.com|xxxxx@gmail.com)"}

*Tags: Errors Debugging, Strategies*

---

**mzaja** - *23:11:33*

Hi all,

I have a greyhound bot which I've been running for a few months using bflw and a self-made scheduler, which starts the bot 5 mins before each race. I would like to port the strategy to flumine. However, flumine expects me to initialise the strategy object with a coarse filter which would subscribe to all greyhound markets at once, but this gets me over the 200 markets per connection limit on most days. What is the best way to go about this? I am only interested in market updates 5 min before the off since there is virtually no activity beforehand, so it seems a waste of subscriptions to spend them on markets which are overwhelmingly inactive.

*Tags: Strategies*

---

## 2022-11-21

**AI Trader** - *02:35:46*

Hi guys,

I'm running my flumine instance in the cloud with the Middleware available in the examples for retrieving EXECUTION_COMPLETE orders whenever the program restarts. When running the same code locally, the middleware fails to retrieve the orders that were placed by the program in the cloud due to the strategy_name_hash `(current_order.customer_order_ref[:STRATEGY_NAME_HASH_LENGTH])` being different (even though the strategy name is the same both locally and in the cloud). I need help figuring out why this is happening, as  `customer_order_ref` should only depend on the strategy name? Any hints would be hugely appreciated as I have spent a lot of time trying to debug this with no success. The fact that orders only store the hash of the strategy name makes it more difficult to figure out if for some undesired reason my code is changing the strategy name.

*Tags: Errors Debugging, Strategies*

---

**liam** - *07:02:58*

What’s being wasted? Streaming is extremely efficient 

*Tags: General Technical*

---

**liam** - *07:34:37*

It will be because your hostname / customer_strategy_ref has changed, you can set it to a default value in the config. This is done so you can run multiple instances of flumine with separate orders 

*Tags: Strategies*

---

**AI Trader** - *15:53:52*

Thanks Liam! 



The hostname / customer_strategy_ref should only be used for filtering the orders on the below part of the code in the middleware , right ? I have removed that part l. My issue is that the order_strategy_ref is different depending on whether I place the orders locally or in the cloud. My understanding is that the first 13 characters of that field should only depend on the strategy name , and not on the hostname. Am I missing something ? 



```resp = client.betting_client.betting.list_current_orders(

                customer_strategy_refs=[config.customer_strategy_ref],

                order_projection="EXECUTION_COMPLETE",

            )```



*Tags: Strategies*

---

**AI Trader** - *16:46:16*

The customer_strategy_refs=[config.customer_strategy_ref], part  (that filters only orders coming from the hostname )

*Tags: Strategies*

---

## 2022-11-22

**AI Trader** - *03:31:32*

Thanks a lot [@U4H19D1D2](@U4H19D1D2), I'll try your solution.

I guess it would be enough to do the below in the script where I instantiate the strategy and add it to flumine?



```from flumine import config



config.customer_strategy_ref = "my_strat_ref"```



*Tags: Strategies*

---

## 2022-11-23

**Jorge** - *08:18:39*

Hi guys, while Simulating, how can I access `market.blotter.get_exposures(self, lookup=selection_lookup)`  when a market is cleared?



I'm looking at the [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|backtestloggingcontrol example](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|backtestloggingcontrol example), but want to save the exposure (profit_if_win and profit_if_lose) on top of the profit

*Tags: General Technical*

---

**liam** - *19:53:55*

BetAngel [https://forum.betangel.com/viewtopic.php?t=26591|v1.60.0](https://forum.betangel.com/viewtopic.php?t=26591|v1.60.0) posting because it has an API! :clap:



Python wrapper anyone? 

*Tags: General Technical*

---

**Unknown** - *23:39:05*

Hey Guys, I have a pretty basic question, so apologies in advance. When I use runner.ex.traded_volume and look at the `size` I see that the size is roughly half of what is displayed on the website. For example this graph shows $30 AUD but runner.ex.traded_volume gives me 16.78, i'm aussming that is GBP, do anyone know of an easy way to convert this?

*Tags: General Technical*

---

## 2022-11-24

**Johnny Boston** - *00:17:36*

Hey Ivan :joy:



Use the conversion rate in the bflw module. That's the only way to do it.

*Tags: General Technical*

---

**Unknown** - *02:04:58*

*UI ORDERBOOK SIZE DIFFERENT FROM STREAMING* 



Hi guys,

Would anyone be able to explain why I see different sizes in the UI for some levels of the book when compared to the orderbook received through the stream?

Here's one example:

For these selections, I don't even see the first two levels of the UI (2.14 and 2.16) in the runner coming from the stream. I have tried to subscribe to all the three

```"EX_BEST_OFFERS", "EX_BEST_OFFERS_DISP", "EX_ALL_OFFERS"```

And all of them have different values from the ones I can see in the UI.



Any help will be much appreciated. Thanks!

*Tags: General Technical*

---

**liam** - *08:18:13*

When streaming if you want best display you need to ask for only best display only, otherwise we default to best offers (what you should be looking at)

*Tags: General Technical*

---

## 2022-11-26

**AI Trader** - *17:12:35*

Hi gents,

Questions related to *BaseStrategy:*

Q1 : It receives  `market_filter` and `market_data_filter` as arguments. Does it automatically update the markets based on these filters? Eg.: If on day 24 Nov the Market M doesn't pass through the filters, but on day 25 Nov it does, will the strategy automatically include this market on Nov 25 without me restarting the code? If not, is there an alternative to restarting the code every X hours? As it would stop the strategy during the restart time, which is risky for trading and would make me loose data for data recording.



Q2: I am trying to find a way to filter markets for which I want to store data using the *MarketRecorder* strategy in the examples.

I want to fetch football matches, but if I filter by:

```market_filter=betfairlightweight.filters.streaming_market_filter(

    event_type_ids=["1"],

    market_types=["MATCH_ODDS"],

    betting_types=["ODDS"],

),```

 I get:

`{"asctime": "2022-11-26 16:40:19,795", "levelname": "ERROR", "message": "[FlumineStream: 2001]: SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 505 markets whereas max allowed number was: 200"}`



I could obviously get a reduced number of  market_ids myself by doing:

```filter = betfairlightweight.filters.market_filter(event_type_ids=[1], market_start_time={'from': datetime_from, 'to': datetime_to})

self.trading.betting.list_events(filter=filter)```

But then I would have static ids and certainly would have to restart the program to fetch the new markets, which is a problem as described above.



What would be the recommended way to reduce the subset of markets while avoiding having static market_ids ?

*Tags: Errors Debugging, Strategies*

---

**liam** - *17:15:13*

Yes

That won’t work when streaming, email bdp and get your limit bumped 

*Tags: General Technical*

---

**AI Trader** - *17:19:32*

Yes for the question where the markets gets updated automatically?



Is there a way to add additional filters to the streaming (for data its fine to get all the 500+ markets, but for trading I would like to limit by time to match for instance)

*Tags: Strategies*

---

**liam** - *17:25:44*

Yes



It’s not additive but in flumine you can supply a list of filters 

*Tags: General Technical*

---

**AI Trader** - *18:08:34*

[@U4H19D1D2](@U4H19D1D2)

I tried to add a `betfairlightweight.filters.market_filter` filter so that I could also filter by market_start_time, but I am getting almost 10_000 markets, whereas I should be getting less than 300. Any ideas why?

I used the below filter as a *market_filter* for the BaseStrategy, which returns the below error:

```filter = betfairlightweight.filters.market_filter(event_type_ids=["1"], market_start_time={'from': '2022-11-26T23:31:33Z', 'to': '2022-11-28T17:31:33Z'})```

`{"asctime": "2022-11-26 17:52:58,912", "levelname": "ERROR", "message": "[FlumineStream: 2001]: SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 9866 markets whereas max allowed number was: 200"}`



It says there are 9866 markets passing this filter, but if I manually check it through:

```all_event_ids = [event.event.id for event in self.trading.betting.list_events(filter=filter)]```

I get only 232 event ids (and there should be a single market_id per event ids as I am only fetching MATCH_ODDS., which is way smaller than the number of market_ids the BaseStrategy complains about (9866)



Below is the code Ive used for the BaseStrategy:

```framework = Flumine(client=client)

strategy = S3MarketRecorder(

    name="WIN",

    market_filter=filter,

    stream_class=DataStream,

    context={

        "local_dir": "./bet_data_temp",

        "force_update": False,

        "remove_file": True,

        "remove_gz_file": True,

        "bucket" : bucket,

    },

)```



*Tags: Errors Debugging, Strategies*

---

## 2022-11-27

**liam** - *07:52:23*

Market filters don’t work with streaming 

*Tags: General Technical*

---

**AI Trader** - *15:29:56*

Thanks ! Is there a way to add a callback and only add market based on information on the market_catalogue? Such as time to start and volume traded? Of course, I could do that check_market_book, but I think I wouldn't even be able to get there in the code as I would get the subscribing to &gt; 200 markets error before.



Is this the right way to go? Or am I missing smth? I guess that approach would work whenever bdp will increase my limit

*Tags: Errors Debugging*

---

## 2022-11-28

**Jorge** - *16:39:04*

Hi, how can I check if my strategy has any executable or pending order in the market?



I thought `market.blotter.has_live_orders`  was good for this, it works in Simulation. But in live trading it is returning True also when there are orders with "Execution complete"... Could it be that it does not work for "FILL_OR_KILL" orders?

*Tags: Deployment, Strategies*

---

## 2022-11-30

**EJono** - *15:18:45*

I posed a question in the issues channel related to linking win margin selection ids to the correct points buckets (selection/team name) for rugby matches. The problem im facing is that the runners list in each games market_catalogue are empty, thus no sort priority or team names are available to link the selection id to the actual outcome it correlates to. Is this something worth contacting betfair directly about in order to sort out or potentially recieve a dump of selection_ids associated with the correct selection/team_name ? Ive confirmed that that the selection ids are not persistnet between games ie the id for home team 1-12+ option is not the same from one game to another and ive also experienced the index of the runners be inconsistent. The draw is the only excpetion with persistent selection_id whcih matches that of the MATCH_ODDS but i am rarely interested in placing on this selection.

Any advice? is it time to start shooting questions to betfair? Thanks

*Tags: General Technical*

---

**EJono** - *16:53:16*

thank you very much, i was being ignorant of this parameter since it was not listed as a required field in betfairs api documentation. Cheers again still got lots to learn

*Tags: General Technical*

---

## 2022-12-01

**Mo** - *10:53:02*

Look at the examples: [https://github.com/betcode-org/betfair/tree/master/examples](https://github.com/betcode-org/betfair/tree/master/examples)

*Tags: General Technical*

---

**MMW** - *10:58:45*

no problem

*Tags: General Technical*

---

**rob smith** - *14:51:12*

When parsing historical data with [https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py) how do you get the market name? Eg 5f Hcap

*Tags: Data Quality*

---

## 2022-12-02

**birchy** - *11:19:05*

In Flumine it's `runner.sp.near_price`

*Tags: General Technical*

---

**Aaron Smith** - *12:34:23*

note: ensure you request SP data in your streaming_market_data_filter, otherwise it ll return None

*Tags: General Technical*

---

## 2022-12-03

**D C** - *14:09:31*

But surely if the market conditions are right then why not. Unless you can be certain of the lag on the feed on which you watched it, you can't 100% match visual event with market response can you? If the race looked innocuous then probably just an error that soaked up everything available? At those high prices the volumes are usually pretty thin from what I see in general. XM can only do so much especially at the higher price end.

*Tags: Errors Debugging*

---

## 2022-12-04

**Tony** - *06:46:37*

Hi, I am very impressed with this forum and I am interested in learning more about backtesting and strategy development. I am a university student and I am unable to afford the pro data, which is quite expensive. Is there an alternative to using pro data for beginners who are just starting out? It seems that some people have run into errors when using only the free data, and while there are a few months of free, full data available, I am not sure if this would be sufficient for testing the significance of a strategy. Thanks all for your help

*Tags: Getting Started, Errors Debugging, Strategies*

---

**birchy** - *11:00:39*

Most of us gather our own data, however you need to make sure that you are placing bets as well to avoid getting blocked.

[https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py|https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py|https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py)



For the bets, this is a good starting point, or you could do something simple like lay runners &lt; 2.0. 

[https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py|https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py|https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py)

*Tags: General Technical*

---

**PeterLe** - *14:52:07*

API documentation has been provided now by betangel Liam.

[https://www.betangel.com/api-guide/](https://www.betangel.com/api-guide/)

*Tags: General Technical*

---

**AI Trader** - *17:01:38*

Hi guys,

What is the correct way to dynamically add markets to my strategy? My criteria are not available through the filters (eg.: traded_amount), so I would like to check every once in a while if these conditions are met and add the market in case it does.



Subscribing to all markets and then checking the condition on `check_market_book` would not be efficient as I would be getting messages from thousands of markets.

My idea would be to have an asynchronous process running every hour populating a file with the markets I want to be trading. Then I would have to embed the logic to check this list in Flumine and remove and add the markets according to this list. What would be the right way to do so? For the Flumine part where I would be checking the list, I woulnd't mind if this was to be done on every book update, as it is a quick check, but I certainly want to avoid subscribing to 10k markets and performing the filter checks every time there's an update in one of these markets.

*Tags: Strategies*

---

**Newbie99** - *17:04:22*

It sounds like you are using Flumine (and therefore streaming), in which case what you are proposing isn't possible (and I suspect most on here would suggest not desirable either).



The way to achieve what you want is to subscribe to all markets that meet some basic criteria and then have your own filter (as you suggested).



The only other way, would be to poll via the REST API, but you may find its just easier to use the streaming approach.

*Tags: General Technical*

---

**AI Trader** - *17:11:54*

Thanks [@UFTBRB3F1](@UFTBRB3F1)!

Are you saying it is not possible to add and remove markets? Because there is the remove_market function from the BaseStrategy class.



Once I am running the strategy, is it possible to add and remove a market from the framework through Flumine? (I want to avoid restarting the framework).

*Tags: Strategies*

---

**Newbie99** - *17:16:31*

Markets are automatically added via the Framework once they are created on the server side. If the market meets your criteria (as specified on startup) then it will get added via Flumine into your cache.



Once the market is closed Flumine receives the message from Betfair and the market is removed.



Flumine is designed to be up and running for days/weeks/months on end, whilst you could stop and re-start frequently, most people wouldn't use it in that way (I suspect).

*Tags: Deployment*

---

**AI Trader** - *17:19:38*

I understand that. What I am trying to achieve here s to select a subset of say, 100 markets, out of 10,000 markets that pass through the basic filter provided by Flumine. I don't want to subscribe to every update of these 10,000 markets as this would be unnecessary and slow. Are you saying that adding and removing markets after the framework is running is impossible?

*Tags: Performance*

---

**Peter** - *18:32:25*

For clarity, the limitation isn't in Flumine, it's in the streaming from Betfair. Once Flumine has subscribed to market stream, it's Betfair that adds or removes markets from the stream depending on the criteria passed to it during the initial subscription. Flumine merely reacts to the changes.

*Tags: General Technical*

---

## 2022-12-05

**Jorge** - *10:48:39*

What is the logic followed in Flumine Simulations to know if a "maker" order that I place would get matched or not?

*Tags: General Technical*

---

**liam** - *10:50:08*

Start [https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/flumine/markets/middleware.py#L183|here](https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/flumine/markets/middleware.py#L183|here)

*Tags: General Technical*

---

## 2022-12-08

**EJono** - *15:48:10*

Im trying to publish the standard output logs from flumine to azure for storage but cant seem to get any outputs. Is there any reason why the below wouldnt output anything.



```from .strategies import test_strategy

from .handlers import AzureLogHandler



logger = logging.getLogger()

logger.addHandler(AzureLogHandler)

logger.setLevel(logging.DEBUG)



framework.add_strategy(test_strategy)



framework.run()```

I can see log messages i set up from inside the "process_market_book" function of "test_strategy" so im sure the handler is working correctly but nothing other than these manual messages are appearing. Do i need to refer to a specifc logger name in the "getLogger(...)" method? Does this have to execute at the root level to work correctly, iniside "__main__"? or something else?

Feel like am im missing something fundamental about logging or otherwise simple.

Cheers!

*Tags: Errors Debugging, Strategies*

---

## 2022-12-09

**liam** - *08:22:26*

What do you mean no outputs? Nothing to stdout or nothing to azure? If you want logs to stdout then you need to add `StreamHandler` as per the examples



```logger = logging.getLogger()



log_handler = logging.StreamHandler()

logger.addHandler(log_handler)

logger.addHandler(AzureLogHandler)

logger.setLevel(logging.DEBUG)```

If you want the proper flumine logs then you also want to add the `jsonlogger` formatter as per the examples

*Tags: Errors Debugging*

---

**EJono** - *16:04:55*

its specifically nothing out to azure (excluding my own [http://logger.info|logger.info](http://logger.info|logger.info)("...") messages). Following the example in the docs would this be the correct way to implement both handlers such that the "proper flumine logs" are outputted  via the AzureLogHandler?



```logger = logging.getLogger()

logger.addHandler(AzureLogHandler)



custom_format = "Internal Flumine Log: %(asctime) %(levelname) %(message)"

stream_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

stream_handler.setFormatter(formatter)

logger.addHandler(stream_handler)



logger.setLevel(logging.DEBUG)```

I would like the behaviour of both handlers.

*Tags: Errors Debugging*

---

## 2022-12-10

**R** - *20:23:50*

This might be a stupid question but it just hit me:  do the betfair selection IDs ever change between markets on the same event?  e.g. Horse A is always Horse A no matter if I am looking at PLACE, WIN, AvB, etc?



I mean I presume not, but thought I would ask to be sure

*Tags: General Technical*

---

**Peter** - *21:18:10*

Haven't seen it differ for horses, but I have seen differences (errors) for golf so it is possible.

*Tags: Errors Debugging*

---

**R** - *21:41:13*

Thanks, much appreciated.  I'll look into adding some checks to avoid problems

*Tags: General Technical*

---

## 2022-12-11

**AI Trader** - *04:40:16*

Hi guys,

Could someone help me to understand how simulated__profit is computed in the SimulatedClient() ? I am using it to backtest a strategy that uses the stream. I guess my main questions are:

1 - Is the simulated__profit the profit of an order at the END of the match? Or is it when I have an order of the same amount and opposite side (eg first back 10 and then lay 10)?

2 - At what time could I consider to have realized the profit? I imagine it won't be in the placed_date field?



My goal is to reconstruct a pnl timeseries of unrealized pnl.

*Tags: Strategies*

---

**Newbie99** - *16:12:22*

I get the impression they only seem to tweet about it when someone points out there's a problem (rather than monitoring and sending out automated tweets).

*Tags: General Technical*

---

**AI Trader** - *22:08:38*

*CANCEL ORDERS BEFORE STOPPING TRADING THE MARKET*

Hi guys,

I want to cancel all my orders before I start trading the market. Currently, the way I "stop" trading it is by placing conditions on `check_market_book`

I am not sure this is the right way though, as it seem that whenever `check_market_book` returns *False*, there is no call to any function like `process_closed_market`, `finish` (see code below from `baseflumine.BaseFlumine._process_market_books`.



Could someone advise:

1 - Where should I add my condition to check if I want to stop trading the market

2 - What is the right function to override in order to close all my orders before exiting the market?



Any help is hugely appreciated. Tks



```for strategy in self.strategies:

    if utils.call_strategy_error_handling(

        strategy.check_market, market, market_book

    ):

        utils.call_strategy_error_handling(

            strategy.process_market_book, market, market_book

        )```



*Tags: Errors Debugging, Strategies*

---

## 2022-12-12

**liam** - *08:57:39*

I don't understand the first sentence, is there a word missing?



As you can see `check_market_book` will only block `process_market_book` and `process_closed_market` will still call, `finish` will be [https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/flumine/strategy/strategy.py#L151|called before flumine ends](https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/flumine/strategy/strategy.py#L151|called before flumine ends)

*Tags: Strategies*

---

**dont** - *10:17:13*

Hey folks, I was just trying out flumine, is there a place to get the username of a betfair account ? I only used the email to login so far, and I can’t see it anywhere in the profile. The `betfairlightweight` api doesn’t seem to be working with the email address.

*Tags: General Technical*

---

## 2022-12-16

**AI Trader** - *02:17:43*

[@U4H19D1D2](@U4H19D1D2) There was a typo, yes. What I meant is that I want to cancel all pending/open orders before I *stop* trading a market.



In that case, should I do the following?



```def check_market_book(self, market, market_book):

    exit_market : boolean = check_exit_market()



    if not exit_market:

        if market_book.status == "OPEN":

            return True

    else:

        for runner in market_book.runners:

            open_orders = market.blotter.strategy_selection_orders(self, runner.selection_id, runner.handicap, order_status=[OrderStatus.EXECUTABLE, OrderStatus.PENDING])

            for order in open_orders:

                market.cancel_order(order)

        self.remove_market(market.market_id)```

*Tags: Strategies*

---

## 2022-12-17

**Matthieu Labour** - *19:03:40*

Hi, when recording market data for 2 sports, do you recommend that I use one strategy for both sports or 2 strategies (one per sport)? In other words, would you create 2 instances of [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py). One per Sport or one only. Is there performance penalty in having multiples and specializing him.

*Tags: Performance, Strategies*

---

**Peter** - *19:52:36*

A possible consideration is the number of markets at the time when you will be instantiating the collection process. You will have a limit on the number of markets to which you can subscribe (initially), and a separation by sport may help you to stay below that limit. Indeed some sports (I'm looking at you soccer) may need more than one collection instance even with a generous limit.

*Tags: General Technical*

---

## 2022-12-19

**Matthieu Labour** - *22:36:09*

How can I access flumine from the Strategy? I want to do something like the following. Thank you!

```    def process_raw_data(

        self, clk: str, publish_time: int, datum: dict

    ) -&gt; None:

        flumine.handler_queue.put(CustomEvent...```

*Tags: Strategies*

---

## 2022-12-20

**shashank bansal** - *11:07:30*

Hi, Has anyone tried streaming using aws or any other serverless? And any recommendations on the same?

*Tags: Deployment*

---

**Mo** - *11:11:20*

I use AWS (EC2)

*Tags: Deployment*

---

**shashank bansal** - *11:15:33*

I am planning to build below mentioned architecture.

[https://aws.amazon.com/blogs/database/store-and-stream-sports-data-feeds-using-amazon-dynamodb-and-amazon-kinesis-data-streams/](https://aws.amazon.com/blogs/database/store-and-stream-sports-data-feeds-using-amazon-dynamodb-and-amazon-kinesis-data-streams/)



But I need to send the sports feed to the API Gateway and thinking of using some serverless approach.

*Tags: Deployment*

---

**Matthieu Labour** - *12:38:41*

In other words, is there a backpointer to flumine in the strategy?

*Tags: Strategies*

---

**liam** - *13:09:24*

The common pattern for this is flumine / marketRecorder -&gt; AWS S3 -&gt; smart_open -&gt; :money_with_wings: :moneybag: 

*Tags: Deployment*

---

**shashank bansal** - *13:39:27*

I want to create an interpreter system where people subscribe to a server and server sends the feed from BF to subscribers. There can be 1000s of connection and can burst out the system with so many odds every few ms. S3 will slow down the latency for sure. So, was thinking towards Dynamodb.

*Tags: Performance, Deployment*

---

**Mikkel** - *22:22:52*

Hi all,



Some time ago I made the research for a profitable system for French horse racing. I'm now looking to expand this to an auto system on Betfair on and on UK races. Does anyone know of a database of fundamental data for uk horse racing - would be nice if it was easy to access through an API. Or does any know of any written guides for automating betting on Betfair using Flumine, etc? :slightly_smiling_face:

*Tags: Strategies*

---

## 2022-12-21

**R** - *08:51:13*

As far as I know:



1. TimeForm ($$$)

2. Proform ($$)

3. Webscrapers ($ / time)

4. Flumine / Market recorder to gather data from the exchange (but you might not call this fundamental data)

For written guides, I guess check out the betcode repos



[https://github.com/betcode-org/betfair](https://github.com/betcode-org/betfair)

[https://github.com/betcode-org/flumine](https://github.com/betcode-org/flumine)



or the betfair developer site itself, which has some docs too (can't get a link right now, good ole VPN issues)

*Tags: Data Quality*

---

**R** - *13:19:00*

Yep, issue with scrapers is if the site layout / schema changes then you need to rewrite.  I avoid them for this reason.



as far as comparison to paid data services, I can't say but I would reason that the services are paid because they are more valuable than what is available on public websites.



on 4) check out the lowestlayer here : [https://github.com/betcode-org/flumine/tree/master/examples/strategies](https://github.com/betcode-org/flumine/tree/master/examples/strategies) and you can pair this with a market recorder

*Tags: Data Quality*

---

## 2022-12-23

**Michael** - *00:43:29*

[@U4H19D1D2](@U4H19D1D2) what does the pricing look like just for your streaming?

*Tags: General Technical*

---

## 2022-12-27

**Liam Querido** - *20:38:50*

Oh really? Can you paper trade using the live app key and flumine?

*Tags: Deployment*

---

**liam** - *20:40:55*

Yes but it’s local paper trading, basically using the flumine simulation engine on live market data 

*Tags: Deployment, Strategies*

---

## 2022-12-28

**Mark Zheng** - *19:52:19*

Hi all, I'm currently learning how to backtest with Flumine by following this tutorial:

[https://betfair-datascientists.github.io/api/How_to_Automate_5/](https://betfair-datascientists.github.io/api/How_to_Automate_5/)



I am running into an issue where several values such as 'profit' and 'size_matched' are all 0, meaning I can't create any meaningful insight. I've tried using the exact "Back the favourite" code, including using a similar dataset but the issue still persists. As far as I can tell, the only things that I have different is that I have the basic data. Has anyone ran into this issue before or has an idea on what might be causing it?

*Tags: Data Quality*

---

## 2022-12-29

**Ralegh** - *20:24:19*

Suggestions on which EC2 instances to use with flumine? Think I’m going to go for C6/7s for simulating but not sure what I need for live (the actual strategies aren’t super maths heavy)

*Tags: Deployment*

---

**Fab** - *20:30:05*

Mo and I couldn't reach an agreement. :smile:



So, if anyone else can help with this, please get in touch.

*Tags: General Technical*

---

## 2022-12-30

**Ralegh** - *10:41:13*

Extra Q; would using the market recorder be a better choice than buying 50ms data? Or is 50ms intervals enough?

*Tags: Data Quality*

---

**Mo** - *10:54:32*

Possibly irrelevant as I don't use flumine and my architecture is quite different but I use m5.2xlarge for trading (multiple sports)

*Tags: Strategies*

---

**PeterLe** - *11:43:22*

I run a few instances locally on my home PC and noticed the last couple of days that I had high latency (circa 0.6)...

Today when i started them, I had no latency messages.

I tend to run them anyway even when I get the messages, so not too concerned

What would you think is the main reason and does everyone get theses from time to time? Thanks

*Tags: Performance*

---

**D C** - *11:51:10*

I use m5.xlarge but it is way more than I need - but like Mo I don't use flumine and run on a much smaller scale (horses only).

*Tags: General Technical*

---

**Ralegh** - *12:17:49*

Thanks for replies, any specific reasons not to use flumine? Or is it just how you guys built your infra out

*Tags: General Technical*

---

**Mo** - *12:19:33*

flumine didn’t exist 12 years ago 

*Tags: General Technical*

---

**Mo** - *12:22:00*

I also have opinions on the design which is not to cast any shade on flumine - more a question of personal preference. Different ways of approaching problems 

*Tags: General Technical*

---

**Ralegh** - *12:32:18*

Ah I see yeah makes sense, I’ve been using it a lot so fairly used to the interface, only annoyance is simulation speed but that’s probs a python issue, just gonna wack on a c6a with joblib and pray haha, much better to be slower and more accurate though, lifesaver

*Tags: Performance*

---

**Mo** - *12:38:49*

Have you read [https://betcode-org.github.io/flumine/performance/](https://betcode-org.github.io/flumine/performance/)?

*Tags: Performance*

---

## 2022-12-31

**Ralegh** - *22:20:58*

Anyone used flumine with pypy? Absolutely chewing up memory

*Tags: Performance*

---

## 2023-01-02

**Fab** - *16:12:36*

Here's a question for Flumine experts, related to the code below, executed on real-time Betfair markets (no backtesting).



I need to capture the winner of a market, but it looks like execution never gets to the `# DO SOMETHING` line.



For the record, the code is definitely reachable: putting a `print(runner.status)` statement just below the `for` line, always prints `ACTIVE` when the market is still open.



*Question: does Flumine trigger `process_market_book` even on the very last message received by Betfair streamer? (market status == CLOSED)*



```def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:

    for runner in market_book.runners:

        if runner.status == "WINNER":

            # DO SOMETHING```

*Tags: General Technical*

---

**AI Trader** - *16:18:50*

*FLUMINE BACKTETER* 

Guys, I wanted to ask a couple of questions regarding the backtester.



1 - How does the backtester decide if an order would be executed? Is it only based on traded_volume? Or it also considers orders executed if the prices in the orderbook would have crossed your odd (even though there was 0 traded volume in the meantime) ?

Example: I put a BACK order at odd 1.4 when the prices in the book are: (best available to back = 1.2 and best available to lay = 1.3). After 10 minutes there have been no trades, but the odds in the orderbook shifted to : best available to back = 1.5 and best available to lay = 1.6 with 0 traded volume in the period.

Assuming the backtester would consider any execution in this scenario,  what are the assumptions regarding the executed volume ? Of course, if I had put 100 million dollars in this order, it wouldn't be completely filled, but would certainly be partially filled in real life.



2 - What are the assumptions on the priority queue? Does the backtester only execute your order if the traded volume crosses your price (meaning if you have a BACK open order at odd 1.4, and there is a trade at 1.4, how does the backtester handle the priority queue? Will it execute anything? Or only executed if there is a trade with odd greater than 1.4?



The reason I am asking is because it seems that the backtester shows way less execution than what I get in reality. Any personal experiences regarding this would be very much appreciated

*Tags: General Technical*

---

## 2023-01-03

**AI Trader** - *02:49:41*

This is extremely helpful to understand [@U4H19D1D2](@U4H19D1D2)  thank you so much 

*Tags: General Technical*

---

**Andrew** - *02:50:32*

I was just searching this chat for the same question :+1::skin-tone-2: :blush: 

*Tags: General Technical*

---

**birchy** - *21:54:58*

Bit of a noob question, but what's the recommended beginner friendly way to manage/automate python venv's on a Linux production machine? To be more specific, I've only used venv's created by Pycharm on my development machine but now I'm getting to a point where I have old bots chugging away in production but need to run others with newer libraries . Venv is the obvious way to achieve this without fucking up the older ones running on the main system, but I'm wondering how best to automate venv installation and setup for spinning up new bot instances on a single machine?

*Tags: Getting Started, Deployment*

---

**foxwood** - *22:03:46*

I got in a right muddle with PyCharm switching back from 3.10 to 3.9 because it broke something. You'd have to create a new project as I understand it for you new bots and set the environment for that new project. Think found my answers at [https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html#modify_interpreter](https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html#modify_interpreter)

*Tags: General Technical*

---

**D** - *22:07:00*

I use virtualenv in production, along with tmux. It's a pretty straightforward way of creating independent python environments.

*Tags: Deployment*

---

**birchy** - *23:03:49*

[@U02RN7YDRQ9](@U02RN7YDRQ9) I'm fine with Pycharm on my development machine. My production stuff is on a headless Linux VPS. :+1: 

[@URMM9463X](@URMM9463X) I'm thinking more along the lines of automating the setup &amp; launch via a script. It seems like a common workflow for which management tools would already exist but Google has muddied the waters and/or I'm not using the correct terminology. My basic manual workflow would be something like:

1. login to remote server via SSH

2. create venv

3. activate venv &amp; install libraries (Flumine, etc)

4. git clone strategy code

5. launch python strategy

6. logout of SSH

*Tags: Getting Started, Deployment, Strategies*

---

## 2023-01-04

**Unknown** - *01:48:00*

I'd suggest [https://pipenv.pypa.io/en/latest/|Pipenv](https://pipenv.pypa.io/en/latest/|Pipenv) for managing the spec for a virtualenv, it can create (by default IIR) a virtualenv under `.venv` (you don't want to commit that), and uses a `Pipfile` and `Pipfile.lock` to manage what goes into the venvs it manages.



The useful thing is that you specify the package constraints in your `Pipfile` (e.g. `pandas ~= 1.1.0`),  then you can tell it to update the lockfile with the latest versions that satisfy the constraints, and you can sync your environments using `pipenv` - perfect for developing services with virtualenvs.

*Tags: Feature Engineering*

---

**Mo** - *07:18:13*

[@U01C12ZEADQ](@U01C12ZEADQ) I assume from the phrasing of your question that you've seen the [https://www.sciencepublishinggroup.com/journal/paperinfo?journalid=155&amp;doi=10.11648/j.ajss.20170506.12|Clarke et al](https://www.sciencepublishinggroup.com/journal/paperinfo?journalid=155&amp;doi=10.11648/j.ajss.20170506.12|Clarke et al) paper promotes the use of the power method but you're looking for more practical evidence?

*Tags: General Technical*

---

**D** - *09:04:11*

[@U016TGY3676](@U016TGY3676) understood - I don't create new environments that often in production so I'm setting up manually. And in dev I'm letting Pycharm create its own envs for me.

*Tags: Deployment*

---

**dont** - *22:55:32*

ansible - bleah. You can just `pip freeze` your venv into a `requirements.txt` which then you get to install on the production machine. Alternatively you can just work with a docker container as Liam says on both dev/prod and deploy the container.

*Tags: Getting Started, Deployment*

---

**dont** - *22:57:30*

`pipenv` related workflows that might help:

[https://github.com/pypa/pipenv/blob/main/docs/advanced.rst](https://github.com/pypa/pipenv/blob/main/docs/advanced.rst)

*Tags: General Technical*

---

**Fab** - *23:16:37*

My suggested approach is also Docker. Here's a simple blueprint to get you started...



Assuming you have two Python programs (`run-oldstrategies.py`, `run-newstrategies.py`), we are on your dev machine (Linux Debian) in the directory that contains those two programs.



*Step 1 - Create separate environment requirement files*

Activate the environment containing libraries for the old strategies then do `pip freeze &gt; requirements-oldstrategies.txt`. Do the same for the new strategies environment to generate `requirements-newstrategies.txt`.



*Step 2 - Create separate Dockerfile(s)*

Create two empty text files: `Dockerfile-oldstrategies`, `Dockerfile-newstrategies`. As an example, here's what you could put inside Dockerfile-oldstrategies (you can easily figure out the other):

```FROM python:3.9-slim-bullseye

RUN apt update -y

WORKDIR /opt/myproject

COPY ./requirements-oldstrategies.txt .

RUN cat requirements-oldstrategies.txt | grep -v '==0.0.0' &gt; requirements-oldstrategies.sanitised.txt

RUN pip install --no-cache-dir --upgrade -r requirements-oldstrategies.sanitised.txt

COPY . .

ENTRYPOINT ["python", "run-oldstrategies.py"]```

*Step 3 - Install Docker*

[https://docs.docker.com/engine/install/debian/](https://docs.docker.com/engine/install/debian/)



*Step 4 - Build separate Docker images for the two programs*

```docker build -t run-oldstrategies -f ./Dockerfile-oldstrategies .

docker build -t run-newstrategies -f ./Dockerfile-newstrategies .```

*Step 5 - Install Docker on your production machin*e

Same as Step 3



*Step 6 - Transfer Docker images from your current machine to the production machine*

```docker save run-oldstrategies | bzip2 | ssh &lt;USER&gt;@&lt;PRODUCTION_IP&gt; docker load

docker save run-newstrategies | bzip2 | ssh &lt;USER&gt;@&lt;PRODUCTION_IP&gt; docker load```

*Step 7 - Log into production machine, make sure images are there*

```docker images```

*Step 8 - Start both Docker containers*

```docker run -d run-oldstrategies

docker run -d run-newstrategies```

*Step 9 - How to check logs for a Docker container*

Run `docker ps -a` to get the hash of each container then you run the following command:

```docker logs &lt;CONTAINER_HASH&gt;```



*Tags: Getting Started, Deployment*

---

**Oliver** - *23:23:20*

Docker/containers have merits, and can be another layer to go with virtualenvs if you want, or you can cut out the virtualenv entirely because you are installing packages straight inside the container instead of a containerised virtualenv. I don't know your deployment experience/environment, but you may find docker is OTT.



I'd say docker/containers (as there are other alternative container runtimes) is an additional isolation/encapsulation layer which can be pretty cool but does have management overhead. You could go in other directions for similar effect, like using packer to produce virtual machine images to then deploy to machines (e.g. EC2/GCP/... instances), and in that or outside of that you could use things like ansible/chef/puppet (ansible probably is best for this kind of smaller scenario thing so can see why Mo mentioned it), and also a layer like terrafrorm (for extra info: terragrunt is also a good layer for it but I doubt so for the scale required in this case) above that to manage the provisioning of your infrastructure (e.g. from the point of spinning up cloud resources if you've gone that way).



I'd still put emphasis on the likes of Pipenv as your first stop as it isn't conceptually (or in operations) much more than the virtualenvs you are using, but has things that are significant for long term management and reproducibility (i.e. exact package version pinning using the lockfile, a bit like using `pip freeze &gt; requirements.txt` but without being a such a mess to untangle when you want to upgrade something). Another good thing is pipenv/virtualenvs will continue to work nicely with editors and you can fire them up easily and without extra permissions on machines (although the permissions is likely not an issue for you).



Just for more info: a thing from before the likes of Pipenv was [https://pypi.org/project/pip-tools/|pip-tools](https://pypi.org/project/pip-tools/|pip-tools) which isn't far removed from the niceness of Pipenv. That will do the dependency locking side of things, but you'd still want to make the environments you install into (fresh VMs, constainers, virtualenvs, or bare metal).



If I blogged or was in consultancy, I'd be inclined to draw out graphs (or a graph) showing the potential dependencies of provisioning with spiel about the different parts...

*Tags: Getting Started, Deployment*

---

## 2023-01-05

**river_shah** - *08:09:02*

If `order_1` fails either flumine internal risk limits or betfair limits, is `order_2` guaranteed to fail?

```with market.transaction() as t:

    t.place_order(order_1)

    t.place_order(order_2)```



*Tags: General Technical*

---

**birchy** - *09:57:08*

Thanks all, I need to get my R&amp;D cap on and see which of your suggestions are best suited to my operation. My main requirements are to be able to easily deploy multiple Flumine instances in a way that allows for future expansion. Not sure if I need a full container setup, but will investigate and ask questions later... :grinning:

*Tags: Getting Started, Deployment*

---

**birchy** - *13:30:55*

Thanks [@U4H19D1D2](@U4H19D1D2). Having read a handful of articles, I was swaying towards Ansible but now I'm not so sure, particularly given that AWS Lightsail (my preferred VPS as it's a simplified EC2 setup) has capability to add containers by simply supplying a docker file. I know you use EC2, but is your preference to run lots of small instances, or to have a larger one with multiple containers? With Docker, are there any issues with running different versions of python on a Linux (Ubuntu) VPS? For example, if my Ubuntu version comes with python 3.9, how easy is it to install 3.11 inside a container? I've had issues in the past with installing newer versions as it can get a bit messy, particularly if we have to resort to using PPA's.

*Tags: Getting Started, Deployment*

---

**dont** - *13:35:42*

It shouldn't be very hard to install a new version. You can also just use one image that comes with python 3.11 preinstalled.

*Tags: Getting Started*

---

**Mo** - *17:44:46*

I'm into low latency stuff so Docker isn't really compatible with that approach - I don't have anything against it though



We use ansible to manage all of our infrastructure - in addition to what Python virtual environments should be set up with what packages installed we also use it to declare what OS packages should be installed, what cron jobs should be created, what services should be running, what directories should exist, which OS users should exist, etc. etc. I certainly wouldn't recommend it if the only thing you need to solve is how to manage your Python virtual environments. But if you wanted a tool that let you manage them on top of everything else I mentioned then it is very powerful

*Tags: Getting Started, Performance*

---

**dont** - *17:46:41*

Sorry I missed your previous comment.



Yeah my point is exactly what you mentioned above - that ansible is much more powerful than just managing your Python environment so (in my opinion) it’s a bit of an overkill to use ansible just for that.

*Tags: General Technical*

---

## 2023-01-06

**Peter** - *09:35:23*

CurrentOrder is a betfairlightweight class that closely follows the data structure used by Betfair. SimulatedOrder and BetfairOrder are Flumine classes that take a more nuanced approach, restructuring and expanding the data available. You can find the price and the size as attributes of their order.order_type objects.

*Tags: General Technical*

---

**Ralegh** - *17:13:40*

Is it common for prices to skip several levels (e.g 3.2-3.6 to 3.8-4.2) in one tick, I feel like I’m not receiving some updates from betfair as I won’t have an update on one contract for a second or two and then it completely jumps, using flumine. Might try just streaming from bflw 

*Tags: General Technical*

---

## 2023-01-07

**Peter** - *09:50:21*

[@U03FS7KM2NL](@U03FS7KM2NL) Re-reading your question ... where are you observing no updates for a second or two? The website UI, bflw polling or flumine?

*Tags: General Technical*

---

**Ralegh** - *12:34:46*

Flumine, I’m logging directly from get_price(available to back). There’s a little bit of logic before that but I’m pretty sure it shouldn’t be skipping prices unless they have a big delay between publish time and current time (&gt;500ms), when I changed the threshold it didn’t change the jumps. 

*Tags: General Technical*

---

**Meow** - *13:02:00*

Hey anyone know how to cancel a particular betid? I have tried this



`cancel_order_filter = betfairlightweight.filters.cancel_instruction( bet_id = '292096868384')`

`cancel_order = trading.betting.cancel_orders(instructions = cancel_order_filter)`



This snippet of code errors out.

*Tags: Errors Debugging, Strategies*

---

**Mo** - *15:55:49*

```cancel_instruction = betfairlightweight.filters.cancel_instruction(bet_id="292096868384")

cancel_execution_report = trading.betting.cancel_orders(instructions=[cancel_instruction])```

*Tags: Strategies*

---

## 2023-01-08

**Unknown** - *01:07:34*

Thanks [@UBS7QANF3](@UBS7QANF3), I have tried that and it still errored out?

*Tags: Errors Debugging*

---

## 2023-01-09

**Andrew** - *09:35:37*

What’s the best way, with flumine, to handle this when both the price and size are to change? Clearly it’s cancel and then place. But if I’m updating too quickly (a feature of my strategy) how do I best avoid a “race” where the order to be cancelled isn’t yet executable (ie. cancellable)?

*Tags: Feature Engineering, Strategies*

---

**Andrew** - *09:39:42*

Half answering my own question, I think o need to implement a wait queue for the new order to wait for the cancellation to be confirmed, but that will introduce a lag on placing the updated new order.

*Tags: General Technical*

---

**Andrew** - *11:02:19*

How do I confirm an order has been cancelled after market.cancel_order(order)? I think I have a race condition trying the place an order before cancellation. On market that’s not a problem but my strategy requires single live order per runner at any moment, plus need to ensure order has cancelled and not executed (before cancel instruction received).

*Tags: Deployment, Strategies*

---

**Artiom Giz** - *13:16:12*

Hi guys!

Does anybody use *Java* for *orders operations* (place/cancel/update/...)?

What is the best/fastest way to get into - simply implement all API calls by myself or there is something ready I can use?

For Python it's done by _betfairlightweight_, looking for something similar for Java :pray:

*Tags: General Technical*

---

**D C** - *13:22:33*

I've not come across anything in a mature state for Java like BFLW is for Python but this repo has some Java code exanples

*Tags: General Technical*

---

**D C** - *13:22:57*

Can't vouch for the quality but if you are dead set on Java it might help you start out.

*Tags: General Technical*

---

**thambie1** - *14:20:35*

I use Java, and started with the codebase [@UUE6E1LA1](@UUE6E1LA1)  linked a few years ago. This is the best starting point for Java I'm aware of. Unless you really need a high performance language, I'd recommend going with betfairlightweight. You'll save a lot of time, particularly with getting a backtesting system out of the box.

*Tags: Performance*

---

**PeterLe** - *15:12:46*

Thanks Alessio, this is just a standard drive (ie not SSD)...so if I slowing increase the setting, at some point it will be too slow and errors would occur?

*Tags: Errors Debugging, Performance*

---

**thambie1** - *15:42:51*

Yeah, that codebase for the rest API calls (placing orders, cancelling orders, etc). And then the following codebase for the streaming market data: [https://github.com/betfair/stream-api-sample-code/tree/master/java|https://github.com/betfair/stream-api-sample-code/tree/master/java](https://github.com/betfair/stream-api-sample-code/tree/master/java|https://github.com/betfair/stream-api-sample-code/tree/master/java)

*Tags: General Technical*

---

**Ralegh** - *17:34:55*

Is there a good way to download historical data straight to AWS? Rather than scp, would be a lot faster, I tried using the link from the website but didn’t work

*Tags: Data Quality, Deployment*

---

**D** - *17:43:13*

How did you do it and what was the error message?

*Tags: Errors Debugging*

---

**Mo** - *17:56:13*

Better to use the API functionality in betfairlightweight no?

*Tags: General Technical*

---

## 2023-01-11

**Andrew** - *20:26:04*

Just had a wonder after doing some reading above. When using FlumineSimulaton does SimulationMiddleware need to be added to the framework? I can’t see in the source where it is add for simulation. The examples don’t show the need to add it.

*Tags: General Technical*

---

**Lee** - *20:37:59*

[https://github.com/betcode-org/flumine/blob/master/flumine/baseflumine.py#L90|https://github.com/betcode-org/flumine/blob/master/flumine/baseflumine.py#L90](https://github.com/betcode-org/flumine/blob/master/flumine/baseflumine.py#L90|https://github.com/betcode-org/flumine/blob/master/flumine/baseflumine.py#L90)

*Tags: General Technical*

---

## 2023-01-12

**Carl** - *14:17:19*

Hi [@U034LK55D3P](@U034LK55D3P) , [@U03FS7KM2NL](@U03FS7KM2NL), Carl from Betfair here. I can send you the full spec, please message me on [mailto:carl.albon@betfair.com|carl.albon@betfair.com](mailto:carl.albon@betfair.com|carl.albon@betfair.com) . Timeform are part of our group so I can get the answers to any ad hoc questions you may have. In terms of cost, the last quote I got was £13k p.a. for the full suite of horse racing data (I can normally get some discount for up-front payment), 12 months minimum subscription. Greyhounds is a little less p.a.

*Tags: General Technical*

---

**R** - *17:18:41*

Doing some due diligence on a strategy at the moment and realized I have not been detecting some last-minute changes in races.



Does anyone know if the `status` field in a `Runnerbook` can be used to check if runners have been dropped out or am I mis-reading the docs?  Just wondering if anyone has experience here before I PR a filter that doesn't actually work!

*Tags: Strategies*

---

## 2023-01-13

**Gooat** - *14:31:33*

Any of the power pandas users swapping out for Polars?

*Tags: Feature Engineering*

---

**river_shah** - *14:52:00*

```Going big...

Do you expect more than 2^32 ~4,2 billion rows? Compile polars with the bigidx feature flag.



Or for python users install pip install polars-u64-idx.```

*Tags: Getting Started, Feature Engineering*

---

**Peter** - *15:01:38*

Looked at it a couple of times, but each time felt that the overhead of adjusting to it outweighed the performance benefits. For context my dataframes can run to several million rows, but I'm a long way short of the billions that polar seems designed to handle.



Worth adding that the real cost of the switch is very difficult to evaluate. It's fairly easy to understand "faster" and to see from the docs what can be done using Polars. But the real pain point would come in the edge cases when I hit an "oh shit" moment as I realise that this thing I do in Pandas may not exist in Polars and I've got to code it up myself or keep using Pandas anyway.

*Tags: Feature Engineering, Performance*

---

**R** - *15:13:06*

also, billions of rows with an in-memory data structure?



I'd be more inclined to use out-of-core compute, e.g. streaming + database / indexed data

*Tags: Performance*

---

**Gooat** - *23:18:20*

Appreciate the input, don't get distracted by the Rust...stick with pandas and some tweaks until you a hit wall.

*Tags: Feature Engineering*

---

## 2023-01-14

**Guy Incognito** - *10:29:18*

I've been looking into market on close order and also limit on close order I'm placing small bets 3 - 4 quid is this big enough to ever move the bsp? I'm using the flumine simulations and aren't fully getting matched, does this mean I would have moved bsp?

*Tags: General Technical*

---

**Ralegh** - *10:48:23*

I had a go rewriting my stuff in polars and exactly as [@U9JHLMZB4](@U9JHLMZB4) said, edge cases meant I ended up going back without finishing. I really like the interface, just couldn’t figure out how to do a few critical things, rolling stuff and reindexing for example (there’s no indexes but I couldn’t find a way to do the equivalent with a column)

*Tags: General Technical*

---

**Jeff Waters** - *14:29:55*

Manchester United are now just one point behind City in the Premier League table: [https://www.bbc.co.uk/sport/football/premier-league/table](https://www.bbc.co.uk/sport/football/premier-league/table)



However, they are miles behind them in the Premier League winner market odds: [https://www.betfair.com/exchange/plus/football/market/1.199506934](https://www.betfair.com/exchange/plus/football/market/1.199506934)



It may well be that United have tricker fixtures still to come in the season, but even so, intuitively the gap seems way too high to me. 15.0 vs 2.1



Anyone agree/disagree?

*Tags: Errors Debugging*

---

**Lee** - *20:47:33*

Looks like for SP bets it fully matches whatever size is asked [https://github.com/betcode-org/flumine/blob/master/flumine/simulation/simulatedorder.py#L306](https://github.com/betcode-org/flumine/blob/master/flumine/simulation/simulatedorder.py#L306)

*Tags: General Technical*

---

## 2023-01-15

**Trex44** - *18:14:42*

Hey all, for the Flumine [https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/examples/strategies/marketrecorder.py|marketrecorder](https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/examples/strategies/marketrecorder.py|marketrecorder). Other than passing the name of the bucket as a string in the context kwarg dict is there anything else that needs to be done to ensure the recorded markets are moved to the bucket? At the moment the files are recorded and zipped but aren't being uploaded to the bucket despite me passing the bucket name. the files is then automatically deleted. I have the correct IAM role assigned to the EC2 instance to allow it to access S3 so I am uncertain what the issue is.

*Tags: Deployment*

---

## 2023-01-16

**Trex44** - *15:40:20*

But I get nothing at all regarding any attempt to move the files to S3 prior to deletion. Tried hard coding the file path into the relevant object too but that didn't work. I am calling the strategy from a strategies folder where it sits in its own script rather than adding it to the strategies script but I don't see why this would have an effect.

*Tags: Strategies*

---

**river_shah** - *16:24:56*

If a worker dies with outstanding positions, upon restart if `customer_strategy_ref` remains the same, does that ensure that `market.blotter` bootstraps all the orders for that market? If that is the case, can I please be pointed to the code how positions are bootstrapped

*Tags: Strategies*

---

**Trex44** - *17:37:14*

Never used it my self but found this on stack overflow



_Until the market is actually closed, the starting price can only be estimated, and Betfair have two methods for this, the *far price* and the *near price*._



[https://sports.stackexchange.com/questions/10697/how-to-calculate-the-far-and-near-price-of-betfair](https://sports.stackexchange.com/questions/10697/how-to-calculate-the-far-and-near-price-of-betfair)

*Tags: General Technical*

---

**Trex44** - *20:18:25*

In the [https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/examples/strategies/lowestlayer.py|LowestLayer](https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/examples/strategies/lowestlayer.py|LowestLayer) example should the check_market_book method be called as soon as framework.run() is called? It doesn't seem to be being called at the moment and I can't get a bet placed.

*Tags: General Technical*

---

**Trex44** - *20:56:59*

```{"asctime": "2023-01-16 20:43:47,370", "levelname": "INFO", "message": "Client added", "username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": null, "transaction_count_total": null, "trading_controls": [], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-01-16 20:43:47,371", "levelname": "INFO", "message": "Adding client control MAX_TRANSACTION_COUNT"}

{"asctime": "2023-01-16 20:43:47,371", "levelname": "INFO", "message": "Adding trading control ORDER_VALIDATION"}

{"asctime": "2023-01-16 20:43:47,371", "levelname": "INFO", "message": "Adding trading control MARKET_VALIDATION"}

{"asctime": "2023-01-16 20:43:47,372", "levelname": "INFO", "message": "Adding trading control STRATEGY_EXPOSURE"}

{"asctime": "2023-01-16 20:43:47,372", "levelname": "INFO", "message": "Adding strategy Lay_Fav_Strat"}

{"asctime": "2023-01-16 20:43:47,372", "levelname": "INFO", "message": "Creating new &lt;class 'flumine.streams.datastream.DataStream'&gt; (2000) for strategy Lay_Fav_Strat"}

{"asctime": "2023-01-16 20:43:47,372", "levelname": "INFO", "message": "Starting flumine", "clients": {"Betfair": {"x": {"username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fa89d18bac0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "Simulated": {}, "BetConnect": {}}, "markets": {"market_count": 0, "open_market_count": 0}, "streams": ["&lt;OrderStream(OrderStream, initial daemon)&gt;", "&lt;DataStream(DataStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140362210664448)&gt;"]}

{"asctime": "2023-01-16 20:43:47,598", "levelname": "INFO", "message": "Client login", "username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fa89d18bac0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-01-16 20:43:47,881", "levelname": "INFO", "message": "Client update account details", "username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fa89d18bac0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-01-16 20:43:47,882", "levelname": "INFO", "message": "Adding worker keep_alive"}

{"asctime": "2023-01-16 20:43:47,882", "levelname": "INFO", "message": "Adding worker poll_market_catalogue"}

{"asctime": "2023-01-16 20:43:47,882", "levelname": "INFO", "message": "Adding worker poll_account_balance"}

{"asctime": "2023-01-16 20:43:47,882", "levelname": "INFO", "message": "Adding worker poll_market_closure"}

{"asctime": "2023-01-16 20:43:47,883", "levelname": "INFO", "message": "BackgroundWorker keep_alive starting", "worker_name": "keep_alive", "function": "&lt;function keep_alive at 0x7fa89d147c70&gt;", "context": {}, "start_delay": 0, "interval": 1200, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-01-16 20:43:47,883", "levelname": "INFO", "message": "BackgroundWorker poll_market_catalogue starting", "worker_name": "poll_market_catalogue", "function": "&lt;function poll_market_catalogue at 0x7fa89d1540d0&gt;", "context": {}, "start_delay": 10, "interval": 60, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-01-16 20:43:47,883", "levelname": "INFO", "message": "BackgroundWorker poll_account_balance starting", "worker_name": "poll_account_balance", "function": "&lt;function poll_account_balance at 0x7fa89d154160&gt;", "context": {}, "start_delay": 10, "interval": 120, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-01-16 20:43:47,884", "levelname": "INFO", "message": "BackgroundWorker poll_market_closure starting", "worker_name": "poll_market_closure", "function": "&lt;function poll_market_closure at 0x7fa89d1541f0&gt;", "context": {}, "start_delay": 10, "interval": 60, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-01-16 20:43:47,884", "levelname": "INFO", "message": "Starting streams.."}

{"asctime": "2023-01-16 20:43:47,884", "levelname": "INFO", "message": "Starting OrderStream 1000", "stream_id": 1000, "customer_strategy_refs": "ip-172-31-38-7", "conflate_ms": null, "streaming_timeout": 0.25, "client_username": "x"}

{"asctime": "2023-01-16 20:43:47,884", "levelname": "INFO", "message": "Starting output_thread (OrderStream 1000)"}

{"asctime": "2023-01-16 20:43:47,885", "levelname": "INFO", "message": "[Register: 1001]: orderSubscription"}

{"asctime": "2023-01-16 20:43:47,885", "levelname": "INFO", "message": "[OrderStream: 1001]: \"OrderStream\" created"}

{"asctime": "2023-01-16 20:43:47,955", "levelname": "INFO", "message": "[OrderStream: 1001]: connection_id: 108-160123204347-4921521"}

{"asctime": "2023-01-16 20:43:47,984", "levelname": "INFO", "message": "[OrderStream: 1002]: SUCCESS (9 connections available)"}

{"asctime": "2023-01-16 20:43:47,985", "levelname": "INFO", "message": "[OrderStream: 1001]: SUCCESS (9 connections available)"}

{"asctime": "2023-01-16 20:43:48,001", "levelname": "INFO", "message": "[OrderStream: 1001]: 0 oc added"}

{"asctime": "2023-01-16 20:43:48,135", "levelname": "INFO", "message": "Starting DataStream 2000", "stream_id": 2000, "market_filter": {"marketIds": ["1.208749303"], "eventTypeIds": ["7"], "marketTypes": ["WIN"], "venues": ["Wolverhampton"], "countryCodes": ["GB"]}, "market_data_filter": {"fields": ["EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP", "EX_MARKET_DEF", "SP_TRADED", "SP_PROJECTED"]}, "conflate_ms": null}

{"asctime": "2023-01-16 20:43:48,135", "levelname": "INFO", "message": "[Register: 2001]: marketSubscription"}

{"asctime": "2023-01-16 20:43:48,136", "levelname": "INFO", "message": "[FlumineStream: 2001]: \"FlumineStream\" created"}

{"asctime": "2023-01-16 20:43:48,212", "levelname": "INFO", "message": "[FlumineStream: 2001]: connection_id: 109-160123204348-5018646"}

{"asctime": "2023-01-16 20:43:48,241", "levelname": "INFO", "message": "[FlumineStream: 2002]: SUCCESS (8 connections available)"}

{"asctime": "2023-01-16 20:43:48,268", "levelname": "INFO", "message": "[FlumineStream: 2001]: SUCCESS (8 connections available)"}

{"asctime": "2023-01-16 20:43:48,285", "levelname": "INFO", "message": "[MarketStream: 2001] 1.208749303 added, 1 markets in cache"}

{"asctime": "2023-01-16 20:43:48,285", "levelname": "INFO", "message": "[FlumineStream: 2001]: 1 mc added"}

{"asctime": "2023-01-16 20:43:48,386", "levelname": "INFO", "message": "Adding: 1.208749303 to markets"}

{"asctime": "2023-01-16 20:43:58,116", "levelname": "INFO", "message": "Created marketCatalogue for 1.208749303", "market_id": "1.208749303", "event_id": "32033309", "event_type_id": "7", "event_name": "Wolverhampton 18th Jan", "market_type": "WIN", "market_start_datetime": "2023-01-18 15:15:00", "country_code": "GB", "venue": "Wolverhampton", "race_type": "Flat", "orders_cleared": [], "market_cleared": [], "closed": false}

{"asctime": "2023-01-16 20:43:58,286", "levelname": "INFO", "message": "Client update account details", "username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fa89d18bac0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}```

*Tags: Deployment, Strategies*

---

**Trex44** - *20:57:06*

The above are the logs from adapting the strategy so that it will place a £2 back bet at the best available price on the first runner (as ranked by runner id). The market only has to be 'OPEN' not in play.  Its filtered for the [https://www.betfair.com/exchange/plus/horse-racing/market/1.208749303?nodeId=32033309|Wolves 15:15](https://www.betfair.com/exchange/plus/horse-racing/market/1.208749303?nodeId=32033309|Wolves 15:15) market on the 18th.

*Tags: Strategies*

---

**Trex44** - *21:01:19*

Ok thanks very much, I will go read the docs again and figure out what stream to pass to the Strategy instance.

*Tags: Strategies*

---

**Trex44** - *21:02:21*

Think I have just copied that over from the Market Recorder start, didn't realise it was a different class of stream for recording data.

*Tags: Data Quality*

---

**Trex44** - *21:18:29*

Right sorted that, thanks for your help Liam, will always post logs in future.  Also congratz to whoever just took my £2 bet on Giorgio Vasari on Wednesday at odds of 1.2. I am sure you got value!

*Tags: General Technical*

---

## 2023-01-17

**Jorge** - *08:18:06*

I'm working in a function to save the EV of my flumine orders, this function is part of the `BaseStrategy` class. Right now, I just call the function on every `process_market_book` iteration, and save the best back and lay prices for every order with EXECUTION_COMPLETE status.



I'm wondering if it is possible to improve it in order to save the partially matched orders as well.



```def register_ev_completed_orders(self, market, selection_id, best_back_price, best_lay_price):

    for order in market.blotter._orders.values():

        if order.status == OrderStatus.EXECUTION_COMPLETE:

            if order.selection_id == selection_id:

                if order.size_matched &gt; 0:

                    if order.date_time_execution_complete:

                        seconds_since_matched = (datetime.datetime.utcnow() - order.date_time_execution_complete).total_seconds()

                        if 10 &lt; seconds_since_matched &lt; 30:

                            if order.bet_id in self.completed_bets_dict:

                                self.completed_bets_dict[order.bet_id]['seconds_since_matched'].append(seconds_since_matched)

                                self.completed_bets_dict[order.bet_id]['ref_prices'].append((best_back_price, best_lay_price))

                            else:

                                self.completed_bets_dict[order.bet_id] = {

                                    'market_id': market.market_id,

                                    'selection_id': selection_id,

                                    'seconds_since_matched': [seconds_since_matched],

                                    'side': order.side,

                                    'ref_prices': [(best_back_price, best_lay_price)],

                                    'size_matched': order.size_matched,

                                    'average_price_matched': order.average_price_matched}```

*Tags: Strategies*

---

**liam** - *08:33:02*

I would say its *much* simpler to do it the 'flumine' [https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/examples/strategies/lowestlayer.py#L60|way](https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/examples/strategies/lowestlayer.py#L60|way)



```# create order

order = trade.create_order(

    side="LAY",

    order_type=LimitOrder(lay, stake),

    notes=OrderedDict(

        best_back_price=best_back_price,

        best_lay_price=best_lay_price

    )

)```

*Tags: General Technical*

---

## 2023-01-19

**liam** - *08:30:25*

The [https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/flumine/streams/orderstream.py#L64|code](https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/flumine/streams/orderstream.py#L64|code) is probably the easiest way of explaining it

*Tags: General Technical*

---

**liam** - *08:58:12*

+ if there are live orders every `streaming_timeout` which is 0.2s I think

*Tags: Deployment*

---

**liam** - *09:36:55*

I have just pushed betcode/faq public :grinning:



[https://betcode-org.github.io/faq/](https://betcode-org.github.io/faq/)



This was created by [@U9JHLMZB4](@U9JHLMZB4) so I can't take ownership but going to look at expanding it to all common questions we get and then into more strategy development.

*Tags: Strategies*

---

**Aaron Smith** - *12:49:55*

Initially i was also surpised by how much liam provided given its a competition, but as of now, i m pretty sure he made a good deal with that. A group within can still heavily benefit from idea sharing and code sharing and in order to get actual meaningful help you also need to provide meaningful help. In such an enviroment some kind of trust needs to be established and from there on you surely can create relations that are helpful in both directions.

*Tags: General Technical*

---

**liam** - *13:53:00*

When I meet people this seems to be question number 1, from profitable through to newbies. I always give the same answer which is that there is no way I would be where I am now without having worked with and spoken to those that I have met through open sourcing bflw/betcode.



You make the assumption that anyone who uses betcode libraries will make money, they are just tools. Open sourcing flumine was always a risk as its far more powerful than it lets on however it has pushed me (and others) to improve it, add features, fix bugs and is now 100x quicker than its predecessor.



The biggest change I have seen in the 'landscape' since this slack is the acceptance that you need to get value to make money. Its a secret world and think this slack is the closest you can see it in public other than the meetups :wink:

*Tags: Errors Debugging, Feature Engineering*

---

**birchy** - *14:25:05*

_You make the assumption that anyone who uses betcode libraries will make money, they are just tools._ 



I've heard this so many times over the years. Most newcomers are very naive and assume that bot == profit but we all know that automating a shit strategy is just a more efficient shit strategy.

*Tags: Strategies*

---

**Trex44** - *21:16:09*

Evening all, for the selection_exposure method of the blotter class what exactly is the strategy parameter that is supposed to be passed? Is it the name of the strat as a string?

*Tags: Strategies*

---

**liam** - *21:19:40*

Strategy object itself I think 

*Tags: Strategies*

---

**Peter** - *22:49:28*

It is indeed the strategy object itself, which as you'll usually call the method from within the strategy would generally just be `self`.

*Tags: Strategies*

---

**Trex44** - *23:29:50*

Thanks both. Just curious - why does the strategy_selection_orders method take a tuple as its second parameter but slice the tuple to remove index 0. Why doesn't it just accept runner_id?

*Tags: Strategies*

---

## 2023-01-20

**Peter** - *13:54:41*

This is [@U4H19D1D2](@U4H19D1D2)’s logic so he may have a different take, but here's mine.



The order lookup is made up of market_id, selection_id and handicap. Although the handicap is 0 for most markets, for handicap markets specifically (e.g. asian line markets) it is needed to differentiate between multiple market lines for the same selection. It's also possible to have the same selection_id (and handicap = 0) in multiple markets at the same time (e.g. correct score in soccer). So to identify uniquely the market line to which an order applies for all market types, we need all three components.



However when the strategy_selection_orders method is invoked, it's done so in the context of the blotter for a specific market and loads the orders for that market only, which is why the market_id isn't needed and is sliced off the tuple, leaving just the selection_id and handicap.

*Tags: Strategies*

---

**Nicholas vizard** - *18:03:07*

Hey gents,

Hope everyone is doing well. Just looking for some help in analysis of Betfair's historical Bz2 stream files.

 *I was wondering if someone had a way to consistently identify the market book update that contains the latest possible snapshot of the trading data for a given market (i.e. last update on runner volumes,ltp, ladders etc.)?* 

I call this the last traded market book (LTMB). This is separate to the  last market update , as trading volumes are wiped before this final update.



It seems that in most markets, the LTMB can be identified as the third last market update in the bz2 file. However, there are some instances in which this is not the case.

Any help in establishing a consistent way to identify the LTMB would be much appreciated.



Best,

Nick

*Tags: Strategies*

---

**Nicholas vizard** - *19:13:38*

yeah that's the plan. Thanks Mo. Ill add in a third condition that the publish time needs to be after the market time and see how I go. Thanks again for your help.

*Tags: General Technical*

---

## 2023-01-21

**Andrew** - *00:50:59*

How does flumine handle abandoned markets? Call to remove_market of BaseStrategy? And possibly process_closed_market? My interest is how best for automation to handle delays, and long delays in start, and then distinguish between those markets and those that are abandoned or cancelled?

*Tags: Strategies*

---

**D C** - *11:08:27*

I'm having a problem with the stream at the moment. Trying to get multiple event types on a single call (US football, Ice Hockey, Basketball and Baseball) with the following filter:



`"marketFilter":{"marketTypes":["MATCH_ODDS"],"eventTypeIds":["6423","7522","7524","7511"]},"marketDataFilter":{"ladderLevels":10,"fields":["EX_ALL_OFFERS","EX_TRADED","EX_TRADED_VOL","EX_LTP","EX_MARKET_DEF","EX_BEST_OFFERS_DISP","SP_PROJECTED"]}`



For some reason it is only returning basketball and Americal football markets. Any ideas as to what could be causing this. I have a 1000 market limit per stream connection and I can't see a problem with that being exceeded (visually checking on the BF website rough numbers). Any suggestions? What normally happens if your required markets exceed your limit? I'd always thought your sub was refused in such cases?

*Tags: General Technical*

---

**James** - *14:12:50*

How easy/time consuming would it be for a novice in python to set up a data recording/storage system such as Liam suggested in another thread (flumine &gt; aws s3)?

*Tags: Deployment*

---

**Aaron Smith** - *14:36:09*

[https://betcode-org.github.io/flumine/](https://betcode-org.github.io/flumine/) has good info for getting started. In the flumine repo under examples, you ll find the S3Recorder, which can be used to safe streaming data in a s3-bucket. The s3 recorder can be set up just like any strategy (so you can follow the steps from the link)

*Tags: Getting Started, Strategies*

---

**Aaron Smith** - *14:48:21*

as someone who also started using flumine early on in his coding career, i want to add that while its all technically "easy", as a beginner you are likely to get stuck on the most dumb things (at least i did), so with this i d like to relativize the word "easy" as to not make you feel dumb, which may happen anyway :D

*Tags: Getting Started*

---

**James** - *14:53:37*

I much prefer learning by doing so I’ll get stuck in. Thanks again for the help

*Tags: General Technical*

---

## 2023-01-22

**Unknown** - *14:38:07*

This isn't something that Flumine handles explicitly. Rather it deals with the information received from Betfair in exactly the same way as it would when the same information is received in a normal race. So the question shifts to what information does Betfair provide for an abandoned race?



The answer is that it suspends the market. Sets all the available volumes to zero, removes all the runners and closes the market. There's no explicit "abandoned" status or reason given for closing the market.



Any orders already placed will be cancelled at the Betfair and Flumine will receive and handle the  updates in the order stream i the normal way.



The file is an example of the WIN market from an abandoned horse race:

*Tags: General Technical*

---

**Peter** - *15:03:09*

A bit puzzled as to why this is a question. What's the architecture of your setup that makes this a potential concern?



I also doubt that anybody here will have an answer to this since if the keep_alive method is being used, implicitly or explicitly, to maintain active sessions it's not something we're likely to come up against. So probably one to ask Betfair.

*Tags: Getting Started, Deployment*

---

**D C** - *15:12:15*

To be plain, I currently use an asynchronous TCP/IP socket component. As such, I use a single session token for my bet placements. I am considering moving to a blocking socket multi-threaded bet placement model. Now I might be able to share a single session token between any/all threads that I create or I might need a login per thread (doubtful but I just don't know). I suppose I a better question would be to ask those who use thread pools for bet placement if they share all threads across a single login session.

*Tags: Strategies*

---

**Adam Momen** - *15:26:23*

What is the time unit measure of the latency? Is it in seconds?

*Tags: Performance*

---

**Trex44** - *18:19:51*

Hello all, does anyone know if there is there anyway to asses from the Betfair API what track is being run at Lingfield? As in if its Turf or All Weather?

*Tags: General Technical*

---

**Mo** - *18:21:52*

[https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/racecard.py|https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/racecard.py](https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/racecard.py|https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/racecard.py)

*Tags: General Technical*

---

**Trex44** - *22:05:31*

Wonder if anyone can help. I was trying to pass a list of market_ids as part of the market filter to a strategy. The id's are generated by filtering the markets based on information in the race_card. At the moment I am logging in using the market filter, getting all the market_ids for races then passing the market_ids to get_race_card. Then using ifnformation from the race_card I filter the market_ids and return a list of filtered market ids  to the strategy. But I get the following error . (see attached). Both the filter and strategy work fine when executed separate to one another.

*Tags: Errors Debugging, Strategies*

---

**Trex44** - *22:05:39*

```  File "/home/ubuntu/Python_projects/initiate_strat.py", line 95, in &lt;module&gt;

    target_arr = get_filter_list()

  File "/home/ubuntu/Python_projects/initiate_strat.py", line 27, in get_filter_list

    results = trading.betting.list_market_catalogue(

  File "/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/endpoints/betting.py", line 233, in list_market_catalogue

    (response, response_json, elapsed_time) = self.request(method, params, session)

  File "/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/endpoints/baseendpoint.py", line 55, in request

    self._error_handler(response_json, method, params)

  File "/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/endpoints/baseendpoint.py", line 81, in _error_handler

    raise self._error(response, method, params)

betfairlightweight.exceptions.APIError: SportsAPING/v1.0/listMarketCatalogue 

Params: {'filter': {'eventTypeIds': [7], 'marketCountries': ['GB'], 'marketTypeCodes': ['WIN']}, 'marketProjection': ['RUNNER_DESCRIPTION', 'RUNNER_METADATA', 'COMPETITION', 'EVENT', 'EVENT_TYPE', 'MARKET_DESCRIPTION', 'MARKET_START_TIME'], 'maxResults': 100} 

Exception: None 

Error: {'code': -32099, 'message': 'ANGX-0003', 'data': {'APINGException': {'requestUUID': 'ie2-ang05b-prd-01191051-000775c81b', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}} 

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0003', 'data': {'APINGException': {'requestUUID': 'ie2-ang05b-prd-01191051-000775c81b', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}```

*Tags: Errors Debugging, Strategies*

---

**Peter** - *22:47:18*

The error message you're getting from Betfair is for an invalid session, so nothing to do with your filter or strategy. This feels more like an issue with how you're initiating your script.

*Tags: Errors Debugging, Strategies*

---

## 2023-01-23

**Trex44** - *14:36:03*

I think its to do with me logging in to Betfair and  then to the Race Card in order to create the filter list before calling run on the framework (which I assume also tries to log in). Even when I logout after getting the Race Card info I get the same problem.

*Tags: General Technical*

---

**Trex44** - *19:55:40*

Is there an example of how to filter market ids by a parameter taken from the race_card when using Flumine?

*Tags: General Technical*

---

## 2023-01-24

**Andrew** - *06:34:53*

Are you using the stream? Prices are on the RunnerBook. And keep historical data in the runner context.

*Tags: Data Quality*

---

**Unknown** - *11:44:55*

OK. Attached are two examples. Example1 is a working script doing what I think you may have been trying to do and creates a list of market IDs from the racecards and feeds that to Flumine. Example2 is how I would personally do it, which embeds the filter condition into the Flumine strategy's check_market_book method.



They both use the same fairly random filter criteria looking for Flat races under a mile - you can insert your own condition.



Points to note are:

• You may need to tweak how the Betfair credentials are passed to the trading object to reflect how you authenticate

• In example two there's no need for an explicit login or call to list_market_catalogues as Flumine takes care of those.

• The market filters have different parameters (because Betfair is inconsistent).

• Where you find the information for your conditions may be a bit different too (ditto).

• The market catalogue may not appear in example2 until several seconds after the market is added to the stream.

• I've turned the order stream off in both example, but you will probably want it on (by removing the parameter when the client is initiated).

*Tags: Strategies*

---

**Nicholas vizard** - *22:41:39*

Hi all,

Apologies in advance for the basic questions: I was looking to confirm if the .total_matched attributes applied to the marketbook.runner and marketbook objects represent the cumulative sum of all back stakes taken on the runner and market respectively until that timepoint? Or does it cumulative sum of the lay liability and back stake taken, or something else all together?

Furthermore, is the currency domiciled in pounds? Or does it depend on the country_code associated with the particular market?



Cheers for any help you may provide!

*Tags: General Technical*

---

## 2023-01-25

**Trex44** - *20:04:03*

Unless I am mistaken though the examples filter based on data from the list_market_catalogue method called on the betting attribute of the trading object. This doesn't have the data I am after which is held in the race_card attribute of the same trading object. The attributes access two different API's. I was trying to

• call betting.list_market_catalogue and return all UK market ids 

• then send this list to the race_card attribute by calling the get_race_card method on it and filtering the list of market ids based on the track surface type (data held in the race_card API not the betting API). 

• Then pass this list of ids to the market filter that is passed to the Strategy object (strategy var) that is passed to framework.add_strategy.  

I can't simply call the race_card.get_race_card() method because it needs market ids as a parameter so I have to make the call on the betting attribute first.

*Tags: Strategies*

---

## 2023-01-26

**liam** - *07:26:04*

I think it’s streaming where it’s only GBP

*Tags: General Technical*

---

**Unknown** - *19:09:51*

Ah, I see. But that still doesn't explain why you're getting what look to be authentication errors. Attached is an example 3, that first gets a list market IDs from the betting attribute, passes it to the race_cards endpoint and generates a new (filtered) list based on a condition found only there, and finally passes that into the Flumine strategy's market filter. The code works with no authentication issues, so you must be doing something subtly different.

*Tags: Errors Debugging, Strategies*

---

## 2023-01-31

**QuickLearner8888** - *13:52:39*

Hi All.



I am new to this channel, and to Flumine in general.



Are there any tutorials out there, to explain how to use the marketrecorder?



I am interested in getting a market-stream, of a specific market_id. As an example, a live soccer-match, I would like to have only best_back_offered and best_lay_offered, and offered volumes at any given time, for every runners in the over/under market.



Any help or pointers in right direction would be a huge help.



 Thanks

*Tags: Getting Started, Deployment*

---

**Crofty** - *14:51:15*

Have you had a look in the FAQs:



[https://betcode-org.github.io/faq/](https://betcode-org.github.io/faq/)



If it's not there, I'm sure someone will be able to help you.

*Tags: General Technical*

---

**Newbie99** - *18:05:19*

Just to add to [@U01HQ97J5HA](@U01HQ97J5HA), there are far better people on here than I to advise, but you could modify this script:



[https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py)



Changing this section:



```event_type_ids=["7"],

        country_codes=["GB", "IE"],

        market_types=["WIN"],```

to:



```event_type_ids=["1"],

market_types=["OVER_UNDER_25"],```

Some things to consider:



1. You need to have the live key enabled for streaming (the delayed key is useless for this unfortunately)

2. You need to be placing real money bets at the same time (even if only in small size), you can't just record, otherwise you will get your account blocked

3. There are A LOT of football markets, so you may find it necessary to use multiple connections (i.e. a different connection for each OVER_UNDER_XX if you are looking at more than one)...you may find you are initially limited to 200 markets per connections and 10 connections, this can be tricky for football as if there are over 200 markets (quite likely) you will get an error! If that happens you can either try to look at market_types that have fewer live markets or speak to Betfair to get your limit increased (but referring back to point 1, you may need to be placing live orders for this to happen).

4. You can loop through the market book to get the price data and need to match the market_id &amp; selection_id (and handicap, but this will always be 0 for football) to the market_catalogue, there is an example here: [https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py)

That hopefully helps you get started, but of course, please feel free to ask away if you get stuck!

*Tags: Errors Debugging, Deployment*

---

**Peter** - *19:13:42*

Broadly what [@UFTBRB3F1](@UFTBRB3F1) said, but with a couple of tweaks.



A minor point on football markets: handicaps will be non-zero for most runners in the Asian Handicap market type, and zero for other markets.



I recommend against trying to filter data from the market recorder as you stream it. Better is to collect it raw (i.e. not just the best prices), compress it and save it, all of which the marketrecorder does by just setting the parameters. Then process the files offline to extract the data you want. The reason for this process is that as you gain more experience in this field, you will probably find that you need more extensive data from which to extract the features that you will use to derive your trading triggers. And while it's easy to filter out the data you're not yet interested in, it's impossible (or at least very expensive if you buy it from Betfair) to infill later the data you wished you'd collected.

*Tags: Data Quality, Feature Engineering, Strategies*

---

## 2023-02-01

**AI Trader** - *02:21:38*

Hi guys,

What is the best way to use Flumine as a "batch process"? I.e. react to market on a fixed frequency (1h for instance) rather than at every book update? I know it wasn't designed for that, but what would be a good "hack" ? I would like this solution to also work with the backtester, so using something as checking if the time is a multiple of 1h might not work since in the backtester the time would be the local time?

*Tags: Errors Debugging*

---

**liam** - *07:20:37*

You want to take advantage of `strategy.conflate`

*Tags: Strategies*

---

**QuickLearner8888** - *08:36:35*

This was extremely helpful, thank you.

Especially the point that I need a live key was essential. I was a bit confused that absolutely nothing happened when I was running the script, no matter what I was editing or adding.



I will most likely only be interested in one line (OVER_UNDER_XX) for each match, and max 20 matches pr day, so I need to get the filters right.



Thanks a lot!

*Tags: Deployment*

---

## 2023-02-05

**JamieK** - *12:01:44*

Hi all :slightly_smiling_face: This all looks very interesting, an opportunity to combine my coding skills with my love of betting. Hoping I can start a journey with this, learn and have some fun. I'm a python newbie, more used to building web apps with Typescript.



Anyone heard of, or use JetBrains DataSpell instead of Jupyter Notebooks (it's supposed to be an enhanced IDE wrapper around Jupyter Notebooks)? I think this IDE might be more appropriate than JetBrains PyCharm

*Tags: Strategies*

---

**Trex44** - *12:05:48*

Thanks very much for this Peter that works. Going to try and adapt it to my strat today and see if it still works, been using a workaround involving scheduling the last week.



It looks the same as what I had built with one exception. The instance of the BetfaireClient object that is created and stored in the client var is instantiated with its orderstream parameter/attribute set to False (default value is True in the Baseflumine class). I just left it as its default value before, do you think this would have caused the error? What does this parameter/attribute actually do? Can't figure it out scouring through the code.

*Tags: Errors Debugging*

---

**Trex44** - *12:12:51*

Hey Jamie, welcome.  I have never heard of DataSpell. I use PyCharm for building Flumine strategies and for remote deployment. I use Jupyter notebook for analysing data that I extract from market data using some custom scripts I built. Although I think you could probably just build a flumine strategy and use it in back test mode to extract the data, no need for a custom script.

*Tags: Deployment, Strategies*

---

**JamieK** - *12:31:22*

Thanks Trex. I'll try PyCharm also to compare, not sure if DataSpell is working as I expected.



TBH, I'm way off having very much of a clue what I'm doing yet. I'm watching some tuts and am aiming to understand how to utilise Flumine, backtest any simple strategy, and then develop my own to backtest. Just need to get my head round Python on the way!

*Tags: Strategies*

---

**Trex44** - *12:32:55*

Same as me really.  Would be much harder to learn Python if it weren't for Betfair, always easier to learn when you have a project you enjoy building.Also pretty proficient with AWS as a result of BF and Flumine.

*Tags: Deployment*

---

**Peter** - *13:54:40*

You will probably want to leave it as the default. Setting it to False turns off the order stream which takes up one of your streaming connections. I just turned it off because it wasn't relevant to the issues that I was demonstrating.

*Tags: General Technical*

---

**foxwood** - *16:26:08*

The free version of PyCharm is a bells-and-whistles IDE - much easier to develop using flumine on a project basis and with class inheritance than any of the notebook approaches.

*Tags: General Technical*

---

**Charaka A** - *18:29:24*

Hi guys, does anyone know how I can unsubscribe from a market stream, after the market has closed when using the betfairlightweight package? I would still like to listen to the other open markets though

*Tags: General Technical*

---

## 2023-02-06

**mzaja** - *11:05:10*

Hi, I have two quick question regarding flumine:

1. Is it possible to have a startegy call both process_market_book() and process_raw_data(). E.g. if I want to simultaneously bet and capture the same stream?

2. Say I create and run two separate strategies, one using MarketStream and the other using DataStream, where stream market/data fllters are identical. Will the streams be shared or will I be subscribed to two different streams on Betfair? If it is the latter, will this count 2x towards my subscription limit, or it does not matter because the markets I am subscribed to are the same?

*Tags: General Technical*

---

**mzaja** - *11:10:26*

Ok, so you say that as long as I run two flumine instances on 2 different connections I should be fine?

*Tags: General Technical*

---

**mzaja** - *11:15:19*

Thanks, I'll do that. Is there a quick way to check how many market subscriptions I am consuming at a given time? Or do I need to try going over the limit and catching the error?

*Tags: Errors Debugging*

---

## 2023-02-07

**Unknown** - *08:22:39*

Anyone have an idea as to why the marketcatalogues returned by betfairlightweight have “None” in the event section?

*Tags: General Technical*

---

**Peter** - *09:08:58*

[@U04980ZT6UX](@U04980ZT6UX) Apologies. Missed your follow up questions when you originally posted them. In case they're still relevant ... market.blotter.live_orders() will give you an iterable of the live orders for that market only, or if ypu're working in your strategy's process_orders() method, the list of orders passed into it will be filtered by the strategy and market combination, but will be all such orders, though easily filtered by OrderStatus.EXECUTABLE.



[@U01D23DDMTQ](@U01D23DDMTQ) is right that to change price and size requires a cancel and new place as Betfair won't allow changing price and size in a replace. Personally I'd fire off the cancel as soon as I knew that I no longer wanted the bet and then watch the response (if working synchronously, or the order stream if working asynchronously) before placing the new order - which may be what he meant by implementing a wait queue.

*Tags: Deployment, Strategies*

---

**Andrew** - *09:49:15*

Sorry a bit late back to this thread. Here's a rough outline of my approach. I make use of market context to cache values from previous check. FYI, I initialise this in middleware



Here's actual code snippet - any IP removed :slightly_smiling_face:



This bit goes in _process_market_book_, _pr is new price



if _pr != market.context['order_price'][runner.selection_id]: # market.context['order_price'][runner.selection_id] is existing target price for runner

                        _sz = &lt;calculate new size based on _pr&gt; # there was some IP here!

                        if _sz &gt; 0:

                            market.context['order_price'][runner.selection_id] = _pr

                            market.context['order_size'][runner.selection_id] = _sz

                            market.context['create_order'][runner.selection_id] = True # This is a flag to help control cancel before create new order



A bit later on in _process_market_book_



for runner in market_book.runners:

            rc = self.get_runner_context(market.market_id, runner.selection_id, runner.handicap)

            if market.context['create_order'][runner.selection_id] and not rc.executable_orders:

                # flagged for create, no executable order

                trade = Trade(market.market_id, runner.selection_id, handicap = runner.handicap, strategy = self)

                order = trade.create_order(

                    side = "LAY",

                    order_type = LimitOrder(

                        price = market.context['order_price'][runner.selection_id],

                        size = market.context['order_size'][runner.selection_id],

                        )

                    )

                market.place_order(order)

                market.context['create_order'][runner.selection_id] = False



And here's the magic in _process_orders_ I figured to handle state and wait for cancel



def process_orders(self, market, orders):

        for order in orders:

            rc = self.get_runner_context(market.market_id, order.selection_id, order.handicap)

            if market.context['create_order'][order.selection_id] and rc.executable_orders:

                # flagged for create, so need to cancel current before create new

                if order.status == OrderStatus.EXECUTION_COMPLETE:

                    market.context['create_order'][order.selection_id] = False

                elif order.status == OrderStatus.EXECUTABLE:

                    market.cancel_order(order)

*Tags: Strategies*

---

**Trex44** - *21:38:33*

Evening all.  Was wondering if anyone can help me solve an issue with back testing a strat. The strat places a bet on a single runner before the race starts then is supposed to place a second bet at 60 seconds to the off if market.blotter.strategy_selection_order has a count greater than one. Which it should do as the first bet was placed successfully and shows as Execution complete. Bizarrely though the market.blotter.strategy_selection_order list turns to an empty list between 104-103 seconds to the off even though the market.blotter.orders list continues to contain the order. See logs attached. I have backtested this strat on 4 markets, 3 it works fine on. Its only this one market that I have seen this behaviour on.

*Tags: Strategies*

---

**Trex44** - *21:39:35*

```{"asctime": "2023-02-07 21:17:54,969", "levelname": "INFO", "message": "Client added", "username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": null, "transaction_count_total": null, "trading_controls": [], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-02-07 21:17:54,969", "levelname": "INFO", "message": "Adding market middleware &lt;flumine.markets.middleware.SimulatedMiddleware object at 0x7f61a0256740&gt;"}

{"asctime": "2023-02-07 21:17:54,969", "levelname": "INFO", "message": "Adding client control MAX_TRANSACTION_COUNT"}

{"asctime": "2023-02-07 21:17:54,970", "levelname": "INFO", "message": "Adding trading control ORDER_VALIDATION"}

{"asctime": "2023-02-07 21:17:54,970", "levelname": "INFO", "message": "Adding trading control MARKET_VALIDATION"}

{"asctime": "2023-02-07 21:17:54,970", "levelname": "INFO", "message": "Adding trading control STRATEGY_EXPOSURE"}

{"asctime": "2023-02-07 21:17:54,970", "levelname": "INFO", "message": "Adding strategy LowestLayer"}

{"asctime": "2023-02-07 21:17:54,971", "levelname": "INFO", "message": "Creating new HistoricalStream (1000) for strategy LowestLayer", "strategy": "LowestLayer", "stream_id": 1000, "market_filter": "/home/ubuntu/Python_projects/tmp/races/1.138465931", "event_id": "28531405", "event_processing": false}

{"asctime": "2023-02-07 21:17:54,971", "levelname": "INFO", "message": "Starting flumine", "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 0, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

{"asctime": "2023-02-07 21:17:54,971", "levelname": "INFO", "message": "Client login", "username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-02-07 21:17:54,971", "levelname": "INFO", "message": "Client update account details", "username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-02-07 21:17:54,972", "levelname": "INFO", "message": "Starting historical market '/home/ubuntu/Python_projects/tmp/races/1.138465931'", "market": "/home/ubuntu/Python_projects/tmp/races/1.138465931"}

{"asctime": "2023-02-07 21:17:54,972", "levelname": "INFO", "message": "[Register: 1000]: marketSubscription"}

{"asctime": "2023-02-07 21:17:54,972", "levelname": "INFO", "message": "[MarketStream: 1000]: \"MarketStream\" created"}

{"asctime": "2023-02-07 21:17:54,972", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:54,974", "levelname": "INFO", "message": "Adding: 1.138465931 to markets"}

{"asctime": "2023-02-07 21:17:54,982", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:54,982", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:54,984", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,000", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,003", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,007", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,014", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,045", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,062", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,070", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,097", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,197", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,229", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,234", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,310", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,391", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}

{"asctime": "2023-02-07 21:17:55,409", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.138465931 added, 1 markets in cache"}```

*Tags: Strategies*

---

**Trex44** - *21:39:43*

```{"asctime": "2023-02-07 21:17:57,216", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.138465931", "selection_id": 10200239, "handicap": 0, "id": "138950974772156791", "customer_order_ref": "4d8a4a5a825e8-138950974772156791", "bet_id": null, "date_time_created": "2018-01-01 13:05:50.071000", "publish_time": "2018-01-01 13:05:50.071000", "market_version": null, "async": false, "trade": {"id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 3.7, "size": 5.0, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null, "price_ladder_definition": "CLASSIC"}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 5.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Pending", "status_log": "Pending", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "0.9912", "market_notes": null, "client": "72f36d32"}

{"asctime": "2023-02-07 21:17:57,216", "levelname": "INFO", "message": "1 order packages executed in transaction", "market_id": "1.138465931", "order_packages": [{"id": "e47d1978-a72c-11ed-93b3-3fe541d536d4", "client": "&lt;flumine.clients.simulatedclient.SimulatedClient object at 0x7f61a3560250&gt;", "market_id": "1.138465931", "orders": ["138950974772156791"], "order_count": 1, "package_type": "Place", "customer_strategy_ref": "ip-172-31-38-7", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}], "transaction_id": 1, "client_username": "72f36d32"}

Entry Bet

{"asctime": "2023-02-07 21:17:57,217", "levelname": "INFO", "message": "Trade status update: Pending", "id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Pending", "status_log": "Pending"}

{"asctime": "2023-02-07 21:17:57,218", "levelname": "INFO", "message": "Order Place: SUCCESS", "bet_id": null, "order_id": "138950974772156791", "status": "SUCCESS", "error_code": null}

{"asctime": "2023-02-07 21:17:57,218", "levelname": "INFO", "message": "Order status update: Executable", "market_id": "1.138465931", "selection_id": 10200239, "handicap": 0, "id": "138950974772156791", "customer_order_ref": "4d8a4a5a825e8-138950974772156791", "bet_id": "100000000001", "date_time_created": "2018-01-01 13:05:50.071000", "publish_time": "2018-01-01 13:05:50.071000", "market_version": null, "async": false, "trade": {"id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Pending", "status_log": "Pending"}, "order_type": {"order_type": "Limit", "price": 3.7, "size": 5.0, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null, "price_ladder_definition": "CLASSIC"}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 5.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": "2018-01-01 13:05:50.294000", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Executable", "status_log": "Pending, Executable", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 25.65, "matched": []}, "notes": "0.9912", "market_notes": "3.7,3.8,3.7", "client": "72f36d32"}

{"asctime": "2023-02-07 21:17:57,218", "levelname": "INFO", "message": "Trade status update: Live", "id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Live", "status_log": "Pending, Live"}

{"asctime": "2023-02-07 21:17:57,220", "levelname": "INFO", "message": "Order status update: Execution complete", "market_id": "1.138465931", "selection_id": 10200239, "handicap": 0, "id": "138950974772156791", "customer_order_ref": "4d8a4a5a825e8-138950974772156791", "bet_id": "100000000001", "date_time_created": "2018-01-01 13:05:50.071000", "publish_time": "2018-01-01 13:05:50.071000", "market_version": null, "async": false, "trade": {"id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Live", "status_log": "Pending, Live"}, "order_type": {"order_type": "Limit", "price": 3.7, "size": 5.0, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null, "price_ladder_definition": "CLASSIC"}, "info": {"side": "LAY", "size_matched": 5.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 3.7}, "responses": {"date_time_placed": "2018-01-01 13:05:50.294000", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Execution complete", "status_log": "Pending, Executable, Execution complete", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0, "matched": [[1514811950696, 3.7, 5.0]]}, "notes": "0.9912", "market_notes": "3.7,3.8,3.7", "client": "72f36d32"}

{"asctime": "2023-02-07 21:17:57,220", "levelname": "INFO", "message": "Trade status update: Complete", "id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Complete", "status_log": "Pending, Live, Complete"}

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;

SECONDS TO START = 104.043

strategy_selection_orders[Order 100000000001: Execution complete]

market.blotter.orders = [Order 100000000001: Execution complete]

market status = OPEN

runner status = ACTIVE

selection exposure = 13.5

stats = {'matched_profit_if_win': -13.5, 'matched_profit_if_lose': 5.0, 'worst_potential_unmatched_profit_if_win': 0.0, 'worst_potential_unmatched_profit_if_lose': 0.0, 'worst_possible_profit_on_win': -13.5, 'worst_possible_profit_on_lose': 5.0}

number of active runners = 8

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;

SECONDS TO START = 103.929

strategy_selection_orders[]

market.blotter.orders = [Order 100000000001: Execution complete]

market status = OPEN

runner status = ACTIVE

selection exposure = -0.0

stats = {'matched_profit_if_win': 0.0, 'matched_profit_if_lose': 0.0, 'worst_potential_unmatched_profit_if_win': 0.0, 'worst_potential_unmatched_profit_if_lose': 0.0, 'worst_possible_profit_on_win': 0.0, 'worst_possible_profit_on_lose': 0.0}

number of active runners = 8

{"asctime": "2023-02-07 21:17:58,369", "levelname": "INFO", "message": "Market 1.138465931 closed", "market_id": "1.138465931", "event_id": "28531405", "event_type_id": "7", "event_name": "Sthl 1st Jan", "market_type": "WIN", "market_start_datetime": "2018-01-01 13:10:00", "country_code": "GB", "venue": "Southwell", "race_type": null, "orders_cleared": [], "market_cleared": [], "closed": true}

{"asctime": "2023-02-07 21:17:58,370", "levelname": "INFO", "message": "Market cleared", "market_id": "1.138465931", "order_count": 1, "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

{"asctime": "2023-02-07 21:17:58,370", "levelname": "INFO", "message": "Market level cleared", "market_id": "1.138465931", "profit": 5.0, "bet_count": 1}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Market closed", "market_id": "1.138465931", "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Removing market 1.138465931", "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Completed historical market '/home/ubuntu/Python_projects/tmp/races/1.138465931'"}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Simulation complete"}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Shutting down Execution (SimulatedExecution)"}

{"asctime": "2023-02-07 21:17:58,372", "levelname": "INFO", "message": "Shutting down Execution (BetfairExecution)"}

{"asctime": "2023-02-07 21:17:58,372", "levelname": "INFO", "message": "Client logout", "username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-02-07 21:17:58,372", "levelname": "INFO", "message": "Exiting flumine", "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;

Profit: 5.00

10200239 2018-01-01 13:05:50.294000 OrderStatus.EXECUTION_COMPLETE 3.7 3.7 5.0 5.0```

*Tags: Errors Debugging, Deployment, Strategies*

---

## 2023-02-08

**liam** - *08:07:43*

Can you share some code and the market? Not sure its possible to debug without

*Tags: Errors Debugging*

---

**Unknown** - *23:32:23*

```import math

from collections import OrderedDict

from flumine import BaseStrategy

from flumine.order.trade import Trade

from flumine.order.order import OrderStatus

from flumine.order.ordertype import LimitOrder

from flumine.utils import get_price

import numpy as np





class LowestLayer(BaseStrategy):

    def __init__(self, *args, **kwargs):

        super().__init__(*args, **kwargs)

        self.max_order_exposure = 100



    def check_market_book(self, market, market_book):

        if market_book.status == "OPEN" and not market_book.inplay:

            return True



    def process_market_book(self, market, market_book):



        # count market matched trades

        market_matched_trades_count = sum(

            [self.get_runner_context(market.market_id, r.selection_id, r.handicap).trade_count -

             self.get_runner_context(market.market_id, r.selection_id, r.handicap).live_trade_count for r

             in market_book.runners])



        # get lowest/highest priced runner (depends on sort settings)

        prices = [

            (r.selection_id, get_price(r.ex.available_to_back, 0))

            for r in market_book.runners if r.status == "ACTIVE"



        ]



        if not prices:

            return



        if market.seconds_to_start &gt; 250:

            return



        prices.sort(key=lambda tup: tup[1], reverse=False)

        selection_id = prices[0][0]



        if prices[0][1] &gt; 12:

            return



        # calculate market under/over round for later analysis

        underround = _calculate_underround(market_book.runners)



        for runner in market_book.runners:



            runner_context = self.get_runner_context(

                market.market_id, runner.selection_id, runner.handicap

            )



            if runner.selection_id == selection_id:



                '''Place initial bet'''

                if market_matched_trades_count == 0 and runner_context.live_trade_count == 0 and 250 &gt; market.seconds_to_start &gt; 0:

                    self.create_trade_create_order(market, market_book, runner, underround, self.context['stake'],

                                                   back_lay='LAY', market_depth=0)

                    print('Entry Bet')



                '''Code only here for investigations'''

                if market.seconds_to_start in [104.043, 103.929]:

                    print('&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;')

                    print(f'SECONDS TO START = {market.seconds_to_start}')

                    print(f'strategy_selection_orders{market.blotter.strategy_selection_orders(self, selection_id, runner.handicap)}')

                    print(f'market.blotter.orders = {[o for o in market.blotter]}')

                    print(f'market status = {market.status}')

                    print(f'runner status = {runner.status}')

                    print( f'selection exposure = {market.blotter.selection_exposure(strategy=self, lookup=(runner.handicap, runner.selection_id))}')

                    print(f'stats = {market.blotter.get_exposures(strategy=self, lookup=(runner.handicap, runner.selection_id))}')

                    print(f'number of active runners = {len([r for r in market_book.runners if runner.status == "ACTIVE"])}')



                '''Try to green up on runner - limit order'''

                if runner_context.trade_count == 1 and runner_context.live_trade_count == 0 \

                        and 30 &lt; market.seconds_to_start &lt;= 60:

                    stake, price, backorlay = self.green_all(market, runner, market_depth=0)

                    self.create_trade_create_order(market, market_book, runner, underround,

                                                   stake=stake, back_lay=backorlay, set_price=price)



                '''If green up was unsuccessful then cancel bets'''

                if runner_context.trade_count &gt;= 1 and runner_context.live_trade_count &gt; 0 and market.seconds_to_start &lt; 30:

                    executable_orders = [o for o in

                                         market.blotter.strategy_selection_orders(

                                             self, selection_id, runner.handicap) if o.status.value == 'Executable']



                    if len(executable_orders) &gt; 0:

                        for order in executable_orders:

                            market.cancel_order(order)

                            print(executable_orders)



                '''Place green up at market price

                Will do this only if matched profit if lose &gt; £1 difference that matched profit if win'''

                if market.seconds_to_start &lt; 25:

                    exposures = market.blotter.get_exposures(strategy=self,

                                                             lookup=(runner.handicap, runner.selection_id))



                    matched_profit_if_win = math.sqrt(exposures['matched_profit_if_win'] ** 2)

                    matched_profit_if_lose = math.sqrt(exposures['matched_profit_if_lose'] ** 2)

                    if math.sqrt((matched_profit_if_win - matched_profit_if_lose) ** 2) &gt; 1:

                        stake, price, backorlay = self.green_all(market, runner, dump_position=True)

                        self.create_trade_create_order(market, market_book, runner, underround,

                                                       stake=stake, set_price=price, back_lay=backorlay)



     



    def process_orders(self, market, orders):

        # kill order if unmatched in market for greater than X seconds

        for order in orders:

            if order.status == OrderStatus.EXECUTABLE:

                if order.elapsed_seconds and order.elapsed_seconds &gt; 100:

                    market.cancel_order(order)



    def create_trade_create_order(self, market, market_book, runner, underround, stake,

                                  back_lay: str, market_depth: int = 1, set_price: float = 0.0):



        stake = float((np.round(stake, 2)))

        '''If market_order = True then a higher market_depth results 

        in a worst price. If it is False the reverse is true'''



        if set_price != 0.0:

            bet_template = [back_lay, set_price]

        elif back_lay == 'BACK':

            bet_template = ['BACK', get_price(runner.ex.available_to_lay, market_depth)]

        else:

            bet_template = ['LAY', get_price(runner.ex.available_to_back, market_depth)]



        # create trade

        trade = Trade(

            market_book.market_id,

            runner.selection_id,

            runner.handicap,

            self,

        )

        # create order

        order = trade.create_order(

            side=bet_template[0],

            order_type=LimitOrder(bet_template[1], np.round(stake, 2)),

            notes=OrderedDict(underround=round(underround, 4)),

        )

        # place order for execution

        market.place_order(order)



    def green_all(self, market, runner, market_depth: int = 0, dump_position: bool = False):

        facts = market.blotter.get_exposures(strategy=self, lookup=(runner.handicap, runner.selection_id))



        selection_exposure = market.blotter.selection_exposure(strategy=self,

                                                               lookup=(runner.handicap, runner.selection_id))

        matched_profit_if_win = facts['matched_profit_if_win']

        matched_profit_if_lose = facts['matched_profit_if_lose']

        print('green all called')

        print(matched_profit_if_win)

        print(matched_profit_if_lose)



        if matched_profit_if_win &gt; 0:

            avg_odds = (matched_profit_if_win + selection_exposure) / selection_exposure

            if dump_position is False:

                return self.calc_green(avg_odds,

                                       current_price_of_runner=get_price(runner.ex.available_to_back, market_depth),

                                       total_staked=selection_exposure) + ('LAY',)

            else:

                return self.calc_green(avg_odds,

                                       current_price_of_runner=get_price(runner.ex.available_to_lay, 0),

                                       total_staked=selection_exposure) + ('LAY',)



        if matched_profit_if_win &lt; 0:

            avg_odds = (matched_profit_if_lose + selection_exposure) / matched_profit_if_lose

            if dump_position is False:

                return self.calc_green(avg_odds,

                                       current_price_of_runner=get_price(runner.ex.available_to_lay, market_depth),

                                       total_staked=matched_profit_if_lose) + ('BACK',)

            else:

                return self.calc_green(avg_odds,

                                       current_price_of_runner=get_price(runner.ex.available_to_back, 0),

                                       total_staked=matched_profit_if_lose) + ('BACK',)



    def calc_green(self, avg_odds, current_price_of_runner, total_staked):

        green_all_stake = avg_odds / current_price_of_runner * total_staked

        return green_all_stake, current_price_of_runner





def _calculate_underround(runners: list) -&gt; float:

    return sum(

        [

            1 / get_price(r.ex.available_to_lay, 0)

            for r in runners

            if r.ex.available_to_lay

        ]

    )```

*Tags: Deployment, Strategies*

---

## 2023-02-09

**Trex44** - *20:46:51*

:weary: So obvious too. Fixed it, thanks very much.

*Tags: Errors Debugging*

---

**Trex44** - *21:01:26*

I know lol, thought that too. Hubris, I had it in my head I had already modified it to fix on the first runner it placed a bet on.

*Tags: Errors Debugging*

---

## 2023-02-10

**AI Trader** - *02:31:07*

Hi guys,

How can I get the market catalogue through the streamer?

I am using the filters

```market_data_filter = streaming_market_data_filter(

    fields=["EX_BEST_OFFERS_DISP", "EX_TRADED_VOL", "EX_MARKET_DEF"],

    ladder_levels=3

)```

But when I get the market in

```process_market_book(self, market, market_book)```

, market.market_catalogue is None.

How can I get it?

*Tags: General Technical*

---

## 2023-02-12

**liam** - *06:43:30*

It’s an order which is ‘live’ / ‘executable’ in the market for all strategies. Any reason you don’t use the helper functions for exposure / runner orders?

*Tags: Deployment*

---

**Johnny Boston** - *06:46:40*

Just so I'm sure we're talking about the same thing, what helper functions are you referring to? Just the properties on the blotter?

*Tags: General Technical*

---

**liam** - *06:47:55*

`strategy_selection_orders()`

*Tags: Strategies*

---

**liam** - *06:52:29*

It filters per strategy but if you look at the code you can create your own version if required 



[https://github.com/betcode-org/flumine/blob/aaf2e1a027a19e1bb2209a77015e16eb8c689040/flumine/markets/blotter.py#L74|https://github.com/betcode-org/flumine/blob/aaf2e1a027a19e1bb2209a77015e16eb8c689040/flumine/markets/blotter.py#L74](https://github.com/betcode-org/flumine/blob/aaf2e1a027a19e1bb2209a77015e16eb8c689040/flumine/markets/blotter.py#L74|https://github.com/betcode-org/flumine/blob/aaf2e1a027a19e1bb2209a77015e16eb8c689040/flumine/markets/blotter.py#L74)

*Tags: Strategies*

---

**Johnny Boston** - *07:00:15*

Thanks for the help.

*Tags: General Technical*

---

**AI Trader** - *16:59:16*

Hi [@U4H19D1D2](@U4H19D1D2), thanks a lot!



I am backtesting the strategy using the Flumine Backtester (along with data collected from Flumine data recorder).

My my market_catalogue is never available (market.market_catalogue is always None).



I know it stores data in two different folders:

• marketCatalogue/

• streaming/

I am only giving passing to the backtester the path to the streaming file (in the market_filter argument, see code below).  My understanding is that the streaming data also contains market_catalogue messages from the exchange, so that shouldn't be a problem. Am I missing something? Any help will be appreciated.



```strategy = MakerStrategy(

    client=client,

    name="bt",

    market_filter={"markets": [path_to_streaming_file]},

    conflate_ms=100,

    max_live_trade_count=1000,

    max_selection_exposure=1000,

    max_order_exposure=1000

)```

*Tags: Deployment, Strategies*

---

**Trex44** - *22:27:37*

Is there a way to add multiprocessing to the FlumineSimulation class (think this would be the correct class to add it to) to get it to speed up back tests? Idea would be to run the simulation on the markets in parallel then collect and merge the results. Mt attempts so far haven't worked.

*Tags: Performance*

---

**Andrew** - *22:53:21*

Taking such an approach means you can’t have bank management across markets. Which market runs first? But to simulate strategy on markets in parallel to collect results I’ve used ProcessPoolExecutor in Python concurrent.futures.

*Tags: Strategies*

---

## 2023-02-13

**Andrew** - *00:31:20*

Borrowed from elsewhere, and modified for my use. Can't recall where from.



`market_files = [f'{MARKET_FILES_PATH}/{path}' for path in os.listdir(MARKET_FILES_PATH,)]`



`def run_process(market_files):`

    `price_recorder = PriceRecorder(`

        `market_filter=`

        `{`

            `'markets': market_files,`

            `'event_type_ids': ['4339'],`

            `'country_codes': ['AU'],`

            `'market_types':['WIN'],`

            `'listener_kwargs': {'inplay': False},`  

        `},`

    `)`



    `framework = FlumineSimulation(clients.SimulatedClient())`

    `framework.add_strategy(price_recorder)`

    `framework.run()`



`def main():`

    `_start = time.time()`



    `processes = os.cpu_count()  # Returns the number of CPUs in the system.`

    `markets_per_process = 8   # 8 is optimal as it prevents data leakage.`



    `_process_jobs = []`

    `with futures.ProcessPoolExecutor(max_workers=processes) as p:`

        `# Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have.`

        `chunk_size = min(`

            `markets_per_process, math.ceil(len(market_files) / processes)`

        `)`

        `# Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs`

        `for markets in (utils.chunks(market_files, chunk_size)):`

            `_process_jobs.append(`

                `p.submit(`

                    `run_process,`

                    `market_files=markets,`

                `)`

            `)`

        `for job in [http://futures.as|futures.as](http://futures.as|futures.as)_completed(_process_jobs):`

            `job.result()  # wait for result`



    `print(f'Total execution time: {(time.time() - _start):.2f} secs')`



`if __name__ == "__main__":`

    `main()`

*Tags: Strategies*

---

**liam** - *07:21:31*

[https://betcode-org.github.io/flumine/performance/|Yes](https://betcode-org.github.io/flumine/performance/|Yes) 

*Tags: Performance*

---

## 2023-02-15

**AI Trader** - *04:08:46*

Hi guys,

I am running the Flumine backtester, and for some reason the backtester never exits.

It gets stuck at self._process_end_flumine() in FlumineSimulation with no error messages or logs. Anyone had a similar issue?

I suspect there could be an error not being shown, as for some reason no erros are being logged when running in Simulation mode (using Pycharm).

*Tags: Errors Debugging*

---

**Charaka A** - *15:17:22*

Hi guys, when using the betfair lightweight package after creating a stream as below, how can I check if all the markets of the stream have been completed (the event is complete)



```self.stream = self.client.streaming.create_stream(

    unique_id=self.streaming_unique_id, listener=self.listener

)```

I would like to use this to close the thread running the stream, since there is no longer any incoming data to listen to.

*Tags: General Technical*

---

## 2023-02-16

**AI Trader** - *02:53:16*

I think I might have narrowed down the error.



To be more precise, when debugging, the code stops at line 34 (in FlumineSimulation.run() on `with self:`) [https://github.com/betcode-org/flumine/blob/aaf2e1a027a19e1bb2209a77015e16eb8c689040/flumine/simulation/simulation.py#L34](https://github.com/betcode-org/flumine/blob/aaf2e1a027a19e1bb2209a77015e16eb8c689040/flumine/simulation/simulation.py#L34), with no errors, but never ends.



I think the reason for the error is that I have created a Child class of Flumine BaseStrategy, and thus I am not reaching BaseFlumine.__exit__.



Will try to produce a minimal example tomorrow

*Tags: Errors Debugging, Strategies*

---

**Johnny Boston** - *03:47:07*

with Flumine, is there a way to see what position your order is in on a specific price in the ladder?



eg If I put out an offer to bet at 2-1 and there is *£*500 on the offer can I see how many individual orders are part of that *£500 ?*

*Tags: General Technical*

---

## 2023-02-17

**AI Trader** - *03:47:32*

Hi [@U4H19D1D2](@U4H19D1D2),

I found a solution for the issue, but I would still like to figure out what was going wrong if you could help.

I solved it by replacing the JupyterLoggingControl in the examples by the BacktestLoggingControl.



To be more precise, it was getting stuck at c.join() in BaseFlumine ([https://github.com/betcode-org/flumine/blob/aaf2e1a027a19e1bb2209a77015e16eb8c689040/flumine/baseflumine.py#L445](https://github.com/betcode-org/flumine/blob/aaf2e1a027a19e1bb2209a77015e16eb8c689040/flumine/baseflumine.py#L445))



`Solution`

```framework.add_strategy(flumine_strategy)

# framework.add_logging_control(JupyterLoggingControl(directory=backtester_orders_path, file_name=f"{market_id}.json"))

framework.add_logging_control(BacktestLoggingControl())

framework.run()```



*Tags: Strategies*

---

## 2023-02-18

**SrFabio** - *16:02:11*

Hey guys what live data feed would you guys recommend for football games? I'm developing an hybrid trading approach with some automated components and it's very sensitive to delay, I'm not sure if you guys have tried anything I just need to know the events (goals, fouls, corners etc..) as they happen with the least delay possible

*Tags: Deployment, Strategies*

---

**AI Trader** - *22:07:05*

Hi guys,

How can I reconstruct the betfair URL (eg

[https://www.betfair.com/exchange/plus/football/market/1.208541766](https://www.betfair.com/exchange/plus/football/market/1.208541766))  from information available in the placed orders (fetched from betfairlightweight.betting.list_current_orders) ?

I need both the event type (eg footbal) and the market_id. The response from betfairlightweight.betting.list_current_orders only includes the market_id. Any advices?



Also, could anyone confirm that market_id is unique across all event types as well?

*Tags: Strategies*

---

## 2023-02-19

**Peter** - *09:10:54*

You can't directly. Betfair's order processing is the same for all orders so doesn't care what the event type is and therefore doesn't include it in the response. Three possible ways around this, each with pros and cons would be:

• a lookup on the market_id (yes they are unique)

• put the event type ID or name in the customer strategy reference so that it is returned with the orders response

• keep a record your end of the orders that you've placed against which you can lookup the responses

*Tags: Strategies*

---

## 2023-02-20

**Unknown** - *17:37:20*

*Backtester*

Hi guys,

I have spent a lot of time reconciling my self-developed backtester against Flumine backtester. I am noticing big inconsistencies that I would appreciate someone helping me to understand.



In many examples, I have a limit order sitting on the queue with a certain piq (according to Flumine), and I see many trades happening at the same price of my order, and my position in the queue only gets reduced by a fraction of these trades.



A complete example follows below:

At timestamp t0,  I have a limit order of size 10 at 3.55 sitting in the book with piq 27.89 (from Flumine).

In the first screenshot (taken at t0), we can see that 33.52 $ have been traded at the price of 3.55.



A few moments later,  at timestamp t1, (second screenshot), we can see that $62.25 have been traded at the price of 3.55, that is, a difference of 28.73$, which I interpret as trades happening at that price.

Surprisingly, my position in the queue (piq) according to Flumine is now 13 and I haven't filled anything. I would have expected to fill in 28.73 - 27.89 = 0.84, which is the difference between the trades happening and my piq of my order, and now have a position in the queue of 0 (which would mean I am at the top of the queue).



My main assumption here is that the difference (in different timestamps) of the traded volume for a given price is the amount traded on that price, and that this amount should either reduce my piq by the exact traded amount or fill my order in case the traded amount is bigger than my piq. Am I missing anything here?

*Tags: General Technical*

---

**AI Trader** - *18:58:48*

Diving deeper in on Flumine, it seems to me that the inconsistency with respect to my own backtester is cause by this function:





```def _calculate_process_traded(self, publish_time: int, traded_size: float) -&gt; float:

    _traded_size = traded_size / 2

    if self._piq - _traded_size &lt; 0:

        size = _traded_size - self._piq

        size = round(min(self.size_remaining, size), 2)

        if size:

            self._update_matched(

                [

                    publish_time,

                    self.order.order_type.price,

                    size,

                ]

            )

        _matched = (self._piq + size) * 2

        self._piq = 0

        return _matched

    else:

        self._piq -= _traded_size

        if logger.isEnabledFor(logging.DEBUG):

            logger.debug(

                "Simulated order {0} PIQ: {1}".format(self.order.id, self._piq)

            )

        return traded_size```

If I understand correctly, we only assume execution if the traded amount is greater than 2 times my piq, and every time there is a trade, my piq only gets reduced by half the traded amount. If this is correct, could someone explain why you decided to make these hypotheses? [@U4H19D1D2](@U4H19D1D2), I understand that might have been done to be conservative on the execution, at the same time, this leads to further inconsistencies such as having a piq that is higher than the volume on the top of the orderbook (which is impossible).

*Tags: Errors Debugging*

---

**liam** - *19:01:44*

Because traded volume is double what is actually matched, flumine is correct 

*Tags: General Technical*

---

**Unknown** - *20:38:48*

*Data collected using Flumine*

Hey there, I'm using Flumine's data recorder to collect streaming data. During my exploratory data analysis, I noticed some potential inconsistencies in the data and would appreciate any thoughts or insights.



To analyze the data, I've organized it into a sorted timestamp dataframe that includes both trades and the orderbook. To account for double-counting, I've divided the size of the trades by two, which has resulted in some interesting observations.



For example, in the screenshot, just before a trade occurred, the best available price to lay at level 0 was 3.55, and the size was 11.9$. At the time of the trade, the trade price was 3.55$  with a size of  5.995 * 2 = 11.99$ (in my table, I display this as 5.995$ after dividing by two, as previously mentioned). The entire level 0 was consumed by this trade, as indicated by the fact that the level with a price of 3.55 no longer existed in the timestamp just after the trade. Based on this, I believe that the actual traded amount was $11.9, and not 0.5 * $11.9 as I initially thought.



However, I've also noticed that in the majority of cases, the volume consumed from the orderbook is only half of the traded amount, which suggests that dividing by two is necessary.





This leads to the strange behavior that sometimes I need to divide the traded size by two, while other times I do not. Do you have any ideas to explain this?

*Tags: Feature Engineering*

---

**AI Trader** - *21:19:00*

*Conflate logic*

Another question guys,

For the Flumine simulation, I am trying to check wether the conflate_ms is working or not, and it seems to me that it is not.

I have added a print of the timestamps (market_book.publish_timestemp_epoch) in the *process_market_book* , and no matter what conflate_ms I use (here I am using 10_000 ms), it always receives the same updates (which seems to be the most granular possible).

Is there anything extra required to make conflate_ms work in simulation mode?



Below is how I am instantiating the FlumineStrategy.

```flumine_strategy = FlumineStrategy(

    strategy = strategy,

    client=client,

    name="bt",

    market_filter={"markets": [market_file]},

    conflate_ms=10_000,

    max_live_trade_count=1000,

    max_selection_exposure=1000,

    max_order_exposure=1000

)```



*Tags: Deployment, Strategies*

---

**D C** - *21:21:48*

Is 10_000 a flumine thing (as opposed to 10000) ?

*Tags: General Technical*

---

**liam** - *21:22:09*

Python thing 

*Tags: General Technical*

---

## 2023-02-21

**liam** - *08:03:20*

[https://github.com/betcode-org/flumine/issues/508|issue](https://github.com/betcode-org/flumine/issues/508|issue)

*Tags: General Technical*

---

**Ke** - *09:05:19*

Is it possible to subscribe extra markets while flumine is running, maybe use worker?

*Tags: General Technical*

---

**liam** - *09:05:48*

fix your initial filter

*Tags: Errors Debugging*

---

**Ke** - *09:14:17*

For the new market which is recently available, we have to restart flumine to pick it up? Any way to include new markets without stopping flumine?

*Tags: General Technical*

---

**Ke** - *09:41:26*

Oh, i didn't realize it works that way. That would be great. Regarding filter, i'm only interested in NBA. But it looks there is no way to filter only NBA games. I can ignore all other games when process market data, but would it cause any latency issue as it streams data of games I'm not interested?

*Tags: Performance*

---

**liam** - *09:46:59*

Assuming you are filtering by NBA marketTypes I think you would struggle to register any latency issues, can't filter by competitionId when streaming :wink:

*Tags: Performance*

---

**Andrew** - *10:10:12*

Want a dollar for that question? :grin: 

*Tags: General Technical*

---

**Ke** - *10:38:32*

If i subscribed other markets which i'm not interested in as i can't filter them out, would it slow down the streaming for the games I'm interested in? Or would the stream speed be affected by the number of the game i subscribed?

*Tags: Performance*

---

**liam** - *10:44:55*

you will be fine, streaming is incredibly lightweight on cpu/memory

*Tags: Performance*

---

**Ke** - *11:40:58*

Does anyone use 3rd party api to incorporate live score in the strategy? Any recommendations?

*Tags: Deployment, Strategies*

---

## 2023-02-22

**jp** - *09:45:55*

Anyone got experience with live/inplay soccer data from [https://www.thesports.com/](https://www.thesports.com/)? In particular I am interested in latency, quality of data and pricing.

*Tags: Performance, Deployment*

---

## 2023-02-27

**AI Trader** - *01:57:47*

Hi guys,

From the streaming data, is it possible to know if a trade was back or lay?

I thought it would be enough to look at the orderbook prior to the trade and see if the traded price was closer to best available to back (or lay), but it happens very often that in the last book update, the best available to lay and back are equally distant to the traded price

*Tags: General Technical*

---

**liam** - *08:23:05*

technically a trade is both a back and a lay but to answer your question no you don't know if the price that is taken is a back or lay

*Tags: General Technical*

---

## 2023-03-04

**Unknown** - *15:53:51*

Problems??

*Tags: General Technical*

---

**Lee** - *17:16:17*

did anyone have any flumine instances that hit 100% cpu and died?

*Tags: General Technical*

---

## 2023-03-05

**Will** - *08:21:25*

hey legends - what is the best way to store/gather past last traded price on a runner. for example, currently i load a dataframe with the  ltp from about 10mins out every time it changes, then perform my predictions. if i load too many markets, i end up with latency issues.

*Tags: Feature Engineering, Performance*

---

**Jonjonjon** - *09:19:19*

Are you doing it on live data with Pandas? If so, Pandas is too slow if you are updating it with every price update. More primitive types should be fastwr

*Tags: Feature Engineering, Performance, Deployment*

---

**Will** - *09:27:59*

Building a list with ltp and seconds to start then converting to a pandas data frame. 

*Tags: Feature Engineering*

---

**Jonjonjon** - *09:34:28*

The conversion to Pandas could be a big bottleneck.

*Tags: Feature Engineering*

---

**PeterLe** - *12:25:13*

Quick question please...if you have more than one account....

When recording more than one sport, ie UK/IE Horses; UK Dogs, UK football, US Horses etc....

Would you record all one on account (Appreciating you can only have 10 connections...) or would you record each sport on a different account?

I think I have the number of markets I can subscribe to lifted anyway

Does it matter?

How do you organise it&gt;?  Thanks

*Tags: General Technical*

---

## 2023-03-08

**Unknown** - *08:56:27*

Hi everyone, apologies for a complete beginner question but I am brand new to Python and betfairlightweight. I have cannibalised some code online to extract the following data from the Betfair Historical Data BASIC soccer files. It has the last pre-play odds and the ltp which is more than I ever expected to get! However, what I would really like is to adjust the code so it produces a similar Excel file but with the last pre-play odds (already got), the half-time odds (if the market is not closed at half-time), the highest price traded and the lowest price traded. If at all possible as a cherry on the top, if all of the recorded odd movements could be put into the columns to the right from the market going in-play until it closes that would be great. Apologies for such a basic question, finding my feet with coding!

*Tags: Getting Started, Data Quality*

---

**Andrew** - *09:44:28*

Hi Richard. Welcome to the quinella - Python and betfairlightweight :blush:  For the extra variables you will need to process stream data. Check out the examples at Betfair Automation Hub run by the wonder AU team. [https://betfair-datascientists.github.io/|https://betfair-datascientists.github.io/](https://betfair-datascientists.github.io/|https://betfair-datascientists.github.io/) You'll help your Python learning by following the examples.

*Tags: General Technical*

---

**Richard Cornish** - *10:06:33*

Thank you very much, these were the tutorials I was following along with which got me to this point :slightly_smiling_face:  I will go through them all again and try to work out the answer. The trickiest thing I have found is a lot of examples don’t work through soccer data which has halftime to deal with. You wouldn’t know of anywhere with worked examples of exporting football data that I would work through? Many thanks for the help! 

*Tags: General Technical*

---

## 2023-03-09

**Richard Cornish** - *13:32:26*

Any chance to have a look at this one? I’m still trying but failing miserably with the various Python libraries :joy:  if I use gigasheet to combine and transform the files that works, but it is slow and labour intensive 

*Tags: Performance*

---

**Unknown** - *19:06:39*

I have narrowed it down to the fact that the code is trying to account for matched volume on the ladder, and as this is a basic download there is no volume. I believe this is what is preventing the inplay min and max from populating, but I cannot for the life of me work out how to just return the inplay high and low. Resource for BASIC files seem to be a bit thin on the ground, especially for soccer

*Tags: General Technical*

---

**Richard Cornish** - *20:12:37*

Thank you so much Liam for taking the time. The tutorial is interesting for someone brand new to the module and Python, but I’m sure it could be much more elegant. I found out today how to inspect the betfairlightweight module which has been another enlightening step! 

*Tags: Getting Started*

---

## 2023-03-10

**liam** - *10:06:35*

Here is a hopefully an easier to understand script and considerably faster, few caveats:



• Half time isn't possible without some assumptions and/or other data

• All odds movements would be a massive list, is that what you actually want?

```import logging

import csv

import smart_open

from unittest.mock import patch as mock_patch

import betfairlightweight



COLUMNS = [

    "market_id",

    "event_date",

    "event_name",

    "country",

    "market_name",

    "selection_id",

    "selection_name",

    "result",

    "actual_sp",

    "pp_min",

    "pp_max",

    "pp_wap",

    "pp_ltp",

    "pp_volume",

    "ip_min",

    "ip_max",

    "ip_wap",

    "ip_ltp",

    "ip_volume"

]



file_path = "/Users/liampauling/Documents/tmp/marketdata/1.177242007.gz"



# setup logging

logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



# create trading instance (don't need username/password)

trading = betfairlightweight.APIClient("username", "password", "appKey")



# create listener

listener = betfairlightweight.StreamListener(

    max_latency=None,  # ignore latency errors

    output_queue=None,  # use generator rather than a queue (faster)

    lightweight=True,  # lightweight mode is faster

    update_clk=False,  # do not update clk on updates (not required when backtesting)

    cumulative_runner_tv=True,

    calculate_market_tv=True,

)



with mock_patch("builtins.open", smart_open.open):

    # create generator

    stream = trading.streaming.create_historical_generator_stream(

        file_path=file_path,

        listener=listener,

    )

    market_data = {}  # store all data



    # loop all markets

    for i in stream.get_generator()():

        for market_book in i:

            market_id = market_book["marketId"]

            if market_id not in market_data:

                market_data[market_id] = {

                    "market_definition": market_book["marketDefinition"],

                    "selections": {

                        i["id"]: {

                            "id": i["id"],

                            "name": i.get("name"),

                            "pp_min": None,

                            "pp_max": None,

                            "pp_ltp": None,

                            "pp_volume": None,

                            "ip_min": None,

                            "ip_max": None,

                            "ip_ltp": None,

                            "ip_volume": None,

                        } for i in market_book["marketDefinition"]["runners"]

                    },

                    "inplay": False,

                    "final_market_book": None,

                }

            market = market_data[market_id]



            # update selection values when prePlay

            if market_book["status"] == "OPEN" and not market_book["inplay"]:

                for selection in market_book["runners"]:

                    selection_data = market["selections"][selection["selectionId"]]

                    if selection_data["pp_min"] is None:

                        selection_data["pp_min"] = selection["lastPriceTraded"]

                        selection_data["pp_max"] = selection["lastPriceTraded"]

                        selection_data["pp_ltp"] = selection["lastPriceTraded"]

                        selection_data["pp_volume"] = selection["totalMatched"]

                    else:

                        selection_data["pp_min"] = min(selection_data["pp_min"], selection["lastPriceTraded"])

                        selection_data["pp_max"] = max(selection_data["pp_max"], selection["lastPriceTraded"])

                        selection_data["pp_ltp"] = selection["lastPriceTraded"]

                        selection_data["pp_volume"] = selection["totalMatched"]



            # update selection values when inPlay

            if market_book["status"] == "OPEN" and market_book["inplay"]:

                for selection in market_book["runners"]:

                    selection_data = market["selections"][selection["selectionId"]]

                    if selection_data["ip_min"] is None:

                        selection_data["ip_min"] = selection["lastPriceTraded"]

                        selection_data["ip_max"] = selection["lastPriceTraded"]

                        selection_data["ip_ltp"] = selection["lastPriceTraded"]

                        selection_data["ip_volume"] = selection["totalMatched"]

                    else:

                        selection_data["ip_min"] = min(selection_data["ip_min"], selection["lastPriceTraded"])

                        selection_data["ip_max"] = max(selection_data["ip_max"], selection["lastPriceTraded"])

                        selection_data["ip_ltp"] = selection["lastPriceTraded"]

                        selection_data["ip_volume"] = selection["totalMatched"]

                        selection_data["actual_sp"] = selection["sp"]["actualSP"]



            # final book

            market["final_market_book"] = market_book



# write data to csv

with open("output_bflw_new.csv", "w") as f:

    writer = csv.DictWriter(f, fieldnames=COLUMNS)

    writer.writeheader()

    for market_id, market in market_data.items():

        market_definition = market["market_definition"]

        selection_lookup = {i["selectionId"]: i for i in market["final_market_book"]["runners"]}

        for selection in market["selections"].values():

            writer.writerow(

                {

                    # market level data

                    'market_id': market_id,

                    'event_date': market_definition["marketTime"],

                    'event_name': market_definition.get("eventName"),

                    'country': market_definition["countryCode"],

                    'market_name': market_definition.get("name"),

                    # selection level data

                    'selection_id': selection["id"],

                    'selection_name': selection["name"],

                    'result': selection_lookup[selection["id"]]["status"],

                    'actual_sp': selection["actual_sp"],

                    "pp_min": selection["pp_min"],

                    "pp_max": selection["pp_max"],

                    # "pp_wap",

                    "pp_ltp": selection["pp_ltp"],

                    "pp_volume": selection["pp_volume"],

                    "ip_min": selection["ip_min"],

                    "ip_max": selection["ip_max"],

                    # "ip_wap",

                    "ip_ltp": selection["ip_ltp"],

                    "ip_volume": selection["ip_volume"],

                }

            )```

*Tags: Getting Started, Errors Debugging, Performance, Strategies*

---

**Richard Cornish** - *14:00:53*

Hi Liam, thank you so much for your help, that is now exporting exactly the data I was after! One final question, I'm just trying to modify the code so it will run through all the folders I have and extract the bz2 files for analysis and pop them all in the one csv file. Can you point me in the direction of amending the code for this? Many thanks again

*Tags: General Technical*

---

**liam** - *14:26:29*

You can just copy the original code



```with mock_patch("builtins.open", smart_open.open):

    for path in glob.iglob(file_path + "**/**/*.bz2", recursive=True):

        # create generator

        stream = trading.streaming.create_historical_generator_stream(

            file_path=path,

            listener=listener,

        )

        ...```

One thing to note is that this will write the data once complete so if there is an error it won't have recorded anything, you might want to refactor to write to file after each market

*Tags: Errors Debugging, Strategies*

---

**Unknown** - *15:11:13*

Sorry to take up so much of your time, I am a bit further along but still getting some error messages when running the following script:

*Tags: Errors Debugging*

---

**liam** - *16:16:58*

and the error?

*Tags: Errors Debugging*

---

## 2023-03-11

**Unknown** - *13:55:27*

Hi, I am a step closer in that this file is now going into the file and exporting the data which is excellent! The only problem now is it is only returning one market. I don't seem to get any error messages, but it just says there is one market in the cache. Apologies for taking up more of your time, but would you be able to cast an eye over this one and see if it is something painfully obvious? I have also uploaded the csv file produced which just has one match shown (and also spacing but this bit I am not as worried about!)

*Tags: Errors Debugging*

---

**ian mcneill** - *17:50:18*

Hi all, apologies for the simple question - I have flumine working and recording locally and all good - is there an example snippet to use an S3 bucket to hold the closed market archives? I’m a c# dev and just learning Python which I’m enjoying but any help on this would be greatly appreciated.



A side q - I was thinking about using Lightsail containers for simplicity but I cant find any option for local storage - so looks like ECS is the only option for this?



Any recommendations would be great - TIA.

*Tags: General Technical*

---

**Lee** - *18:17:48*

[https://github.com/betcode-org/flumine/blob/9dc0bf01ac211f925c49fcc05a8617e0c2a61b5b/examples/strategies/marketrecorder.py#L200|https://github.com/betcode-org/flumine/blob/9dc0bf01ac211f925c49fcc05a8617e0c2a61b5b/examples/strategies/marketrecorder.py#L200](https://github.com/betcode-org/flumine/blob/9dc0bf01ac211f925c49fcc05a8617e0c2a61b5b/examples/strategies/marketrecorder.py#L200|https://github.com/betcode-org/flumine/blob/9dc0bf01ac211f925c49fcc05a8617e0c2a61b5b/examples/strategies/marketrecorder.py#L200)



*Tags: General Technical*

---

## 2023-03-12

**birchy** - *13:41:24*

Regarding Lightsail, I use these instances as they're far easier to setup than EC2. You can add extra storage (see screenshot), however I just run the marketrecorder on a low cost instance and push to S3.

*Tags: Getting Started, Deployment*

---

## 2023-03-13

**Andy Bason** - *09:19:47*

Hi guys, say I want to compare the LTPs of all runners in a horse race at time a and time b. What is the most efficient way to store/compare the data? My first attempt uses Pandas but it's clear now it's not up to the job. (btw I'm using betfairlightweight) Cheers

*Tags: Feature Engineering*

---

**Andy Bason** - *09:33:10*

It'll be my beginner knowledge level. Thanks for the help

*Tags: Getting Started*

---

**Mo** - *09:34:38*

Maybe I'm not understanding the problem but just record a dictionary of selection ID to LTP at time a then at time b do the comparison

*Tags: General Technical*

---

**Alex** - *11:21:21*

You can then turn the dictionary into a pandas Series with `pd.Series(ltp_dict)` (assuming ltp_dict) holds the prices.

*Tags: Feature Engineering*

---

**Lucas Torres** - *11:52:02*

Hi guys, I'm new to the developer application of the bf, but I'm likeing to discover this. I just want the data from US horse racing and I just want the BSP of the runners. Could someone help me?

*Tags: Getting Started*

---

**Unknown** - *12:23:49*

Just one more question, do you know why my BSP seems like multiplied x 10^5 ?

*Tags: General Technical*

---

**ian mcneill** - *18:25:42*

Thanks - I seem to remember there was example code to set this up - is that still available - I know boto3 config is needed to connect to the bucket but as a python noob I’m struggling to hook this together - any pointers most welcome :slightly_smiling_face:

*Tags: General Technical*

---

**Michael** - *19:29:26*

Birchy are you writing to a server on the lightsail instance or doing it another way? I’m currently writing data to mysql on lightsail but its for scraping so not sure how it will react when I’m writing data every second for a market recorder

*Tags: Data Quality, Deployment*

---

**birchy** - *20:10:03*

[@U027P3N2WMQ](@U027P3N2WMQ) I'm using the S3MarketRecorder example which uses boto3 to push to S3. The only extra setup from your end is to create a `.aws` folder in your home directory for your credentials. 

[https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html|https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html|https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)

*Tags: Getting Started, Deployment*

---

**birchy** - *20:11:59*

[https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html|https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html|https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)

*Tags: Deployment*

---

## 2023-03-16

**rob smith** - *09:44:11*

Hi, can anyone see what the issue is with my place bet order? I am trying to place a lay at BSP but just get a failure response. Thanks

```limit_order = filters.limit_order(size=10, price=1.01)

instruction = filters.place_instruction(order_type="LIMIT_ON_CLOSE", selection_id=runner.selection_id, side="LAY", limit_order=limit_order)

place_orders = trading.betting.place_orders(market_id=market_book.market_id, instructions=[instruction], customer_strategy_ref="Lay")```



*Tags: Strategies*

---

**rob smith** - *09:53:24*

Seems there's more than one issue as I still get the failure message. Changing limit_order to limit_on_close_order gives me a 32602/DSC-0018 error

*Tags: Errors Debugging*

---

**ian mcneill** - *10:03:18*

Thanks folks - v helpful - yes I copied the S3MarketRecorder into my source

*Tags: General Technical*

---

## 2023-03-21

**Tom** - *10:11:03*

Hey guys, I was around about 6 months ago looking at a betfair bot. Since then have done CS50, CS50p, some data-science, maths and half a dozen udemy courses and am now building the bot.  Between betfairlightweight and flumine it just blows my mind how this stuff is out there (not to mention the type of statistical libraries and things that are out there).  Can already see a use for betfairutil as well so cheers [@UBS7QANF3](@UBS7QANF3) and [@U4H19D1D2](@U4H19D1D2).



 It seems like flumine is the quickest way to login to the API and start streaming, although I'd done it all in betfairlight in the past. Is there any wisdom on the best way to use combinations of these to get what you want from it? I'd say my strategy is most like market-making (in horseraces) in terms of how it executes

*Tags: Strategies*

---

**Tom** - *10:14:12*

I can also get markets e.t.c. from the API, but run into this trying to execute stream.start()



betfairlightweight.exceptions.ListenerError: connection_id: 107-210323101234-6265894, data: {"op":"status","id":2,"statusCode":"FAILURE","errorCode":"NOT_AUTHORIZED","errorMessage":"AppKey is not configured for service","connectionClosed":true,"connectionId":"107-210323101234-6265894"}



I've contacted betfair about it and just waiting but thought there could be some wisdom here on it

*Tags: Errors Debugging*

---

**D C** - *10:26:26*

When I wanted to use stream API I had to make an application for it - even though I had been using the SOAP and NG API for years. They may have changed things and made streaming accessible by default but based on your error message it is probably not the case so if you have been in touch with Betfair about it you probably just need to wait for approval.

*Tags: Errors Debugging*

---

**Peter** - *10:33:41*

Flumine wraps around Betfairlightweight. It provides a trading framework with BFLW providing the connection to Betfair in the background. So the the best way to combine them is simply to work in Flumine.

*Tags: Strategies*

---

## 2023-03-22

**Tom** - *02:52:17*

What's the best way to familiarise oneself with all the different functionalities and options? I'm looking on GitHub and the docs but I'm running myself in circles - I'm sure that a lot of what I want to do are the classic problems that have been overcome in the libraries - but where do you find and understand all the functionality?



getting market_percentage for example, I'm trying to use betfairutil (and I'm sure this is straightforward), but I'm having trouble going from the dataframe I've created to what's needed



```market_books = betfairutil.read_prices_file(path_to_prices_file)```

are prices_files standardised outputs from betfair?



I thought market_books themselves were betfair objects?



I seem to be confusing myself chasing different ideas of how it works without really sitting down and trying to understand the libraries first - I have a clear idea of the strategy e.t.c. but in terms of structuring software it's tricky because I can't conceptualise the end result (which parts to make modular, how to structure the different working bits and pieces etc)

*Tags: Feature Engineering, Strategies*

---

**Mo** - *05:51:56*

1. Prices files could be self recorded or purchased from Betfair. The format is the same

2. It's not entirely clear what you mean by market books being "betfair objects" but the answer is probably yes. The Betfair API defines what a MarketBook object is [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-MarketBook](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Betting+Type+Definitions#BettingTypeDefinitions-MarketBook)

3. betfairutil has a calculate_book_percentage function [https://github.com/mberk/betfairutil/blob/525dc4dc9d87bc9438e72c24fd024d9bcab29b6c/betfairutil/__init__.py#L1518-L1540](https://github.com/mberk/betfairutil/blob/525dc4dc9d87bc9438e72c24fd024d9bcab29b6c/betfairutil/__init__.py#L1518-L1540)

*Tags: Strategies*

---

**Mo** - *05:54:09*

If you have a clear idea of your strategy then suggest you start with the flumine examples - [https://github.com/betcode-org/flumine/tree/master/examples/strategies](https://github.com/betcode-org/flumine/tree/master/examples/strategies) - and adapt for your needs

*Tags: Strategies*

---

**birchy** - *13:03:22*

LowestLayer is a good starting point from [@UBS7QANF3](@UBS7QANF3)'s link and book % example in that. For the book calculation, just be wary that PLACE, etc markets can have multiple winners, so the 0EV book will be 100% * number_of_winners, e.g. 200% for 2TBP markets.

[https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py#L73|https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py#L73](https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py#L73|https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py#L73)

*Tags: General Technical*

---

## 2023-03-23

**JC** - *12:08:09*

Hi, when streaming market data using flumine to S3, is there a recommended way to collect and store metadata and marketcatalogue data into a lookup table in S3 or some database? Cheers

*Tags: General Technical*

---

**Peter** - *12:20:43*

Not explicitly, but the catalogue is just a json file so it's easy to read, filter and persist the data you may want to look up afterwards. Personally I write key data from each to a database table together with a list of the winners (obtained by reading the last few lines of the associated stream file). That table is then used to compile lists of candidate markets for backtesting.

*Tags: General Technical*

---

**JC** - *12:24:49*

Thanks [@U9JHLMZB4](@U9JHLMZB4). Do you update your database on the fly with `on_market_close` or have a process that crawls through recorded streaming files to update your database periodically e.g. daily? I used to do the former but thinking of moving to the latter as I’m moving all storage to s3.

*Tags: General Technical*

---

**Peter** - *12:36:43*

It's a process that runs independently on an EC2 instance. But that's mainly because of how I constructed it originally. It could just as well have been contained by a logging control as the market was closed, with pros and cons each way. E.g. logging control would avoid the need to re-visit the files, but re-visiting the files confirms that they made it to S3 and helps handle those with closing wrinkles (e.g, multiple closures or none recorded).

*Tags: Deployment*

---

**JC** - *13:07:07*

Cheers [@U9JHLMZB4](@U9JHLMZB4), makes sense. [@U4H19D1D2](@U4H19D1D2) do you add to the market table in the db within flumine or with another process?

*Tags: General Technical*

---

**liam** - *17:04:29*

Flumine with a custom loggingcontrol that communicates via API to my db 

*Tags: General Technical*

---

**Jesus Perdomo** - *22:11:46*

Hey guys, so glad I found this library, very cool!



Just trying to figure out what would be best practice for my use (I read the official betfair stuff but doesn't really answer the question).

My understanding is that there is a limit to how many login requests the API can receive before soft-banning. The 'best practices' page states that a single login session can carry out multiple API calls as long as it's alive. I'm trying to make an app that gets some information from the API, hopefully in the future extend it to allow betting through the app as well. The app will make some requests to find out if a particular markets exists (by searching for a team name), and what the odds are at the time of request. The app will be using non-interactive logon as it is hands-off



Would best practice be as follows? Logon --&gt; Get Data --&gt; Logout?

Or should I just keep the connection open for the next API call? - I'd say the time in-between calls can be variable, sometimes in quick succession (seconds to minutes), but most of the time it would be in the minutes to hours range.



If the best thing to do is to keep the connection open, does the API send an exception code or some other response if a session is already open?

I'm just trying to figure out how to code the logic: If already logged-in, send call, otherwise, attempt login and get a session id

*Tags: Errors Debugging, Deployment, Strategies*

---

## 2023-03-24

**Tom** - *01:23:50*

I'm still waiting on Betfair to allow my API to stream Delayed Data; it looks like they might have even completely restricted my connection to the Betfair API to make calls.



I am testing code with it, although had throttled it to a single (horseracing) event and the markets within it; but was calling odds every minute to get market percentage - which I presume is why because it's streaming type behaviour.



I've emailed them 3 times about the delayed app key, so it's a bit tricky to get the strategy going; I'm working on other bits and pieces in the meantime but it's tricky to see how flumine works without the streaming.

*Tags: Strategies*

---

**Herugrim** - *07:50:25*

Information from the Best Practice page:



• Logins sessions last up to 12 hours by default and you can use [https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909|Keep Alive](https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909|Keep Alive) to extend the session beyond the stated session expiry time. 

• A single session can be used across multiple API calls/threads simultaneously.  *Please note:*  *you don't need to make a new login request for every API call*.

• You should ensure that you handle the *INVALID_SESSION_TOKEN* error within your code by creating a new session token via the [https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909|API login](https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909|API login) method.

• If[https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909#Login&amp;SessionManagement-LoginRequestLimits| login limits ](https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909#Login&amp;SessionManagement-LoginRequestLimits| login limits )are exceeded, you'll be automatically prevented from making further login requests for a period of 20 minutes. During this time all existing sessions will remain valid.



*Tags: Errors Debugging, Deployment*

---

**Jesus Perdomo** - *11:50:23*

I'll be placing bets as usual (not using the API but manually) - I'm just using the API for some alerts and a prompt on my phone to place a bet. Do I need to place a bet through the API to avoid a ban? Also, thanks for advising on the session. I guess the best way would be to try and send the API request, and if failed with exception INVALID SESSION or NOT LOGGED IN then attempt to login first

*Tags: Errors Debugging*

---

**liam** - *13:01:25*

Thats a very wide reaching question and ultimately boils down to market capacity. I personally run multiple strategies with similar models but different executions (the important part) however I know some centralise to try and have more control over execution / double taking etc

*Tags: Strategies*

---

**Tom** - *13:26:10*

Yeah I don't know how to think about it - on the one hand you'd think that there is some threshold of betting activity towards full book optimisation if you have too much going on in a single, closed event; but then it might detract from the risk/reward profile of each individual decision.

*Tags: Strategies*

---

## 2023-03-26

**Thomas JAMET** - *20:13:23*

How can I compute market book analytics (like moving averages) with Flumine? Is it something that's advisable?

*Tags: Feature Engineering*

---

**Peter** - *21:06:29*

You can certainly do that with Flumine. It will deliver the near-real-time state of the market as it updates from which you can calculate your analytics. But how you would compute them would depend on the specific analytics you want and possibly what you want to use them from.

*Tags: Deployment*

---

## 2023-03-27

**Peter** - *17:37:50*

Creating a middleware child class with a  `___call___()` method is exactly how I would do it. You can write the results to the market.context dictionary and they'll be available to every strategy that consumes that market.



Not sure why you would want a timer-based callback in this context.

*Tags: Strategies*

---

**Thomas JAMET** - *17:45:28*

To compute time-based analytics?

If there are no market-book updates, the middleware won't get called and the analytics won't get updated and become stale.

If there are no market-book updates (and no strategy order), the strategy won't get evaluated either I believe, but that's another question.

Is there a Flumine facility to have a timer-based callback?

I am now leaning towards creating a separate BackgroundWorker for this. Any suggestion?

*Tags: Strategies*

---

**Peter** - *17:50:27*

When you setup a strategy to subscribe to a market, you can set the streaming_timeout (seconds) parameter. That will force the a snapshot to be taken and evaluated at that frequency if no market updates have been received. [https://betcode-org.github.io/flumine/strategies/#parameters](https://betcode-org.github.io/flumine/strategies/#parameters)

*Tags: Getting Started, Strategies*

---

## 2023-03-28

**RDr** - *00:25:19*

Hi, I am only getting started and found some codes to get stream racing data using befairlightweight.

It sounds like Flumine could facilitate the process and run different market_ids streams in parallel/concurrently and sequentially.

Could someone assist and let me know what the Flumine codes would be to run and get/save different streaming data in parallel (different venues) and also in sequence (same venue for different races once the previous race market closes) ?

Thanks!



#===================================================

# Sample section of codes for betfairlightweight version (only 1 stream worked for me, not sure how to adapt the codes or use existing Flumine functions to record multiple markets in  parallel or sequentially)



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener)



# create filters (AU WIN racing)

# market_filter = streaming_market_filter(

#    event_type_ids=["7"], country_codes=["AU"], market_types=["WIN"],

# )

my_market_ids = [..., ...., ...]

market_filter = filters.market_filter(market_ids=my_market_ids)



market_data_filter = streaming_market_data_filter(

    fields=["EX_MARKET_DEF", "EX_LTP", "EX_BEST_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "SP_TRADED", "SP_PROJECTED"], ladder_levels=3

)



# subscribe

streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=1000,  # send update every 1000ms

)



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()



# Open a file for the market stream data

filename = f"{market_id}.txt"

filepath = os.path.join("data", filename)

with open(filepath, "w") as f:



    # check for updates in output queue

    while True:

        market_books = output_queue.get()

        print(market_books)



        for market_book in market_books:

            # print(

            #     market_book,

            #     market_book.streaming_unique_id,  # unique id of stream (returned from subscribe request)

            #     market_book.streaming_update,  # json update received

            #     market_book.market_definition,  # streaming definition, similar to catalogue request

            #     market_book.publish_time,  # betfair publish time of update

            # )



            # write data to file

            f.write(str(market_book.streaming_unique_id) + '\n')

            f.write(str(market_book.streaming_update) + '\n')

            f.write(str(market_book.market_definition) + '\n')

            f.write(str(market_book.publish_time) + '\n')

*Tags: Getting Started, Deployment, Strategies*

---

**RDr** - *08:37:37*

Hi Liam, thanks for your response. I did start running the sample codes in [https://betcode-org.github.io/flumine/quickstart/](https://betcode-org.github.io/flumine/quickstart/) and then kept waiting for framework.run() to show something after the initial msg 'starting strategy 'ExampleStrategy'.  So most likely me not starting with and running the right example (and knowing what I am doing yet).

*Tags: Strategies*

---

**RDr** - *13:01:25*

Hi Liam,

I've started to search for older msg on the channel and downloaded a copy of the older slack but it will take me some time to go through all the resources available and examples to find what's relevant for me now. So I need you guidance again please.



1. Are the examples that could help with my question in the folder [https://github.com/betcode-org/flumine/tree/master/examples](https://github.com/betcode-org/flumine/tree/master/examples) ?

2. Should the files in sub-folder [https://github.com/betcode-org/flumine/tree/master/examples/strategies](https://github.com/betcode-org/flumine/tree/master/examples/strategies) be manually copied (I saw the following for setup [https://github.com/betcode-org/flumine/blob/master/setup.py](https://github.com/betcode-org/flumine/blob/master/setup.py))?

*Tags: Getting Started*

---

## 2023-03-30

**Jesus Perdomo** - *19:31:56*

Hey guys, random question (relatively new to python).



Went through the betfairlightweight source to try and see if there was any implementation to use proxies with the API connection - I don't think this is the case.

Reason I am asking is because my development VPS is located in the US but I'm in the UK. All API calls unfortunately route through US IP address which betfair doesn't allow.



I was hoping I could implement a proxy to all the requests calls, but seems like I'd have to edit the original source code for this (and therefore miss out on future updates to the package etc...)



Has anyone come accross this issue? - Probably controversial as it could be used to circumvent Betfair rules of access I suppose?

*Tags: Getting Started*

---

**Jesus Perdomo** - *21:05:03*

Have given it a go now - seems like the SSL cert for non-interactive login doesn't play nice with a proxy. I keep getting SSL errors when trying to login unfortunately :disappointed:



Was worth a try

*Tags: Errors Debugging*

---

**Trex44** - *23:22:25*

This might sound like a dumb question but what does the market_book.total_matched and runner.total_matched (for runner in market_book.runners) measure?



I would have thought the first measures the total volume traded on the market and the second the total volume traded on a given runner. I am getting zero for the market_book.total_matched when backtesting and getting very low numbers for the runner total matched figures (that go up and down!). I am backtesting on Pro data not recorded data, uncertain if that makes a difference. See code and logs attached.

*Tags: General Technical*

---

**Trex44** - *23:22:48*

```from collections import OrderedDict

from flumine import BaseStrategy

from flumine.order.trade import Trade

from flumine.order.order import OrderStatus

from flumine.order.ordertype import LimitOrder

from flumine.utils import get_price

import numpy as np





class LowestLayer(BaseStrategy):

 

    def check_market_book(self, market, market_book):

        if market_book.status == "OPEN" and not market_book.inplay and 0 &lt; market.seconds_to_start &lt; 0.2:

            print('----------------------------------------------')

            print(f'Seconds to start = {market.seconds_to_start}')

            print(f'market_book.total matched = {market_book.total_matched}')

            print(f'[r.total_matched for r in market_book.runners] = {[r.total_matched for r in market_book.runners]}')

            print(

                f'sum([r.total_matched for r in market_book.runners]) = {sum([r.total_matched for r in market_book.runners])}')

            return True```

*Tags: Strategies*

---

**Trex44** - *23:23:22*

```{"asctime": "2023-03-30 22:09:29,717", "levelname": "INFO", "message": "Client added", "username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": null, "transaction_count_total": null, "trading_controls": [], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-03-30 22:09:29,717", "levelname": "INFO", "message": "Adding market middleware &lt;flumine.markets.middleware.SimulatedMiddleware object at 0x7f68114aa380&gt;"}

{"asctime": "2023-03-30 22:09:29,718", "levelname": "INFO", "message": "Adding client control MAX_TRANSACTION_COUNT"}

{"asctime": "2023-03-30 22:09:29,718", "levelname": "INFO", "message": "Adding trading control ORDER_VALIDATION"}

{"asctime": "2023-03-30 22:09:29,718", "levelname": "INFO", "message": "Adding trading control MARKET_VALIDATION"}

{"asctime": "2023-03-30 22:09:29,718", "levelname": "INFO", "message": "Adding trading control STRATEGY_EXPOSURE"}

{"asctime": "2023-03-30 22:09:29,718", "levelname": "INFO", "message": "Adding strategy LowestLayer"}

{"asctime": "2023-03-30 22:09:29,719", "levelname": "INFO", "message": "Creating new HistoricalStream (1000) for strategy LowestLayer", "strategy": "LowestLayer", "stream_id": 1000, "market_filter": "/home/ubuntu/Python_projects/tmp/races/1.161864856", "event_id": "29441541", "event_processing": false}

{"asctime": "2023-03-30 22:09:29,719", "levelname": "INFO", "message": "Starting flumine", "clients": {"Betfair": {}, "Simulated": {"6bb3cddd": {"username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f68114aa590&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 0, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140085059739648)&gt;"]}

{"asctime": "2023-03-30 22:09:29,719", "levelname": "INFO", "message": "Client login", "username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f68114aa590&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-03-30 22:09:29,720", "levelname": "INFO", "message": "Client update account details", "username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f68114aa590&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-03-30 22:09:29,720", "levelname": "INFO", "message": "Starting historical market '/home/ubuntu/Python_projects/tmp/races/1.161864856'", "market": "/home/ubuntu/Python_projects/tmp/races/1.161864856"}

{"asctime": "2023-03-30 22:09:29,720", "levelname": "INFO", "message": "[Register: 1000]: marketSubscription"}

{"asctime": "2023-03-30 22:09:29,720", "levelname": "INFO", "message": "[MarketStream: 1000]: \"MarketStream\" created"}

{"asctime": "2023-03-30 22:09:29,720", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,722", "levelname": "INFO", "message": "Adding: 1.161864856 to markets"}

{"asctime": "2023-03-30 22:09:29,728", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,731", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,732", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,733", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,734", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,735", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,736", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,754", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,759", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,785", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,822", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,917", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:29,952", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:30,040", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:30,083", "levelname": "WARNING", "message": "Runner 12077917 (13.81) removed from market 1.161864856"}

{"asctime": "2023-03-30 22:09:30,154", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:30,480", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:30,529", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

{"asctime": "2023-03-30 22:09:30,648", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.161864856 added, 1 markets in cache"}

----------------------------------------------

Seconds to start = 0.176

market_book.total matched = 0

[r.total_matched for r in market_book.runners] = [24835.62, 3965.73, 2802.5, 593.19, 29.79, 153.78, 0]

sum([r.total_matched for r in market_book.runners]) = 32380.609999999997

----------------------------------------------

Seconds to start = 0.063

market_book.total matched = 0

[r.total_matched for r in market_book.runners] = [24835.62, 4521.7, 2802.5, 593.19, 29.79, 153.78, 0]

sum([r.total_matched for r in market_book.runners]) = 32936.579999999994

----------------------------------------------

Seconds to start = 0.003

market_book.total matched = 0

[r.total_matched for r in market_book.runners] = [24835.62, 4521.7, 2802.5, 593.19, 29.79, 45.97, 0]

sum([r.total_matched for r in market_book.runners]) = 32828.77

{"asctime": "2023-03-30 22:09:33,018", "levelname": "INFO", "message": "Market 1.161864856 closed", "market_id": "1.161864856", "event_id": "29441541", "event_type_id": "7", "event_name": "Sedge 29th Aug", "market_type": "WIN", "market_start_datetime": "2019-08-29 18:20:00", "country_code": "GB", "venue": "Sedgefield", "race_type": null, "orders_cleared": [], "market_cleared": [], "closed": true}

{"asctime": "2023-03-30 22:09:33,019", "levelname": "INFO", "message": "Market cleared", "market_id": "1.161864856", "order_count": 0, "clients": {"Betfair": {}, "Simulated": {"6bb3cddd": {"username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f68114aa590&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140085059739648)&gt;"]}

{"asctime": "2023-03-30 22:09:33,019", "levelname": "INFO", "message": "Market level cleared", "market_id": "1.161864856", "profit": 0, "bet_count": 0}

{"asctime": "2023-03-30 22:09:33,019", "levelname": "INFO", "message": "Market closed", "market_id": "1.161864856", "clients": {"Betfair": {}, "Simulated": {"6bb3cddd": {"username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f68114aa590&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140085059739648)&gt;"]}

{"asctime": "2023-03-30 22:09:33,020", "levelname": "INFO", "message": "Removing market 1.161864856", "clients": {"Betfair": {}, "Simulated": {"6bb3cddd": {"username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f68114aa590&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140085059739648)&gt;"]}

{"asctime": "2023-03-30 22:09:33,020", "levelname": "INFO", "message": "Completed historical market '/home/ubuntu/Python_projects/tmp/races/1.161864856'"}

{"asctime": "2023-03-30 22:09:33,020", "levelname": "INFO", "message": "Simulation complete"}

{"asctime": "2023-03-30 22:09:33,020", "levelname": "INFO", "message": "Shutting down Execution (SimulatedExecution)"}

{"asctime": "2023-03-30 22:09:33,020", "levelname": "INFO", "message": "Shutting down Execution (BetfairExecution)"}

{"asctime": "2023-03-30 22:09:33,020", "levelname": "INFO", "message": "Client logout", "username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f68114aa590&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-03-30 22:09:33,021", "levelname": "INFO", "message": "Exiting flumine", "clients": {"Betfair": {}, "Simulated": {"6bb3cddd": {"username": "6bb3cddd", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f68114aa590&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140085059739648)&gt;"]}

time in mins = 0.05507066069791714

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;

Profit: 0.00

total = 0 

 max = 0 

 min = 0```

*Tags: Strategies*

---

## 2023-03-31

**Trex44** - *07:10:45*

Hey I was looking to do this to but was implementing the computation in the strategy class. What are the benefits/reasons for implementing it in the middleware ?

*Tags: Strategies*

---

**Jorge** - *08:35:15*

Hi guys, I am trying to access `market.event_name` in [https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py#L109|check_market_book](https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py#L109|check_market_book) but it is equal to None. Any way I could access it?

*Tags: Strategies*

---

**Thomas JAMET** - *10:26:33*

I can see a couple of reasons:

1. Separation of code.

2. The middleware get evaluated before the strategies. It might help if you have multiple strategies.

If you don't care about these points, there might be no benefit.

*Tags: General Technical*

---

## 2023-04-04

**RDr** - *14:02:00*

Hi guys,  I am checking the delays from the scheduled time for some horse races and wondering what indicators I should look for in the historical streaming data (pro version) to get the actual time the races started (or just before in-play if applicable)?

*Tags: General Technical*

---

**Ralegh** - *14:47:47*

Question from someone on 40% PC, any good software for churning commission (like doing simple arbs) that they can set up, I’d write something on flumine for them but I’m strapped for time

*Tags: General Technical*

---

## 2023-04-07

**Tom** - *03:59:56*

Just looking for a bit of help with this error.



I was using the broad market filter

#     market_filter = streaming_market_filter(

#         event_type_ids=['7'],

#         country_codes=['AU'],

#         market_types=['WIN']

# ),



Then thought I would try to limit it to the number of markets I am interacting with in testing (a single event, all markets). I'm loading them in _init_ and attempting to loop through that list in the market filter with a lambda function (is that the problem?)



This is the error. It could also be because I am not logging out properly?- I have 2 files I am running, one is collecting data and the other is running strategy in flumine; I haven't figured out how to run both at the same time yet, but the idea is that the first one would manage data (inc. which markets to stream/process for bets).



betfairlightweight.exceptions.ListenerError: connection_id: 206-070423024222-349976, data: {"op":"status","id":3002,"statusCode":"FAILURE","errorCode":"MAX_CONNECTION_LIMIT_EXCEEDED","errorMessage":"You have exceeded your max connection limit which is: 2 connection(s).You currently have: 3 active connection(s).","connectionClosed":true,"connectionId":"206-070423024222-349976"}

*Tags: Errors Debugging, Strategies*

---

**Tom** - *07:34:17*

The old data isn't a huge deal for what I want to do to get it going - the goal is just to get the machinery functioning at this point - I think it's to do with the market filter but I'm not sure how to modify it, just looking through betfairlighweight docs now (but not completely sure if it will fix it)

*Tags: Errors Debugging*

---

**Mo** - *07:45:46*

1. Turn off everything

2. Wait

3. Start ONE flumine instance with the above market filter

*Tags: General Technical*

---

**Tom** - *08:08:14*

2023-04-07 17:07:41,638 - ERROR - [MarketStream: 2001]: SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 242 markets whereas max allowed number was: 200



I'm getting this bad boy, which seems like the most obvious problem

*Tags: Errors Debugging*

---

**Tom** - *08:46:20*

I had to make a list and put market_ids=market_ids into the filter.



But clunky but at least it works



I already process the data that gives me the list of Market ID's I want in the BaseStrategy class init, so might just have to recreate it until I get the live access

*Tags: Deployment, Strategies*

---

**Trex44** - *11:52:21*

Hey guys, what's the correct way to print the results of a strategy to a log (e.g. a text file). Is this available in the blotter somewhere?

*Tags: Strategies*

---

**Lee** - *12:16:53*

Logging control [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

*Tags: General Technical*

---

## 2023-04-08

**Michael** - *15:52:37*

Whats the best way to handle data streaming? Are people just writing to a bucket using the S3 data recorder and then accessing the .json files or writing the rows to a db?

*Tags: General Technical*

---

## 2023-04-09

**Derek C** - *09:07:57*

fyi, as I'm a lazy person I use this one: [https://aws.amazon.com/s3/storage-classes/intelligent-tiering/](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)

*Tags: Deployment*

---

**Aravind** - *15:14:11*

Hey guys - my name is Aravind and I am the founder of [https://www.dev.goated.com/](https://www.dev.goated.com/) - a sports trading exchange that is launching soon. Our api docs are here: [https://api.dev.goated.com/swagger-ui/](https://api.dev.goated.com/swagger-ui/) and we have built an sdk here: [https://docs.goated.com](https://docs.goated.com).



I love what the community have done here with *betfairlightweight* and would love to get a similar high quality sdk for Goated so that existing betfair traders can also connect to Goated easily. Are there people here who might be exited in working with me on this?

*Tags: Strategies*

---

**Michael** - *20:47:25*

I’m still getting the following error: 2023-04-09 20:43:47,803 - *ERROR* - *Error* accessing S3 bucket: An error occurred (AccessDenied) when calling the ListBuckets operation: Access Denied when trying to use Boto3 even though I created a user with S3FullAccess

*Tags: Errors Debugging*

---

## 2023-04-10

**Aravind** - *23:01:48*

Good question 

*Tags: General Technical*

---

## 2023-04-11

**ShaunW** - *09:27:51*

Thanks for the reply [@U04SRDB8FKJ](@U04SRDB8FKJ) but I don't think that addresses the question. As I said we see a lot of proposed exchanges and it's all very clever stuff but most only have a plans to attract pros but not actual customers.  Betfair have a high public awareness and the might of Paddy Power's advertising budget but still struggle so what makes you different? I'm obviously disinclined to invest my time in something and find the only people there are us, and most we see only seem to have plans to fund raise and sell up rather than anything approaching an actual sustainable business.   



That might seem a little more pointed than questions about fees and perks, but I'm sure these are questions your investors have asked, unless of course they're looking for a quick return rather than being involved in a long term business.

*Tags: General Technical*

---

**Peter** - *09:44:32*

Have to agree with [@UEA14GBRR](@UEA14GBRR). Kudos to you for doing this and for the quality of your implementation and documentation. But while the tech side looks great, I'm not seeing the long-term business rationale and without that, there's no incentive for me to invest time learning the ins and outs of a new platform.



I don't see a USP that would attract recreational bettors, or anything in your answers that suggests that attracting them is being considered. Being an exchange version of another platform who themselves don't seem to have a USP, is a bit underwhelming.



On the crypto point, I would see bypassing conventional banking deposits as a limiting factor and certainly not a USP?

*Tags: General Technical*

---

**Jonjonjon** - *10:33:54*

If it was crypto deposits with zero annoying KYC I would not see that as a problem. But the lack of a diversified eco system of users would be.

*Tags: General Technical*

---

**ShaunW** - *10:55:12*

I agree about the crypto Peter. I don't actually know anyone who uses it for day to day transactions and if anything I just hear idioligical opposition and mistrust.  



The tech isn't the difficult part of setting up an exchange it's understanding how you'll become the name people think of when they want to place a bet. Taking a leaf from the bingo playbook, betting needs a compelling reason to "play" because the over complicated exchange model offering value nobody cares about is unlikely to take much business from tradional operators.  



I'm genuinely trying to be constructive and not just critical, I just feel without a novel and compelling customer offering that sells itself, the advertising budget alone to gain traction will be an obstacle.

*Tags: Strategies*

---

**Jonjonjon** - *12:11:18*

Are there rumours of something changing to UK gambling laws?

*Tags: Deployment*

---

**D C** - *12:12:44*

Oh right - you know that there is a government white paper coming out on UK gambling soon? There is talk of employing a single customer view for betting accounts. You must be aware of all the talk of people having to provide proof of income documentation that has been happening? Seems its mostly happening to losing accounts and bookmakers rather than the exchanges, but if this stuff becomes law then dumb money will evaporate very quickly

*Tags: Strategies*

---

**ShaunW** - *12:19:13*

Genuine best wishes [@U04SRDB8FKJ](@U04SRDB8FKJ).  You don't need yes men at this stage so I hope any tough questions will help to avoid the mistakes of the others who've tried.

*Tags: General Technical*

---

**D C** - *12:22:35*

[@UPMUFSGCR](@UPMUFSGCR) It is a question that currently has no answer - bookies are not legally obliged to check source of funds at all. I am hoping that you are right though about it being depositors only that get checked. If not, my account will be gone as my non-betfair income per year is £0. This is the frustrating thing though because any legislation will be implemented by people who probably know jack shit about betting and will fail to see the subtle differences between exchange and bookmaker business models. I do worry what will happen if no distinction is made between casino/sports bettinge/exchange betting but we will soon see

*Tags: Strategies*

---

**Graham** - *14:29:05*

the constant reference to [http://stake.com|stake.com](http://stake.com|stake.com) "insane" success followed up by questions on best-practices regarding KYC are ironic and not lost on me, if indeed the joke was intended.

*Tags: General Technical*

---

**Graham** - *14:34:50*

the documentation and technology is nice. The bottom line is that to exist as a p2p exchange with the sole USP being crypto-based deposits just sends the value to near-zero. For no other reason that you appear to want to operate in legal-waters. In the anecdotal venn-diagram of bettors, those who want a viable exchange are betting where liquidity is greatest; Betfair. Those who want to use crypto don't want any KYC.



I'm not sure the middle-ground, being a licenced operator (and therefore lawfully compliant to enforce KYC) is sufficient enough to mimic the massive success of [http://stake.com|stake.com](http://stake.com|stake.com)



Whomever successfully turns their back on the regulator and adopts a fair and honest -- but crucially close-to-anonymous exchange gets all the sweets.

*Tags: Strategies*

---

## 2023-04-16

**Trex44** - *00:01:49*

Thanks guys. What would you recommend as the best sport to trade? I like football a lot but have struggled to get the data I felt I needed for models.

*Tags: Strategies*

---

**Felix Vasquez** - *02:35:56*

Great question!

*Tags: General Technical*

---

**Trex44** - *11:24:22*

Footy always the most engaging, best sport in the world. Need data to build the models though, always been the issue. Glad to hear that pro is doable without horses though.

*Tags: Strategies*

---

**Trex44** - *12:54:11*

I have no ethical problem in beating other people who bet against me. I am taking on risk, if I screw up and they win then I have nothing against them. I am not limiting winners and only hearding mug punters into my 200% over round book.  My ethical problems with gambling lay in the fact that I think bookies should have been phased out long ago and everything should be exchanged based.  Thanks for the info on the 20/80 split DC.

*Tags: General Technical*

---

## 2023-04-17

**Jorge** - *10:51:33*

Hi guys, I'm having an issue with my flumine Strategy. I save the details of every order I place, after it goes to EXECUTION_COMPLETE status, using:

```def save_completed_bets_data(self, market, selection_id):

    """

    Saves data from bets after an order in EXECUTION_COMPLETE.

    Note that an order can be partially matched many times before going to EXECUTION_COMPLETE status.



    """

    for order in market.blotter._orders.values():

        if order.size_matched &gt; 0:

            if order.status == OrderStatus.EXECUTION_COMPLETE:

                if order.selection_id == selection_id:

                    if order.date_time_execution_complete:

                        if order.bet_id not in self.completed_bets_dict:

                            self.completed_bets_dict[order.bet_id] = {

                                'market_id': market.market_id,

                                'selection_id': selection_id,

                                'side': order.side,

                                'average_price_matched': order.average_price_matched,

                                'size_matched': order.size_matched,

                                'execution_complete_ts': order.date_time_execution_complete,

                            }



                            df = pd.DataFrame.from_dict(self.completed_bets_dict, orient="index")

                            df.to_csv(self.completed_bets_csv_path)```

My issue is that for some orders (5% of them) the size_matched does not match the stake I get quering `list_cleared_orders`. Moreover, the stake from `list_cleared_orders` is always bigger than the one I recorded. Anyone has any clue why this would happen?

*Tags: Feature Engineering, Strategies*

---

## 2023-04-18

**Andrey Luiz Malheiros** - *02:12:47*

Hey guys, my name is Andrey and I'm starting to use Flumine.

I want to run my strategies on a Django server and be able to activate and deactivate them using Flumine's API.

Currently, I'm using Django and Celery to start an asynchronous task when I call an API. Once this task initializes, Flumine starts running for a strategy that is passed in the body of the API request. However, the issue with this approach is that every time I call the API to start a strategy, I need to run another Flumine instance. I'm not sure if this is the best approach for my situation. Does anyone have a similar infrastructure or any ideas on how I can handle running my strategies on a server?

*Tags: Deployment, Strategies*

---

**Adrian** - *03:04:07*

Hi guys, I'm back after a break and see that the app is renamed betcode. Happy to see everyone here from bflw again

*Tags: General Technical*

---

**Adrian** - *03:07:14*

feeling like a total newb and wondering how to get the bz2 files from betfair into the correct format or if theres a helper function that can work directly with these files? would be grateful if someone could point me in the right direction. thank you!

*Tags: General Technical*

---

**Mo** - *05:48:40*

Standard practice is to patch `open` with `smart_open.open` and then `flumine` can read the `bz2` files directly: [https://betcode-org.github.io/flumine/performance/#file-location](https://betcode-org.github.io/flumine/performance/#file-location)

*Tags: Performance*

---

**Mo** - *05:52:08*

Depends how many strategies you're planning on running. If it's a lot then you might run into the limits on number of streaming connections Betfair allows

*Tags: General Technical*

---

**liam** - *06:05:36*

First question is why? Do you see the need to start / stop strategies? 

*Tags: General Technical*

---

**Adrian** - *06:24:03*

thank you Mo. I will try to implement some code that Liam created but it looks like there might be problems with the historical data

*Tags: Data Quality*

---

**Mo** - *06:30:22*

What kind of problems?

*Tags: General Technical*

---

**Adrian** - *06:52:33*

i think i actually wrote some code a couple years back to make the historical data work with flumine but can't remember. i know there was an issue with the cumulative volume being calculated differently

*Tags: Data Quality*

---

**Mo** - *06:55:50*

Just set your listener arguments appropriately: [https://github.com/betcode-org/betfair/blob/00e9abb2b4d228408eea1620cbeb55cbaed0e0c6/betfairlightweight/streaming/listener.py#L107-L108](https://github.com/betcode-org/betfair/blob/00e9abb2b4d228408eea1620cbeb55cbaed0e0c6/betfairlightweight/streaming/listener.py#L107-L108)

*Tags: General Technical*

---

**Jorge** - *09:24:58*

I found out the issue I posted above is related to Orders going to [https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L39|OrderStatus.VIOLATION](https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L39|OrderStatus.VIOLATION) .

1. I place an order and it gets accepted, so the Status is EXECUTABLE.

2. The Market gets suspended.

3. I cancel the order and get the following log: "Order status update: Violation", "Order has violated: MARKET_VALIDATION Error: Market is not open". But, the order's info is `"info": {"side": "BACK", "size_matched": 0.0, *"size_remaining": 20,* "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}` .

I thought [https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L39|OrderStatus.VIOLATION](https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L39|OrderStatus.VIOLATION) is part of [https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L49|COMPLETE_STATUS](https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L49|COMPLETE_STATUS) so the order.size_remaining would be 0. How are you guys handling orders that go to VIOLATION status?

*Tags: Errors Debugging*

---

**Andrey Luiz Malheiros** - *19:58:22*

I came up with this solution because I don't want to execute my strategies by running a Python script on my terminal or a Jupyter notebook, for example, because I can't automate a process this way. I want to pass the predictions that I generate on a daily basis as context to run my strategies, and I want to do this in an automated way

*Tags: General Technical*

---

## 2023-04-19

**liam** - *05:32:00*

Ok, best practice would be to have a single instance / strategy which reads your predictions either from a text file or an API. You can use a worker for this that polls every x seconds and updates your context.

*Tags: Strategies*

---

**Andrey Luiz Malheiros** - *18:18:28*

I was reading in the documentation about workers, and this really seems to be the ideal way to update the context according to data in my database. Thank you for your help, Liam. Regarding the situation of adding and renewing a strategy, is it possible to use any approach with workers and CustomEvent to add or remove strategies? I ask this because of the following situation: let's suppose I have 12 strategies for 4 different sports being executed. If I use one instance of Flumine for each strategy, I will end up with 12 different clients. Would this pose any issues with the connection limit allowed by Betfair, for example? Another option would be to run all the strategies in the same instance of Flumine. However, if I wanted to stop running only one of these strategies, it wouldn't be possible. I would have to remove all of them and then restart Flumine. Is there any solution to address this?

*Tags: Strategies, Multi Client*

---

**Andrey Luiz Malheiros** - *19:28:28*

In the example I gave with 12 strategies, to initialize these strategies, we must pass a Betfair client to each instance of Flumine. If I have 12 strategies, each with a different instance, then Flumine will perform 12 logins.

*Tags: General Technical*

---

**Jonjonjon** - *20:12:09*

Thanks Thambie. Does Flumine simulate the lapse?

*Tags: General Technical*

---

**thambie1** - *20:12:28*

No clue, I don't use Flumine

*Tags: General Technical*

---

**Andy B** - *22:09:45*

Hi All, What's the best way of getting the next event start time from the stream?  I am doing Aus greyhounds and I have found a scenario where I need to periodically update data from an external site to use in my strategy.  My though is to insert this at a point where I have 5 minutes or more until the next race, but I'm not sure how to tell that from within the stream.

*Tags: Strategies*

---

## 2023-04-20

**Peter** - *14:01:15*

Flumine maintains a list of market objects that you could iterate over to find the one with the smallest seconds_to_start attribute.



But that's not how I'd do it personally. Cleaner would be to treat each market separately and within the strategy do this processing when each market's seconds_to_start &lt;= 600 (you could set a flag in market.context to ensure that it's only done once).



Even cleaner would be to put this test and associated processing into the call() method in a custom middleware class.



However, if your external site is slow to respond you may need to push this out to another process to avoid holding up stream processing.



Yet another approach would be to make the external calls in a worker function, in which case rather than trying to infer the next race from the stream, I'd recommend making a single call at the beginning of the day using betfairlightweight to load all the catalogues and save a local list of the day's start times and then work through that to trigger your calls to the external site.

*Tags: Performance, Strategies*

---

**EJono** - *16:37:25*

Can more than one flumine strategy access the same market stream on a single account simultaneously?



Ive spun up three flumine strategies for testing which all want to be accesing and running on the same sorts of football markets.

Thus the streaming market filter and class set up looks like the following:



```SMF = streaming_market_filter(

        event_type_ids=event_type_ids,

        country_codes=country_codes,

        market_types=["MATCH_ODDS"]

)



strategy_one = StrategyOne(market_filter=SMF, ... )

strategy_two = StrategyTwo(market_filter=SMF, ... )

strategy_three = StrategyThree(market_filter=SMF, ... )```

I have noticed that sometimes a market will drop from the logs of one strategy and start up on another. Im slightly confused and wondering if there is somehting fundamental im missing such as "a market can only be streamed by one strategy at a time".

Any help greatly appreciated

*Tags: Strategies*

---

**Andrey Luiz Malheiros** - *19:40:21*

Yes, I agree that using strategies with different instances of Flumine is not ideal, but is there a way to add or remove strategies from Flumine after running `flumine.run()`?

To explain further why I want this, let's say I have 4 strategies running, and one of them is performing VERY poorly, and I want to pause only that specific strategy to prevent further losses.

Now, in another scenario, let's say I have 4 strategies running, and I want to start an additional strategy.

In both of these scenarios, it would be very helpful to have a way to add or remove strategies from Flumine after the instance has already been initiated. Do you know if there is a way to do this?

*Tags: Strategies*

---

**liam** - *20:17:57*

Not really however I would look at an active flag in the strategy context which you update using a worker / text file. Adding strategies is a limitation of flumines design however maybe there is scope to make this possible 

*Tags: Strategies*

---

**Andrey Luiz Malheiros** - *23:43:52*

Thank you very much, Liam. I will likely use this approach, with a flag in the context indicating which strategies are active. I will also take a look at the `__enter__` and `__exit__` methods of `BaseFlumine`, as I was reviewing them this week and it seems that by creating some methods and making slight modifications to these two, it may be possible to achieve this. If I am successful, I will share what I have done with the group.

*Tags: General Technical*

---

## 2023-04-21

**EJono** - *07:32:29*

the rest of the arguments passed to the strategy class look like the following:



```strategy_one = StrategyOne(

    max_order_exposure=30,

    max_live_trade_count=20,

    max_selection_exposure=30,

    market_filter=streaming_market_filter(

        event_type_ids=["1"],

        country_codes=country_codes,

        market_types=["MATCH_ODDS"],

    ),

    name='instance_1',

    context={

        "event_type_ids": ["1"],

        "market_types": "MATCH_ODDS",

        'number_of_markets_for_placement': eval(number_of_markets_to_place_on),

        "cancel_all_bets": eval(cancel_all_bets),

        'lower_limit': 0.0,

        'upper_limit': 0.33333333,

        'max_liability': 11,

        'strategy_type': 'first_to_score',

        'market_ids_for_placement': []

    }

)```

Strategy 2 and 3 are exactly the same apart from variations in "name", "lower_limit", and "upper_limit"

*Tags: Deployment, Strategies*

---

## 2023-04-22

**Mo** - *10:37:27*

[https://github.com/betcode-org/betfair/blob/00e9abb2b4d228408eea1620cbeb55cbaed0e0c6/betfairlightweight/endpoints/scores.py#L16-L41](https://github.com/betcode-org/betfair/blob/00e9abb2b4d228408eea1620cbeb55cbaed0e0c6/betfairlightweight/endpoints/scores.py#L16-L41)

*Tags: General Technical*

---

**Unknown** - *21:13:47*

About this discussion, from what I understand, Flumine can handle multiple strategies in the same market, but I would like to know the correct way to do this. I ask this because I can imagine 2 possible scenarios to instantiate different strategies for the same market. Let's suppose we want to test three different strategies in the same markets, and the scenarios are as follows:



1. We can instantiate 3 `streaming_market_filter` using the same parameters, and then pass each instantiated `streaming_market_filter` to each of the 3 strategies. My question in this scenario is whether Flumine will perform 3 different subscriptions to stream for the same markets, so that the same information is obtained from Betfair 3 times, with each one being passed to each market?



2. We can instantiate 1 `streaming_market_filter`, and then pass the same instance of `streaming_market_filter` to the 3 strategies.



I will also send code illustrating the 2 scenarios:

*Tags: General Technical*

---

## 2023-04-23

**Trex44** - *07:25:19*

Morning all. Just wondered if there is a way to call Betfair's own Cashout function via Flumine?

*Tags: General Technical*

---

**river_shah** - *09:38:22*

Depends on context but I am not sure such a great idea. I would suggest do computation inside your strategy, building transaction and fire hedges yourself.



For larger exposure relative to market size, inbuilt cash out can be very inefficient

*Tags: Strategies*

---

**John** - *10:04:31*

Hi all and [@U4H19D1D2](@U4H19D1D2), I was trying to use `max_live_trade_count` (=1). I notice that even the existing orders have been cancelled by the code in `process_orders()` after 2 seconds for all runners, my code still thinks there are live orders executable for all runners (so below line is true):



[https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/strategy/strategy.py#L199](https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/strategy/strategy.py#L199)



hence my code doesn't place any new bets, though I'd expect the code to start placing new bets or those runners when their bets had been cancelled. Please could anyone give me a pointer if I am missing something very obvious?



Many thanks!

*Tags: Deployment, Strategies*

---

**Tom** - *11:18:30*

I'm interested to know how you guys are working the bet placement process with Flumine.



My lines are blurring between a linear computing process, OOP (which I'm only slowly getting the hang of), the RunnerContext object, Trade object, order itself, order-packaging and Trade execution - do I push the order execution calling functions in flumine, or create it to the objects and write general code in process_orders to process all orders? I was executing trades, until I started building an output for results comparison e.t.c. and I feel like I stepped over a layer of complexity and having trouble contextualising it all now

*Tags: Performance*

---

**Unknown** - *13:21:41*

Hi, here is the log:



2023-04-22 08:26:33 |  Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failed: live_trade_count (1) &gt;= max_live_trade_count (1)



I manually/visually checked the market (on betfair website). There were no orders from this strategy (as they all had been cancelled by the code).

*Tags: Errors Debugging, Deployment, Strategies*

---

**Lee** - *13:23:12*

Can you share the full logs? It’s easier to debug when we can see the orders placed/cancelled/status’s changing

*Tags: Errors Debugging*

---

**Peter** - *16:53:18*

Hi [@U042NB80A1X](@U042NB80A1X) It feels to me as though you're overthinking this. I recommend taking a step backwards and looking at how it's done in the examples provided in the repo (especially lowest layer).



Think in terms of triggers and actions. Something is going to trigger a desire to place an order which you'll most likely detect in your strategy's `process_market()` method, so just use `market.place_order()` to execute the trade there. Other actions, such as replace or cancel, may be triggered by analysis of your order book so are done in your strategy's `process_orders()` method, or could be in response to changes in the market and so again done in  `process_market()` and it really doesn't need to get complicated.

*Tags: Strategies*

---

## 2023-04-25

**Peter** - *16:42:02*

It can indeed get complicated quickly. But Flumine has features that help keep this to a manageable level.



In case it helps, my feedback and end-of-day processes lean heavily on logging controls. I have a version that writes all orders to a database and updates those entries with the result when markets are complete. This keeps the monitoring logic completely separate from strategy execution (except arguably for the single `add_logging_control()` statement), but makes the data I need available for daily reconciliation scripts and ad hoc analyses.

*Tags: Feature Engineering, Strategies*

---

## 2023-04-26

**Tom** - *00:07:17*

Yeah that is really handy to know, thank you very much for the help Peter. I got the trades working so thank you very much. Onto a more difficult problem of making them profitable lol



And this never ending quest of finding the elegant solution

*Tags: General Technical*

---

## 2023-04-28

**Unknown** - *03:52:55*

Hi, I am comparing the live total matched by runner_id from the polling API vs the traded volume (tv) from streaming (using flumine market_recorder). The total matched from 'Polling / Matched' is the same as what was displayed on Betfair website for the race. However, the totals from the streaming are different.

Any ideas what assumption I am getting wrong that would explain the discrepancies or what I should check?  Thanks

*Tags: Deployment*

---

**James** - *11:35:05*

Ever since the clocks changed market catalogue race times are showing -1 hour (13.30 shows as 12.30, etc). Does anyone know how to fix this? Thanks 

*Tags: Errors Debugging*

---

**Mo** - *11:45:33*

What do you mean by "fix this"? The time is in UTC as it always has been

*Tags: Errors Debugging*

---

**Philip Joss** - *22:18:48*

I'm like to get a list of tennis events that aren't doubles matches. As all doubles match event names have `/` in them I figure I need to use the `text_query` parameter in a `market_filter`. However, I can't figure out the syntax to use to exclude all events that have `/` in the event name. `betfairlightweight.filters.market_filter(event_type_ids=[2])` gets me a list of all tennis events. Playing around I tried ``betfairlightweight.filters.market_filter(event_type_ids=[2], text_query="/")` to filter to only doubles matches. This isn't what I need but it didn't actually return any events at all. What would I use to exclude `/` and thus exclude doubles matches?

*Tags: General Technical*

---

## 2023-04-29

**RDr** - *05:35:21*

As MitchM mentioned, the Polling API total matched would be for the sum of back and lay. After some tests backing a few runners and checking the data from flumine market recorder (Streaming API) and  the matched bets from the Polling API, I saw about 5% being systematically added to my back bets and recorded as traded (at least kind of explain why Polling Matched &lt; 2 * Streaming/tv in the snapshot). Would these 5% be for commission or something and included only for Betfair streaming API data?

*Tags: Data Quality*

---

**Trex44** - *11:41:22*

In python you can replace the the tzinfo of the utc date time object to account for this.

```for market in market_catalogue:



    market_id = market.market_id

    venue = event.event.venue

    start_time_utc = market.market_start_time

    uk_timezone = pytz.timezone('Europe/London')

    start_time_uk = start_time_utc.replace(tzinfo=pytz.UTC).astimezone(uk_timezone)```



*Tags: General Technical*

---

## 2023-05-02

**Adam Momen** - *15:32:44*

[@U4H19D1D2](@U4H19D1D2) Have you considered writing rust version of _flumine_ for speed gains_?_



I’m running backtesting across thousands of markets everyday, the processing time slow down as the data increased, following all the performance guidelines and tips in the [https://github.com/betcode-org/flumine/blob/master/docs/performance.md|doc](https://github.com/betcode-org/flumine/blob/master/docs/performance.md|doc) and installing flumine with c &amp; rust liberaries (`flumine[speed]` ) helped but it was still relatively slow.



I wanted faster processing speed, so I’ve run a simple benchmark [https://www.notion.so/Flumine-Backtesting-Benchmark-c3b20cd40fd4487388fd2d73ed23f72f?pvs=4|experiment](https://www.notion.so/Flumine-Backtesting-Benchmark-c3b20cd40fd4487388fd2d73ed23f72f?pvs=4|experiment) to compare file reading speeds from disk in Python3.11 and Rust, why I chose rust?  It’s safe and fast, and the learning curve not that steep.



*Rust was 35x faster.*



&gt; Note: I ran the benchmark on my M1 Air, I admit that it’s the not the best way of recording the result, but it was good rough estimation.

*Tags: Getting Started, Performance*

---

**liam** - *15:34:21*

The file reading isn't the bottle neck, its the python object creation, [https://github.com/tarb/betfair_data|this](https://github.com/tarb/betfair_data|this) is really quick but not maintained to the level I would require

*Tags: General Technical*

---

**Adam Momen** - *15:36:08*

Thanks for the link, I agree, python garbage collector is slowing it down.

*Tags: Performance*

---

**Mo** - *15:44:05*

What's the source code for the Python implementation?

*Tags: General Technical*

---

**Adam Momen** - *15:45:41*

[https://www.notion.so/Flumine-Backtesting-Benchmark-c3b20cd40fd4487388fd2d73ed23f72f?pvs=4|here](https://www.notion.so/Flumine-Backtesting-Benchmark-c3b20cd40fd4487388fd2d73ed23f72f?pvs=4|here)

*Tags: General Technical*

---

**Mo** - *15:48:41*

1. No one uses `ujson` any more, use `orjson`

2. From what I can tell, you do things differently in Python than rust. In Python you read the entire contents of the file into memory then start deserialising line by line. In rust you are deserialising as you read the lines. I assume the latter will be more efficient as it lets the CPU get to work on deserialisation while you are waiting on I/O

*Tags: Performance*

---

**Mo** - *15:51:19*

Also looks like you don't do anything with the deserialised line in rust whereas in Python you are constructing a big list of the deserialised objects

*Tags: General Technical*

---

**Mo** - *15:55:49*

So basically you've got all of the memory management overhead of constructing 3840 x 2 lists which are each at least 2k elements long none of which is done in rust



I totally agree with you we should aim to speed up flumine (and betfairutil) using rust ideally by building on [https://github.com/tarb/betfair_data](https://github.com/tarb/betfair_data) but I think the current comparison is unfair

*Tags: Performance*

---

## 2023-05-03

**Trex44** - *08:55:21*

Thanks Mo, yes that would be my definition of profitable. I thought though that if my lifetime comms generated weren't X% of my lifetime profits (X being 20% or 40% depending on the band) I would incur PC on winnings each week until they hit that band.



I can see a scenario there where say you win 1 in 10 bets but are below the life time comms level you are then charged PC on the winning bet which effectively brings your commission paid on that bet to well over 5%.  If the net gain from winning bets as part of a strategy is much lower but you are still losing the same amount on the losing bets in the sequence then it can turn a strategy unprofitable no?

*Tags: Strategies*

---

**Trex44** - *11:32:48*

Thanks Mo. I know its on the aggregate weekly winnings but assume these consist solely of the winning bet and nothing else. Then it effectively increases the coms you pay on the winnings of that bet without taking into account that bet was the only 1 in 10 you won, and assume the 9 losses were the previous week/s. As you are now netting less on your winning bets than you were before PC was applied your strategy may now be a losing strategy as the winners don't cover the losses.



I can see how this may still be a strategy that you may want to run, if running other profitable high hit rate strategies that don't generate much coms,  as it generates coms that compensate for those strategies. But still there is the possibility that if you had _*only*_ that one strategy, as described above ,you could find it becomes unprofitable once you hit PC. Unless I have misunderstood something.



I am no where near the lifetime allowence yet. I will have a deficit to work through too. I am just planning ahead and trying to figure out what to concentrate on next.

*Tags: Strategies*

---

**Alex** - *14:15:24*

very basic question: what is the most simple way to get the aggregated exposure per runner given a runner object?

*Tags: General Technical*

---

**Brøndby IF** - *16:07:48*

I had even started a method using the `open_date` in games that pass through the `in_play_only=True` filter (It's useful because the last `open_date` saved record is exactly the moment the ball started rolling in the game), and when the `_datetime_created - open_date` was greater than `45:00`, we would be in additional time, and when the `Half Time` market closed, that would mean the first half was end.



However, this way I would not be able to know when the second half started, limiting it to only the first half. ((And this method has several flaws in it, it was just an attempt to find an option))

*Tags: Deployment*

---

**D C** - *16:56:34*

I've noticed from manual recreational bet placement that for certain football matches, the inplay delay is reduced a lot while a game is in half time break (about 1 second). I pretty much only bet on prem games so no idea about other leagues. Not sure why this is nor how consistent it is but it might be something to help you identify the beginning of second half.

*Tags: General Technical*

---

**Trex44** - *20:06:01*

Thanks Mo. I went over the maths of it and yea I see that actually even in the scenario I posed you can't actually turn the strategy into a losing one because of the coms like you said. That wasn't immediately apparent to me.



Tried another scenario where I made the hit rate 50% too with a loss in one week and a win the next. Even though the coms weren't at X% the PC applied to the winning bet still left the strategy in profit. So I guess the maths behind PC was designed with all this in mind.

*Tags: Strategies*

---

**Jonjonjon** - *20:33:51*

Christmas has come early. Numba now works with Python 3.11. Has anyone tested it yet?

*Tags: General Technical*

---

## 2023-05-04

**Jonjonjon** - *10:25:01*

Nice [@U01B8031PM1](@U01B8031PM1). I'm hoping to build it on my Dev box in the following week. Have you witnessed.any performance gains?

*Tags: Performance*

---

## 2023-05-06

**Tom** - *02:58:00*

I am no expert on this at all - but I saw this recently, which looks pretty interesting in regards to speed of Python [https://www.theregister.com/2023/05/05/modular_struts_its_mojo_a/](https://www.theregister.com/2023/05/05/modular_struts_its_mojo_a/)

*Tags: Performance*

---

## 2023-05-07

**Jeff Waters** - *23:15:39*

I've put some code in process_market_book to log the WOM when my orders are at least partially matched:



```for order in market.blotter.live_orders:

    runner = Repository().get_runner_from_order(order)

    if order.size_matched &gt; 0 and not hasattr(order, "wom"):

        order.wom = Wom().get_wom_for_runner(runner)```

When I print my results to a spreadsheet, I use:



```wom = order.wom if hasattr(order, "wom") else 0```

When I print the results, for orders that were matched, some of my entries are zero and some are the wom. I don't understand why it's sometimes the case that the order object is given a 'wom' attribute when money has been matched, and sometimes it isn't?



I wondered whether it might be due to race conditions, as I'm using multi-processing. I'm aware that, with multi-processing, each process has its own memory space, but Chat GPT assures me that race conditions can nevertheless happen (and yes, I'm aware that GPT isn't an authoritative source! :smile:). Anyway, I applied a lock to everything in process_market_book, but that didn't fix the issue.



Any suggestions would be appreciated.



Thanks



Jeff

*Tags: Errors Debugging, Performance, Deployment*

---

## 2023-05-08

**Nick** - *08:20:42*

Hi All - A couple of questions for those using Flumine for running backtests on their strategies and their automation needs:

1. How close do your backtests compare with reality once you go live on a strategy?

2. What are some of the factors that influence differences between backtests and live results?

3. Any experiences / comparisons people want to share?

*Tags: Deployment, Strategies*

---

**Peter** - *09:10:47*

Sort of need to answer question 2 first. The most important factor is whether you are making or taking a price. If you're taking then the results will probably be quite accurate, as long as you don't keep taking the same liquidity. However if you're making prices (offering) then there are two conflicting forces that can send your actual results in either direction (or cancel out).



I've had strategies where the actual fill rate is less than that implied by backtesting, and those that are matched have a higher propensity to lose. This is known as adverse selection. Doesn't necessarily invalid the strategies, but makes them less profitable.



On the other hand some people here report seeing better results when running live than in backtesting, as their offers attract additional funds into the market giving them a better match rate.



So there's not a clear answer and by offering prices you are affecting the market in ways that backtesting can't anticipate. What that effect is will depend on how tight the spreads are how liquid the market is.

*Tags: Deployment*

---

**liam** - *09:42:06*

To add there are some limitations in the [https://betcode-org.github.io/flumine/quickstart/#simulation_1|docs](https://betcode-org.github.io/flumine/quickstart/#simulation_1|docs) 

*Tags: General Technical*

---

**liam** - *10:06:53*

Too many variables, best practice is to compare your simulations to the real world for each strategy/marketType/time of day etc, you will find it will be very consistent over the long run in terms of if it will under/over state matching/pnl

*Tags: Strategies*

---

**Jorge** - *11:50:10*

Hey guys. Using flumine my Strategy behaves strangely when Latency is high. Could it happen that my [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L118|market.blotter.live_orders](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L118|market.blotter.live_orders) gets updated but my [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L74|market.blotter.strategy_selection_orders](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L74|market.blotter.strategy_selection_orders) does not get updated? For example, in the event of an order filled, I'm thinking [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L118|market.blotter.live_orders](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L118|market.blotter.live_orders) could change but [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L74|market.blotter.strategy_selection_orders](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L74|market.blotter.strategy_selection_orders) doesn't and therefore my exposure calculation is wrong.

*Tags: Performance, Deployment, Strategies*

---

**Trex44** - *12:25:18*

Yes. Thanks very much, its buried in there. Is there a way to call this data inside a Flumine strategy? I can't find it in the market object or subclasses of the market class.

*Tags: Strategies*

---

**liam** - *12:28:24*

Ideally you set it up in a worker similar to [https://github.com/betcode-org/flumine/blob/master/examples/workers/inplayservice.py|this](https://github.com/betcode-org/flumine/blob/master/examples/workers/inplayservice.py|this) as imagine you will want it called periodically

*Tags: General Technical*

---

**liam** - *13:10:31*

Workers suit polling / long running process where as a moving average would need every MarketBook so suits being processed in a strategy and then stores in market context 

*Tags: Feature Engineering, Strategies*

---

**Matthieu Labour** - *15:52:26*

Has anyone had success incorporating the betdaq API or Smarkets API into flumine?

*Tags: General Technical*

---

**Beeblebrox** - *19:11:42*

I've seen strange things happen with strategy_selection_orders() not returning orders. I posted about it a month ago, but no one replied so I assumed it was just me ([https://betcode-org.slack.com/archives/C4H05ML2E/p1679771428625779](https://betcode-org.slack.com/archives/C4H05ML2E/p1679771428625779))

*Tags: Strategies*

---

## 2023-05-09

**Jose Maquia** - *01:45:45*

Hello! I'm using the following filters to place bets but want the bet to be placed normally and if partially filled I want it to automatically cancel after 5 seconds, what filters should I change. I tried FILL_OR_KILL but that doesn't work for me as I want it to stay for a few seconds instead of instantly cancel. I'm passing the constant CANCELLATION_TIME to the function but I'm unsure if it's actually doing anything.



# Define a limit order filter with time_in_force and min_fill_size

        limit_order_filter = betfairlightweight.filters.limit_order(

            size=order_size,

            price=order_price,

            persistence_type='LAPSE'

        )



        # Define an instructions filter

        instructions_filter = betfairlightweight.filters.place_instruction(

            selection_id=str(bet_object.selection_id),

            order_type="LIMIT",

            side="BACK",

            limit_order=limit_order_filter

        )



     bet_thread = threading.Thread(

            target= bfl_trading.betting.place_orders,

            args=(

                str(bet_object.match.market_id),

                [instructions_filter],

                CANCELLATION_TIME,

            ),

        )

        # Start the bet placement thread

        bet_thread.start()

*Tags: Strategies*

---

**Peter** - *09:55:10*

Sounds like something that should be raised as an issue in the [https://github.com/betcode-org/flumine/issues|github repo](https://github.com/betcode-org/flumine/issues|github repo).

*Tags: General Technical*

---

**liam** - *10:48:11*

Without some code to replicate not sure what is going on here, I imagine some [user] code is causing the latency to get high and then cause issues rather than what is described

*Tags: Performance*

---

**Beeblebrox** - *11:43:39*

This is it. If I could easily replicate it I'd log something, but unfortunately that's not the case.



I suspect it is something to do with latency as it only tends to happens when there are lots of markets being streamed (~5000) at the weekend. In my logs though there are only ever latency peaks of ~1 second which quickly fall away (within ~.01 seconds).



Like I say though, if I stop and restart it, it then works fine for the rest of the weekend, so no idea what's going on.

*Tags: Performance*

---

**Andy** - *14:03:40*

I had this problem too. The betfair blog and other places talk about fill_or_kill and fill_and_kill being exactly this - [https://apps.betfair.com/learning/fill-or-kill-dont-wait-around-for-a-price-to-be-matched/|https://apps.betfair.com/learning/fill-or-kill-dont-wait-around-for-a-price-to-be-matched/](https://apps.betfair.com/learning/fill-or-kill-dont-wait-around-for-a-price-to-be-matched/|https://apps.betfair.com/learning/fill-or-kill-dont-wait-around-for-a-price-to-be-matched/) but then the documentation only talks about a minFillSize which is the min stake amount to fill before cancelling - [https://docs.developer.betfair.com/plugins/servlet/mobile?contentId=3833942#content/view/3833942|https://docs.developer.betfair.com/plugins/servlet/mobile?contentId=3833942#content/view/3833942](https://docs.developer.betfair.com/plugins/servlet/mobile?contentId=3833942#content/view/3833942|https://docs.developer.betfair.com/plugins/servlet/mobile?contentId=3833942#content/view/3833942). No mention anywhere I can see of what parameter/filter to use to use the feature as I would have assumed would have been one of it’s primary use cases (kill after X number of seconds) 

*Tags: Feature Engineering*

---

**Jose Maquia** - *15:36:20*

thanks for the help

*Tags: General Technical*

---

**liam** - *15:36:27*

very easy if you are using flumine

*Tags: General Technical*

---

**Jose Maquia** - *15:38:58*

I'm using betfairlightweight mostly but yeah, don't think it'll be a problem

*Tags: General Technical*

---

## 2023-05-10

**Lee** - *09:09:40*

Office start time [https://github.com/betcode-org/flumine/blob/master/flumine/markets/market.py#L143|here](https://github.com/betcode-org/flumine/blob/master/flumine/markets/market.py#L143|here)

*Tags: General Technical*

---

**liam** - *09:11:20*

flumine can't see into the future (yet)

*Tags: General Technical*

---

## 2023-05-11

**rob smith** - *14:44:27*

This is probably a basic question so apologies in advance.... how do I convert an unmatched limit order into a limit on close order? I want to back at SP if the order is not matched first. Thanks

*Tags: General Technical*

---

## 2023-05-13

**Dennis** - *09:52:48*

did something change in the list_cleared_orders endpoint? I have some code that was working fine for ages but then fell over a few months back, now it just times out with an error

```cleared_orders = trading.betting.list_cleared_orders(

                bet_status="SETTLED",

                record_count=10,

                from_record=0,

                settled_date_range=range_dates

            )```

and I get an 'UNEXPECTED_ERROR' after a long time

*Tags: Errors Debugging, Strategies*

---

**Trex44** - *10:44:19*

Morning all. Does anyone have a good resource where you can learn how to work with big data files (4-40GB) in Python. Struggling to work with this Data using AWS Jupyter instances at the moment because of its size (and my lack of knowledge).

*Tags: Deployment*

---

## 2023-05-14

**Derek C** - *06:25:49*

[@U03N4QBJ0TV](@U03N4QBJ0TV) that's a tough question because there are so many options and it depends on how your data is structured and what you're preferences are in terms of analysis tools.



If your issue is just disk space, then using S3 instead of local storage is a good way to go.



If the issue is analysing large datasets then something I do quite a lot of, is to keep my data files in Amazon S3 and use Athena SQL to query them. It's a little tricky to set up but once you have defined your 'database' as this set of files (e.g. CSV files) you can then run SQL queries against it like any other DB. Of course this relies on you being comfortable with AWS/S3/SQL in the first place.



Another SQL option would be to get a trial Snowflake account and load the data into Snowflake - more user-friendly than Athena but more expensive once the trial expires.

*Tags: Deployment*

---

**R** - *14:06:09*

Forgive my laziness in checking docs, but is there a quick way to get the flumine simulator to read a directory of market data or is this a case of code the directory parsing yourself with your own market filter?



I have a structure like you get from betfair : YEAR//MONTH//DAY//MEETING_ID/MARKET_ID



I'm actaully guessing on the "meeting_id" part, I'm not sure what this ID relates to from betfair

*Tags: General Technical*

---

**R** - *14:39:45*

that makes sense, cheers!  So shoudl I assume that I need to write some logic to handle parsing through the directory or can flumine accept a directory similar to above and parse through markets itself?



E.G. run a backtest using flumine and it will read the directory (in sequence) and simulate on each market it finds?

*Tags: General Technical*

---

**Newbie99** - *14:49:45*

Yes essentially, I suspect most use glob.glob (or os.path...) depending on how your function is structured and filter based on your criteria.



My structure is similar, I start at the top level with sport, then year, month, date, then country code, then market (I group by event using event processing within flumine), but ultimately you need to pass as few markets as possible (i.e. ideally just the ones you actually want to test against)

*Tags: General Technical*

---

## 2023-05-17

**vtaco** - *02:37:34*

Hello, thanks for all the great work over the years on this framework.  Question - what is the difference between the two sample strategies  [https://github.com/betcode-org/flumine/blob/master/examples/strategies/pricerecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/pricerecorder.py) and [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py)

*Tags: General Technical*

---

**liam** - *06:45:10*

`pricerecorder` is writing some basic data to a csv whereas `marketrecorder` is storing all streaming updates to allow market playback through the simulation and/or bflw

*Tags: Data Quality*

---

**vtaco** - *20:14:08*

so if i'm running a market recorder, safe to say theres no reason to run pricerecorder (other than the convenience of all price updates dumped to csv)?

*Tags: Data Quality*

---

**vtaco** - *20:18:07*

thanks man.  Related question - if I wanted to take the marketrecorder output files and shove into a sql database (one table for each event's metadata, one table for all the updates), is there a resource/example that would be best to reference?

*Tags: General Technical*

---

## 2023-05-19

**Paul** - *13:45:44*

Hello all! We are doing a little survey to help plan the next event/meet-up. If you have 2 minutes, would appreciate it if you could fill this in: [https://forms.gle/DWXRMqZjkc9xhVgC9|https://forms.gle/DWXRMqZjkc9xhVgC9](https://forms.gle/DWXRMqZjkc9xhVgC9|https://forms.gle/DWXRMqZjkc9xhVgC9)

*Tags: General Technical*

---

**Unknown** - *14:36:02*

Good morning, everyone. Yesterday, while using _*betfairlightweight*_, I noticed that all soccer matches, once they appear as _*InPlay*_ on the Betfair website, take approximately 1 minute to show up for the first time in the API when you select to display `in_play_only=True` events. Additionally, it takes 2 minutes or more for them to stabilize and continuously appear without interruption.



```market_filter = betfairlightweight.filters.market_filter(

        event_type_ids=['1'],in_play_only=True

    )

    return trading.betting.list_events(filter=market_filter)```

Is this a normal occurrence when using the API with calls like the one above or similar to the test video I made using [https://docs.developer.betfair.com/visualisers/api-ng-sports-operations/](https://docs.developer.betfair.com/visualisers/api-ng-sports-operations/)?



Im asking because, for example, in _*Geeks Toy Software*_, once it shows as _*InPlay*_, it stays that way, and the same goes for the Betfair website. However, when using the API, this oscillating occurs.

*Tags: Getting Started, Strategies*

---

**thambie1** - *15:35:16*

I verified the same behavior happens for me, so at least the problem isn't specific to you.

*Tags: General Technical*

---

**Jonjonjon** - *21:10:58*

I finally installed it.

My personal suite of tests takes 55 seconds on Python 3.8. 49 seconds on Python 3.11. So about 11% faster

*Tags: Getting Started*

---

## 2023-05-22

**Jorge** - *10:11:33*

I would like to populate my Backtest results with some info from the market catalogue. I am loading the marketCatalogue files in the backtest using [https://github.com/betcode-org/flumine/blob/master/examples/middleware/marketcatalogue.py|marketcatalogue.py](https://github.com/betcode-org/flumine/blob/master/examples/middleware/marketcatalogue.py|marketcatalogue.py) , but how can I access these market_catalogues in [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|backtestloggingcontrol.py) ? One solution would be to add it to the orders context, but I still wouldn't be able to access it in [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py#L83|_process_cleared_markets](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py#L83|_process_cleared_markets)

*Tags: General Technical*

---

**Jorge** - *10:34:37*

That's good but I still cannot access it in[https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py#L83|_process_cleared_markets](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py#L83|_process_cleared_markets)

*Tags: General Technical*

---

## 2023-05-23

**Peter** - *07:56:04*

If you're using Flumine it will handle creating a session and keeping it alive. If you're using betfairlightweight or betconnect you'll need to keep the session alive yourself.

*Tags: Deployment*

---

**Jonjonjon** - *08:35:36*

Suppose I am running 10,000 backtests.

I want to store the results.

I currently store the results as gzipped JSON, one per market. But then I end up with 10,000 files.

I've tried writing to sqlite instead, but that uses a lot of disk space.

I'm thinking of trying to write all results to a single zipped test file. But I've read that most libraries such as gzip will give poor performance if write to it incrementally. What else should I try?

*Tags: Performance*

---

**Jonjonjon** - *09:17:18*

From prior work, if each process opens the file (with lock), writes a little to it, then closes it... The overall compression will be poor due to the way most compression algorithms work. Some are better than others for streaming writes.

*Tags: General Technical*

---

**Jonjonjon** - *09:31:07*

[https://stackoverflow.com/questions/18097107/python-gzip-appending-to-file-on-the-fly|https://stackoverflow.com/questions/18097107/python-gzip-appending-to-file-on-the-fly](https://stackoverflow.com/questions/18097107/python-gzip-appending-to-file-on-the-fly|https://stackoverflow.com/questions/18097107/python-gzip-appending-to-file-on-the-fly)

*Tags: General Technical*

---

## 2023-05-25

**Andy B** - *05:25:51*

I have some code for placing thoroughbred bets that I cobbled together combining my python stuff and BFLW.  30 seconds out from scheduled start time, I get some variables from by database, get the current "to back" price from BF and run them through a ML algorithm to get predictions.  This works okay, but I want to shift this into Flumine and run it closer to start time.

For back testing purposes, I am pulling the data out of my database into a dataframe and then as I process each market in flumine, I select the subset of rows for the specific market and run the ML algorithm over it.  This works okay to this point, but I am now running into an issue where it is trying to continually run the process until the race goes in-play, so I can see that the way I currently have my code designed isn't right.

I was wondering how others implement the processing of ML algorithms in Flumine, or if I am better off forcing it to not process the market again once an order is placed.  If that is the better option, how would I do that?

*Tags: Feature Engineering*

---

**Andy B** - *06:00:28*

Thanks Liam,

    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:



        if market.seconds_to_start &lt;= 5 and market_book.inplay == False:



            [http://logging.info|logging.info](http://logging.info|logging.info)(f"Inside the last 5 seconds for this market and race is not inplay")

            ThisMarket = TodaysRunners[TodaysRunners['MarketID'] == market_book.market_id]



            for runner in market_book.runners:

				if runner.status == "ACTIVE" and runner.ex.available_to_back:

					row_index = ThisMarket[(ThisMarket['MarketID'] == market_book.market_id) &amp; (ThisMarket['SelectionID'] == runner.selection_id)].index

					if len(row_index) &gt; 0:

						# Update the value in the BSP column for the matching row

						ThisMarket.loc[row_index, 'BSP'] = runner.ex.available_to_back[0]['price']



            RaceFull = ThisMarket[['TrackName','RaceNumber','Distance','HorseName','Num','MarketID','SelectionID']].copy()

            scaled_features = StandardScaler().fit_transform(Race.values)

            ModelName = 'HorseModel2M.h5'

            ModelName = tf.keras.models.load_model(ModelName)

            Predictions = ModelName.predict(scaled_features)

            RaceFull['Probability'] = Predictions

            RaceFull['Model'] = 'Model9'

            RaceFull = RaceFull.sort_values(by = 'Probability', ascending=False)

            RaceFull = RaceFull.fillna(0)

*Tags: Feature Engineering, Strategies*

---

**Andy B** - *07:57:37*

I think that I am accessing the keras model to make predictions too fast when back testing against the market stream:

2023-05-25 16:21:56,369:CRITICAL:Unknown error bad allocation in process_market_book (1.212040559)

Traceback (most recent call last):

  File "C:\Python\lib\site-packages\flumine\utils.py", line 239, in call_strategy_error_handling

    return func(market, market_book)

  File "C:\Python\Scripts\flumine\FlumineSims.py", line 228, in process_market_book

    ModelName = tf.keras.models.load_model(ModelName)

  File "C:\Python\lib\site-packages\keras\utils\traceback_utils.py", line 70, in error_handler

    raise e.with_traceback(filtered_tb) from None

  File "C:\Python\lib\site-packages\h5py\_hl\files.py", line 533, in __init__

    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)

  File "C:\Python\lib\site-packages\h5py\_hl\files.py", line 226, in make_fid

    fid = h5f.open(name, flags, fapl=fapl)

  File "h5py\_objects.pyx", line 54, in h5py._objects.with_phil.wrapper

  File "h5py\_objects.pyx", line 55, in h5py._objects.with_phil.wrapper

  File "h5py\h5f.pyx", line 106, in h5py.h5f.open

  File "h5py\h5fd.pyx", line 155, in h5py.h5fd.H5FD_fileobj_get_eof

  File "h5py\h5fd.pyx", line 155, in h5py.h5fd.H5FD_fileobj_get_eof

MemoryError: bad allocation



That makes sense if it is multi-processing and trying to open it n number of times I guess, but I am not sure if there is a way to do this without slowing down the stream.  Is there a better way to do this?

*Tags: Errors Debugging, Performance, Strategies*

---

**liam** - *08:24:00*

Not sure how keras/h5 works but to start with I imagine loading the model is slow and is shared so could be done in `Strategy.add`



```def add(self):

    self.model = tf.keras.models.load_model(ModelName) ```

*Tags: Performance, Strategies*

---

**Andy B** - *09:36:58*

I think I am going to have to reconsider this as I keep running into memory issues, despite having more than enough available memory.  Thanks for your help [@U4H19D1D2](@U4H19D1D2)

*Tags: Performance*

---

**Andy B** - *09:48:49*

Yes, but I got the same error.  I am assuming I have done it correctly, but my assumptions are not always great.

*Tags: Errors Debugging*

---

## 2023-05-26

**Michael** - *00:27:05*

Is there any endpoint I can hit when running cleared_orders to get the BSP of the selection I placed? I am streaming data seperately but just want to populate this BSP for selection in my db so maybe this would be easier to do?

*Tags: General Technical*

---

**Tom** - *02:14:43*

Is it possible to create these so that Flumine uses my modified version in its automatic functions? I have called my RunnerContext, RunnerData, then realised it wasn't updating trades/orders e.t.c.



If I call them RunnerContext, will it use my modified version? I'm sorry I'm still a newb with coding, OOP, although getting some parts of it.

*Tags: General Technical*

---

**AI Trader** - *19:12:44*

Hi guys,

What is the physical location of betfair servers? Asking because I want to run my strategies in the same location for lower latency. Is it London?

*Tags: Performance, Deployment*

---

**Jonjonjon** - *22:28:58*

I'm not sure if this is correct or not, but will [https://github.com/betcode-org/betfair/blob/e4d58fd369312408c04519be1520e35b0dfc61dc/betfairlightweight/endpoints/betting.py#L241|list_market_book](https://github.com/betcode-org/betfair/blob/e4d58fd369312408c04519be1520e35b0dfc61dc/betfairlightweight/endpoints/betting.py#L241|list_market_book) do it? Disclaimer: I haven't tried it.

*Tags: Strategies*

---

## 2023-05-27

**Tom** - *09:02:55*

So I used RunnerContext to create RunnerData as a subclass but it doesn't update with trades/orders when they are placed, so it's a little redundant. I was wondering if I called it RunnerContext instead of RunnerData if Flumine would recognise and update them in that case, but obviously not I guess because it wouldn't know where to find them.



No biggie, just figuring things out still.

*Tags: General Technical*

---

**Mo** - *16:17:01*

I’m curious what latency edge you’re hoping to get by asking this in a public forum rather than working it out yourself?

*Tags: Performance*

---

## 2023-05-30

**Jorge** - *13:34:09*

Hi, in the [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|marketrecorder.py) , I would like to access market.event_name (or other market_catalogue fields) to be able to filter the recorded markets. Potentially inside [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py#LL64C9-L64C25|process_raw_data](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py#LL64C9-L64C25|process_raw_data) . Is this possible?

*Tags: General Technical*

---

## 2023-05-31

**liam** - *08:31:55*

So on the initial image you get the streaming definition, so you could do something like this:



```    def process_raw_data(self, clk: str, publish_time: int, data: dict):

        market_id = data.get(self.MARKET_ID_LOOKUP)



        if "marketDefinition" in data:

            if data["marketDefinition"]["venue"] == "Ripon":

                if market_id not in self.markets_i_want_to_process:

                    self.markets_i_want_to_process.append(market_id)



        if market_id not in self.markets_i_want_to_process:

            return



        file_directory = os.path.join(self.local_dir, self.recorder_id, market_id)

        with open(file_directory, "a") as f:

            f.write(

                json.dumps(

                    {"op": "mcm", "clk": clk, "pt": publish_time, "mc": [data]},

                    separators=(",", ":"),

                )

                + "\n"

            )```



*Tags: General Technical*

---

## 2023-06-01

**Trex44** - *10:47:17*

[@U04NWADNCFR](@U04NWADNCFR) Thanks very much for the answer. I currently use csv's stored in S3 then load them into a Jupyter note book in AWS Sagemaker and use Pandas to explore them. I have considered moving everything over to an SQL database but I like the graphical output I can get from using Pandas. Loading into the notebook takes an age and the kernel dies if I try to manipulate large data sets as the instance doesn't have the memory needed.

*Tags: Feature Engineering, Performance, Deployment*

---

**Derek C** - *16:13:41*

[@U03N4QBJ0TV](@U03N4QBJ0TV) I do a lot of the same thing, except that I use Athena to populate pandas DataFrames in SageMaker from my files in S3. The benefit is that I can filter the rows and the columns that will be brought into memory in the SQL logic, and also sometimes do grouping to further reduce the size of the dataset that lands in Pandas. The benefit of Athena is that you don't have to actually migrate anything into a SQL database - you just tell it where the csv files are in S3 and they 'become' the database that you can then query. The awswrangler library makes it easy to integrate Pandas and Athena/S3 if you're not already using that.

*Tags: Feature Engineering, Performance, Deployment*

---

## 2023-06-02

**Trex44** - *10:00:12*

[@U04NWADNCFR](@U04NWADNCFR) When you say you use Athena to populate the pandas DF's in Sagemaker; can you do this via Sagemaker or is it a case of running the query using Athena then manually loading the output file from the given S3 bucket into a Jupyter notebook. In other words, if I change the data filters in the Pandas script I am using (e.g. focus on events taking place on Sunday only) in a Jupyter instance on Sagemaker will it run a new query via Athena and I can then visualise the output in Sagemaker? Or is it the case I would have to vary the filters in Athena then load the new results into Sagemaker for visualisation?

*Tags: Feature Engineering*

---

**Derek C** - *14:12:21*

Everything runs inside Jupyter, using sagemaker studio. I guess you might be using the older version of Sagemaker? Inside SageMaker Studio



`import awswrangler as wr` 



`df = wr.athena.read_sql_query(sql=my_sql_text, database="bf", boto3_session=session, keep_files=False)`

*Tags: Deployment*

---

**George** - *15:50:45*

When using the Betfair API, is there any way of placing the bet in a currency other than GBP?

If not, does everyone in non-UK countries have to request the exchange rate repeatedly?

*Tags: General Technical*

---

## 2023-06-03

**thambie1** - *03:18:06*

Anyone know if it's possible to login using the API using a trading master account? Currently getting error "TRADING_MASTER" when I try.

*Tags: Errors Debugging, Strategies*

---

## 2023-06-04

**Andy** - *05:24:09*

Will check out. I’m currently just streaming and getting updates whenever there’s an update. If there was some parameter to set to avoid in play bets that would’ve been ideal 

*Tags: General Technical*

---

**thambie1** - *05:26:58*

Whether or not the match is in play is provided in the streaming data. That is by far the simplest method. There is no such parameter to set when placing a bet.

*Tags: General Technical*

---

**Jonjonjon** - *09:41:32*

Must be some sort of super hft strategy if they want to remove the if statement. Reminds me of something I read about a guy who would parse order stream messages before they fully arrived (on an Asian financial exchange, not Betfair).

*Tags: Strategies*

---

**Jonjonjon** - *11:11:11*

Does anyone do that with Flumine?

*Tags: General Technical*

---

**D C** - *11:16:56*

I used to do something similar but that was with stuff off a serial port at low baud rate where you could get one byte at a time - and this was with fixed format message protocol where you had specific end of message identifier charactetr. Higher level stuff probably won't give you that kind of granularity. I'd imagine JSON parsing would be a lot trickier but my inner nerd is intrigued how you would go about doing it with a  more open ended message structure. Would you be just looking to parse interior structures as they arrive and assemble once you identify end of message?

*Tags: Errors Debugging*

---

**Trex44** - *16:40:50*

Ahh ok. I was using the Notebook tab in AWS's Sagemaker service to create and use Jupyter notebooks. I wasn't actually using SageMaker studio its self. I think I steered away from it because I couldn't figure out how much it was going to cost me where as when you launch a stand alone notebook you can instantly see the resources you are using e.g. ml.t3.2xLarge so you know how much it will cost.



How much does using a Sagemaker studio instance cost? I see from the tutorial videos you can launch standalone notebooks in the Studio instance that utilise different resources so I am guessing you get charged for that. But is there a charge simply for keeping a Sagemaker Studio instance running even when there are no notebooks active?

*Tags: Deployment*

---

## 2023-06-05

**Mo** - *11:42:06*

flumine?

*Tags: General Technical*

---

**Tom** - *11:43:11*

wap is flumine.utils

The volume I think is betfairlightweight?



Should the input be a flumine input?

*Tags: General Technical*

---

**Mo** - *11:43:35*

I’m asking if you’re doing this in flumine or just bflw 

*Tags: General Technical*

---

**Tom** - *11:43:59*

I've imported wap from flumine but most of this code is bflw

*Tags: General Technical*

---

**Mo** - *11:44:30*

So then my question is have you asked for the traded volumes in your price projection?

*Tags: General Technical*

---

**Tom** - *11:46:21*

I thought that this code

volume = runner.ex.traded_volume



Got like a PriceSize list or something like that which was input to wap.



It could be a better/quicker question - of the best way to get VWAP/Volume for individual runners

*Tags: General Technical*

---

## 2023-06-07

**C0rnyFlak3s** - *12:30:27*

Hello guys, it’s been a while since I was active in this slack, now I’m returning to the field in full steam. I was constructing an extensive odds feed database myself to validate my value models over the last couple months. So far so good, but now I want to backtest my models on my data feed. I have written a custom game matcher which matches games from different bookmakers, however what I am still struggling with is a good source for historical results data. Sure, I can extract many result stats from my data myself, but I would rather enjoy an API service that lets me pull results (including corners etc.  besides just goals).



So my question to you guys is, are you using 3rd party services to get results data on already finished games, and if you do, can you maybe point me to a place which offers a great sports/league coverage? It doesn’t need to be a free service, paid services are also welcome if they offer an API endpoint to pull historical results. Thanks.

*Tags: Strategies*

---

## 2023-06-11

**PeterLe** - *13:21:54*

Hi, sorry newbie question...can someone please explain this setting in simple terms :

```multi_order_trades: bool = False,```

If you were trading the same market, how would it work in conjunction with :

```max_live_trade_count: int = 15,```

Thanks in advance

*Tags: Deployment, Strategies*

---

## 2023-06-13

**Alejandro Pablos Sánchez** - *22:48:44*

Hi, I'm struggling to get the names of the runners in a given market, for a specific event, but I don't get it. The first thing I do is to get the market book associated to my target market, requesting best offers. Then, I iterate over the different market books returned (just one) and inside of it, the market runners or selections. However I cannot know the name of each of these runners. There is a RunnerDescription class in the resources module that seems to have a name field, but I don't know how to link my code with it. I'd be very glad to receive any kind of help. Thanks a lot :smile:

*Tags: General Technical*

---

**Unknown** - *22:50:33*

This my code for this. Inside the nested loop for runners, I would like to do something like runner. name or runner.description.name, but it is not working even though I know it should be somehow possible to get the runner name. Thanks for the help ! :relaxed:

*Tags: General Technical*

---

## 2023-06-14

**John Foley** - *00:16:12*

wait, i just saw your screenshot :slightly_smiling_face: my answer is assuming you are using flumine, but maybe you’re not?

*Tags: General Technical*

---

**Guy Incognito** - *05:28:30*

Does anyone know how to get the list of all market_id's at once without doing some sort of appending through process market book?

*Tags: General Technical*

---

**liam** - *05:33:08*

In flumine? / if so where are you?

*Tags: General Technical*

---

**Guy Incognito** - *05:52:50*

Yea in flumine, I'm currently in process market book, but it can be in any part of the code

*Tags: General Technical*

---

**Peter** - *06:21:10*

A few things to unpick here.



Firstly if you're going to append them to a global list, better to do it in market middleware, which has an add() method that is run just once when the market is first encountered.



Secondly you don't need to do that as there is a flumine.markets list containing all the markets that you can iterate over to (for example) create a list of of all the market_ids, or just access directly.



Thirdly, what is your global list used for? If it's to find related markets (e.g. the win and place markets for a horse race) would market.event, which already brings them together, already provide the information you're seeking?



Finally, if you still need it, where are you wanting to use your global list (or flumine.markets) as that would influence the best way to access / build it?

*Tags: General Technical*

---

**Alejandro Pablos Sánchez** - *13:33:29*

Hi ! I'm not using flumine, I'm using betfairlightweight. Are there substantial advantages in using flumine?

*Tags: General Technical*

---

**Peter** - *14:08:30*

Yes, you're using list_market_book to the get prices, but need to use list_market_catalogue to get the runner names.



And yes, Flumine requires a whole different way of thinking, but once you're over that, it's way easier to work with as it takes care of much of the infrastructure for you (such as including the runner names in the market object) with the added advantage of giving you near continuous price updates.

*Tags: General Technical*

---

## 2023-06-17

**AI Trader** - *10:54:50*

Hi guys,

I am looking for a solution to test strategies with small amounts of money. I know I. can limit that through software and using the concept of strategies in the orders and etc, but I was wondering if there is any way to have anything similar to a sub-account (as in crypto exchanges), so regardless of software bugs, I can guarantee I will be using a limited amount of money for a strategy. Any ideas on this? [https://support.betfair.com/app/answers/detail/a_id/94|Betfair seems not to allow multiple accounts per client](https://support.betfair.com/app/answers/detail/a_id/94|Betfair seems not to allow multiple accounts per client)

*Tags: Errors Debugging, Strategies, Multi Client*

---

## 2023-06-19

**Guy Incognito** - *08:28:41*

Does `streaming_timeout` individual to each market e.g. if a market is added later on will it be desynced with the streaming time out of other markets or is streaming_timeout synced across all markets? And is streaming timeout reset by a marketbook event or is it on its own cycle that isn't affected by process market book

*Tags: General Technical*

---

**liam** - *08:55:05*

Its per stream, nothing complicated, its simply going to give you eevry MarketBook received or a stale book if seconds since latest is greater than your `streaming_timeout`



[https://github.com/betcode-org/flumine/blob/d2e7d8e6cbc65687d2a0fb41e8f24bf133081f59/flumine/streams/marketstream.py#L59](https://github.com/betcode-org/flumine/blob/d2e7d8e6cbc65687d2a0fb41e8f24bf133081f59/flumine/streams/marketstream.py#L59)

*Tags: General Technical*

---

**Peter** - *09:40:56*

A stream is a connection to Betfair through which will be passed the updates for all markets subscribed to in that stream. So the answer to your question is yes.

*Tags: General Technical*

---

**D C** - *18:58:38*

Yeah I guess - but it's just sales patter isn't it? [@UBS7QANF3](@UBS7QANF3) that model has been around for ages now hasn't it - do you know if they actively work on it to improve it? I enquired years ago about how much it was to subscribe to but was told it was available to bookies only - do many bookies actually use it?

*Tags: Strategies*

---

## 2023-06-20

**D C** - *11:41:09*

Yeah I saw that [@UQL0QDEKA](@UQL0QDEKA) although it was mostly for pennies up in the top end prices. Does anyone know if bookies apply a latency on their odds for non-logged in accounts to deter scraping? If so probably no point me watching at all.

*Tags: Performance*

---

**George** - *12:09:52*

Is it a bad idea to put a 'slow' (maybe a few seconds) function call inside `process_market_book`?  I imagine that would cause a lot of problems when (potentially) betting on large numbers of markets?

*Tags: Performance, Strategies*

---

**liam** - *13:35:24*

[https://github.com/betcode-org/flumine/tree/master/examples/workers](https://github.com/betcode-org/flumine/tree/master/examples/workers)

*Tags: General Technical*

---

**Alejandro Pablos Sánchez** - *21:27:36*

Hello mates !! Since yesterday night, I'm getting the AccountAPING exception error when using the API client. Is anyone experiencing the same issue? Apparently it says "INVALID_SESSION_INFORMATION" but I didn't change the credentials neither the key, so I can't figure out why this is happening. Any help? 



Thanks :wink:

*Tags: Errors Debugging*

---

**Alex** - *23:36:14*

Any reason why line 191 in markets.blotter reads `for order in self.strategy_selection_orders(strategy, *lookup[1:])` ? Why is disregarding the first element in lookup? I suspect this is a bug because strategy_selection_orders would normally take strategy, selection_id, and handicap as parameters. In the above it would just get strategy and selection_id=handicap and handicap defaulting to 0. [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L191](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L191)

*Tags: Errors Debugging, Strategies*

---

## 2023-06-21

**Peter** - *07:48:04*

Is this with Flumine or Betfairlightweight?

*Tags: General Technical*

---

**Alejandro Pablos Sánchez** - *07:50:08*

betfairlightweight

*Tags: General Technical*

---

**Peter** - *07:53:07*

Is it possible that your session token expired? With betfairlightweight you need to call the keep_alive method yourself to keep it active.

*Tags: Deployment*

---

**Guy Incognito** - *09:01:00*

I've spent about an hour looking for this and as soon as I posted the question I found it within 10 seconds

*Tags: General Technical*

---

**liam** - *09:13:06*

Or



```def incognito_worker(context: dict, flumine) -&gt; None:

    for strategy in flumine.strategies:

        print(strategy.context)```

*Tags: Strategies*

---

**casper** - *12:21:41*

Hey [@U4H19D1D2](@U4H19D1D2)!



I think in the latest betfairlightweight release (2.17.2) you haven’t actually upgraded orjson to 3.8.7 as per release notes? It’s still at 3.8.5?



[https://github.com/betcode-org/betfair/blob/master/requirements-speed.txt#L2](https://github.com/betcode-org/betfair/blob/master/requirements-speed.txt#L2)

[https://github.com/betcode-org/betfair/pull/519/files](https://github.com/betcode-org/betfair/pull/519/files)



The latest orjson version is now 3.9.1, maybe worth bumping to that? They (orjson) seem to have been doing loads of minor releases recently.

*Tags: Performance*

---

**Alejandro Pablos Sánchez** - *16:29:06*

Coming back to this, I'm still not able to fix it with the keep_alive method

*Tags: Errors Debugging, Deployment*

---

**Alejandro Pablos Sánchez** - *16:29:14*

`*APIError*: AccountAPING/v1.0/getAccountFunds` 

Params: {}

Exception: None

Error: {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}}

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}}, 'id': 1}

*Tags: Errors Debugging*

---

**Alejandro Pablos Sánchez** - *16:29:49*

This is the exact error message I get, is the same for whatever API function I wanna use

*Tags: Errors Debugging*

---

## 2023-06-22

**Tom** - *03:02:19*

Hey guys, what is the best way to amend orders in Flumine? Is there any dummy code for this available? It's just checking price and size and re-establishing the order to new market conditions. I've got to get my head around this blotter as well - is that how most of you are managing your orders? Will it retrieve all existing orders if you re-start your strategy?

*Tags: Strategies*

---

**Peter** - *06:39:36*

Flumine has an `order.replace` replace method, but it only allows you to change the price, not the size, as that's all that Betfair allows on an existing order. You will of course also lose your place in the matching queue, re-entering at the back of your new price's queue. Because it's limited, I suspect that most people simply do an `order.cancel` and then place a new bet with the adjusted price and size. Using that approach though, you need to be aware that there may be a delay before your cancelation is executed, and indeed it can still be matched in the small window between you requesting the cancellation and it taking effect, so it's prudent to verify the cancellation before placing the adjusted bet.



The blotter is certainly very useful, I'd say equally with the orders object received by a strategy's `process_orders` method. Note an important difference is that the blotter has all orders for the current market, but the `process_orders` method receives only the orders for the current strategy.



Yes, existing orders will be retrieved and added to the blotter if you restart your strategy, but there may be a small delay as those with execution complete status won't come through the order stream, so instead Flumine makes an API call to Betfair to get them and add them to the blotter. Better practise though is to design your strategies to run continuously (whilst acknowledging that during development that may not be feasible or desirable).

*Tags: Strategies*

---

**liam** - *06:50:55*

In addition to above note that flumine won't add 'execution complete' orders by default without the example [https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|middleware](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|middleware)

*Tags: General Technical*

---

**Tom** - *10:57:56*

Really appreciate the help gentlemen

*Tags: General Technical*

---

## 2023-06-23

**Guy Incognito** - *06:31:22*

If the market book is a stale market book will this trigger check market book to run, I have streaming_timeout set to 5 with a print statement in check market book but most markets that haven't had an update don't seem to be printing out every 5 seconds

*Tags: General Technical*

---

## 2023-06-24

**Andy** - *03:00:33*

Anyone running their code in VSC getting these oom errors and VSC crashing periodically? The program doesn’t seem to be using large amounts of memory if I check task manager and the json files I’m reading and writing to aren’t more than a few mb in size. Have tried the google suggestions but hasn’t improved things - [https://stackoverflow.com/questions/71614897/vscode-crashed-reason-oom-code-536870904#:~:text=If%20you%20are%20using%20VSCode,Optimizations%22%20and%20mark%20the%20checkbox.&amp;text=It's%20easy%20than%20you%20think,will%20see%20that%20it%20works|https://stackoverflow.com/questions/71614897/vscode-crashed-reason-oom-code-536870904#:~:text=If%20you%20are%20using%20VSCode,Optimizations%22%20and%20mark%20the%20checkbox.&amp;text=It's%20easy%20than%20you%20think,will%20see%20that%20it%20works](https://stackoverflow.com/questions/71614897/vscode-crashed-reason-oom-code-536870904#:~:text=If%20you%20are%20using%20VSCode,Optimizations%22%20and%20mark%20the%20checkbox.&amp;text=It's%20easy%20than%20you%20think,will%20see%20that%20it%20works|https://stackoverflow.com/questions/71614897/vscode-crashed-reason-oom-code-536870904#:~:text=If%20you%20are%20using%20VSCode,Optimizations%22%20and%20mark%20the%20checkbox.&amp;text=It's%20easy%20than%20you%20think,will%20see%20that%20it%20works).

*Tags: Errors Debugging, Performance, Strategies*

---

**Trex44** - *07:28:59*

Morning all. How do you access the Runner name via a strategy when backtesting? I can see it in RunnerCatalogue but can't figure out how to get there. Market Catalogue is 'none'.

*Tags: Strategies*

---

## 2023-06-26

**Jorge** - *09:24:50*

Hi, I'm trying to understand how would my flumine Strategy work in case the latency suddenly is very high. I am placing an order using [https://github.com/betcode-org/flumine/blob/master/flumine/markets/market.py#L79|place_order](https://github.com/betcode-org/flumine/blob/master/flumine/markets/market.py#L79|place_order) and then checking the selection's exposure using [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L185|market.blotter.get_exposures(self, lookup=selection_lookup)](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L185|market.blotter.get_exposures(self, lookup=selection_lookup)). Could it happen that the order gets matched and goes to EXECUTION_COMPLETE, but being the latency very high, the selection's exposure doesn't consider this order? In this case, my strategy would think this order never existed and place it again

*Tags: Performance, Strategies*

---

**Jorge** - *10:04:01*

With high Latency I mean I see "High latency between current time and MarketBook publish time" in the logs. In these cases [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L185|market.blotter.get_exposures(self, lookup=selection_lookup)](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L185|market.blotter.get_exposures(self, lookup=selection_lookup)) returns a wrong exposure, my guess is that it does not consider the order because the EXECUTION_COMPLETE message does not arrive on time

*Tags: Performance*

---

**liam** - *10:09:32*

Yeah if you get that then you are in deep trouble, fix the cause don’t try and debug this outcome. What are you doing to cause it? Have you profiled? 

*Tags: Errors Debugging*

---

**Jorge** - *10:14:09*

I guess my model is too slow and when there are many live games it takes too long to run process_market_book. For sure increasing the number of CPUs and optimizing the model would help me. I'd just like to add another safety check in case this ever happens again

*Tags: Performance, Deployment, Strategies*

---

**Jorge** - *12:09:03*

I see a lot of [https://github.com/betcode-org/flumine/blob/master/flumine/baseflumine.py#L140|"High latency between current time and MarketBook publish time"](https://github.com/betcode-org/flumine/blob/master/flumine/baseflumine.py#L140|"High latency between current time and MarketBook publish time") messages but very few [https://github.com/betcode-org/betfair/blob/master/betfairlightweight/streaming/stream.py#L69|"Latency high"](https://github.com/betcode-org/betfair/blob/master/betfairlightweight/streaming/stream.py#L69|"Latency high") ones. What is the difference between the 2 of them?

*Tags: Performance*

---

## 2023-06-27

**Guy Incognito** - *04:54:46*

This is some reproducible code there are 3 market ids I put into streaming market filter. One is just a random market that was a few minutes from the jump and two are from races a few days away. The two markets that are quite far away basically have no trading activity while the one jumping soon is very active. When I chick the logging file only the very active market has check market book and process market book running. The two other markets don't have anything logged, but when that very active market went inplay and then afterwards  settled, that's when streaming time out seemed to start working for the two other markets and they started logging stuff every 5 seconds. How can I get all markets to process every 5 seconds? 



# Logging

```logging.basicConfig(filename = 'streaming_timeout.log', level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO), format='%(asctime)s:%(levelname)s:%(lineno)d:%(message)s')



class OverWatch3(BaseStrategy):

    def start(self) -&gt; None:

        print("starting strategy")



    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:

        [http://logging.info|logging.info](http://logging.info|logging.info)(f"Running check_market_book for market.market_id: {market.market_id}")

        return True

        

    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:

        [http://logging.info|logging.info](http://logging.info|logging.info)(f"running process_market_book for market.market_id :{market.market_id}")



ow3 = OverWatch3(

    market_filter=streaming_market_filter(

        market_ids = ['1.215545086','1.215545081','1.215501829']

    ),

    streaming_timeout = 5,

)



framework.add_strategy(ow3)



framework.run()```

*Tags: Strategies*

---

## 2023-06-28

**George** - *16:05:41*

[https://github.com/betcode-org/flumine/blob/master/examples/workers/inplayservice.py#L13](https://github.com/betcode-org/flumine/blob/master/examples/workers/inplayservice.py#L13)

Can I ask, why does this function require `context` as an input? It doesn't appear to be used within the function?

*Tags: General Technical*

---

**liam** - *17:18:37*

It’s the default, see the [https://github.com/betcode-org/flumine/blob/master/flumine/worker.py|code](https://github.com/betcode-org/flumine/blob/master/flumine/worker.py|code), just not used in this example

*Tags: General Technical*

---

**George** - *17:34:15*

Now I think I understand workers and my code is working - thanks very much. But I still have an issue.

Whilst the simulation is of course much faster than "real-time", there is no equivalent way to speed up the worker code which is inherently slow (e.g. calls to an external database or API).

This means that the worker output - which writes to `market.context` - is totally misaligned (in terms of 'clock') with the market-book. Isn't this a huge issue for some users?

Now I am lucky, because my worker output does not change frequently over time. However, I do need the worker to execute its "first pass" before the first `process_market_book`  is called. I can, of course, instruct my worker to start 10 "minutes" before `check_market_book` returns True, but 10 simulation-minutes is much quicker than the time it takes for the worker output to be generated, so it solves the issue in live trading but not in simulation.

I think that I can workaround this issue by checking the `market.context` during the `check_market_book`. However, it is not ideal, because `check_market_book` will return False in the simulation, whereas, in real life it would have returned True.

Depending on how slow the worker is, most of the simulated trading window could have been missed, just waiting for the worker's first pass!

I imagine this is not a new question - what is the standard answer to dealing with this?

*Tags: Performance, Deployment, Strategies*

---

**Aaron Smith** - *18:09:14*

where are they then? in the bought data? Before i start a strategy, i normally populate a selection_id_to_name dict if i need a name, but this always seemed weird to me. Is this what everyone else does aswell?

*Tags: Strategies*

---

## 2023-06-29

**George** - *17:30:58*

Is there a BF order type that ensures the order (if EXECUTABLE) will be automatically cancelled when the market status goes to SUSPENDED?

If not, what is the most flumine-like way to achieve that behaviour?

(I have coded up a way to do it, but I think it's unnecessarily complicated.)

*Tags: General Technical*

---

**George** - *17:43:45*

Having checked the BF docs, it looks like there is no such order type. But, it would be very useful to have a persistenceType = LAPSE_ON_SUSPENSION.

I imagine experienced Flumine users have a good way to do it though?!

*Tags: General Technical*

---

## 2023-06-30

**George** - *09:21:54*

OK, so basically have an ever growing dictionary inside the strategy object mapping market ID to number of active runners?

*Tags: Strategies*

---

**Stef** - *10:24:46*

Hi all,

maybe this question was already answered, in that case apologies in advance&gt;

I have a live key, which I activated a while ago before Betfair started charging £299. It works fine with my old scripts, but it doesn't seem to be working with streaming. AM I doing something wrong?



Thanks :)

*Tags: Deployment*

---

**George** - *17:41:47*

Is there a function in flumine (or betfairutil) which returns the green/red "potential PNL" numbers that you see on the Betfair GUI?

I looked in the Blotter, there is `get_exposures` which has `matched_profit_if_win` but it seems that the code only considers the orders for the particular runner in question. So, then it seems that orders on other runners that could result in a potential profit/loss are not taken into account. Which means it would not match the green/red values displayed on the screen?

*Tags: General Technical*

---

**Gary Cook** - *20:20:34*

Please, Is there any hard and fast rules on how much/often we can access the free API? I got suspended once years ago apparently but I didn't realise at the time. I think I was logging one game at a time and all its odds through a 3rd party app that implements the API. I don't intend to do that. Im just getting market snapshots right now but I have no idea how often it is OK to do that. FYI: I am using betfairlightweight.

*Tags: General Technical*

---

**D C** - *20:32:15*

I hear that you can run into problems if you are only pulling data and not placing bets. If you were suspended, did they not provide the reason for the suspension? I guess if you were using a 3rd party app, you are at the mercy of whoever wrote that?

*Tags: General Technical*

---

## 2023-07-01

**James** - *03:25:52*

I have run into issues in the past when getting an error placing a bet (eg my account balance is too low to place the. bet) and then the bot kept retrying in a loop. But that surfaced as API charges on my account rather than a suspension.

*Tags: Errors Debugging*

---

**Michael** - *17:39:49*

This is an extremely basic question but I’m having a lot of trouble implementing it. For those using market recorder, I’m trying to write the metadata to a SQL database. I’m calling a function in the process_closed_market function which takes the market definiton, connects to the sql database and then writes it, but nothing is being written (or showing up on logs). When I implement this, I get the following error:

```2023-06-17 22:27:22,654 - ERROR - _get_cleared_market error

Traceback (most recent call last):

  File "/home/ubuntu/.local/lib/python3.8/site-packages/urllib3/connection.py", line 174, in _new_conn

    conn = connection.create_connection(

  File "/home/ubuntu/.local/lib/python3.8/site-packages/urllib3/util/connection.py", line 72, in create_connection

    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):

  File "/usr/lib/python3.8/socket.py", line 918, in getaddrinfo

    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):

socket.gaierror: [Errno -3] Temporary failure in name resolution



During handling of the above exception, another exception occurred:```

It feels like the market is never closing and then I am stuck with a full /tmp file and hence the above error occurs. How does everyone else implement this?

*Tags: Data Quality, Errors Debugging*

---

**liam** - *17:44:12*

Any context to this error / got the full stack trace?

*Tags: Errors Debugging*

---

**Michael** - *19:00:37*

It was a couple of weeks ago, I reverted the changes so let me re-run and cause the error again. Its definitenly to do with writing to the database however as removing it fixed it

*Tags: Errors Debugging*

---

## 2023-07-03

**Michael** - *15:14:00*

I have done just that, I think its how I’ve written it / my OOO knowledge becoming a limitation. I’ve effectively rewrote the entire process_closed_market function in a class which inherits from the S3MarketRecorder strategy, I assumed this would then add the functionality of the new function without entirely overwriting the old function in the main strategy

*Tags: Strategies*

---

**liam** - *19:31:56*

Looks like a connection problem to me 

*Tags: General Technical*

---

## 2023-07-05

**Michael** - *22:02:59*

I was writing metadata to a mysql database, it was getting stuck connecting. Once I reverted changes and fixed it then I get this no more

*Tags: Errors Debugging*

---

## 2023-07-06

**Mo** - *07:15:29*

If it was a system wide connection issue then you would have had problems with your connection to Betfair as well



I had exactly this problem in the past - ie a temporary failure in name resolution when connecting to a database - where the fact it was a connection issue stretched credibility. It was in fact a problem with the name resolver itself



It sounds like you’ve “fixed” it by no longer connecting to the database which doesn’t sound like a fix at all?



What version of Ubuntu are you running?

*Tags: Errors Debugging*

---

**Michael** - *23:53:21*

I think this was genuinely a error in the code, it never wrote in some tests either. I still connect to the database, but call during process_closed_markets now which works with no issue. I may well run into it in future if it is a issue you’ve had so if you could share how you solved it that’d be great, you said you’d take a look at how you solved it but never came back. My linux version is Ubuntu 20.04

*Tags: Errors Debugging*

---

## 2023-07-07

**Andy B** - *05:58:16*

Hi All, Just wondering how to get the 'name' value from market_definition so that I can exclude Pace and Trot races from my back tests?  I can get the event_name by calling market.event_name, but I didn't see a property for the 'name' value.  I was hoping to do something like:

def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:

            if 'Trot' not in market.name and 'Pace' not in market.name:

                       Do the back test........

*Tags: General Technical*

---

**Mo** - *06:08:02*

I believe the error is related to how frequently you are connecting to the database so if you have reduced that frequency then that is a workaround



Another workaround I was going to suggest would be to use the IP address of the database server, assuming it is fixed, to avoid having to resolve the name



In my case, although I recall I had to mess around with the default name resolver setup in Ubuntu, that doesn't seem to be reflected in my current configuration so I assume it was fixed at some point. I am running 22.04. Upgrading should be a permanent solution

*Tags: Getting Started, Errors Debugging, Deployment*

---

**liam** - *07:56:27*

Is this self recorded or betfair historical data?

*Tags: Data Quality*

---

**Andy B** - *08:43:35*

It's the betfair pro files for Australian thoroughbreds.  I could see the values were in the market files, but I tried every way except the right way to get the details out.  I thought I had been through the flumine py files thoroughly too, but I hadn't.

*Tags: General Technical*

---

**Unknown** - *20:33:40*

Apologies for this question as I know its been asked before...in the case of horse racing, does a horse retain its selection id over the course of a season (lifetime) ? Thanks in advance

*Tags: General Technical*

---

## 2023-07-08

**Alex** - *16:09:09*

I'm looking at a historic data file where total matched declines from one step to another. Is this just a data error or do I fundamentally misunderstand the concept of total matched. I assumed it was just CUMSUM(volume) in the market (double counted), i.e. dividing it by 2 and diffing it gives you an idea of volume traded

*Tags: Errors Debugging*

---

## 2023-07-09

**river_shah** - *15:33:15*

Can you use streaming for connectivity and keep it to less than 1K transactions an hour and try to generate some commissions? That should be fair usage likely

*Tags: General Technical*

---

**Gary Cook** - *15:36:26*

I gather some data to inform my trades. I'm actually very good at it but I didn't mind losing money while testing some things.  I don't place trades with the API. I don't think that should matter though? The money is still going through Betfair just their normal web UI. The reason I mention that is because just gathering data I don't really need streaming. Im just doing snapshots, not in-play live trading with the API.

*Tags: Deployment, Strategies*

---

**river_shah** - *15:38:03*

I think that is likely the problem. Your usage may look like just scraping the platform to inform some other commercial offering. I suggest using streaming, not snaps and betting through the api, generate commissions and you should not run into issues.

*Tags: Strategies*

---

**Gary Cook** - *15:39:38*

Ok well I appreciate the help and chat but it's still all a little vague and subjective.

*Tags: General Technical*

---

**Gary Cook** - *15:47:12*

So if I snapshot all markets from say 50 games a day I should be fine? Of course, I will be giving them some of my money through betting/trading. I would imagine that is less demanding than watching a whole game with streaming.

*Tags: Strategies*

---

**Gary Cook** - *15:52:42*

Have you guys seen a way to index the markets? I made a JSON with the order I want the markets in for my database. They seem to be fetched in a random order otherwise. I guess since the market_id changes. Doesnt really matter now so just wondering if I missed something on that.

*Tags: General Technical*

---

## 2023-07-10

**Matthew** - *15:42:55*

Ok, thank you. I have some other questions on it sorry if I bother you. 



1) what you are referring to Traded volumes in point 2 are the tradedvolume inside the market book -&gt; runner[ex][“tradedVolume”] is that cumulative right? It seems so for me 



2) apart from point 1, I explain again my logic so maybe you have a better idea of how to develop a more solid solution. I’m trying to get for each market update, the traded prices and sizes of that update, so not cumulative, so for example, if I set a discrete period of having updates every 2 seconds, I want to know for each runner the traded prices and sizes in those 2 seconds. To do that. I save every time the last update on the prices and volumes traded by the runners, which i found in the runner market book, then I save the current prices and volumes which should be the same as before with new prices if new prices are traded, or larger sizes on the already traded ones. And I do a diff. Then, all the prices that have a size &gt; 0 are the traded prices in those 2 seconds. Sometimes because of scratching this logic has some fallacy, but overall it seems to work. Do you have a better option? Am I complicating my life? 



Thank you :pray:  

*Tags: General Technical*

---

**Matthew** - *17:53:38*

I add some things that are not totally clear to me. From the listener of the betfair lightweight I can enable the cumulative market volume and runner volume but this is discouraged, why? How these two parameters behaves when I read from bz2 files vs live feed?



There is also the streaming updates list in the market book where I can see some volumes which does not seem to be cumulative, maybe I can pick those one, even if it is not a friendly list 

*Tags: Deployment*

---

## 2023-07-11

**Andy B** - *06:11:29*

Just coming back to this post incase it helps anyone else, you need to use double quotes aroung processed, not single quotes, as:

```if 'processed' not in market.context:

    market.context["processed"] = True

    code…```

It took me a while to realise this and I couldn't understand why the code wasn't working for me until I added double quotes.

*Tags: General Technical*

---

**Matthew** - *08:55:46*

1) By cumulative, I mean the total amount of money bet up to that market update for each runner. 



2) let me try to be more concise, for every market book update. I have a dictionary for each runner where the key are the prices and the value are the sizes. I have the previous one and the current one, then basically if I do the diff key by key, the deltas that remain &gt; 0 are the prices and sizes traded in my window. This has a couples of problems, the main ones that I found are

• currency conversion 

• Scratching which change the key and values of my dict and the comparison with the previous one

3) in the doc string is highly recommended if using betfair pro data to keep it true 

*Tags: General Technical*

---

**Matthew** - *09:21:18*

For example, if a scratching happens, the matching between dicts is not possible to do, because the prices are adjusted (if you know exactly how I will be happy!) I know that I have to use the adjustment factor but I don’t find the proper formula 



If there are currency conversions, prices changes and same problem as before, with also sizes that decrase instead of increasing, so when I do the diff sometimes I get negative values 





*Tags: General Technical*

---

**George** - *17:45:53*

apologies [@U4H19D1D2](@U4H19D1D2) but after several hours of debugging I still cannot understand why `market.market_catalogue` is None. the `poll_market_catalogue` worker is active and I can see that it is doing its job.

but, flumine's `_process_market_catalogues` never seems to get called. do you know why could this be happening?

*Tags: Errors Debugging*

---

**George** - *18:00:02*

well that's the problem - it isn't

*Tags: General Technical*

---

**liam** - *18:01:29*

So no logs? Error logs? Or you haven’t set up logging as per examples? 

*Tags: Errors Debugging*

---

**George** - *18:02:19*

the error in the logs  is an error coming from me trying to access the market_catalogue when it's None

*Tags: Errors Debugging*

---

**George** - *18:03:12*

I get loads of these but it's not an error:

{"asctime": "2023-07-11 16:32:58,689", "levelname": "DEBUG", "message": "BackgroundWork er poll_market_catalogue executing", "worker_name": "poll_market_catalogue", "function" : "&lt;function poll_market_catalogue at 0x103c73820&gt;", "context": {}}

*Tags: Errors Debugging*

---

**George** - *18:04:15*

but flumine's `_process_market_catalogues` never fires, so the market never gets its market_catalogue applied

*Tags: General Technical*

---

**George** - *18:05:10*

[https://github.com/betcode-org/flumine/blob/master/flumine/baseflumine.py#L235](https://github.com/betcode-org/flumine/blob/master/flumine/baseflumine.py#L235) this never seems to get called

*Tags: General Technical*

---

**George** - *18:08:45*

```22 def run():

23   start_time_seconds = 43000

 24   stop_time_seconds = 1800

 25   setup_logger(logging.DEBUG)

 26   trading = login_with_bflw()

 27   client = clients.BetfairClient(trading)

 28   framework = Flumine(client=client)

 29   strategy = get_strategy(start_time_seconds, stop_time_seconds)

 30   pred_worker = get_prediction_worker(framework, start_time_seconds)

 31   logging_control = MarketOrderControl()

 32   framework.add_strategy(strategy)

 33   framework.add_worker(pred_worker)

 34   framework.add_logging_control(logging_control)

 35   with patch.object(config, 'raise_errors', True):

 36     framework.run()

 37   return framework```

*Tags: Getting Started, Errors Debugging, Strategies*

---

**George** - *18:10:39*

the error occurs because `strategy.process_market_book` requires access to the runner names which should be in `market.market_catalogue`

but `market.market_catalogue` is `None` even though the `poll_market_catalogue` worker is active

*Tags: Errors Debugging, Strategies*

---

## 2023-07-13

**Harry Corrigan** - *11:53:25*

Is there any way to update a flumine strategies subscriptions after it has been executed?

*Tags: General Technical*

---

**liam** - *12:02:47*

No, fix your subscription 

*Tags: Errors Debugging*

---

**Harry Corrigan** - *12:03:23*

So if I want a strategy that changes market halfway through execution, that's not possible?

*Tags: Strategies*

---

**Harry Corrigan** - *12:20:27*

I'm trying to create a scheduler that takes a list of events and subscribes to each one before it starts, these events may start on different days as the system is intended to run forever 

How would I do this if I can't update a strategies subscriptions? Would I have to remove one strategy and create a new one with a different filter when trying to subscribe to a new market?

*Tags: Strategies*

---

**George** - *12:27:24*

you can code your strategy in such a way that it changes its behaviour halfway through.

*Tags: Strategies*

---

**liam** - *12:28:25*

Open up your subscription and just filter, streaming isn’t designed for what you are describing 

*Tags: General Technical*

---

**George** - *12:29:39*

```def process_market_book(self, market, market_book):

    if first_half_of_execution:

        self.do_strategy_1()

    else:

        self.do_strategy_2()```

*Tags: Strategies*

---

**Peter** - *17:21:54*

I use.`_process_order` to record orders in a database and `_process_cleared_orders_meta`  to update that entry with the information such as the final amount matched and the profit from the trade.



It would be slightly more complex if you're writing to a file, so maybe just capture the data in  `_process_cleared_orders_meta`  when all the data that will ever be available is there for you to capture.

*Tags: General Technical*

---

## 2023-07-14

**Johnny** - *11:00:36*

Cool.  Guessing s3 performance is unaffected by number of objects in a bucket then... I currently have the same structure as Peter but all on HDD at home.  Are I/O charges a concern at all?

*Tags: Performance*

---

## 2023-07-15

**JazzMan** - *10:27:35*

Use a database to save your data but it should be designed to only save the data that has changed, otherwise it can become very large.

*Tags: General Technical*

---

**Peter** - *10:38:49*

[@U012CS8MP52](@U012CS8MP52), [@U4H19D1D2](@U4H19D1D2) isn't suggesting keeping market data in a database, since as you say, that would get very large. He's just advocating keeping enough information about races, markets and trades as an index that can be filtered to get a list of market_ids to pass to Flumine when running strategy simulations. The market data is kept in its raw form in a compressed file per market.

*Tags: Strategies*

---

## 2023-07-17

**V** - *11:33:29*

Does anyone know how betfair attributes comms generated across market streams? For example, if you consume two sports but only really trade on one, is that problematic?

*Tags: General Technical*

---

**Guy Adini** - *13:07:01*

Hi guys, basic question, I think :slightly_smiling_face:

When downloading historical betfair data (football), I can figure out the names of the teams from the 1x2 market - but I don't know how to get further information about the event.

For example - is there any way to get the competition name and id?



Thanks!

*Tags: General Technical*

---

## 2023-07-18

**Harry Corrigan** - *11:54:40*

Hi, does anyone know how to properly display betfair markets, i seem to have a problem with getting event names, specifically in tennis, where there are events like Set 01, Set 02 etc i want to tie these sets to the event they belong too (Player 1 v Player 2) how would i do that?

*Tags: General Technical*

---

**Harry Corrigan** - *11:57:59*

The problem is the Sets, listed as events such as this one with the id 32494059 (Set 01)

*Tags: General Technical*

---

## 2023-07-19

**Mo** - *13:40:08*

For non-Australian races, the race card might help with 2)

*Tags: General Technical*

---

**Jorge** - *14:00:06*

Hi, from my flumine Strategy, how can I exit the Strategy so it does not trade anymore and closes the program. I am looking at [https://github.com/betcode-org/flumine/blob/d2e7d8e6cbc65687d2a0fb41e8f24bf133081f59/flumine/baseflumine.py#L429|baseclient.exit()](https://github.com/betcode-org/flumine/blob/d2e7d8e6cbc65687d2a0fb41e8f24bf133081f59/flumine/baseflumine.py#L429|baseclient.exit()) but no idea how to call from within process_market_book().

*Tags: Strategies*

---

**river_shah** - *14:54:10*

You can maintain a handle to the framework and then:



```from flumine.events.events import TerminationEvent



...

framework.handler_queue.put(TerminationEvent(framework))```

*Tags: General Technical*

---

**river_shah** - *14:55:41*

It seems that `BaseStrategy` does not expose framework so pass the framework into your derived strategy and then strategy can terminate program as needed

*Tags: Strategies*

---

**Gary Cook** - *21:44:07*

Does anybody know how to get the short names for teams with longer names, please? Is it something we can access from the API? For example, the team called _*"Hapoel Beer Sheva"*_ is called that in the event name and market names but in the runners it's called _*"H Beer Sheva"*_. Thanks in advance for any help on this matter.

*Tags: General Technical*

---

**Gary Cook** - *22:59:59*

Ive got no idea how to handle this. Not in a reasonable way. I would perhaps need to know every possible team in advance to handle it gracefully

*Tags: General Technical*

---

**Vlad Gavrilov** - *23:11:25*

[https://www.datacamp.com/tutorial/fuzzy-string-python|https://www.datacamp.com/tutorial/fuzzy-string-python](https://www.datacamp.com/tutorial/fuzzy-string-python|https://www.datacamp.com/tutorial/fuzzy-string-python)

*Tags: General Technical*

---

## 2023-07-20

**Gary Cook** - *00:29:08*

Cheers for the help. So far I think it's just in Half-time/Full-time so I might just switch the names for home and away based on their sort priority. Weird though.

JFYI: It's the same in the online APING. I was just confusing myself.

*Tags: General Technical*

---

**Peter** - *08:40:11*

A helpful step for accuracy is to remove from the matching those that don't need it so that there's a limited pool of possible matches to be handled.

*Tags: General Technical*

---

## 2023-07-21

**Alessio** - *09:15:16*

In my opinion, the situation with non-1st-tier football is quite messy. You get databases of a lot of duplication (90% of the time they use "H A B" sometimes they use "Hi A B") and leagues that don't even align when you go into regional

*Tags: General Technical*

---

**anh** - *10:03:54*

hello everyone, I'm new to Python and coding in general and am in the process of moving from BA/Excel to BFLW (whilst learning on the job).  I'm very much a newbie so if betcode isn't the place for this question please let me know... I have implemented a few strategies so far but my next one requires me to check open orders (matched and unmatched) on different horse racing win markets. To do this I am trying to use list_current_orders - which I think is the right one - as follows: `open_bets = trading.betting.list_current_orders(market_ids=[market_id])`. This seems to return a list of objects but is where I'm stuck. How do I get the bet details from these objects?

*Tags: Getting Started, Strategies*

---

**Mo** - *10:06:30*

Look at the `CurrentOrder` definition: [https://github.com/betcode-org/betfair/blob/e4d58fd369312408c04519be1520e35b0dfc61dc/betfairlightweight/resources/bettingresources.py#L627](https://github.com/betcode-org/betfair/blob/e4d58fd369312408c04519be1520e35b0dfc61dc/betfairlightweight/resources/bettingresources.py#L627)

*Tags: Strategies*

---

**Mo** - *10:07:10*

Also suggest you run the code in a debugger using and IDE such as PyCharm. That will make it easy to inspect objects

*Tags: Errors Debugging*

---

**Joe Bloggs** - *20:40:35*

hi, does anyone have any 50ms historical data they are willing to share

*Tags: Data Quality*

---

## 2023-07-22

**Alejandro Pablos Sánchez** - *23:13:22*

I guess I could do this by looping through all the markets and saving somewhere all runner names in each of the markets but idk if this is the best solution, i don't think so. If anyone could help, it would be great !! Thanks to everyone :wink:

*Tags: General Technical*

---

## 2023-07-23

**Unknown** - *00:33:31*

Hey guys, I hope you are having a great weekend.

I was testing the example "examplestreaming.py" in the betfairlightweight library ([https://github.com/betcode-org/betfair/blob/master/examples/examplestreaming.py](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaming.py)). After running the script, I saw in the console that the function _on_status() on the Listener is responsible for printing in the console the message in the following image:

My doubt is:

Why have TWO MarketStreams been created and why is this happening, or am I wrong in interpreting that two MarketStreams are being created?

Only one MarketStream should be created.

*Tags: General Technical*

---

**Felix Vasquez** - *02:49:25*

[@UPMUFSGCR](@UPMUFSGCR) How can I record the data for $5 per month? Is it a third-party service?  or using Flumine?

*Tags: General Technical*

---

**Unknown** - *06:18:51*

Guys, please another question:

I am modifying the logging message in the function `_process()` of the `MarketStream` class in the betfairlightweigth library (below).

The original message in the function is `"[%s: %s]: %s added, %s markets in cache"` and I modified it to `"[%s: %s]: %s added, %s markets in cache (MY SELF)"`.

The problem is that after making the changes, the Console doesn't show the new message, it keeps printing the original message.



```def _process(self, data: list, publish_time: int) -&gt; bool: 

        caches, img = [], False

        for market_book in data:

            market_id = market_book["id"]

            full_image = market_book.get("img", False)

            market_book_cache = self._caches.get(market_id)



            if (

                full_image or market_book_cache is None 

            ):  # historic data does not contain img

                img = True

                if "marketDefinition" not in market_book:

                    logger.warning(

                        "[%s: %s]: Missing marketDefinition on market %s resulting "

                        "in potential missing data in the MarketBook (make sure "

                        "EX_MARKET_DEF is requested)"

                        % (self, self.unique_id, market_id)

                    )

                market_book_cache = MarketBookCache(

                    market_id,

                    publish_time,

                    self._lightweight,

                    self._calculate_market_tv,

                    self._cumulative_runner_tv,

                )

                self._caches[market_id] = market_book_cache

                logger.info(

                    "[%s: %s]: %s added, %s markets in cache (MY SELF)"

                    % (self, self.unique_id, market_id, len(self._caches))

                )

            market_book_cache.update_cache(market_book, publish_time, True)

            caches.append(market_book_cache)

            self._updates_processed += 1

        self.on_process(caches)

        return img```

Below is the message in the console without reflecting the changes made in the code:



```INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: 1.216273367 added, 1 markets in cache

INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: 1.215746166 added, 2 markets in cache

INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: 1.216265892 added, 3 markets in cache

INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: 3 mc added```

What could be happening?

Thanks for your help!

*Tags: General Technical*

---

**Jonjonjon** - *09:31:51*

Using the market recorder example in the Flumine codebase. Then run it on AWS or a similar service

*Tags: Data Quality, Deployment*

---

**Guy Adini** - *11:39:07*

Do you know which datacenter offers the lowest latency when connecting to Betfair?

I'm already on AWS so London or Dublin make the most sense for me, but it would also be good to know in general - if I want the very lowest latency in the future, if there's some other recommended provider.

*Tags: Performance, Deployment*

---

## 2023-07-24

**Andy B** - *04:30:06*

I am trying to work out how to find numberOfWinners from the market in flumine, but I think it is possibly called something else.  Any pointers on this one?

*Tags: General Technical*

---

**Mo** - *07:43:02*

1. No, delayed key is useless

2. You cannot just collect price data under any circumstances, there needs to be some betting activity on your account

3. You shouldn't be polling, use streaming

*Tags: Strategies*

---

**river_shah** - *09:40:04*

[https://huggingface.co/docs/transformers/main/model_doc/llama2](https://huggingface.co/docs/transformers/main/model_doc/llama2)

*Tags: Strategies*

---

**Simon Chan** - *09:45:15*

Thank you all for the reply. I guess I will have to look for somewhere else to scrape data in order to build models. Any recommended sites? The technicality is not a big issue because I do quite a lot of web scraping on my day job (software dev)

*Tags: Strategies*

---

**liam** - *11:18:48*

Just released flumine 2.4.0 which has a change to the simulation engine allowing the matching of passive orders (requires a config change) against available prices. Its very [https://github.com/betcode-org/flumine/blob/91694ebfa0321e75c71f404ac76f3af8853df0d3/flumine/simulation/simulatedorder.py#L473|rudimentary](https://github.com/betcode-org/flumine/blob/91694ebfa0321e75c71f404ac76f3af8853df0d3/flumine/simulation/simulatedorder.py#L473|rudimentary) by simply matching the order against what is available (in addition to traded volume) and doesn't factor in double counting however would be good to get some initial feedback on if it is actually more accurate and should become the default or needs further work etc.



```config.simulation_available_prices = True```

FYI [@U02QDJCK3NW](@U02QDJCK3NW) tested on your example and the order now matches at 1.11

*Tags: General Technical*

---

**Peter** - *11:43:05*

I recommend using an IDE with a debugger. [@UBS7QANF3](@UBS7QANF3) recommends PyCharm. I use Visual Studio Code. Both allow you to set a breakpoint (e.g. at the start of `process_market` or `process_orders` ) and then examine all the data that's available to you at that point. It can be a real eye opener.

*Tags: Errors Debugging*

---

**Mo** - *15:59:10*

Assuming you don't need tick data to build models, this resource is great: [https://promo.betfair.com/betfairsp/prices](https://promo.betfair.com/betfairsp/prices)

*Tags: Strategies*

---

## 2023-07-25

**Felix Vasquez** - *02:06:01*

Hi [@U4H19D1D2](@U4H19D1D2)!, Thanks for your response!

The reason for this change is that I'm in a discovery process, studying all files and what is doing every function, imported library, decorators, and so on. I'm new in programming, python, betting, trading, and all this world. Basically, I am learning Python through betfairligthweigth library and further in my path with Flumine, to finally built some strategies there.

I was running it from the debug window in VSCode, I modified the `examplestreaming.py` file by adding calls to different endpoints, playing around with the response, and analyzing the data provided by de API. And all this worked fine, but I was surprised when saw the mentioned behavior.

What you say has total sense to me, and maybe the problem is that `examplestreaming.py` is importing the library and isn't using the code I download to my PC. I will change it to a relative import and see if this solves the problem.

Finally, you ask me why am I running from terminal?, is there any other way to run it?

Any advice to facilitate the learning process will be appreciated.

Cheers

*Tags: Errors Debugging, Strategies*

---

**liam** - *16:56:44*

Like Lee I do a bit of both, all market files in a s3 bucket and then a mysql db with flat market / market cat / order / trade / strategy data all behind a django api. I find this is then the best way to get market data required for simulation:



market filter -&gt; api -&gt; return marketIds / data -&gt; download locally if required -&gt; simulate

*Tags: Strategies*

---

## 2023-07-26

**Matthew** - *08:35:48*

Hello, do you guys know how to get the state in which the race has been ran? For example for a US horse racing, I’m not able to find the state, sometimes I can find something in the marketDefinjtion inside the time zone, sometimes not. Is there a better way?

*Tags: General Technical*

---

**Mo** - *08:39:27*

Curate your own database of race tracks? There's not that many of them

*Tags: General Technical*

---

**Michael** - *16:24:49*

Forgive my ignorance, but why are you using a django api to query the database essentially? Is there a benefit

*Tags: General Technical*

---

## 2023-07-27

**liam** - *08:19:27*

I have multiple instances of flumine running all communicating with it, sending/receiving market / order / trade / strategy / notes etc to it



I have a simple SPA running on top that allows me to view everything at a glance and update settings etc



I have a few lambda scripts that communicate on a daily basis to update / reconcile / check / record various things



Simulation scripts use it to pull in the relevant marketId's / data required (catalogues / sports data etc)



However when using juypter / pandas I communicate directly to the db with SQL as its quicker, whats your frustration? Speed?

*Tags: Feature Engineering, Performance, Strategies*

---

**liam** - *08:20:36*

EC2.nano, setup and forget, much easier than managing it yourself

*Tags: Getting Started, Deployment*

---

**AI Trader** - *12:53:38*

Hi guys,

I am trying to reconstruct trades from the orderbook data collected using Flumine.

After sorting the orderbook snapshots by timestamp, I realized that sometimes, the runner_volume decreases (from quantities ranging from cents to multiple pounds). This would then generate trades of negative amount, which is absurd. Does anyone know what could be going on here? Thanks

*Tags: General Technical*

---

**AI Trader** - *16:00:59*

Very helpful, thanks a lot [@UBS7QANF3](@UBS7QANF3)

*Tags: General Technical*

---

## 2023-07-30

**Unknown** - *14:08:35*

Just noticed a few errors on my stuff...

*Tags: Errors Debugging*

---

**Mo** - *17:58:14*

[https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/racecard.py](https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/racecard.py)

*Tags: General Technical*

---

## 2023-07-31

**mzaja** - *14:40:09*

Hi everyone! I wrote this package for indexing captured Betfair stream data which you may find useful: [https://github.com/mzaja/betfair-database](https://github.com/mzaja/betfair-database)



If you have market catalogues stored alongside your stream captures, it parses them and stores them inside an SQL table together with the catalogue/stream data file paths. Then, you can write SQL queries to retrieve the paths to whatever subset of data you like.



Have a go at it, like, share, maybe even contribute... :blush:

*Tags: General Technical*

---

**R** - *18:50:48*

[@U4H19D1D2](@U4H19D1D2) I was about to code similar, regex the market name and you're good to go.



But [@UBS7QANF3](@UBS7QANF3)’s suggestion makes it nice and clean.  Although I can't see this raceCard endpoint on the betfair API so that's confusing.



I'll try the API endpoint out first and if that works we're in business!  Cheers guys

*Tags: General Technical*

---

**Rishab** - *22:16:09*

Basically my query is- if I'm starting from strach what are the steps to backtest the tick-by-tick data in python. (Links for sample codes, etc)

*Tags: General Technical*

---

**Paul** - *22:18:43*

Bz2 is short for “bzip2”. It’s a compressed format like zip. Python has a bz2 module or you can decompress it using a bzip compatible decompressor.

*Tags: General Technical*

---

**Rishab** - *23:03:54*

Do you have a sample code to convert it in the time-series format? Also does using default libraries like betconect/flumine make the whole process easier?

*Tags: General Technical*

---

## 2023-08-01

**Mo** - *06:35:18*

Check out [https://github.com/mberk/betfairutil](https://github.com/mberk/betfairutil). In particular, I think the `prices_file_to_data_frame` function will help you produce that time series

*Tags: General Technical*

---

**Mo** - *06:37:00*

&gt; Basically my query is- if I'm starting from strach what are the steps to backtest the tick-by-tick data in python. (Links for sample codes, etc)

Use flumine for backtesting. Look at the examples, especially this one: [https://github.com/betcode-org/flumine/blob/master/examples/simulate.py|https://github.com/betcode-org/flumine/blob/master/examples/simulate.py](https://github.com/betcode-org/flumine/blob/master/examples/simulate.py|https://github.com/betcode-org/flumine/blob/master/examples/simulate.py). And read the documentation: [https://betcode-org.github.io/flumine/|https://betcode-org.github.io/flumine/](https://betcode-org.github.io/flumine/|https://betcode-org.github.io/flumine/)

*Tags: General Technical*

---

## 2023-08-02

**rob smith** - *14:32:23*

Hello, is it possible to get the VWAP using bflw? Not for historical data but when trading live. Cheers

*Tags: Data Quality, Deployment, Strategies*

---

**liam** - *15:20:50*

Its all private for processing the streaming data, can you share some code (no usernames/pwds)

*Tags: General Technical*

---

**Trex44** - *16:48:09*

Hey guys, if I save something to the market.context using a strategy will another strategy acting on the same market access the same market object and so the same context with the saved data if it calls market.context? I didn't think it did but now I am not so sure.

*Tags: Strategies*

---

**George** - *16:53:18*

I am using Flumine, and I am sometimes sending orders which are rejected by Betfair due to INSUFFICIENT_FUNDS (don't worry this isn't a sob story with a GoFundMe).



I am curious as to how Flumine stores those order objects. According to my logs, I believe these orders are stored with `bet_id` = None and `size_remaining` = full size and `size_cancelled` = 0. Is that correct?



Shouldn't they have `size_remaining` = 0 and `size_cancelled` = full size?

*Tags: General Technical*

---

**Newbie99** - *17:10:27*

Yes, market.context can be shared between strategies, so for a simple example, lets say you store your fair value in market context and you have 1 back strategy and 1 lay strategy, both could see the fair value level in market.context and place orders (or not) accordingly.

*Tags: Strategies*

---

**Peter** - *17:11:24*

... provided both strategies are running within the same Flumine instance.

*Tags: General Technical*

---

**Trex44** - *18:26:05*

Thanks guys. All the parameters are  very light computationally. I have built my own workers before so know how to do that but haven't built a middleware script. I see some examples on Github for middleware but no information on the 'Advanced' section of the Flumine page on how to implement it. How would I implement it once built?

*Tags: General Technical*

---

**Newbie99** - *20:12:57*

Then create a class something like:



```class CustomMiddleware(Middleware):

    def __init__(self, flumine):

        self.flumine = flumine

    def __call__(self, market):

        check_and_calc(market, strategy_list=self.flumine.strategies)



def check_and_caclc(market, strategy_list):

....do stuff```



*Tags: Strategies*

---

**Rishab** - *23:56:10*

Im using the MarketRecorder file available online to record data. The cell with framework.run() keeps on running until I kill the process. I believe its because it keeps on saving a timeseries data(I desire the same). If i kill the cell instatly after running, I'm able to load the JSON that was saved from the process (But it just has the data for one timestamp, cuz I didn't see data for any runner twice). However, if I keep on running the cell for a long time &amp; then kill it manually I see an error when loading the json - JSONDecodeError: Extra data: line 2 column 1 (char 16137)

*Tags: Errors Debugging*

---

## 2023-08-03

**Unknown** - *03:06:54*

Hi [@U4H19D1D2](@U4H19D1D2), Thanks for the advice! I made the git clone. But the problem persisted. After a while, I realized that the betfairlightweight library had been installed and imported from there.

This behavior didn't let me debug the things happening inside the library, just limited to the file where I was running the example.

To solve this, I put the libraries as in the image, added the _`__init__`_ necessary files, changed the route for importing, and voila! It worked! and now I can debug throughout both libraries! :raised_hands::tada:

*Tags: Getting Started, Errors Debugging*

---

**liam** - *07:12:10*

By cell I am going to assume you are trying to run this in a notebook, the idea with flumine / recorder is to run the program on its own process and just leave it (not designed for stop / start) 



Have you seen the raw streaming data before? / read the docs? The files are not valid json, the lines are, at a guess you want to be using bflw or flumine to read the files.



It’s recording the streaming data so the level will be based on how active the market is assuming you are using your live key. 

*Tags: Deployment*

---

**Rishab** - *15:14:55*

I found this online from a really old post, is it the optimal way to load data? trading.streaming.create_historical_generator_stream(

    file_path="tmp/bafd8260/1.216640376",

    listener=listener,

) [@U4H19D1D2](@U4H19D1D2)

*Tags: Strategies*

---

**George** - *15:18:40*

Has anyone ever coded a trading control to validate that the account has sufficient funds to place the bet?

It seems like the latency involved makes it impossible. `get_account_funds` takes a really long time to return.

Has anyone solved this problem?

*Tags: Performance, Strategies*

---

**George** - *15:38:05*

i had it when testing something and the other controls didn't pick it up

the max-transaction-count froze me out for an hour instead

that's not ideal as i'd rather the strategy knows about the lack of balance and just doesn't send the order

*Tags: Strategies*

---

**liam** - *15:38:57*

So why is speed an issue? Just use a worker to disable the strategy when balance &lt; x however just fix the issue 

*Tags: Errors Debugging, Performance, Strategies*

---

**liam** - *15:41:20*

strategy.context seems more applicable 

*Tags: Strategies*

---

**George** - *15:50:52*

indeed! i made the numbers up for simplicity and to demonstrate the need for some way of disabling a strategy based on balance. it wasn't quite like what i described when i experienced it. i had a low exposure per market but had a very unlucky run of a large number of markets, followed by a lucky run which didn't get monetised properly ( although with tiny stakes)

*Tags: Strategies*

---

**D C** - *15:55:49*

FWIW if I run anything new I manually monitor it by running it locally on my machine for as low stakes as I can get away with verbose console logging enabled and fingers on CRTL and C at the ready. I don't use flumine but have similar exposure limits which I know will prevent me getting a total burnout but things still can go to s*it. I'd never run something totally "new" without being there at the first run.



Maybe I am paranoid but tearning through a bankroll just once in life due to one's own coding errors changes a man.

*Tags: Errors Debugging*

---

**George** - *15:56:31*

totally agree. my strategy had passed the "sit and watch" stage by that point!

*Tags: Strategies*

---

**George** - *16:51:53*

really i was just doing a little bit of testing guys - this was not a "deployed" strategy. i was just messing around with a few £ here and there. i totally agree about bankroll to stake ratio.

but point taken

*Tags: Deployment, Strategies*

---

**Unknown** - *19:47:04*

I did. We are talking about a difference of more than 40 minutes though. From the documentation, it should lag ~100ms

*Tags: General Technical*

---

## 2023-08-04

**Peter** - *09:41:39*

The reason I suggested that middleware might be preferable to a worker, in this specific case, was because the `__call__` method is executed on every market update, and it seemed likely that you would want to update your parameter each time the market updated.



By contrast, if you built this into a worker, it would be run on a schedule independently of market updates, so would be out-of-date some of the time, and may be being re-calculated unnecessarily at others.



Moreover, market middleware automatically receives the current state of the market and can save the calculated parameter straight back to the market.context where it would be available to any strategy interested in that market, whereas a worker would have be be told which markets interest you and would would then need to be coded to go digging into the flumine object to find them.

*Tags: Strategies*

---

**AI Trader** - *10:38:47*

[@U01DVUAE2G1](@U01DVUAE2G1), did you ever compare historical data of raw vs virtual prices ? Did you also find instances such as the one I am sharing? I have many others, so I am not sure I am parsing the data incorrectly (even though I have double checked the raw data)

*Tags: Data Quality*

---

**Unknown** - *11:45:18*

I think I've found the reason for such an apparent annomaly.



After exploring the virtual prices, I found out that the minimum size shown in the best level is 1 pound.



Upon looking at one of the apparent annomalies, it turns out that the best price available in the raw data has a size of 55 cents. And upon looking at the virtualised prices, we see a size of 13.85, which is exactly the sum of the size on the first and second levels of the raw data. So it seems betfair is showing the price of the second best level in the raw data for the virtualised data, and summing up their sizes.



I couldn't find this in the documentation though. If anyone has seen it somewhere and could share, that'd be great

*Tags: General Technical*

---

**joe taylor** - *12:19:57*

Hi guys! I want to work on some strategy around inplay horse racing. Had a few queries around this: how can I get real-time race data like runner distance left in race or position of each runner in race/speed 2. historical data for each runner with different features-timeform type data &amp; inplay data(like speed in last x fraction of race left etc)

*Tags: Data Quality, Feature Engineering, Performance, Strategies*

---

**Mo** - *12:49:52*

This is called Rollup and defaults to the minimum stake size ie £1. I don’t believe you can override this with streaming, only listMarketBook

*Tags: General Technical*

---

**AI Trader** - *13:06:08*

thanks [@UBS7QANF3](@UBS7QANF3). Out of curiosity, could you point me to the documentation regarding this behaviour in the streaming? Thanks

*Tags: General Technical*

---

**Mo** - *13:40:56*

The data provider is [https://www.totalperformancedata.com/|https://www.totalperformancedata.com/](https://www.totalperformancedata.com/|https://www.totalperformancedata.com/)



Typically you would get access to this via Betfair



Historic data is available via an API but it is largely useless

*Tags: Performance*

---

**D C** - *13:42:14*

Thanks for this explanation chaps this has confused me for ages. Ages ago I wrote a half-arsed price grid market replay display application that allowed manual step through each delta and a toggle for virtual/actual odds. Many times saw worse virtual prices but always assumed it was a coding error on my part.

*Tags: Errors Debugging*

---

**Rishab** - *13:52:23*

Hey [@UBS7QANF3](@UBS7QANF3) thanks for replying. So I have the betfair api, how do I get access to realtime horse position and all using this totalperformacedata?

*Tags: General Technical*

---

**Riccardo Fresi** - *13:59:31*

Hi, sorry to bother you, i don't get some things about the library:

1. is there any chance to have the score on real time (football for instance)

2. what is the purpose of streaming?

3. delayed or not delayed? how much are delayed the data?

4. any chance to have a free repository with data history since in italy seems we cannot purchase anything?

Thank!

*Tags: General Technical*

---

**Mo** - *14:14:37*

1. inplayservice is OK for this: [https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py|https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py](https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py|https://github.com/betcode-org/betfair/blob/master/betfairlightweight/endpoints/inplayservice.py)

2. To receive price updates as efficiently and quickly as possible

3. not delayed. The delayed data is useless

4. you’ll have to collect this yourself

*Tags: General Technical*

---

**ShaunW** - *15:23:10*

Probably a question for a meetup (which I didn't go to) but how the heck do you get an AM?  Been profitable for years and not even a sniff of one. Maybe commission generated is the trigger? It's not like I actually need one, already have a very helpful contact, but for the sake of a fragile ego it would be quite nice.

*Tags: General Technical*

---

## 2023-08-05

**Andy** - *07:02:52*

I recently got it moved from 200 (default) to 1000. Was still having some issues at 1000 and asked for 1500. I was told it's only possible for them to set 1000 max on their end or unlimited.. and that only a few select large clients are unlimited.. I questioned that as hard to believe as it would surely just be a user variable change...sounds like suspicions confirmed :thinking_face:

*Tags: General Technical*

---

**Riccardo Fresi** - *09:32:51*

But for streaming I need a payment subscription?

*Tags: General Technical*

---

**Trex44** - *21:56:35*

What are the bariers do you think? I was looking into it recently and was surprised at how much easier and cheaper it is to get high quality data for Financial markets that for Betfair. Probably means there is less alpha from it but I was still surprised.



I have also wondered if there is a lot of stuff that gets left on the table by the big guys because it wouldn't have a meaningful impact on their overall performance. Kind of how value investing in smaller market cap companies is something (according to Joel Greenblat) that managers of smaller portfolio sizes can do well at because the bigger portfolios ignore the companies as they can't get enough money into them.

*Tags: Performance*

---

**George** - *22:12:27*

where are you going to get the live data stream from?

*Tags: Deployment*

---

**Trex44** - *23:20:18*

All brokers provide live data. interactive brokers have menagerie to pick from depending on what markets you want to trade [https://www.interactivebrokers.com/en/pricing/research-news-marketdata.php](https://www.interactivebrokers.com/en/pricing/research-news-marketdata.php). You can also pay for for market depth etc. I think they lower or wave fees once you trade over a certain volume. You can get historical data from them and other sources too e.g. Kinetic. Its the historical stuff that I was surprised how cheap it is relative to BF historic data.

*Tags: Data Quality, Deployment*

---

**George** - *23:40:12*

Honestly I doubt there is a service aimed at the 'little guy' that offers both market data and order placement direct to exchange via python API, which is what we have with BF.

If there was - how would such a broker make money? They would have huge costs and no revenue.

You have to pay a commission or a margin somewhere.

It's possible to find positive EV in financial markets on paper for sure but my guess is it would be wiped out by the cost of execution.

*Tags: General Technical*

---

## 2023-08-06

**Kurosakura Cactus** - *09:45:53*

hi guys, strange question, but is anyoen else worried that using a library from online which takes id/pw as input might send your information to a different endpoint, and steal your info, and emulate the output?

*Tags: General Technical*

---

**Mo** - *09:54:05*

It's unclear if you're asking in general terms or about a specific repository but in the case of betfairlightweight, yes I went through the code to check it was suitable for my needs but no not specifically to check if it was stealing my credentials

*Tags: General Technical*

---

**Mo** - *10:03:48*

I think it depends on your definitions of:



1. Successfully

2. Trade

3. Financial markets

Does having a low frequency automated strategy for harvesting risk premia in an ISA count?



Do crypto markets count?

*Tags: Strategies*

---

**AI Trader** - *11:45:43*

*Virtual prices streaming data parsing*



*Hi guys,*

So far I have been using raw prices data. I am now collecting virtual price data. I have a question on how to reconstruct the orderbook. There are some issues in the way I am doing it.

*Question 1:*

In the below update message, we see *[9,0,0] (bdatl for selection* 5817456). According to the format in the documentation, it should be [level, price, size]. Does it mean I've received an update of price 0? As far as I knew, prices were bounded between 1.01 and 1000. What does it mean?

{"op":"mcm","clk":"AL3t1wMA6rPdBACj+8sE","pt":1691192775815,"mc":[{"id":"1.216538196","rc":[{"bdatl":[[1,1.44,192.1],[2,1.46,22.47],[3,1.47,5.31]],"id":10782},{"bdatb":[[1,4.9,13.72],[2,4.8,6.8],[3,4.7,34.91],[4,4.5,15.72],[5,4.4,3.58],[6,4.2,2.38],[7,1.03,2],[8,1.01,6],[9,0,0]],"id":58805},{"bdatl":[[6,17,1.95],[7,18.5,2.38],[8,1000,0.17],[9,0,0]],"id":5817456}]}]}



*Question 2:*

When reconstructing the book, would it be correct to keep a cache with the [level, price, size] and update the level every time I get an update (this would assume the levels are consistent accross different updates)? I was doing something different before: I would keep a cache of [price, size] and ignore the level, and always construct the book snapshot by using the best prices in my cache. The problem it seems I am having with this approach is that I seem to be missing updates removing some levels, ie, I get an update with a level 0, but my cache still shows better prices with non-zero volume.

*Tags: General Technical*

---

**D C** - *11:52:09*

In question 1 you see [9,0,0] because that means that there is no price for that level. You can see this from the prior entry [8,1000.0,0.17] which is the max price available. Just because the stream can return up to 10 levels, it doesn't mean that all of those levels will contain a price. A price of 0 just means that there is nothing available for that level.

*Tags: General Technical*

---

**Mo** - *12:22:06*

Aren’t you using betfairlightweight ?

*Tags: General Technical*

---

**AI Trader** - *12:33:33*

[@UBS7QANF3](@UBS7QANF3), I am using it for my live deployment, but for research purposes I am reconstructing the historical orderbook data. I will try to find where this happens on betfairlightweight to see the logic being used

*Tags: Deployment*

---

**liam** - *12:59:00*

Yeah why are you not using bflw for this? 



[https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py|https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py|https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py)

*Tags: General Technical*

---

**Alejandro Pablos Sánchez** - *14:16:33*

Hi guys !! While working with the different markets available in the exchange for football, I came up with the following question. When a particular market is not listed for a selected football match, is it because it is not available for that match, or because no one has put an order in that market? I'm saying this because a very popular market such as

"Both teams to score (BTTS)" appears as unavailable for all matches (it does not appear in the Betfair exchange betting interface). Then I wonder if this is due to it not being available or just because I need to be the first placing an order on it. Thanks !! :smile:

*Tags: Strategies*

---

## 2023-08-07

**Jorge** - *08:41:46*

Hi, I am using the streaming_timeout parameter in my flumine strategy to force an update periodically after some seconds even if there's no update within this period. I would like it to force a first update on all the markets that I am subscribed to, instead of waiting until we receive the first market_book for the market. Is this possible with flumine?

*Tags: Strategies*

---

**Rishab** - *13:02:39*

Query regarding marketrecorder from flumine, if I run it for whole day would it automatically pause updating the files saving data for races that have already ended &amp; when want to ultimately stop the recorder(the whole process), is there any conditional attribute in the function that can be used or manually killing the process will work as well?

*Tags: General Technical*

---

**Peter** - *16:01:19*

It's designed to run continuously. Some of us have literally been running variants of it for years without stopping.



For your specific questions ... yes it will stop recording markets when they are closed. The last few lines you will normally see in a data file will be the messages showing the results and then the market closure. If you wish to stop it, yes, you can just kill the process. Alternatively you can use the termination middleware (see the examples folder) to stop the Flumine instance once all markets for the day have closed.

*Tags: General Technical*

---

**AI Trader** - *19:28:32*

Hi guys,

Has anyone tried different approaches to sending multiple orders to Betfair at once? I am currently doing that thorugh creating multiple threads to send all my orders. Sometimes more than 1,000 orders at the same time in the startup. I am creating these threads in python, and I have realized that the latency to spin up a thread is quite significant (could be as high as 30ms). When doing that for hundreads of orders, it becomes meaningful. How do you usually do that? Asyncio? Thanks

*Tags: Performance*

---

## 2023-08-08

**joe taylor** - *11:29:53*

I have recorded data using flumine- Can I simulate my strategy on this recorded data?

*Tags: Strategies*

---

**joe taylor** - *12:19:50*

I’m working on a quoting strategy will it be able to factor in the fact that if I’m able to quote on a level before everyone else I should be the one to get the fill first on that level?

*Tags: Strategies*

---

**joe taylor** - *14:46:15*

I think it’ll be helpful for beginners if someone can share an implementation/Anaylsis notebook of a basic strategy using flumine. I read the documentation but not much clear from there.  

*Tags: Getting Started, Strategies*

---

**Lee** - *14:54:41*

the [https://betcode-org.github.io/flumine/|docs](https://betcode-org.github.io/flumine/|docs) have a simple example, which bit is not clear?

*Tags: General Technical*

---

**joe taylor** - *14:56:41*

So I’m running it on a saved data. How do I see things like how strategy performs with time/ if I want to do some plotting on type of bets it’s taking Etc 

*Tags: Strategies*

---

**Lee** - *14:58:56*

You can use this [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|logging control](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|logging control) as a starting point to output bets into a csv, from there you just need to read the csv and plot how you’d like

*Tags: General Technical*

---

**Rishab** - *15:57:45*

[@UUCD6P13J](@UUCD6P13J) where do I need to put backtestloggingcontrol as an input? Im assuming it works similar to marketrecorder(create a strategy with marketrecorder &amp; then framework.addstrategy())

*Tags: Strategies*

---

**MacrcoPolo** - *22:45:27*

Stupid question - can details of a runner in a market be altered without market being suspended?

*Tags: General Technical*

---

## 2023-08-09

**Simon Chan** - *13:39:27*

Hi, is Postgresql a good database stack to store and modelling data? I'm mostly familiar with Postgres because of work (web dev), but I want to know if someone has used a different stack

*Tags: Strategies*

---

## 2023-08-10

**Jorge** - *12:28:15*

Hey guys. How do you organize the recorded market files in S3? Until now I'm following the [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|marketrecorder.py)  and my file structure is {event_type_id}/{streaming or marketCatalogue}/{year}/{month}/{day}/{market_id}.gz



I'm thinking of adding another level to separate by market_type, so it is easier/faster to backtest strategies only in a specific market_type (until now I had a condition in the check_market_book).

*Tags: General Technical*

---

**Jorge** - *12:48:55*

I don't have experience with dbs but I guess it makes sense.. Any post about how to implement this lookup?

*Tags: General Technical*

---

**Jorge** - *15:27:55*

Do you create the table directly through Python? what kind of db do you use?

*Tags: General Technical*

---

**Mo** - *15:29:27*

Personally I use Postgres. I have a market catalogue table that gets inserted into by scraping market catalogues on an hourly basis. I also have a database table for indexing S3 objects of different types, one of which is scraped prices files. One of the columns in this index table is market ID that can be used to join on the market catalogue table and then filter on anything contained in the market catalogue

*Tags: General Technical*

---

**PeterLe** - *21:00:51*

I noticed something strange on the 20:23 At Salisbury. Two of my python strats fired in bets (which should be for inplay only) at around 20:06 (So before it actually went in play). It also happened on one of my C# programs as well. I can only assume that the inplay flag must have been set momentarily for a split second? Just wondered if anyone else noticed it&gt;? Thanks

*Tags: General Technical*

---

**AndyL** - *21:20:50*

the market was also Suspended at 20:05:11 and triggered an error for me:

```Order has violated: MARKET_VALIDATION Error: Market is not open```

*Tags: Errors Debugging*

---

## 2023-08-11

**Nacho Uve** - *09:36:23*

[@U0155J92A7Q](@U0155J92A7Q) I have not many stream files. So by now, I am happy using a simple CSV. I read it with pandas and make my filters to select markets to backtest or analyze.

*Tags: Feature Engineering*

---

## 2023-08-12

**BYD BET** - *18:16:30*

Hey guys,



I use in_play_service.get_scores on soccer match and sometimes i have no data for a football match.



For example :

`event_list = [32425345, 32425347, 32425348, 32425366, 32466972, 32504596, 32504599, 32504600, 32504601, 32504602, 32504603, 32504604, 32504605, 32504607, 32504608, 32504609, 32524091, 32536472]`

`[http://trading.in|trading.in](http://trading.in|trading.in)_play_service.get_scores(event_ids=event_list)` 



I never got any data for match 32504596 which took place yesterday. I have noticed this problem and I have not been able to explain why but it happens quite frequently.



Is it possible that betfair does not offer data for certain matches? This is very strange.



Thanks

*Tags: Strategies*

---

**PeterLe** - *18:24:45*

quick question please..re `max_selection_exposure=30`,

Given this scenario: two horse race, both horses laid at 1.8 for £1.

Does it calculate the exposure on each selection taking into account the bets on the the other selections?

Ie would it be calculated in flumine as an exposure on each at - 0.80p or a +0.20 on each?

I'm sure its the latter but wanted to check. Would anyone know please? Thanks



Edit ; Its the former.

*Tags: General Technical*

---

**Rishab** - *20:32:31*

I have been trying to  arrange my files created from market recorder but struggling a bit rn. Let's say I want to create a folder wrt event_Id_type. I tried changing the process_raw_data function : directory = os.path.join(self.local_dir,str(data.get('marketDefinition').get('eventTypeId')),market_id). So this line ensures that my data is saved in the correct location. But the location is created in the add function first and there's no 'data' argument passed to it so I can't find eventTypeId there. What solution are you guys using here?

*Tags: Data Quality*

---

**Mo** - *20:42:26*

Doesn’t matter. Read the thread. Point is - no need to have any meta data in the file system. Put it in a database instead

*Tags: General Technical*

---

**George** - *21:03:02*

the bigger question is why are you paying 5% commission on Betfair?

*Tags: General Technical*

---

**Joe Bloggs** - *21:57:43*

*Cunningham's Law* states "the best way to get the right answer on the internet is not to ask a question; it's to post the wrong answer."

*Tags: General Technical*

---

**joe taylor** - *22:05:58*

Okay but being on the laying side shouldn’t this help me generally?

*Tags: General Technical*

---

## 2023-08-14

**Unknown** - *03:20:49*

Hi guys! While debugging the cache mechanism, I found this `'con': True` attribute in the `market_change` on an UPDATE message. In the `update_cache()` function, there is no `'con'` equivalent. What is this `'con'` attribute? and why is Betfair sending it?

*Tags: Errors Debugging*

---

**liam** - *08:31:03*

No, [https://betcode-org.github.io/betfair/streaming/#snap|snap](https://betcode-org.github.io/betfair/streaming/#snap|snap) the listener

*Tags: General Technical*

---

**Carlos Vx** - *11:54:44*

I have been checking flumine and I didn't find an easy way to get orders in streaming that are not attached a BaseStrategy. I would like to process all orders and not only the orders created using the BaseStrategy. Is there an easy way to achieve it?

*Tags: Strategies*

---

**mzaja** - *12:11:07*

Hi, has anybody had similar experience to this? I have been placing FILL_OR_KILL orders through Flumine and noticed that the order stream update for setting the order status to "Execution complete" is sometimes severely delayed. A FILL_OR_KILL order is either matched instantaneously or rejected, so it should go straight from "Executable" to "Execution complete". Normally, it looks like this, and one can see that the delta between the last two updates is 52 ms:

```Time                       Selection ID  Bet ID        Order status        Side      Matched    Remaining  Trade status

-----------------------  --------------  ------------  ------------------  ------  ---------  -----------  --------------

2023-08-13 10:35:25,142        55258054  None          Pending             BACK         0            2     Live

2023-08-13 10:35:25,340        55258054  316818562580  Executable          BACK         2            0     Pending

2023-08-13 10:35:25,392        55258054  316818562580  Execution complete  BACK         2            0     Live```

However, for a sizeable proportion of the time, the "Execution complete" status update is received several minutes later, when the market gets resulted. Note that the bet is already fully matched when it reaches the status of "Executable", but it takes almost 9 minutes for the "Exection complete" update to arrive.

```Time                       Selection ID  Bet ID        Order status        Side      Matched    Remaining  Trade status

-----------------------  --------------  ------------  ------------------  ------  ---------  -----------  --------------

2023-08-13 10:11:58,988        58973557  None          Pending             BACK            0            2  Live

2023-08-13 10:11:59,296        58973557  316815184967  Executable          BACK            2            0  Pending

2023-08-13 10:20:39,839        58973557  316815184967  Execution complete  BACK            2            0  Live```

Any thougts on this? I am trying to figure out whether this is

1. A problem on Betfair's side.

2. A problem with Flumine.

3. A problem with my setup.

*Tags: Getting Started, Deployment*

---

**Carlos Vx** - *12:13:57*

I would like to use flumine to create a subscription and have all the orders tracked including orders that are not created using a BaseStrategy (directly on the betfair platform).

I used the example "betcode-org/[https://github.com/betcode-org/flumine/tree/master|flumine](https://github.com/betcode-org/flumine/tree/master|flumine)/[https://github.com/betcode-org/flumine/tree/master/examples|examples](https://github.com/betcode-org/flumine/tree/master/examples|examples)/example.py"

but I didn'tfind an easy way to achieve it. I can only see orders created on process_market_book function that are attached to the strategy implemented.

*Tags: Strategies*

---

**liam** - *12:15:59*

You could modify [https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|this](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|this) worker example to pull them in

*Tags: General Technical*

---

**mzaja** - *12:16:29*

How would I place it incorrectly? This is from the same strategy which uses only FILL_OR_KILL bets, so there is no risk of accidentally mixing them with regular orders. Moreover, the bet is clearly fully matched when it lands in the market, and I cannot find a reasonable excuse for the "Execution complete" update to take 9 minutes from then onwards.

*Tags: Strategies*

---

**mzaja** - *12:23:29*

The way I see it, it is either:

1. Betfair erroneously does not send an update, hence I am asking whether someobody else experienced the same thing.

2. Betfair sends an update but it somehow gets dropped before it reaches me.

3. Betfair sends an update and it is received on my end, but Flumine does not process it.

The core issue is that Flumine leaves the trade in live state in the order does not complete, which then intereferes with the trading controls.

*Tags: Deployment, Strategies*

---

**casper** - *12:40:07*

Hey [@U4H19D1D2](@U4H19D1D2)! Is there a particular reason not to bump the requests module for bflw? urllib3 2.0 issues?

[https://github.com/betcode-org/betfair/pull/525](https://github.com/betcode-org/betfair/pull/525)

*Tags: General Technical*

---

**Mo** - *14:47:43*

AWS Dublin

*Tags: Deployment*

---

**Mona** - *15:14:55*

Hi all, this is quite basic, can anyone help with what `price_projection` should look like in the `list_market_book` function, I am trying to get the best offers for each runners for a particular market  and the result doesn't seem to show anything other than last traded price. Appreciate the help

*Tags: General Technical*

---

**Mo** - *15:34:35*

[https://github.com/betcode-org/betfair/blob/7118b471e61282d87630bcdc5d6de6b69a59cfa6/examples/exampleone.py#L55C3-L57](https://github.com/betcode-org/betfair/blob/7118b471e61282d87630bcdc5d6de6b69a59cfa6/examples/exampleone.py#L55C3-L57)

*Tags: General Technical*

---

**Mona** - *15:51:29*

that is very helpful, thank you Mo

*Tags: General Technical*

---

**mzaja** - *16:32:14*

[@U4H19D1D2](@U4H19D1D2) I believe this example would work if you change line 59 to `order_type=LimitOrder(lay, self.context["stake"], time_in_force="FILL_OR_KILL"),` . All that is needed is to place FILL_OR_KILL orders and check the order stream.

[https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/examples/strategies/lowestlayer.py#L59](https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/examples/strategies/lowestlayer.py#L59)

*Tags: General Technical*

---

**mzaja** - *16:33:39*

[@UUCD6P13J](@UUCD6P13J) I am using default Flumine's logging controls. Log it to JSON, read and extract data of interest in a script.

*Tags: General Technical*

---

**mzaja** - *16:45:21*

The reason why I even dived into this matter is because I noticed that Flumine was rejecting order placement with

```"strategy.validate_order failed: live_trade_count (1) &gt;= max_live_trade_count (1)"```

when I tried to reuse the Trade to place a closing order. This never happened in the simulation so I investigated further and found out that a percentage of opening bets does not change status to "Execution complete" when the bet is fully matched (and even if it isn't, it should be cancelled). Therefore, it cannot be down to a delay in the logging controls because Flumine's behaviour on a live market confirms it.

*Tags: Errors Debugging, Deployment, Strategies*

---

**mzaja** - *17:03:55*

Sorry, what do you mean by "normal logs"? This is all from the root Flumine logger. Looking at the code, `BaseOrder.__update__status()` logs a message whenever its status is updated. If the message is not logged, it means the order status did not change. The status should change to "Execution complete" because the order is fully matched, and most of the time it does. But, occasionally it does not for some reason. I am trying to figure out what the reason is.

*Tags: General Technical*

---

**Lee** - *17:06:07*

Yeah the flumine logger

*Tags: General Technical*

---

**Lee** - *18:38:57*

Looks like i’ve narrowed the issue down to [https://github.com/betcode-org/flumine/blob/master/flumine/order/process.py#L88|here](https://github.com/betcode-org/flumine/blob/master/flumine/order/process.py#L88|here). order.bet_id is None with status PENDING, current_order.bet_id is set and status is EXECUTION_COMPLETE.

*Tags: General Technical*

---

**mzaja** - *19:05:25*

[https://github.com/betcode-org/flumine/issues/681](https://github.com/betcode-org/flumine/issues/681)

*Tags: General Technical*

---

## 2023-08-15

**Mo** - *06:06:12*

`streaming_update` field of the market book gives you the raw streaming update so you could check that for which runners have changes

*Tags: General Technical*

---

**D C** - *14:36:55*

Surely for fill or kill, the order status should go straight from "pending" to "execution_complete" ? And would you not only get "pending" if placing it with the async flag? I'm assuming here you are talking about pure bet operations and that "pending" refers to bet status and is not artificially set by flumine of course?

Edit: Or "Pending" to "Expired" in case of failure to match.

*Tags: General Technical*

---

**mzaja** - *20:41:29*

[@UUE6E1LA1](@UUE6E1LA1) That is what I think as well. In this PR [https://github.com/betcode-org/flumine/pull/655](https://github.com/betcode-org/flumine/pull/655)  I assumed that they all go PENDING -&gt; EXECUTION_COMPLETE, but it seems they can also go to EXPIRED as well. It would be good to capture raw responses of the Betfair order stream with bflw and check.

*Tags: General Technical*

---

**mzaja** - *21:45:07*

Calling listCurrentOrders will do no good because it'll miss any intermediate states, I've read the docs and you are right, there is no mention of EXPIRED anywhere. However, I do wonder if the documentation is perhaps not up to date.



I have not been placing the orders with the async flag. Don't really see a point in doing it because the orders are submitted to the thread pool anyway, so from Flumine's perspective it's all asynchronous anyway.

*Tags: General Technical*

---

**mzaja** - *22:12:18*

I wonder where that EXPIRED comes from... AFAIK, Flumine uses order streaming and not poling to retrieve bet states. I have just done an experiment with placing FOK bets and they go to EXECUTION_COMPLETE regardless of whether they are matched or not. Here is the data. The first order is matched, the second is cancelled ("sm"/"sc"). They both go straight to "EC" for status, which stands for EXECUTION_COMPLETE.

```{'id': '1.216407624',

 'orc': [{'id': 39674645,

          'uo': [{'id': '317109322476',

                  'p': 1.94,

                  's': 1,

                  'side': 'L',

                  'status': 'EC',

                  'pt': 'L',

                  'ot': 'L',

                  'pd': 1692133287000,

                  'md': 1692133287000,

                  'avp': 1.94,

                  'sm': 1,

                  'sr': 0,

                  'sl': 0,

                  'sc': 0,

                  'sv': 0,

                  'rac': '',

                  'rc': 'REG_GGC',

                  'rfo': '',

                  'rfs': ''}],

          'ml': [[1.94, 1]]}]}

{'id': '1.216407624',

 'orc': [{'id': 39674645,

          'uo': [{'id': '317109323812',

                  'p': 1000,

                  's': 1,

                  'side': 'B',

                  'status': 'EC',

                  'pt': 'L',

                  'ot': 'L',

                  'pd': 1692133289000,

                  'sm': 0,

                  'sr': 0,

                  'sl': 0,

                  'sc': 1,

                  'sv': 0,

                  'rac': '',

                  'rc': 'REG_GGC',

                  'rfo': '',

                  'rfs': '',

                  'cd': 1692133289000}]}]}```

*Tags: General Technical*

---

**mzaja** - *22:29:23*

I assume it is a state Flumine internally assigns to all orders which have been sent to the thread pool for execution. One needs to register a bet as soon as it's been submitted because otherwise the bot will go haywire and place hundreds of them in a second.

*Tags: General Technical*

---

**mzaja** - *22:44:58*

[@UUE6E1LA1](@UUE6E1LA1) The PENDING status I am getting is Flumine's PENDING status, not Betfair's. Flumine puts the order into PENDING as soon as it's submitted for execution, for the reasons I explained about. I guess that is what the confusion was about. Those states displayed in the original post are Flumine's internal bet states, not Betfair's.

```

class OrderStatus(Enum):

    # Pending exchange processing

    PENDING = "Pending"

    CANCELLING = "Cancelling"

    UPDATING = "Updating"

    REPLACING = "Replacing"

    # Completed

    EXECUTABLE = "Executable"  # an order that has a remaining unmatched portion

    EXECUTION_COMPLETE = "Execution complete"  # an order that does not have any remaining unmatched portion

    EXPIRED = "Expired"  # order is no longer available for execution due to its time in force constraint

    VIOLATION = "Violation"  # order never placed due to failing controls```

*Tags: General Technical*

---

**Rishab** - *23:38:38*

order_type=LimitOrder(price=runner.ex.available_to_back[5]['price'], size=12.0) is there any restrictions on size? my strategy in simulation works fine when size is small but doesn't execute with a big size(not able to place orders). Any clue?

*Tags: Strategies*

---

**Rishab** - *23:52:33*

Wasn't generating it earlier. Logs solved the issue, had to change the max_order_exposure. Thanls

*Tags: General Technical*

---

## 2023-08-16

**liam** - *06:00:10*

How are you writing to the db? Within a strategy?



Common process is to store raw data using the market recorder and then process to db/csv etc 

*Tags: Data Quality, Strategies*

---

**Michael** - *10:12:34*

Yeah, directly within the strategy. I do it within process_closed market, passing the market catalogue to the db write function and then writing to db from there. The DB is just storing the path and then certain parts of the market catalogue to help me filter during backtesting.

If you store the raw data and then process to db, do you not lose the ability to retrieve the market catalogue? Really, it doesn’t need to write to db instantly after the market is closed as long as it does it eventually

*Tags: Strategies*

---

**George** - *16:31:18*

I have noticed that, when using a logging control, `_process_closed_market` and `_process_cleared_orders_meta` often get called twice. The second call is often around an hour later than the first.



In theory, this should not be a big deal - I can just ignore the repeated log lines that I don't need. However, I want to start uploading my log files to s3. I am planning to put an s3_upload call at the end of `_process_cleared_orders_meta`. But if I do this, I'll upload to s3 twice, and the second upload will overwrite the first.



I can turn on versioning, but that's not really the point, because the first log file is the 'correct' one and the second should not (in my mind) be appearing at all.



Has anyone noticed this problem and found a way to resolve it?

*Tags: General Technical*

---

**Mo** - *16:58:37*

I'm bumping up against the limitation of my flumine knowledge here but personally I handle this kind of situation by having the S3 upload handled by a completely separate process (cron job) which will allow time for these kind of appends to log files before compressing and uploading the log files to S3

*Tags: General Technical*

---

**Frenkie** - *19:09:30*

Hi all, a newbie here. I am trying to go through the documentation of Flumine in order to deploy my own strategy, I wanted to check if there's any way to fetch my current exposure at each runner.

One way is for me to calculate this locally by tracking all my previous matchings but I wanted to know if there's any method/attribute that provides this, or how do you guys fetch this info?

*Tags: Deployment, Strategies*

---

## 2023-08-17

**George** - *08:14:00*

I don't think it's silly to think about putting an s3 upload at the end of process_cleared_orders_meta. If I'd done that I would have overwritten all my log files. So it could have been an issue!



In general I find logging the most confusing part of the whole coding universe. It doesn't help that python's logging module is very weird.

*Tags: General Technical*

---

**Frenkie** - *08:19:24*

got it, thanks for your help

*Tags: General Technical*

---

**liam** - *08:23:49*

I think we have our wires crossed, if you are using AWS then you want to be using cloudwatch, there a few plug and play python modules that literally stream the logs straight to CW. For example I use docker, few lines of code and it does it all.



Not sure why you would be overwriting, just updating with the latest surely? Often results get changed. 

*Tags: Deployment*

---

**George** - *13:47:18*

I think the bigger issue with the current exposure function is like this:

Let's say we back runner 1 for £25 and lay runner 2 for £10 @ 1.5

Now let's say we want to know our exposure on runner 2.

The function will return £5 because of the lay order only.

I would argue the correct value is £30 and therefore the exposure management in flumine is not applied correctly.

*Tags: General Technical*

---

## 2023-08-18

**Mona** - *18:03:39*

Hi guys, is there a way to retrieve runner names (horse names) from the MarketBook objects? I noticed the runner names are available in the MarketCatalogue objects but they return None sometimes from the streaming using Flumine?

*Tags: General Technical*

---

**liam** - *22:08:05*

I assume not because of flumine / bflw being incorrect?

*Tags: General Technical*

---

## 2023-08-19

**PeterLe** - *09:33:27*

No it wasn’t anything wrong with flumine/ bflw. It seems crazy that I can only put 5000 tx combined total across all my sub accounts (500 each) whereas my wife who only has a single account can put 5000 through hers. Betfair could do with bumping it up a bit. I’ve only incurred a charge twice in recent times as most offset against comm. 

*Tags: General Technical*

---

**Mo** - *23:13:25*

1. `flumine.utils.price_ticks_away(2.1, -2)`

2. `betfairutil.make_price_betfair_valid(2.0657, betfairutil.Side.LAY)`

*Tags: General Technical*

---

## 2023-08-20

**Y B** - *14:03:53*

Hi everyone, just getting started with flumine. Wondering how can I feed the market data from bz2 files downloaded from betfair into it?

```strategy = ExampleStrategy(

    market_filter={"markets": ["/tmp/marketdata/1.170212754"]}

)```

What exactly does the `markets` override accept?



is flumine capable of reading raw data as is or do I need to pre-process it by using e.g. [https://github.com/mberk/betfairutil](https://github.com/mberk/betfairutil)?

*Tags: Getting Started, Strategies*

---

**Mo** - *15:07:00*

1. Give the list of files in the `market_filter` under the `markets` key

2. Yes it can read the raw data files but you probably need to patch `open` as described here: [https://betcode-org.github.io/flumine/performance/#file-location](https://betcode-org.github.io/flumine/performance/#file-location)

*Tags: Performance*

---

**Y B** - *15:10:25*

That's very helpful, thanks

*Tags: General Technical*

---

**Y B** - *15:27:59*

*Context:* I'm trying to run a very simple strategy (always back the first runner at a certain level) on *basic* data (i.e. just last_price_traded without having access to order book depth). And my orders don't seem to ever get filled.

*Questions:*

• Is there anything that needs to be done to make orders get filled when backtesting on "basic" data?

• Is it even possible to get backtesting work on basic data? I'm fine with either assuming infinite depth at a certain level and/or some const value

• In basic data mode, if I place an order (back)  with "limit=X" 

    ◦ and currently the market (i.e. last traded price) is at A (A &lt; X)

    ◦ the next mkt data tick provides last traded price is at B (A &lt; X &lt; B) -- *will my order get filled?* 

*Tags: Data Quality, Strategies*

---

**Mo** - *18:17:35*

Conflation happens when you are not pulling the data off the socket fast enough. By definition, if it's happening then you are spending too much time handling the data. Whether it matters or not is another question

*Tags: General Technical*

---

**AI Trader** - *21:43:49*

Hi #Flumine users,

Could someone explain why this is required?



```            if (

                order.bet_id and order.bet_id != current_order.bet_id

            ):  # replaceOrder handling (hacky)

                order = markets.get_order_from_bet_id(

                    market_id=current_order.market_id,

                    bet_id=current_order.bet_id,

                )```

[https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/flumine/order/process.py#L63](https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/flumine/order/process.py#L63)

*Tags: General Technical*

---

**AI Trader** - *21:49:27*

Not sure I understand the question, but I don't think you can cancel a size higher than the size of any individual order. I think in your example, you need to internally figure out that you need to cancel entirely the order of size 10 and partially cancel 3 USD out of the 5 USD of the second order.

*Tags: General Technical*

---

## 2023-08-21

**Frenkie** - *08:39:45*

ok got it, thanks for your help [@U04H54Q392N](@U04H54Q392N) [@U04980ZT6UX](@U04980ZT6UX)

*Tags: General Technical*

---

**Rishab** - *11:24:47*

looked at the git examples and the market.py in flumine, dont see anything that can increase the size of order. Confused if it's bec when u increase the size u lose priority. So is that the reason why it's not there or simply I'm missing some function?

*Tags: General Technical*

---

## 2023-08-22

**Unknown** - *10:48:31*

Hi Guys,

i'm trying to figure out if is it possible to get an edge with a model prediction that predict a status SUSPENDED when is it OPEN, at the moment i collect no much data, unfortunately seems i cannot download histic data from italy so i have to collect from inplay.

this is the confusion matrix

the idea is to place an order when i predict suspended and it is still open (probably will be suspended in some moments), it make sense for you?

*Tags: Strategies*

---

## 2023-08-23

**Trex44** - *10:50:55*

Hey all. Is there a way to use Multiprocessing with [https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py|this example script](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py|this example script) for recording market data from recorded markets/historical data? My attempts to implement it so far aren't working.

*Tags: Data Quality*

---

**Andrew** - *21:49:00*

With thanks once again to Liam, isn’t this what you’re looking for. Works for me. [https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/](https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/) I create a CSV output per market. Subsequently you can merge or leave as 1:1 for your market PRO stream files.

*Tags: Performance*

---

## 2023-08-24

**liam** - *08:14:04*

I assumed [@U03N4QBJ0TV](@U03N4QBJ0TV) wasn't a flumine user (yet)

*Tags: General Technical*

---

**Trex44** - *14:41:55*

Hey both. Thanks very much. I am actually a Flumine user (just not a good one). Had issues trying to build a strategy that logged data and had that other script bookmarked from a while back so went with it. I will try and implement the Flumin version. thanks very much for your help both of you.

*Tags: Strategies*

---

## 2023-08-25

**Unknown** - *10:53:41*

hi guys, need some help



i implemented a kelly method function

```def kelly_method(odds):

    b = odds - 1

    p = 0.94 #0.9851668726823238  # precision value for prediction model or historical win rate

    f = ((odds * p) - 1) / b

    return odds, b, p, f```

i have some doubt: p should be the % winning probability of the bet, i'm not sure to use the precision of model or the ex-post % based on actual historical results



in the first case i have a very bad performance

in the second case, not so bad;



by the way also the function give me negative results if the odds has more implicit probability respect the p variable (it makes sense to you?) so i use a 0.5 fixed for negative value (it also make sense to you?)



Other thing: the one i've loss, i suspect that i collect very bad the live odds for this single event, may i ask you if you can send me the historical data for that since seems in italy we cannot retrieve historical data? of course if is not a problem

*Tags: Data Quality, Errors Debugging, Performance, Deployment, Strategies*

---

**Mo** - *12:33:37*

1. Use your prediction

2. Don't bet at full Kelly stake. Use a fraction such as one-half or one-quarter (I prefer the latter). Fractional Kelly protects you against uncertainty in your predictions and also greatly reduces volatility for only a relatively small cost in long term expected wealth

*Tags: Strategies*

---

**Riccardo Fresi** - *13:48:28*

got the point, too risky, but we still talking on a almost certain odds, maybe can be reasonable to have a cap of the fraction, let me think about it;

now i'm running on sequential bet, how kelly should work on concurrent event, any ideas?

*Tags: Strategies*

---

**Riccardo Fresi** - *14:04:15*

separate football match, i mean, the balance is an input for kelly method, but it can't be the same, if the fraction to invest for 2 concurrent event is 0.8 i cannot invest 1.6  of my initial balance. so should i prioritize which event? the one with hyphotetical higher fraction the recalculate with this priority set and netting the balance for every step?

*Tags: Strategies*

---

**Mo** - *14:04:46*

Standard practice is to have a separate bankroll for each event

*Tags: General Technical*

---

**Riccardo Fresi** - *14:13:55*

this seems intresting

• page 17 [http://www.eecs.harvard.edu/cs286r/courses/fall12/papers/Thorpe_KellyCriterion2007.pdf](http://www.eecs.harvard.edu/cs286r/courses/fall12/papers/Thorpe_KellyCriterion2007.pdf)

• [https://vegapit.com/article/numerically_solve_kelly_criterion_multiple_simultaneous_bets](https://vegapit.com/article/numerically_solve_kelly_criterion_multiple_simultaneous_bets)



*Tags: Strategies*

---

**Riccardo Fresi** - *14:41:20*

also, i saw you post on goto_conversion, should i use that as input for kelly method? seems the one i need, i still not convinced to use my precision prediction value

*Tags: Strategies*

---

**Riccardo Fresi** - *14:49:08*

i have implemented few ML prediction model with sklearn library (hope you know), this library set the model and run it splitting train set and test set, (X_train, Y_train, X_test, Y_test)

X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=43214)

model.fit(X_train, y_train)

then predict a new dataset of result, called Y_pred, based on X_test

y_pred = model.predict(X_test)

the precision is the % of correct Y_Pred on Y_test

precision = precision_score(y_test, y_pred)



I assume that the precision is equal to probality

*Tags: Strategies*

---

**Riccardo Fresi** - *15:10:07*

y_pred are my forecasted data, a list of value [true,false,false,true,false]

y_test are actual data, an other list of value for same events [false,false,false,true,false]

on this sample i got 4 on 5, so precision 0.8, that is also my model probability

*Tags: Strategies*

---

## 2023-08-26

**Alex A** - *06:51:40*

You probably want to use the uncertainty, which should be associated with that individual prediction, not with the model as a whole.

*Tags: Strategies*

---

**Riccardo Fresi** - *08:47:15*

Yes, still not sure if it is the theoretical right value, I got so far 4 options:

1. Uncertainty of single prediction based on prediction model

2. Uncertainty of whole model

3. Implicit probability based on odds (I.e shin method, here Mo have already posted something about that)

4. real probability based on historical prediction, cumulative calculus (not convinced at all)

I got all of this, but I miss there theory so I don’t know what is the best approach

*Tags: Strategies*

---

**liam** - *14:27:17*

Flumine user?

*Tags: General Technical*

---

**Pos** - *14:56:05*

[@UUE6E1LA1](@UUE6E1LA1) noob question, when you search for markets with same event name/date, do you use listMarketCatalogue?

*Tags: General Technical*

---

**liam** - *15:01:23*

Are you not streaming?

*Tags: General Technical*

---

## 2023-08-29

**Matthew** - *09:25:52*

Which kind of metrics do you use for tracking how well is doing really your model against the market? I mean, a metric which is able to determine if the model is having some negative variance or if it is really performing bad. I know about looking at the profit at BSP, I wonder if there are other methodologies. Thank you :pray:  

*Tags: Strategies*

---

**Mo** - *09:32:27*

It’s more about expected value assuming BSP is true rather than profit at BSP



For me, this is the approach but the problem is what to use as fair value in contexts where BSP is not available. For example, in play

*Tags: General Technical*

---

**Mo** - *09:41:39*

Also depends on why you think you’re finding value in these situations. If you think the market is completely bonkers for long periods of the event then it’s not helpful to use the market to calculate EV. On the other hand if you think you’re profiting off short term mispricing then it makes perfect sense to evaluate the bets once the market has adjusted

*Tags: General Technical*

---

**Jorge** - *13:15:27*

Hi, I want to extract the prices from my recorded file just before a dog race. I know how to do this in a Soccer game, I search for the moment where the market_book['inplay'] is True and I take the previous market_book. But no clue on how to do it in dog races

*Tags: General Technical*

---

**AI Trader** - *15:51:48*

Hi guys,

Using betfairlightweight, if I subscribe to the order stream with a conflate parameter equals to None, what will be used by betfair? I tried to look up the documentation and could not find a default value. I also tried to check the messages to see if I could find this values somewhere, and I couldn't. Thank you so much

*Tags: General Technical*

---

**AI Trader** - *17:24:09*

*Trading System*



Hi guys,

I wrote my own trading system using betfairlightweight to keep the stream connection. One of the issues I face sometimes is that I get the marketbook stream update before the order stream update. For instance, when I manually cancel an order thorugh the UI, I might not replace it because the marketbook update came before the order update. Have you guys had this issue? How did you deal with it (Other than tweaking the conflate) ?

*Tags: Strategies*

---

**Riccardo Fresi** - *17:29:19*

Guys,

what's wrong on this function?

```def place_order(size, price, selection_id, trading, market_id, lay="LAY"):

    # placing an order

    limit_order = filters.limit_order(size=size, price=price, persistence_type="LAPSE")

    instruction = filters.place_instruction(

        order_type="LIMIT",

        selection_id=selection_id,

        side=lay,

        limit_order=limit_order,

    )

    place_orders = trading.betting.place_orders(

        market_id=market_id, instructions=[instruction]  # list

    )

    return place_order```

```U.place_order(1, 1.1, 1.217703269, trading, 4940266, lay="BACK")```

```APIError: SportsAPING/v1.0/placeOrders 

Params: {'marketId': '4940266', 'instructions': [{'orderType': 'LIMIT', 'selectionId': '1.217703269', 'side': 'BACK', 'limitOrder': {'price': 1.1, 'persistenceType': 'LAPSE', 'size': 1}}]} 

Exception: None 

Error: {'code': -32602, 'message': 'DSC-0018'} 

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32602, 'message': 'DSC-0018'}, 'id': 1}```



*Tags: Errors Debugging, Strategies*

---

**Johnny** - *17:31:19*

Hi all, hope you had a fruitful bank holiday... potential noob question again so apologies if it's documented somewhere (in which case please could you point me in the direction of the page).



Using marketrecorder.py and flumine, mainly based on the examples, I have recorded something like 30,000 markets.   It looks like the market catalogue files (.json.gz) are saved for only about 10,000 of them, so there are a load of market stream text files which don't have the metadata stored.  Is there a way to get the market catalogue data retrospectively?  And does it sound like I am using the recorder incorrectly for this to happen in the first place?

*Tags: General Technical*

---

**Johnny** - *17:56:43*

... and now as a side quest I will go and learn about how to search for strings in massive text files

*Tags: General Technical*

---

**AI Trader** - *19:10:51*

[@U4H19D1D2](@U4H19D1D2) I keep a local cache with my open orders. If for some reason, I manually cancel an order though the UI and I receive the orderbook update thorugh the stream before I receive the order update, my internal cache will think I still have that order open and do nothing. I only react to book updates (not order updates), so when I finally receive the order update, I will update the order cache, but I will only replace the order when I receive a next orderbook update thorugh the stream (which in small markets might take quite a while). This is not a problem only in case of manual cancels thorugh the UI, but also if there is an order fill.

*Tags: General Technical*

---

**AI Trader** - *19:11:37*

Error:

Hi guys,

Over the past few days I have noticed I have been receivng the error &lt;head&gt;&lt;title&gt;502 Bad Gateway&lt;/title&gt;&lt;/head&gt; when trying to place orders. Did this happen to any of you? Any tips on how to solve?

*Tags: Errors Debugging*

---

**liam** - *20:01:04*

So in flumine we use a timeout on the queue to push a snap of the stream cache, you can see the logic here or just migrate to flumine and never think about this again…



[https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/flumine/streams/marketstream.py#L59|https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/flumine/streams/marketstream.py#L59](https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/flumine/streams/marketstream.py#L59|https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/flumine/streams/marketstream.py#L59)

*Tags: General Technical*

---

**Andrey Luiz Malheiros** - *20:23:51*

Hey guys.

Let's suppose I run 2 different strategies for the same markets within the same instance of Flumine, Strategy A and B, both in paper trade mode. Let's suppose I placed an order using Strategy A. If I later place a new order using Strategy B, does Flumine take into account the bet previously placed with Strategy A in the market to handle new bet?

*Tags: Strategies*

---

## 2023-08-30

**Riccardo Fresi** - *10:06:10*

my personal issue is interpret the error. it's not really clear (to me of course)

*Tags: Errors Debugging*

---

**Andrey Luiz Malheiros** - *19:07:44*

Thank you, George!

Following this idea, another question arose. When using FILL_OR_KILL, does Betfair attempt to match my order with the best available prices? For instance, if I place an order with a size of 10 and odds of 5 using FILL_OR_KILL, and in the order book there are bets with a size of 5 and odds of 6, and another bet with a size of 5 and odds of 5, will my average_price_matched end up being 5.5?

*Tags: General Technical*

---

## 2023-08-31

**Andre Mello** - *02:48:09*

Hi guys

help me

I'm having trouble with this certificate, I've never done it

``````

`openssl req -new -config openssl.cnf -key client-2048.key -out client-2048.csr`





`Country Name (2 letter code) [AU]:BR`

`State or Province Name (full name) [Some-State]:PR`

`Locality Name (eg, city) []:PARANAGUÁ`

`Organization Name (eg, company) [Internet Widgits Pty Ltd]:[http://yourcompany.com|yourcompany.com](http://yourcompany.com|yourcompany.com)  ????? *[https://www.betfair.com/sport/](https://www.betfair.com/sport/) ?? I don't know what to put*`

`Organizational Unit Name (eg, section) []:Security Team - *I don't know what to put*`

`Common Name (e.g. server FQDN or YOUR name) []:Test API-NG Certificate  *I don't know what to put*`

`Email Address []:[mailto:my.name@mydomain.com|my.name@mydomain.com](mailto:my.name@mydomain.com|my.name@mydomain.com) - *MY email betfair??*`

 

`Please enter the following 'extra' attributes`

`to be sent with your certificate request`

`A challenge password []:  -` _*`my passord betfair??`*_

`An optional company name []: - my name??`

*Tags: Deployment*

---

## 2023-09-01

**George** - *17:46:10*

What is the best/fastest way to grab a snapshot of a market book at X minutes before the market start time?

Of course I could load the market in flumine and loop through the updates until I get to the desired timestamp, but that seems to be a relatively slow process.

Would it be a terrible idea just to use the BASIC data instead?

*Tags: Data Quality, Performance*

---

**liam** - *17:54:14*

 Bflw in lightweight mode or the rust betfair library, don’t use basic data 

*Tags: Data Quality*

---

**Peter** - *21:02:24*

Trying to use the rust-based betfair_data package. Sadly it barfs on the first line of each of the marketrecorder files that I feed it with the following (example) parse error:

`file: 1.217507712 err: (JSON Parse Error) invalid type: null, expected a borrowed string at line 1 column 25`



Example first line:

```{"op": "mcm", "clk": null, "pt": 1692873297553, "mc": [{"id": "1.217507712", "marketDefinition": {"bspMarket": true, "turnInPlayEnabled": true, "persistenceEnabled": true, "marketBaseRate": 5, "eventId": "32583319", "eventTypeId": "7", "numberOfWinners": 1, "bettingType": "ODDS", "marketType": "WIN", "marketTime": "2023-08-26T12:50:00.000Z", "suspendTime": "2023-08-26T12:50:00.000Z", "bspReconciled": false, "complete": true, "inPlay": false, "crossMatching": false, "runnersVoidable": false, "numberOfActiveRunners": 6, "betDelay": 0, "status": "OPEN", "runners": [{"adjustmentFactor": 46.361, "status": "ACTIVE", "sortPriority": 1, "id": 46821369}, {"adjustmentFactor": 19.97, "status": "ACTIVE", "sortPriority": 2, "id": 36503217}, {"adjustmentFactor": 15.785, "status": "ACTIVE", "sortPriority": 3, "id": 41122645}, {"adjustmentFactor": 8.596, "status": "ACTIVE", "sortPriority": 4, "id": 2507254}, {"adjustmentFactor": 6.617, "status": "ACTIVE", "sortPriority": 5, "id": 122409}, {"adjustmentFactor": 2.673, "status": "ACTIVE", "sortPriority": 6, "id": 26486375}], "regulators": ["MR_INT"], "venue": "York", "countryCode": "GB", "discountAllowed": false, "timezone": "Europe/London", "openDate": "2023-08-26T12:50:00.000Z", "version": 5406653488, "raceType": "Flat", "priceLadderDefinition": {"type": "CLASSIC"}}, "img": true}]}```

Anybody know how to work around this?

*Tags: Errors Debugging, Strategies*

---

**Mo** - *21:11:03*

I have an idea for how to do this that should be quite fast, will add it to betfairutil tomorrow 

*Tags: General Technical*

---

**Peter** - *22:07:59*

It does. Though every update (line) has `"clk": null` . I'm hoping that I can avoid the need to unpack and pre-process every file before streaming the updates. I'm not understanding why `null` is being treated as an invalid type.

*Tags: General Technical*

---

**Mo** - *22:48:59*

I think because of this line: [https://github.com/tarb/betfair_data/blob/5babe169ef625c193bec405580eaf04daaeb6e5e/src/ids.rs#L7](https://github.com/tarb/betfair_data/blob/5babe169ef625c193bec405580eaf04daaeb6e5e/src/ids.rs#L7) if you want to try to fix it

*Tags: Errors Debugging*

---

## 2023-09-02

**foxwood** - *09:45:03*

All my files recorded with flumine have a value for that field eg "clk":"AJjuvAEAhcOpAQCIxccB" - maybe a recorder issue ? Sort of thing I'd write a mm for once the recorder was fixed and run through all the files setting the field to a fixed string. Pain but future proofs it.

*Tags: Errors Debugging*

---

**Mo** - *10:03:21*

[https://github.com/betcode-org/flumine/issues/522|https://github.com/betcode-org/flumine/issues/522](https://github.com/betcode-org/flumine/issues/522|https://github.com/betcode-org/flumine/issues/522)

*Tags: General Technical*

---

**Mo** - *10:06:15*

But you might never have picked up the change depending on when you created your personal market recorder

*Tags: Data Quality*

---

## 2023-09-04

**Jorge** - *10:37:20*

Hi, I'd like my Strategy to cancel all open orders when stopping the script (i.e. Ctrl+C but also in case of a bug). Would you put this inside process_market_book() or is there a better place for it?



Also, I save the exposures per runner/market_id when the Strategy stops, so I could continue from there if I need to restart it.

*Tags: Errors Debugging, Strategies*

---

**Jorge** - *11:07:46*

This is what I have tried, but got an error: `raise RuntimeError('cannot schedule new futures after shutdown')`



```    framework.add_strategy(strategy)



    try:

        framework.run()



    finally:

        log_error('Exiting. Canceling all open orders and saving state of the bot...')

        for market in framework.markets:

            for order in market.blotter:

                if order.status == OrderStatus.EXECUTABLE:

                    log_error('Canceling order {}', order)

                    market.cancel_order(order) ```

*Tags: Errors Debugging, Strategies*

---

**Jorge** - *11:21:03*

Ah, I found the [https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py#L154C13-L154C13|finish(self, flumine)](https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py#L154C13-L154C13|finish(self, flumine)) function, so disregard my question :smile: (maybe worth to add it to the FAQ?)

*Tags: Strategies*

---

**liam** - *11:22:07*

[https://github.com/betcode-org/faq/pulls](https://github.com/betcode-org/faq/pulls)

*Tags: General Technical*

---

**Jorge** - *13:14:32*

I'm trying to keep the state of my flumine Strategy after a restart. I'd like market.blotter.get_exposures(self, lookup=selection_lookup) to return exposures considering the previous Strategy. Is there any way of doing this inside flumine? Maybe saving market.blotter._orders and overwriting it after the restart?

*Tags: Strategies*

---

**Jorge** - *13:31:19*

Is it this one [https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|orders.py](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|orders.py) ? Will try it

*Tags: General Technical*

---

**Jorge** - *14:09:54*

Seems to be working. Does it only add orders from strategies with the same customer_strategy_ref as config.customer_strategy_ref?

*Tags: Strategies*

---

**Y B** - *15:17:37*

Could somebody please help with a n00b question regarding historical football market data: why are there multiple unique MarketStartTime within a single event?

*Tags: General Technical*

---

## 2023-09-05

**Mo** - *16:47:19*

Sounds like you are trying to compare the state of the cache with one streaming update?

*Tags: General Technical*

---

**Mo** - *17:02:17*

Yeah but what I mean is, look at what you have for the previous timestamp then work out what you should get if you apply the streaming update in the line you shared

*Tags: General Technical*

---

**John Foley** - *17:11:36*

the streaming update in the line i shared - those zeroes mean there’s no longer any liquidity available for any of the prices in those tuples, right? as opposed to “no update”

*Tags: General Technical*

---

**Mo** - *17:21:39*

Obviously (?) betfairutil is just using betfairlightweight to read a series of market books; prices_file_to_data_frame is concatenating a series of data frames, one for each market book. Technically there are two market books with that publish time but one contains no prices so you're not seeing that one, you're seeing the previous one

*Tags: General Technical*

---

**sartux** - *18:50:58*

Hello guys, I have a problem with cashing out through betfairlightweight.



When I need to close a position, I execute an opposite entry calculating the size in relation to the price.



If I make a request to see the open positions, I get 2 positions instead of 0. How can I only see the positions that are actually still open, discarding those where I carried out the cash out?

*Tags: General Technical*

---

## 2023-09-06

**Jorge** - *07:09:41*

Done [https://github.com/betcode-org/faq/pull/3](https://github.com/betcode-org/faq/pull/3) [@U9JHLMZB4](@U9JHLMZB4)

*Tags: General Technical*

---

**Steve** - *08:15:07*

Hi, I've been remotely hosting a python script for a little while. I use Azure which has been costing me circa $180AUD (90 Pound, $115USD) a month per virtual machine. I run each strategy I have on a different VM, and these costs have been prohibihiting me from trading some of my more marginally profitable strategies. Are other people paying something similar? And does anyone have some advice for how I can remotely host some python scipts (with access the certs that BF requires) in a cost effcient manner?

*Tags: Strategies*

---

**liam** - *08:16:36*

I ran on AWS EC2 for about £5 a month for a few years, what instance type are you using / why?

*Tags: Deployment*

---

**Peter** - *08:22:25*

Same approach as [@U4H19D1D2](@U4H19D1D2). I have a small collection of AWS EC2 instances costing me about £5 each mainly based around different sports. My main spend is on RDS (hosted database) not the VMs running the strategies.

*Tags: Deployment*

---

**Trex44** - *08:38:06*

I use AWS EC2 as well. 3 of the micro instances, one for market recording, one for live strats and another for experimenting. Median daily cost is $2.23 a day and the instances account for just over half that. My main spend each month is on SageMaker and Athena which can easily add another $20 daily depending on what I am doing.

*Tags: Deployment*

---

**D C** - *09:05:39*

I hear you [@U03N4QBJ0TV](@U03N4QBJ0TV) . I've heard plenty of similar but it is hard to separate signal from noise with these stories. I read something about a well known punter  in the horse game on the BA forum falling victim, but in this story it is important to note that he REQUESTED his account be closed because he had no intention of providing financial details. We need to know real detail of what will/will not happen if we fail AC before even thinking about supplying such data.

This is part of the problem though - there are no "rules" as such so bookies are just doing these things in a seemingly haphazard and inconistent manner. Combine that with the fact that 95% of social media as absolute nonsense and the picture becomes even more fuzzy. I've still yet to actually converse with ANYONE directly who has been asked to provide details for an affordability check - everything is a third party account from questionable sources.

I was deeply concerned for a while and it still is worrying but I've given up worrying about it too much now because there are just too many unknowns.

*Tags: General Technical*

---

**Michael** - *09:35:06*

I don't think there's much cause for concern about any of us falling foul of the checks because of not having other income, all reference I've seen to this talks about net losses rather than just X amount per week, account managers will help with any snags, it'll be fine.



I imagine some less consistent players might have to manage their funds carefully, keeping plenty of float to avoid paying in or something like that. No biggy.



The effect on liquidity is unknowable, obviously the purpose of the legislation is to reduce the money wagered, but on the other hand it might prove a barrier to entry for new automated players and I love those.



For my own part I'd rather not be taking anyone's last penny so I'm all for affordability checks and if it reduces my income I'll suck it up.

*Tags: General Technical*

---

**Mo** - *09:36:13*

On what basis did you decide:



1. You needed one VM per strategy

2. You needed that particular type of VM (which must be quite well resourced to be costing you that much per month)

*Tags: Strategies*

---

**D C** - *09:47:38*

I hope you are right Michael. From memory, the white paper states a non-invasive credit check would be made on as little as a £150 loss over 24 hours or £1K across 30 days (might have those figures wrong I am going from memory).

I share your sentiment about not taking people's last penny but people with a problem (the people that these changes are supposed to protect) will still gamble. I say that as someone who was once very close to having a serious gambling problem, who even took out payday loans at times to bet, and was a cigarette paper's width from having to get an IVA due to using credit cards to gamble. Based on my own betting patterns I know for a fact that you can spot a certain type of problem gambler just from deposit + bet patterns. Bookies and casino's have known this for ages (if not they are criminally stupid) but were too greedy for too long. I still find it unbelievable that these morons at the GC think their proposed measure will do anything other than relocate the problem.

*Tags: Performance, Strategies*

---

**sartux** - *10:27:56*

where can i find the blotter class documentation?



is it included at betfairlightweight?

*Tags: General Technical*

---

**George** - *10:29:34*

are you using flumine?

*Tags: General Technical*

---

**sartux** - *10:30:15*

i use only betfairlightweight from pip

*Tags: General Technical*

---

**George** - *10:33:23*

probably best to use [https://github.com/betcode-org/flumine](https://github.com/betcode-org/flumine) check it out

*Tags: General Technical*

---

**PeterLe** - *10:36:30*

Check out AWS lightsail too, not quite as many functions as EC2 but good enough and much cheaper 

*Tags: Deployment*

---

**AI Trader** - *12:07:35*

[@U4H19D1D2](@U4H19D1D2), does bflw allows the creation of a historical stream using multiple files? To simulate the strategy running in multiple markets? It seems like the function create_historical_stream only takes a file_path, not multiple as input. Thanks again

*Tags: Strategies*

---

**Mo** - *12:08:46*

Doesn't seem like a responsibility for betfairlightweight

*Tags: General Technical*

---

**liam** - *12:09:08*

Flumine does and it’s awesome 

*Tags: General Technical*

---

## 2023-09-08

**Alejandro Pablos Sánchez** - *12:55:20*

Hello everyone ! I'm working on my project and I got a couple of questions. First, before placing an order for the MATCH_ODDS market for instance, I like to check the trading price for a given runner. However, I do this via the _last_price_traded_ attribute of the corresponding runner, and I think that I'm getting its corresponding LAY price instead of the BACK one, which is the one I would like to get. How could I get the BACK one? Any clue? 



And another thing, say I wanna bet a selection with size 20€ at a price of 1.70€ but there are 15€ available to match at higher price (1.75€ for instance) and 10€ at 1.72€. Is there any way to set the matching process at the highest available possible and hence increase the initially planned price?  



Hope I made myself clear enough. Thanks for your help :innocent: !

*Tags: Strategies*

---

**liam** - *13:25:06*

And BPE is the default, are you using flumine?

*Tags: General Technical*

---

**Alejandro Pablos Sánchez** - *13:52:42*

No, I am using betfairlightweight

*Tags: General Technical*

---

## 2023-09-09

**Mo** - *08:38:23*

Are you aware that flumine does queue positioning? If so, do you think there's something wrong with the way it's implemented?

*Tags: General Technical*

---

**Steve** - *13:25:20*

I'm using D3_v2 through Azure batch. It has 32gb of memeory which I think is what is killing me. Sounds like I need to be checking out AWS.

*Tags: Deployment*

---

**Steve** - *13:26:58*

Now just need to figure out how to hook AWS up to pycharm.

*Tags: Deployment*

---

**Steve** - *13:28:31*

Memory charges on azure are also a bit steep. Will start shopping around for a better database too.

*Tags: Performance*

---

**liam** - *19:48:57*

32gb? I average about 100mb per strategy

*Tags: Strategies*

---

## 2023-09-10

**James** - *08:47:07*

I’ve read a few posts that suggest you can buy betfair data cheaper by going to bdp directly. I’ve started recording my own data but there’s a lead time to build up a decent database. Is my understanding correct? If so, any idea of the cost? Pro data for uk and aus dogs and horses is what I’m interested in. 

*Tags: General Technical*

---

**Jonjonjon** - *11:03:07*

Maybe a dumb question, but what sort of products on Binance expire at short, frequent (daily or less) intervals.

*Tags: General Technical*

---

**Mo** - *12:53:31*

Like you say, for one thing there is a question of discipline



Not everyone will have the skills, knowledge and discipline to avoid blowing up on their way to profitability. Perhaps some have the discipline but it takes them a long time and sizeable losses that we get to enjoy before they get there



Perhaps the backtest looks good but they don't understand market capacity so they give a lot back when they try to scale up



Perhaps they go live too quickly, lose £50 and never come back. All we need in that case is enough churn



Without putting too fine a point on it, you want to give them enough rope to hang themselves with

*Tags: Deployment*

---

**Dave** - *13:31:51*

Yep, fully agree with your final point - making it free certainly drops the barrier somewhat and may even encourage someone who otherwise may not take the leap due to  lack of representative data. Oh..and making the historical data portal more robust and responsive..

*Tags: Data Quality*

---

## 2023-09-11

**D C** - *09:18:36*

Yeah makes no sense at all. The feed as it was offered must still exist and presumably is how Betfair access it (emphasis on presumably). I can understand them offering a price tiered subscription model integrated with OTS trading software but removing the option for the raw feed at the same time sounds like throwing money away.

*Tags: Strategies*

---

**D C** - *09:30:22*

Yeah charging for the API was something I couldn't believe when I first heard about it. Feels like pure greed - same with charging for historical data. I can't understand creating obstacles to automation like that - I remember when I first started dabbling with automation years ago I would lose hundreds of quid on a new strategy while ironing out the bugs. Probably says more about my slapdash approach to things back then but I can't be the only one who has lost more than £299 learning the hard way along the way.

*Tags: Data Quality, Errors Debugging, Strategies*

---

**PeterLe** - *10:04:04*

Morning, just on TPD, would anyone be willing to share a basic example of how to incorporate the feed into flumine please? Thanks

*Tags: General Technical*

---

**liam** - *10:42:09*

[https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py](https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py)

*Tags: General Technical*

---

## 2023-09-12

**Kishore Kumar** - *16:55:28*

Hi I am new to Betfair APIs and flumine. I am trying to get list of events for today and using below. From response I am not getting total available liquidity for that event. Is there any parameter I need to pass to get that , or I need to get list_market_catalogue and sum all?

`trading.betting.list_events({"eventTypeIds":[2]})`

*Tags: Getting Started, Strategies*

---

**D C** - *17:05:44*

Have you looked at the betfair API page for the listEvents operation? If not here it is below - you can see from this exactly what the operation returns



[https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listEvents](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/listEvents)

*Tags: General Technical*

---

**D C** - *17:18:55*

You'll need listMarketBook then, or better still request stream API access (assuming you don't already have streaming access).

*Tags: General Technical*

---

**Alejandro Pablos Sánchez** - *21:46:26*

Hi everyone. I'm struggling to get ex and sp attributes but they are None all the time. Anyone knows how to properly get them to later find the prices and stakes for both back and lay sides for the different runners in a market?

*Tags: General Technical*

---

**Mo** - *22:06:17*

Or use the filters instead of a raw dictionary, as per the examples: [https://github.com/betcode-org/betfair/blob/0aadd60ab2c2dcebeedfbc8b661bd257aec46435/examples/exampleone.py#L56](https://github.com/betcode-org/betfair/blob/0aadd60ab2c2dcebeedfbc8b661bd257aec46435/examples/exampleone.py#L56)

*Tags: General Technical*

---

**Alejandro Pablos Sánchez** - *22:33:25*

Thanks a lot [@UBS7QANF3](@UBS7QANF3) , I'll check that around and see it it works. You're always so helpful :innocent:

*Tags: General Technical*

---

## 2023-09-13

**rob smith** - *14:46:07*

This is a basic question but I'm a self-taught beginner and can't work it out. I moved my scripts to a vps and they worked as normal for the first few days before returning a "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host" error at login. Betfair's support have asked me for the json request and the response. This is where I'm stuck. I'm using trading.login_interactive() to login. How do I get the json request/response? Thanks

*Tags: Getting Started, Errors Debugging, Strategies*

---

**liam** - *14:52:32*

Add this to the top of your script and you will get lots



```import logging

from http.client import HTTPConnection



HTTPConnection.debuglevel = 1

logging.basicConfig()

logging.getLogger().setLevel(logging.DEBUG)

requests_log = logging.getLogger("requests.packages.urllib3")

requests_log.setLevel(logging.DEBUG)```



*Tags: Errors Debugging*

---

**Unknown** - *23:56:39*

I'm using flumine loggingControl to log live orders to CSV. Attached is the main lump of my code minus [http://logging.info|logging.info](http://logging.info|logging.info)() calls. The main idea is to add BSP's to my CSV data for preplay strategies. The CSV file is missing most (but not all) of the markets I participated in and looking at my logs, it seems that there is some sort of race condition between `_process_cleared_orders_meta()` and `_process_closed_market()`. The weird thing is that my variable `self.csv_data` gets populated as expected via `_process_cleared_orders_meta(),` _however_ the `_process_closed_market()` call often fails to find the market_id in `self.csv_data` and I can see that there are currently 30+ markets sat in there waiting to be written to CSV long after the markets have closed. I suspect that closed_orders is getting called before cleared_orders_meta.



Can someone please explain:

a) the difference between `_process_cleared_orders()` and  `_process_cleared_orders_meta()`

b) if `_process_closed_market()` is the only event in loggingControl that exposes the marketBook/BSP

c) if/why my logic is flawed

*Tags: Deployment*

---

## 2023-09-14

**foxwood** - *17:04:24*

I raised something similar recently - basically some matched orders are not showing as `EXECUTION_COMPLETE` in `process_closed_market()` despite the trade in the log showing the bet was immediately logged with zero outstanding size - but status still sitting at `EXECUTABLE`. This shows in the info log before the market is closed so somehow the match status was known at that time. The info log of the bet changing status appears after the market is logged as closed. This only started happening from early August when I updated flumine to 2.4.1 - prior to that I was running some earlier version (not sure which) that dated back to early 22 - never had the problem prior to that update.

*Tags: General Technical*

---

**birchy** - *17:36:56*

Interesting, because I've been using the same logging control for a couple of years but with separate bsp information. It's only this week that I updated flumine and then decided to update my logging control to include the bsp data. Based on what [@U4H19D1D2](@U4H19D1D2) said above, I'm planning to build 2 lookup dicts - one for my orders and one for BSP's and then amalgamate and write the CSV once I have both for each market. The only issue I can foresee is that I'll need to avoid a memory leak due to having BSP's in my lookup for markets on which I had no orders but I can probably manage that with a TTL or orders/market check.

*Tags: Performance*

---

**foxwood** - *18:54:54*

Unless BF have changed something recently it could be a timing / threading / priority issue. Do you know what version of flumine you were using before - doesn't seem to be a way of finding out with PyCharm as far as I can tell - most I can find is a reference to version 1.21.4 in another project so probably a lot changed since then.

*Tags: General Technical*

---

**birchy** - *19:00:28*

I last updated production flumine about 6 months ago, so probably version ~2.3.5

*Tags: Deployment*

---

**birchy** - *19:01:24*

[https://github.com/betcode-org/flumine/blob/master/HISTORY.rst|https://github.com/betcode-org/flumine/blob/master/HISTORY.rst](https://github.com/betcode-org/flumine/blob/master/HISTORY.rst|https://github.com/betcode-org/flumine/blob/master/HISTORY.rst)

*Tags: General Technical*

---

**AndyL** - *19:02:53*

I approached this just the other week:

logging control -&gt; runner.csv

market close BSP -&gt; bsp.csv

Then merged with Pandas

*Tags: Feature Engineering*

---

## 2023-09-15

**Andy Bason** - *09:50:29*

What's the easiest way to get your liability per runner using bflw? I've been trying `price_size` from `list_current_orders` but I think I must be missing something obvious given that this must be a common requirement. Cheers

*Tags: General Technical*

---

**liam** - *10:22:47*

If using [https://github.com/betcode-org/flumine|flumine](https://github.com/betcode-org/flumine|flumine) :wink:

*Tags: General Technical*

---

**Andy Bason** - *10:26:05*

Thanks guys. I'm still cutting my teeth of bflw and haven't progressed to flumine yet

*Tags: General Technical*

---

**liam** - *10:26:36*

Just use flumine and let it do all the hard work

*Tags: General Technical*

---

**liam** - *10:33:45*

How come? / what would help?

*Tags: General Technical*

---

**Andy Bason** - *10:36:55*

The problem's with the user rather than flumine! I prefer to learn by getting stuck in and doing but need to step back and learn about basics like classes, etc

*Tags: General Technical*

---

**liam** - *10:37:45*

Perfect, flumine uses classes for your strategy creation, so get stuck in

*Tags: Strategies*

---

**Andy Bason** - *10:47:22*

It's a laying strategy which places multiple bets pre-off

*Tags: Strategies*

---

**George** - *10:48:59*

sure, but there are multiple different ways that you can define the concept of "liability". Flumine might give you a different answer than the one you wanted. Just a heads up.

*Tags: General Technical*

---

## 2023-09-17

**AI Trader** - *06:04:26*

Guys, has anyone had issues creating stream connections using betfairlightweight in EC2 instances? I guess it's more an AWS question (sorry), but when I run my docker in AWS ECS Fargate, it works fine. Whenever I try to run the same container on ECS using an EC2 instance, I get a Timeout error (max retries exceeded). Error in the comments. I have tried to ssh into the instance and ping betfair server to make sure it wasn't some firewall or connectivity issue, but the ping works.

*Tags: Errors Debugging, Deployment*

---

**AI Trader** - *06:04:42*

Exception: HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x7fd25870c290&gt;, 'Connection to [http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com) timed out. (connect timeout=30)'))

Exception: HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x7fd25870c290&gt;, 'Connection to [http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com) timed out. (connect timeout=30)'))

*Tags: Errors Debugging*

---

**PeterLe** - *10:11:45*

Morning,

I noticed i had some STRATEGY_EXPOSURE errors  yesterday (This is a new strategy; I was expecting a lot more bets to be placed than it did)

I have a number of strategies all running on a single system...

Each one of which is run from the command line...

The code at the bottom of each strat is like so:



`strategy = Sub6(`

    `name="WIN",`

    `market_filter=betfairlightweight.filters.streaming_market_filter(`

        `event_type_ids=["7"],`

        `country_codes=["GB", "IE"],`

        `market_types=["WIN"],`

    `),`

    `max_order_exposure=50,`

    `max_selection_exposure=100,`

    `max_trade_count=5000,`

    `max_live_trade_count=8`

`)`

`framework.add_strategy(strategy)`



`framework.run()`



I can also see that we also have these settings in strategy.py too :



 `def __init__(`

        `self,`

        `market_filter: Union[dict, list],`

        `market_data_filter: dict = None,`

        `sports_data_filter: List[`

            `str`

        `] = None,  # 'raceSubscription', 'cricketSubscription'`

        `streaming_timeout: float = None,`

        `conflate_ms: int = None,`

        `stream_class: Type[BaseStream] = MarketStream,`

        `name: str = None,`

        `context: dict = None,`

        `max_selection_exposure: float = 15, # 15`

        `max_order_exposure: float = 2,`

        `max_trade_count: int = 1000,`

        `max_live_trade_count: int = 15,`

        `multi_order_trades: bool = False,`

    `):`



So my question is: Am I correct in setting the exposures in the individual strategy files and if so, do these setting take precedence/over ride over any other areas of code?

Thanks in advance

*Tags: Errors Debugging, Deployment, Strategies*

---

**Peter** - *12:27:40*

The exposures are evaluated against the strategy-market-runner-handicap combination so yes, it's correct to set them for the individual strategy.



They don't really over-ride other code. They're used as parameters by the trading controls when order placement is assessed.



I'd recommend looking at your logs, since each time they prevent an order being placed an order violation with explanatory message is logged.

*Tags: Strategies*

---

**Andy B** - *12:52:52*

Hi All, I have a question about whether it is possible to place a PLACE bet, while placing a WIN bet.  The scenario I am trying to figure out is a strategy where the criteria for a WIN bet is fulfilled and the bet is placed, but I also then want to place a PLACE bet.  I know the market_id values are different, but if I know the market_id and selection_id for the PLACE market, is it possible to return the runner.ex.available_to_back[0]['price'] value from the PLACE market while still processing the WIN market?  I only want to place a PLACE bet if I have successfully placed a MarketOnClose bet for the WIN.

Sorry if that seems garbled, I do suspect I am not thinking about the right way to go about this, but I have been going in circles all day trying to figure out the right execution strategy.

*Tags: Strategies*

---

**Alejandro Pablos Sánchez** - *13:00:49*

Hello all ! Hope you're spending a nice Sunday :slightly_smiling_face: . I'm studying about the bet matching process in general and I came up with the following question: Suppose I wanna bet 35$ to Over 2.5 goals to a given match in the betfair exchange, to a football  match. It turns out that there are 20$ to match at a price of 1.4$ and 15$ at 1.35$. How would the matching process take place in this situation?. Can this process be controlled by us somehow with certain parameters? Or it depends completely on the exchange internal working? Would love if anyone could provide a detailed explanation on this or point out some resource where all this stuff is explained. Thanks a lot :wink: .

*Tags: General Technical*

---

**George** - *13:11:13*

sounds like you simply want to use limit orders.

but first, you should define specifically what you want to do. then after that we can help you understand how to achieve it.

*Tags: General Technical*

---

**liam** - *13:16:47*

Flumine user?



`Market.event["PLACE"]`

*Tags: General Technical*

---

**Andy B** - *13:34:28*

Thanks Liam, yes I am a flumine user (finally!).  I'll give this a crack.

*Tags: General Technical*

---

**Riccardo Fresi** - *16:12:57*

is there in flumine some sample of recording both price and live stats?

*Tags: Deployment*

---

**Peter** - *17:07:58*

It's done very commonly, but there's not a neat example of it as it depends on how the live stats are being consumed. However, you have the [https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py|market recorder example](https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py|market recorder example) for prices and for live event data the [https://github.com/betcode-org/flumine/blob/master/examples/tennisexample.py|tennis example](https://github.com/betcode-org/flumine/blob/master/examples/tennisexample.py|tennis example) shows how you can poll an external API, while the [https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py|sportsdata example](https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py|sportsdata example) shows how to handle streamed event data if you have access to that.

*Tags: Data Quality, Deployment*

---

**Riccardo Fresi** - *18:13:50*

thanks!

i runned the market recorded, even if i need to understand better the output json (not so clear to me)

tha sample for event live event does not save anything to local file right? i have to implement that in the strategy?

is it better to run different script with different strategies or the tool foreseen to run different strategies' if yes, how?

*Tags: Deployment, Strategies*

---

## 2023-09-18

**Guy Incognito** - *03:43:28*

Can betfairlightweight or flumine check the matched bets that I have placed on the website but haven't been settled yet?

*Tags: General Technical*

---

**Peter** - *13:34:26*

The market recorder provides the changes to the market only in a raw format. To see the actual state of the market you would run this through Flumine which creates and maintains an internal record of the state of the market as it would be after each update and makes it available to your trading strategies.



No the sample for the live event doesn't save the data to a local file. But once you have the data and have decided in which data and in what format, writing it out to a file is the easy bit. One tip for doing that would be to ensure that you record the time that the data is received, so that you can match that with the market data later.



It's easier to run separate scripts to collect event and market data. especially as the market recorder is already written for you. But there will come a point at which you want to be able to collect both together in order to make trading decisions and place bets. So it's worth working out how to do that early. If you're looking at external sources of data, you'll want to look at how Flumine supports works and middleware. These can be very useful for polling external data sources and merging event data with market data (via the market.context attribute) and feeding it to your trading strategies. There's not an easy way to answer the "how?" part of this as it depends a lot on how your accessing event data and how you plan to use it in your trading strategies.

*Tags: Data Quality, Deployment, Strategies*

---

**Peter** - *13:41:39*

Yes.



Betfairlightweight supports the Betfair API's listCurrentOrders, which delivers all orders (with a 1000 per page limit) irrespective of how you placed them. So your website bets can be filtered based on the absence of customerStrategyRef.



Flumine's `market.blotter.orders` will return all orders (including those placed on the website) for the specific market for which you're receiving an update. Note though that the orders delivered to a strategy's `process_orders` method, will be limited to orders for that specific strategy and so would not include your website orders.

*Tags: Deployment, Strategies*

---

## 2023-09-19

**Guy Incognito** - *07:27:38*

Is there some way I can view the total traded volume of a market that has settled. I tried with betfairlightweight and I always get total volume equal to zero when asking for the market book

*Tags: General Technical*

---

**Mo** - *07:35:08*

[https://github.com/mberk/betfairutil/blob/18dcd187b5cab8fa4be25694fb64e8045bf3214e/betfairutil/__init__.py#L2178](https://github.com/mberk/betfairutil/blob/18dcd187b5cab8fa4be25694fb64e8045bf3214e/betfairutil/__init__.py#L2178)



&gt;  Working out the total volume traded on a market is surprisingly tricky given Betfair's shenanigans around the market

&gt;     closure. Specifically, the last few updates in the price stream result in data getting zeroed out which means if you

&gt;     were to just look at the last available market book it would look like the total volume traded was zero. In fact, it

&gt;     appears there is no consistency as to how many price stream updates are involved in this zeroing of data and so it's

&gt;     not as simple as having a rule such as "look at the second to last market book available".

&gt; 

&gt;     If memory usage is not an issue then the most robust method would be to read the entire set of market books into

&gt;     memory then iterate them in reverse order looking for the first market book that has a non-zero total volume traded.

&gt;     However, in many use cases memory usage is a significant concern. For example, if you want to process tens of

&gt;     thousands of markets and use parallel processing to speed this up then you end up needing to hold the entire set of

&gt;     market books for multiple markets in memory simultaneously and, depending on the number of cores your machine has

&gt;     and the number of parallel processes you use, it is easy to exhaust the total memory available.

&gt; 

&gt;     The solution presented here is to use a deque

&gt;     ([https://docs.python.org/3/library/collections.html#collections.deque](https://docs.python.org/3/library/collections.html#collections.deque)). Think of this as a limited length list

&gt;     where when a new item is appended to the list, another item is removed for the start of the list. This is used when

&gt;     reading the prices file to leave us with the X last market books which can then be iterated in reverse order to find

&gt;     the last one which has a non-zero volume traded. Based on my own analysis of prices files I have settled on a

&gt;     default maximum length of the deque of 8 - i.e. at most this function will store 8 market books in memory at any one

&gt;     time. This is a trade-off between giving correct results and drastically cutting down on memory usage versus reading

&gt;     the entire set of market books into memory. If memory usage is not of concern to you then you can simply set the

&gt;     deque_len argument to None and the function will read the entire set of market books into memory ensuring

&gt;     correctness. If you observe any prices files where a deque of length 8 doesn't give correct results then please

&gt;     create a GitHub issue here: [https://github.com/mberk/betfairutil/issues](https://github.com/mberk/betfairutil/issues) so I can investigate.

*Tags: Performance*

---

**Andy B** - *12:46:38*

I haven't been able to figure this out, so looking for a bit of assistance if possible.  My existing strategy looks like:



class MyStrat (BaseStrategy):



    def start(self) -&gt; None:

        print("starting strategy")



    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:

        # process_market_book only executed if this returns True

        if market_book.status != "CLOSED":

            return True



    # If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially

    # After the first inital time process_market_book has been run, every single time the market ticks, process_market_book will run again

    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:



        if market is not None:

            if market is not None and market.market_catalogue is not None and market.market_catalogue.market_name is not None:

                RaceNumber = market.market_catalogue.market_name.split(' ')[0].replace("R","").strip()



        if market.seconds_to_start &lt;= 5 and market_book.inplay == False:

            TrackName = market.venue.lower()

            for runner in market_book.runners:

                RunnerName = [x.runner_name for x in market.market_catalogue.runners if x.selection_id == runner.selection_id]

                RunnerName = RunnerName[0].split('. ')[1].lower()

                if runner.status == "ACTIVE" and runner.ex.available_to_back: # and runner.ex.available_to_lay[0]['price']:

                    filtered_row = pd.DataFrame()

                    if market.market_type == 'WIN':

                        filtered_row = GreysList[(GreysList['TrackName'] == TrackName) &amp;

                            (GreysList['RaceNumber'] == int(RaceNumber)) &amp;

                            (GreysList['DogName'] == RunnerName)]



                    if not filtered_row.empty or market.market_type == 'PLACE':

                        if not filtered_row.empty:

                            runner_odds = filtered_row['ODDS'].iloc[0]

                            current_price = runner.ex.available_to_back[0]['price']

                            bet_price = get_nearest_price(round((current_price),2))

                            if runner_odds &lt; current_price and runner_odds &gt; 0 and current_price &gt;=2.5 and current_price &lt;= 10 and market.market_type == 'WIN':

                                print(market.market_type)



                                if market.market_type == 'WIN':

                                    stake = round((3/current_price),2)

                                    if stake &lt; 0.1: stake = 0.1

                                    print(TrackName, RunnerName, runner_odds, current_price, stake)

                                    trade = Trade(

                                    market_id=market_book.market_id,

                                    selection_id=runner.selection_id,

                                    handicap=runner.handicap,

                                    strategy=self,

                                    )

                                    order = trade.create_order(

                                        side="BACK", order_type=MarketOnCloseOrder(liability=stake)

                                    )

                                    market.place_order(order)

									

									# At this point, I want to also make a place bet but I need to get the value of runner.ex.available_to_back[0]['price'] from the PLACE market.

									# At the start of my script, I get a list of WIN and PLACE markets and store them in a dataframe called TodaysRaces.

									# I can locate the PLACE market id by querying this dataframe, but I don't know how to utilise Market.event["PLACE"] to get the value of runner.ex.available_to_back[0]['price'], so that I can set up the trade and order to place the bet.

									# Can someone point me in the right direction please?

*Tags: Feature Engineering, Strategies*

---

## 2023-09-20

**Mona** - *18:32:23*

It is weird that I get high latency between current time and MarketBook publish time with my newly subscribed LIVE_APP_KEY.

*Tags: Performance, Deployment*

---

**liam** - *20:37:27*

Well it sounds like that is all very slow.



If you are recording data best practice is to record the raw streaming data for simulation after, are you not using the market recorder?

*Tags: Data Quality, Performance*

---

**Mona** - *20:40:47*

I wanted to record this "structured data" in a file that is easy for me to debug. I am not using market recorder, can it be used to record processed data&gt;?

*Tags: Data Quality, Errors Debugging*

---

**Mona** - *21:05:43*

sorry, I am a bit new to this, do you mean you still think the problem is the processing of the data? I will change it to use marketrecorder eventually as you suggested, but still can't understand, is it because the DELAY_APP_KEY we receive much less market books hence the time difference eventually build up is not that significant?

*Tags: Getting Started*

---

**Mona** - *21:07:55*

I see, if I add another strategy like the pricerecorder in your example that process the data and store it in a csv, would it uses a different thread and solve the problem?

*Tags: Strategies*

---

## 2023-09-21

**liam** - *08:36:35*

No, all strategy execution is within the main thread, have you profiled your code? You must be doing something very inefficient

*Tags: Strategies*

---

**Unknown** - *13:34:05*

Am I reading this correctly? On any MarketBook event, Flumine will call all strategies, irrespective of whether that strategy is subscribed to that particular market? So if I have one strategy subscribed to football markets and one subscribed to horse racing, both of them will get called on both football and horse racing market updates?

*Tags: Strategies*

---

**Emze_93** - *14:10:07*

Is anyone experiencing Flumine not handling market book event at all? All seemed fine this morning until this very moment. handle_output in OrderStream seems to be hanging in a forever loop with no exit. Weird enough as market to address are open now.

*Tags: General Technical*

---

**liam** - *14:12:47*

No it will check if the strategy is subscribed to that particular stream

*Tags: Strategies*

---

**mzaja** - *14:41:33*

Thanks, I see where I went wrong now. It calls `strategy.check_market` , not `check_market_book` , which does what I'd expect it does.

*Tags: Strategies*

---

**Emze_93** - *15:13:19*

Not using orjson.



Digged in a little and it seems there are no open markets.



the handle_output method from OrderStream class in orderstream.py when calling self.flumine.markets simply does not return any open market. The filer I am using allows a huge availability at the moment (horse racing in UK and Ireland).



Is it a limit I am given from using delayed API keys? I have no clue of why I am seeing an empty market list.

*Tags: General Technical*

---

**Mona** - *15:17:59*

Hi thanks, I haven't profiled it yet, but did a simple profiling on `process_market_book` function after I removed the writing of the structure data into csv, the whole function takes less than tenths of a second to execute, the high latency issue still persists, is this still not efficient enough

*Tags: Performance*

---

**liam** - *16:53:09*

All delayed key questions and use is banned here, it’s a huge waste of everyone’s time 

*Tags: General Technical*

---

## 2023-09-22

**casper** - *09:10:18*

Getting quite a few Status Code Error 502 lately and some “Connection aborted” messages when using trading.betting endpoints. Anyone experiencing similar things? Yesterday had particularly a lot of those.

*Tags: Errors Debugging, Strategies*

---

**D C** - *10:04:47*

I had between 10 and 20 on certain API operations yesterday and quite a few "Connection reset by peer" errors. That said I am testing out a new codebase and the latter could be down to me. Previous code bases don't clock the HTTP result codes so I don't know how common the 502s are in general to compare it with.

*Tags: Errors Debugging*

---

## 2023-09-23

**Trex44** - *15:05:24*

Hey, just wondering. It it common to get Latency High warning such as the ones below.

```{"asctime": "2023-09-23 14:03:32,716", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.9683074951171875"}

{"asctime": "2023-09-23 14:03:32,720", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.864220142364502"}

{"asctime": "2023-09-23 14:03:32,725", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.8646314144134521"}

{"asctime": "2023-09-23 14:03:32,741", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.6966798305511475"}

{"asctime": "2023-09-23 14:03:32,743", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.6270527839660645"}

{"asctime": "2023-09-23 14:03:32,748", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.5797228813171387"}

{"asctime": "2023-09-23 14:03:32,759", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.5898082256317139"}```

*Tags: Performance*

---

**Peter** - *15:07:58*

This is [https://betcode-org.github.io/faq/deploying-strategies/#problems|covered in the FAQ](https://betcode-org.github.io/faq/deploying-strategies/#problems|covered in the FAQ).

*Tags: Deployment*

---

## 2023-09-24

**Rishab** - *00:05:08*

Hey! In the flumine backtest I'm getting MAX_TRANSACTION_COUNT Error, was trying to figure out how is 1 hour considered in backtest for total no. of transactions?(Lets say 50 races are backtested within an hour , would all the trades in this time period be considered as trades in 1 hour?)

*Tags: Errors Debugging*

---

**Unknown** - *11:33:39*

I came across this library which may be of interest:

[https://github.com/TotalPerformanceData/gmaxfeed/](https://github.com/TotalPerformanceData/gmaxfeed/)



I haven't used it, but it appears to be what you are looking for i.e access to the raw data with a TPD licence. Has anyone else used this?

*Tags: Performance*

---

**TT** - *11:36:55*

I've not delved into the code too much but I saw this:

[https://github.com/TotalPerformanceData/gmaxfeed/blob/87e58cf32fcb2e2c3ea147dcdeaa15fbd4f96091/gmaxfeed/feeds/record_live.py](https://github.com/TotalPerformanceData/gmaxfeed/blob/87e58cf32fcb2e2c3ea147dcdeaa15fbd4f96091/gmaxfeed/feeds/record_live.py)

*Tags: Performance, Deployment*

---

**TT** - *11:48:06*

Yeah, I seem to recall discussions about historic data not matching what you receive live. So without access to TPD through betfair, recording the stream yourself and integrating into flumine with middleware seems the only option.



Do you know what the requirements are for getting a TPD feed through betfair are? Is it for PC customers only?

*Tags: Deployment*

---

**Justice** - *12:02:27*

[@UBS7QANF3](@UBS7QANF3) Really? I did not know that. I wanted to mess around and train a reinforcement learning model using Betfair price data and TPD tracking data, but guess that's useless if that's true. Do you know why that is the case? That's very poor. But tbh, my whole experience with the TPD (albeit several years ago) was very poor in general

*Tags: Strategies*

---

**Justice** - *12:48:56*

[@UBS7QANF3](@UBS7QANF3) Very annoying. I personally found the data to be extremely inaccurate. I developed a piece of software for a client that used the data, back when it was only available as raw data.  The data was all over the place and TPD were evasive and condescending when I questioned the accuracy of the data. We decided to cut our losses on the project

*Tags: General Technical*

---

**Justice** - *12:52:41*

[@UBS7QANF3](@UBS7QANF3) I sympathise with that. I created a machine learning model that had the racing post and timeform ratings as a feature. Back testing showed huge profits. Thankfully, I thought better of it. After looking into it, turns out these ratings are revised retrospectively, causing data to leak into the model

*Tags: Feature Engineering, Strategies*

---

## 2023-09-25

**D C** - *07:44:49*

When I started I wrote a GUI app with a price grid trading tool with the view to using it as a visual aid to manually trading. In retrospect this was rather stupid for a variety of reasons, but I am actually quite glad I started off like this because watching the rendered GPS in real time made it very clear what kind of "surprises" the data can throw at you. Especially when watching a race replay after and comparing with the GPS visuals. I don't want to criticize though  - there is still plenty of information there despite the errors.

*Tags: Errors Debugging, Strategies*

---

**Jorge** - *13:44:06*

Hi, looks like the [https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py#L10|OrdersMiddleware](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py#L10|OrdersMiddleware) has a bug because it is not filtering by market.market_id (in [https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py#L23|list_current_orders](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py#L23|list_current_orders) ) when processing orders. Is anyone else using it?



This is the code I use to add the Middleware:

```....



framework.add_strategy(strategy)

framework.add_market_middleware(RestoreOrdersMiddleware(framework))

framework.run()```

*Tags: Errors Debugging, Strategies*

---

**Beeblebrox** - *15:12:30*

In my local version of OrdersMiddleware I have a filter for the market id. According to my git history I added that in a day after I'd copied the example, so I must have found the same issue as you, but I never pushed it back to the flumine repo.

*Tags: General Technical*

---

## 2023-09-26

**Mona** - *16:16:25*

Hi if I set the `conflat_ms` in strategy initialization, does that apply to `MarketStream`  only or does it also apply to its `OrderStream`. is there a way to set different `conflat_ms` values in `MarketStream` and `OrderStream`?

*Tags: Strategies*

---

**Trex44** - *17:38:13*

Thanks [@U9JHLMZB4](@U9JHLMZB4). I am running the servers on an EC2 instance and I think this may have been related to CPU credit balance running out.

*Tags: Deployment*

---

## 2023-09-27

**Trex44** - *17:48:21*

Hey guys. Has anyone ever had an issue where the 'profit' column in their logs shows as 0 even though the bet was a win or a loss? The last 3 bets in the information below were all wins or losses on 1 pound bets . I have been using a StandardLoggingControl class for months with no issue. Its a modified version of the [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|BacktestLoggingControl](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|BacktestLoggingControl). It only started doing this yesterday so wondering if the issue actually lays with Betfair. Strangely the first bet of the day logged correctly.

```bet_id,strategy_name,market_id,selection_id,trade_id,date_time_placed,price,price_matched,size,size_matched,profit,side,elapsed_seconds_executable,order_status,market_note,trade_notes,order_notes

322399496435,back_A,1.218753189,58855793,66405328-5c73-11ee-8cd1-b9568a354032,2023-09-26 13:48:40.490287,3.55,3.55,1,1,2.55,BACK,0.023866,Execution complete,"3.5,3.55,3.6",,"{'secs_to_start': 79.845224} 

22403591681,back_C,1.218753209,58928490,47d0dbec-5c78-11ee-8cd1-b9568a354032,2023-09-26 14:23:36.850423,5.2,5.2,1,1,0.0,BACK,13.330706,Execution complete,"5.1,5.2,5.1",,"{'secs_to_start': 83.423703}

22413152508,back_B,1.218753428,1224675,29a2fde9-5c83-11ee-8cd1-b9568a354032,2023-09-26 15:41:30.800658,3.45,3.45,1,1,0.0,BACK,0.9553,Execution complete,"3.4,3.45,3.4",,"{'secs_to_start': 209.332741} 

322414591274,back_A,1.218753244,18565272,bd43c7fd-5c84-11ee-8cd1-b9568a354032,2023-09-26 15:52:48.223042,2.78,2.78,1,1,0.0,BACK,11.308602,Execution complete,"2.76,2.78,2.76",,"{'secs_to_start': 132.004036} ```

*Tags: Strategies*

---

## 2023-09-29

**Andy B** - *03:50:37*

Hi All, I am trying to create a backtest based on the Aus PRO greyhound files from betfair.  I can see that the box\trap number is included in the bane value in the raw files, but I can't find a way to get it out in the back test.  In the live stream I would do something like:



Box = [x.runner_name for x in market.market_catalogue.runners if x.selection_id == runner.selection_id]

Box = Box[0].split('. ')[0]



As far as I can work out, there is no market_catalogue when using the PRO files to backtest, although maybe I have missed something logical.  I could provide the data I need from an alternative source, but I don't really want to do that because a:) it is very memory intensive to store large amounts of data in a multi-threaded backtest and b:) the data is in the raw files, so I should be able to access it.  I've searched through previous threads in here and I've looked at the betcode FAQ on github, and I am feeling like I may not be able to do this simply using flumine, but I am sureother people have found a way around this, so I am curious how it is being done.

*Tags: Performance, Deployment*

---

**Good Job** - *07:24:23*

Anyone ran in to missing historical data for certain events? Finding that even English premier league seems to have around a 100 games quite literally nowhere to be found on basic and where I can test, even on other tiers. Any ideas?

*Tags: Data Quality*

---

**liam** - *07:25:43*

Are you using flumine?

*Tags: General Technical*

---

**Andy B** - *08:06:40*

Yep, I'm a flumine convert.  I am missing something here as I can't see how to return it from the market.definition.

*Tags: General Technical*

---

**Andy B** - *08:17:20*

I tried debugging it by printing market.market_book.market_definition and I get:

&lt;betfairlightweight.resources.streamingresources.MarketDefinition object at 0x000001D6DA25CB80&gt;



So I can see that it is an object and VSCode tells me it is a variable.  If I wanted to get the market_id, I know that I can do print(market.market_book.market_id), but I have no clue how to process the market_definition values.

*Tags: Errors Debugging*

---

**Unknown** - *08:18:30*

What about when you use the debugger?

*Tags: Errors Debugging*

---

**liam** - *08:30:08*

Nice debugger can be a game changer when learning 

*Tags: Errors Debugging*

---

**Mo** - *13:24:10*

I’m not aware of such a resource but look under the “Discount Price Plans” heading here: [https://historicdata.betfair.com/#/home|https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home|https://historicdata.betfair.com/#/home) for an indication of which months have problems 

*Tags: General Technical*

---

## 2023-09-30

**Andy B** - *08:14:00*

I have this code working well in the live stream using flumine, but I can't get it going in a back test.  In my live code I use:

place_market = market.event["PLACE"]

place_market_book = place_market[0].market_book



So I initially tried using the same in a back test, but no dice.  I then tried it as:

place_market = market.market_book.market_definition.market_type["PLACE"]

place_market_book = place_market.market_book



But again it didn't work and it throws the error TypeError: string indices must be integers.



I tried looking at it in the debugger and both market.event and market.market_book.market_definition.market_type show the value as 'WIN', which is correct, and leads me to think that maybe the context of the place market either doesn't exist in the recorded data, or there is yet another way to access it.  Any thoughts?

*Tags: Errors Debugging, Deployment*

---

**liam** - *08:17:41*

You need to simulate using event processing so that both markets are available 



[https://betcode-org.github.io/flumine/quickstart/#event-processing|https://betcode-org.github.io/flumine/quickstart/#event-processing](https://betcode-org.github.io/flumine/quickstart/#event-processing|https://betcode-org.github.io/flumine/quickstart/#event-processing)

*Tags: General Technical*

---

**Peter** - *20:34:18*

I use vscode too and would be lost without the debugger. Highly recommended.

*Tags: Errors Debugging*

---

## 2023-10-01

**Andy B** - *09:06:07*

Thanks (again) Liam.  I'm still not getting the response I was expecting though.  My strategy looks like:



strategy = BoxPercSim(

        market_filter={

            "markets": markets,

            'market_types':['WIN','PLACE'],

            "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

            "event_processing": True

            },

    max_order_exposure= 50, # Max bet sizes of $50

    max_trade_count=1, # Max of trade/bet attempt per selection

    max_live_trade_count=1, # Max of 1 unmatched Bet per selection

    )



And then the rest of my code is:



place_market = market.event["PLACE"]

                            print(place_market)

                            for a in place_market:

                                print('event:', a.event, 'market_type:', a.market_type, 'place_market_type:', type(place_market))

                            place_market_book = place_market[0].market_book



I put that bit of debug in there to try and see what is being returned and I can see that place_market is indeed a flumine object as per my code in the live stream - &lt;flumine.markets.market.Market object at 0x000001B593B74AF0&gt;.



In the second part of the debug, I can see the output as:

event: defaultdict(&lt;class 'list'&gt;, {'WIN': [&lt;flumine.markets.market.Market object at 0x000001B592FECDF0&gt;], 'PLACE': [&lt;flumine.markets.market.Market object at 0x000001B593B74AF0&gt;]}) market_type: PLACE place_market_type: &lt;class 'list'&gt;



Using the vscode debugger, I can see that market.event = 'WIN" and the value of place_market is an empty list i.e. [].  Presumably I need to be getting the value for place_market from somewhere else, but looking through the debuger output in vscode isn't giving me the answers I expect.  I did wonder if the market I was accessing was only a WIN market, but it does this for every market when I let the back test run, so that can't be the answer.  It has to be the way I am trying to access it.

*Tags: Errors Debugging, Deployment, Strategies*

---

**Andy B** - *09:18:08*

I have it set up in flumine like this:

# Searches for all betfair data files within the folder sample_monthly_data_output

data_folder = 'Greyhounds\\Data'

data_files = os.listdir(data_folder,)

data_files = [f'{data_folder}/{path}' for path in data_files]



def run_process(markets):

    """Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data.



    Args:

        markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv

    """

    # Set Flumine to simulation mode

    client = clients.SimulatedClient()

    framework = FlumineSimulation(client=client)



    # Set parameters for our strategy

    # strategy = FlatIggyModel(

    strategy = BoxPercSim(

        market_filter={

            "markets": markets,

            'market_types':['WIN','PLACE'],

            "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

            "event_processing": True

            },

    max_order_exposure= 50, # Max bet sizes of $50

    max_trade_count=1, # Max of trade/bet attempt per selection

    max_live_trade_count=1, # Max of 1 unmatched Bet per selection

    )

    framework.add_strategy(strategy)



    framework.add_logging_control(

        BacktestLoggingControl()

    )

    framework.run()



# Multi processing

if __name__ == "__main__":

    all_markets = data_files  # All the markets we want to simulate

    processes = os.cpu_count()  # Returns the number of CPUs in the system.

    markets_per_process = 8   # 8 is optimal as it prevents data leakage.



    _process_jobs = []

    with futures.ProcessPoolExecutor(max_workers=processes) as p:

        # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have.

        chunk = min(

            markets_per_process, math.ceil(len(all_markets) / processes)

        )

        # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs

        for m in (utils.chunks(all_markets, chunk)):

            _process_jobs.append(

                p.submit(

                    run_process,

                    markets=m,

                )

            )

        for job in [http://futures.as|futures.as](http://futures.as|futures.as)_completed(_process_jobs):

            job.result()  # wait for result



So it should be populating the list of markets correctly I think.

*Tags: Deployment, Strategies*

---

**liam** - *09:19:21*

flumine is good but not that good :joy:

*Tags: General Technical*

---

**Andy B** - *09:30:39*

If I understand correctly, the problem is because I simply have all of the files dropped in one folder and flumine has no concept of where the next market is because it hasn't loaded all markets in the current thread?  I guess I have missed something in the doco that explains how I should set the markets up?

*Tags: General Technical*

---

**Andy B** - *09:41:46*

I had avoided that as I thought the memory overhead would be a problem, but I think you are right, it's possibly the simplest way forward.

*Tags: Performance*

---

**Good Job** - *15:48:34*

Also, to the best of my knowledge it is quite literally the best price data there is in most if not all terms, and some of it being missing is quite unfortunate. I have not come across a paper using said exchange data to test model  profitability, but I have seen quite a few using stuff like B365 etc...

*Tags: Strategies*

---

## 2023-10-02

**Felix Lucien** - *04:21:39*

Does the betfairlightweight exchange scraper handle outages?

*Tags: General Technical*

---

**liam** - *06:13:03*

What is the 'betfairlightweight exchange scraper'?

*Tags: General Technical*

---

**Felix Lucien** - *07:28:43*

[https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py) is what I'm talking about

*Tags: General Technical*

---

**liam** - *07:29:13*

That is flumine, which does handle outages 

*Tags: General Technical*

---

## 2023-10-03

**Troy Edwards** - *05:59:36*

Hey guys are there any JSON gurus out there?  I am scraping [http://TheGreyHoundRecorder.com.au|TheGreyHoundRecorder.com.au](http://TheGreyHoundRecorder.com.au|TheGreyHoundRecorder.com.au) for its results to store in my database ie [https://www.thegreyhoundrecorder.com.au/results/ballarat/223151/1](https://www.thegreyhoundrecorder.com.au/results/ballarat/223151/1).  I was scraping this successfully for about a year now but the website has changed it webcoding.   Now using [http://VB.NET|VB.NET](http://VB.NET|VB.NET) and the DOM (ie  functions like HtmlElement and HtmlElementCollections) I can get scrape the data but it takes me 10 individual request to get the 10 races from Ballarat AUS whereas all of the data for the 10 races is also shown in the webpage source as a JSON response.  There is also additional data in this JSON response that I might like to investigate - such as general commentary like this "MAN OF MAGIC (4) was narrowly defeated in a tougher maiden race last start and looks primed to break his maiden status".





If you view the source you can find the following JSON data such as  .....   "3.25","8.42","Q/11",29.3,[1917],{"code":1840,"__typename":1841},{"id":1919,"name":1920,"colour":1876,"sex":1877,"sire":1921,"dam":1922,"__typename":1880},"3846228","Man Of Magic","My Redeemer","Darley Park" which aligns with Starting Price, Split, Position in Run, Sire and Dam etc





It looks as if the website source might contain the original JSON request as well as the JSON response OR maybe I am just confused :slightly_smiling_face:  What I would like to do is setup some classes and then decode this webbrowser JSON string into the class in one go.





Otherwise are there any good JSON tools from the MS store OR even online tools that could help determine the start and end of the JSON strings.

*Tags: Getting Started*

---

**George** - *11:14:15*

Can I just ask how do people get the 'race number' in Flumine for AUS horse racing for PLACE (and other non-WIN) markets?



I think the WIN markets have the race number in the market description but the PLACE (and other) markets don't?

*Tags: General Technical*

---

**ShaunW** - *11:50:08*

Can't vouch for Flumine [@UCQB6S222](@UCQB6S222) but you might have to tie it back to the win market using the scheduled start time and venue.

*Tags: General Technical*

---

**George** - *11:54:34*

I wrote some code to do this a few months ago... then I scrapped it because it was extremely complicated.

Also, don't forget the scheduled start time can (and does) change.

I would imagine someone much smarter than me has solved this problem in an elegant way?

*Tags: General Technical*

---

**Peter** - *12:10:43*

That works for win and place markets. For more exotic markets it may fail (or will fail for the GB &amp; IE forecast market).



For those I use a worker to pull all the catalogues for the day's races, create a Pandas data frame and the group by start time and venue.



But it could equally be done without Pandas by using list comprehension.

*Tags: Feature Engineering*

---

**Clive** - *21:52:37*

Do people generally run the recorder alongside a main strategy or in a separate process?

*Tags: Strategies*

---

## 2023-10-04

**Andy B** - *10:51:39*

Quick question.  When I set up a strategy and set max_trade_count and max_live_trade_count to 1 in a simulation, is that only for the specific strategy?  If I have 5 strategies, could I in theory place 5 bets on the same runner, or is it governed by the max_trade_count and max_live_trade_count setting in another strategy?  The code I am currently using is:



strategy_A = SimsAll_A(

        market_filter={

            "markets": markets,

            'market_types':['PLACE'],

            "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

            "event_processing": True

            },

    max_order_exposure= 50, # Max bet sizes of $50

    max_trade_count=1, # Max of trade/bet attempt per selection

    max_live_trade_count=1, # Max of 1 unmatched Bet per selection

    )

*Tags: Deployment, Strategies*

---

**liam** - *10:52:37*

Its per strategy instance, so yes 5 strategies could place 5 bets, one bet per runner per strategy

*Tags: Strategies*

---

**Johnny** - *11:00:03*

Do you guys mean a separate Docker container, or a separate strategy added to the existing framework?

*Tags: Strategies*

---

**Johnny** - *11:00:41*

...or just another instance of Python

*Tags: General Technical*

---

**Andy B** - *11:31:39*

Actually, it's not a bug, it's because I set this in another strategy:

if 'processed' not in market.context:

                market.context['processed'] = True

*Tags: Errors Debugging, Strategies*

---

**liam** - *13:03:08*

Sounds like you should be using `strategy.context` if its strategy data

*Tags: Strategies*

---

**Mo** - *15:03:49*

I don’t use Docker. I mean another instance of Python

*Tags: General Technical*

---

**foxwood** - *16:46:55*

It's strategy data specific to the market. It makes it possible for different strats to talk/check with each other when they know the name and meaning of values of the other strat eg `if market.context["strat999"]["someValue"] == 0 :`  Not used in anger but there if needed. I use the strategy context for immutable parameters - usually. It was either that or have dictionary of markets and their data in the strategy context - design decision. Or is there a better way ?

*Tags: Strategies*

---

## 2023-10-05

**Andy B** - *06:36:37*

If there is a better way to do my back tests, I'm happy to learn.  I have three strategies in the one back test, and I want to be able to place no more than 1 bet on a runner in each strategy.  I was using the following to speed up market processing:

if 'processed' not in market.context:

                market.context['processed'] = True



This has the impact of preventing other strategies from processing that market and that means my test results are incorrect.  I changed that code to:



if 'processed' not in self.context:

                self.context['processed'] = True



This appears to allow each strategy to process the market, although I haven't definitively proved that yet.    If there is a better way to do this, let me know.

*Tags: Performance, Strategies*

---

**liam** - *06:38:53*

Just copy the [https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/examples/strategies/lowestlayer.py#L46|example](https://github.com/betcode-org/flumine/blob/796ec51060328edac7e14678c6defd63a964d250/examples/strategies/lowestlayer.py#L46|example) but change the if to



```if runner_context.trade_count == 0:```

ie. just place one trade/bet per runner per strategy

*Tags: Strategies*

---

**Andy B** - *06:39:33*

Ah okay, so the runner.context is exclusive to the strategy?

*Tags: Strategies*

---

**Johnny Boston** - *09:23:45*

Hey, has anyone played with streaming logs from a logging handler through to a UI or something for better monitoring of multiple strategies (with flumine obv)

*Tags: General Technical*

---

**liam** - *09:25:04*

Pipe to AWS cloud watch (I use the docker plugin), very easy to then view / search / graph :ok_hand: 

*Tags: Deployment*

---

**liam** - *09:40:12*

[https://docs.docker.com/config/containers/logging/awslogs/](https://docs.docker.com/config/containers/logging/awslogs/)

*Tags: Deployment*

---

**Andy B** - *09:41:44*

That doesn't help with speeding up the simulation unfortunately.

*Tags: Performance*

---

**liam** - *09:47:18*

[https://github.com/betcode-org/flumine/blob/master/HISTORY.rst#250-2023-10-05|flumine 2.5.0 ](https://github.com/betcode-org/flumine/blob/master/HISTORY.rst#250-2023-10-05|flumine 2.5.0 )released, lots of improvements (smart open now integrated) and bug fixes but let me know if anything breaks

*Tags: Errors Debugging*

---

**Andy B** - *10:04:30*

Good question, it's about 20 months of Aus greyhounds

*Tags: General Technical*

---

**Andy B** - *10:11:22*

14 cores \ 20 logical processors.  I have flumine set to 8 markets per process

*Tags: General Technical*

---

**rob smith** - *10:23:14*

Hi, I've been trying to download data from Betfair using this script: [https://github.com/betcode-org/betfair/blob/master/examples/examplehistoricdata.py](https://github.com/betcode-org/betfair/blob/master/examples/examplehistoricdata.py). I've added my login details but not made any other changes. I keep getting the following error: `Traceback (most recent call last):`

 `File "C:\Users\Rob\AppData\Local\Programs\Python\Python37\lib\site-packages\betfairlightweight\endpoints\historic.py", line 206, in request`

   `response_json = json.loads(response.content.decode("utf-8"))`

`orjson.JSONDecodeError: unexpected character: line 4 column 1 (char 6)`



`During handling of the above exception, another exception occurred:`



`Traceback (most recent call last):`

 `File "E:/Documents/PycharmProjects/bfData/bfDataDownloadv1.py", line 53, in [module](module)`

   `file_type_collection=["M"],`

 `File "C:\Users\Rob\AppData\Local\Programs\Python\Python37\lib\site-packages\betfairlightweight\endpoints\historic.py", line 148, in get_file_list`

   `(response, response_json, elapsed_time) = self.request(method, params, session)`

 `File "C:\Users\Rob\AppData\Local\Programs\Python\Python37\lib\site-packages\betfairlightweight\endpoints\historic.py", line 208, in request`

   `raise InvalidResponse(response.text)`

`betfairlightweight.exceptions.InvalidResponse: Invalid response received:`





`[!DOCTYPE html](!DOCTYPE html)`

`[html](html)`

`[head](head)`

`[meta name="viewport" content="width=device-width" /](meta name="viewport" content="width=device-width" /)`

`[title](title)ngErrorRedirect[/title](/title)`

`[/head](/head)`

`[body](body)`

`[div](div)`

`Error`

`[/div](/div)`

`[script defer src="<https://static.cloudflareinsights.com/beacon.min.js/v8b253dfea2ab4077af8c6f58422dfbfd1689876627854](script defer src="<https://static.cloudflareinsights.com/beacon.min.js/v8b253dfea2ab4077af8c6f58422dfbfd1689876627854)" integrity="sha512-bjgnUKX4azu3dLTVtie9u6TKqgx29RBwfj3QXYt5EKfWM/9hPSAI/4qcV5NACjwAo8UtTeWefx6Zq5PHcMm7Tg==" data-cf-beacon='{"rayId":"811491476cdf0742","version":"2023.8.0","b":1,"token":"d048f65d27954a24aa6b1d7d2ddcb256","si":100}' crossorigin="anonymous">[/script](/script)`

`[/body](/body)`

`[/html](/html)`

*Tags: Errors Debugging*

---

**Andy B** - *10:38:59*

Dumb question, how do I create and share the profile?

*Tags: General Technical*

---

**liam** - *10:40:26*

I use [https://github.com/ymichael/cprofilev|cprofilev](https://github.com/ymichael/cprofilev|cprofilev)



```python -m cprofilev simulation.py```

*Tags: General Technical*

---

**Andy B** - *12:02:05*

It should have been 50ish.  There must be a problem on my end in that case.

*Tags: General Technical*

---

**Andy B** - *12:17:01*

That timing indicates there must be an issue, as it actually takes around a minute for 50 markets.  I can't get cProfile to run without errors when executing against my flumine script, but it works with other none flumine scripts, and I think it might be the multi-processing causing the issue.



concurrent.futures.process._RemoteTraceback:

"""

Traceback (most recent call last):

  File "C:\Python\lib\multiprocessing\queues.py", line 245, in _feed

    obj = _ForkingPickler.dumps(obj)

  File "C:\Python\lib\multiprocessing\reduction.py", line 51, in dumps

    cls(buf, protocol).dump(obj)

_pickle.PicklingError: Can't pickle &lt;function run_process at 0x00000251473E35E0&gt;: attribute lookup run_process on __main__ failed

"""



The above exception was the direct cause of the following exception:



Traceback (most recent call last):

  File "C:\Python\lib\runpy.py", line 197, in _run_module_as_main

    return _run_code(code, main_globals, None,

  File "C:\Python\lib\runpy.py", line 87, in _run_code

    exec(code, run_globals)

  File "C:\Python\lib\cProfile.py", line 180, in &lt;module&gt;

    main()

  File "C:\Python\lib\cProfile.py", line 173, in main

    runctx(code, globs, None, options.outfile, options.sort)

  File "C:\Python\lib\cProfile.py", line 19, in runctx

    return _pyprofile._Utils(Profile).runctx(statement, globals, locals,

  File "C:\Python\lib\profile.py", line 62, in runctx

    prof.runctx(statement, globals, locals)

  File "C:\Python\lib\cProfile.py", line 100, in runctx

    exec(cmd, globals, locals)

  File "C:\Users\elmob\OneDrive\Punting\Scripts\Betfair\BoxPercSimsAll.py", line 945, in &lt;module&gt;

    job.result()  # wait for result

  File "C:\Python\lib\concurrent\futures\_base.py", line 433, in result

    return self.__get_result()

  File "C:\Python\lib\concurrent\futures\_base.py", line 389, in __get_result

    raise self._exception

  File "C:\Python\lib\multiprocessing\queues.py", line 245, in _feed

    obj = _ForkingPickler.dumps(obj)

  File "C:\Python\lib\multiprocessing\reduction.py", line 51, in dumps

    cls(buf, protocol).dump(obj)

_pickle.PicklingError: Can't pickle &lt;function run_process at 0x00000251473E35E0&gt;: attribute lookup run_process on __main__ failed

*Tags: Errors Debugging*

---

**Andy B** - *12:22:17*

I think this one is a future me problem, my brain is fried tonight.  Thanks for taking time to help me Liam.

*Tags: General Technical*

---

**Andy B** - *12:39:45*

That's my script that contains the flumine config and my strategies.  Obviously I have configured that wrong if you weren't expecting it.

*Tags: General Technical*

---

**liam** - *12:49:28*

ok so its pandas

*Tags: Feature Engineering*

---

**liam** - *12:50:10*

what are you doing in pandas? / my understanding is that you are only placing one bet so surely you just need to do the dirty pandas stuff once per market/strategy?

*Tags: Feature Engineering, Strategies*

---

**Andy B** - *12:57:06*

Okay, that does make a bit of sense.  At the start of the script I read in the event_id, time and distance to a pandas df, and then I check the df to get the distance when it opens a new market.  At some point I have pushed that down the processing order until it got to the point where I had it checking it as it processed each runner.  If I bump it up so that it is only checking once per market, I'm certain that would make this run quicker.  Only one way to find out.

*Tags: Feature Engineering*

---

**liam** - *13:01:42*

The site uses the API so doesn't really solve the problem

*Tags: General Technical*

---

**river_shah** - *15:28:07*

`flumine[speed]`just ensures that `befairlightweight[speed]` is used. No other optimisations in flumine itself?

*Tags: Performance*

---

**river_shah** - *15:32:17*

Serious rust envy here at the moment, learning but not enough time. I think with rust + `maturin` we could fully rewrite `bflw / flumine` into one package

*Tags: General Technical*

---

## 2023-10-06

**Andy Bason** - *08:14:44*

Hello guys, with Betfairlightweight is there a way of streaming at, eg, 20ms but also forcing an update every 1000ms where there has been no stream update. These terms might not be correct but what I am trying to do is receive the data as fast as possible and when the market goes quiet continue to collect data at 1000ms until it picks up again. Cheers

*Tags: General Technical*

---

**liam** - *08:15:45*

Copy flumines [https://github.com/betcode-org/flumine/blob/9c65ce90d7574482cd11f97f2c785497edc66533/flumine/streams/marketstream.py#L59|logic](https://github.com/betcode-org/flumine/blob/9c65ce90d7574482cd11f97f2c785497edc66533/flumine/streams/marketstream.py#L59|logic) with a timeout on the queue and a snap

*Tags: General Technical*

---

**Muhammad Adeel Zahid** - *08:40:21*

Hello. I am using the simple strategy from the Flumine tutorial. Below are some code snippets

```strategy = LayStrategy(

    market_filter=streaming_market_filter(

        event_type_ids=["7"], # Greyhounds

        country_codes=["AU", "NZ"], # Australia

        market_types=["WIN"], # Win Markets



    )





)

def check_market_book(self, market: Market, market_book: MarketBook) -> bool:

    # process_market_book only executed if this returns True

    if market_book.status != "CLOSED":

        return True



# If check_market_book returns true i.e. the market is open and not closed then we will run process_market_book once initially

#  After the first initial time, process_market_book runs every single time someone places, updates or cancels a bet

def process_market_book(self, market: Market, market_book: MarketBook) -> None:

    if market.seconds_to_start <= 120:

        for runner in market_book.runners:

            logging.info(f'runner.selection_id ,{runner.selection_id},Status :{runner.status},Matches{[(m.price, m.side)for m in runner.matches]}'

                         f'Orders:{[(o.price, o.size_matched, o.size_cancelled) for o in runner.orders]},TMthd{runner.total_matched},HdCpd{runner.handicap}'

                         f',SP:{[runner.sp.actual_sp, runner.sp.lay_liability_taken, runner.sp.near_price, runner.sp.far_price, runner.sp.actual_sp]}')

            logging.info(f'Complete orders = {runner.orders}')```

I am trying to log the matched orders for each market but no order shows up in the log file. Can anyone guide what I am doing wrong?

*Tags: Strategies*

---

**Unknown** - *10:42:30*

Just following up on this. Attached is the logger code (see next post), should have included in the initial post, sorry about that. Could scaling the amount of strategies being run cause this. I was running 10 but I am now runing 20 via a single Flumine instance.

*Tags: General Technical*

---

**Trex44** - *10:42:45*

```import csv

import logging

from flumine.controls.loggingcontrols import LoggingControl

from flumine.order.ordertype import OrderTypes

import datetime



logger = logging.getLogger(__name__)

today_date = datetime.datetime.now().strftime("%d-%m-%Y--%H:%M")



FIELDNAMES = [

    "bet_id",

    "strategy_name",

    "market_id",

    "selection_id",

    "trade_id",

    "date_time_placed",

    "price",

    "price_matched",

    "size",

    "size_matched",

    "profit",

    "side",

    "elapsed_seconds_executable",

    "order_status",

    "market_note",

    "trade_notes",

    "order_notes",

]





class StandardLoggingControl(LoggingControl):

    NAME = "Standard_Logging_Control"



    def __init__(self, *args, **kwargs):

        super(StandardLoggingControl, self).__init__(*args, **kwargs)

        self._setup()



    def _setup(self):

        with open(f"orders_{today_date}.txt", "w") as m:

            csv_writer = csv.DictWriter(m, delimiter=",", fieldnames=FIELDNAMES)

            csv_writer.writeheader()



    def _process_cleared_orders_meta(self, event):

        orders = event.event

        with open(f"orders_{today_date}.txt", "a") as m:

            for order in orders:

                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:

                    size = order.order_type.size

                else:

                    size = order.order_type.liability

                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:

                    price = None

                else:

                    price = order.order_type.price

                try:

                    order_data = {

                        "bet_id": order.bet_id,

                        "strategy_name": order.trade.strategy,

                        "market_id": order.market_id,

                        "selection_id": order.selection_id,

                        "trade_id": order.trade.id,

                        "date_time_placed": order.responses.date_time_placed,

                        "price": price,

                        "price_matched": order.average_price_matched,

                        "size": size,

                        "size_matched": order.size_matched,

                        "profit": order.profit,

                        "side": order.side,

                        "elapsed_seconds_executable": order.elapsed_seconds_executable,

                        "order_status": order.status.value,

                        "market_note": order.trade.market_notes,

                        "trade_notes": order.trade.notes_str,

                        "order_notes": order.notes_str,

                    }

                    csv_writer = csv.DictWriter(m, delimiter=",", fieldnames=FIELDNAMES)

                    csv_writer.writerow(order_data)

                except Exception as e:

                    logger.error(

                        "_process_cleared_orders_meta: %s" % e,

                        extra={"order": order, "error": e},

                    )



        [http://logger.info|logger.info](http://logger.info|logger.info)("Orders updated", extra={"order_count": len(orders)})



    def _process_cleared_markets(self, event):

        cleared_markets = event.event

        for cleared_market in cleared_markets.orders:

            [http://logger.info|logger.info](http://logger.info|logger.info)(

                "Cleared market",

                extra={

                    "market_id": cleared_market.market_id,

                    "bet_count": cleared_market.bet_count,

                    "profit": cleared_market.profit,

                    "commission": cleared_market.commission,

                },

            )```

*Tags: Getting Started, Errors Debugging, Strategies*

---

**Bookworms Inbox** - *14:33:37*

I have had developed a bot that looks to a certain telegram alert service for football games. The bot then sends the games to a csv for another software called BF Bot Manager to place the bets.



The problem we had was that the team naming convention from the alert service did not comply with that of Betfair and as such the bets were not placed.



We attempted to get round this by having the bot take the team names from the alerts and then comparing them with Betfair, and then interchanging the teams names to suit Betfairs naming convention, then the bot would send the amended team names to the csv for the other software to place the bets.



Problem is, it works well but does not match well, with very obsecure team names being returned, not even related tot he alert team names..



Please let me know, hopefully you can help

*Tags: General Technical*

---

**Peter** - *16:15:59*

I don't believe that there is a convention for Betfair football names. I see inconsistencies, duplications, the occasional error and even no longer accurate historical names. You'll need to create and maintain a mapping between Betfair's and the alert service's names.

*Tags: Errors Debugging*

---

**joe taylor** - *17:51:20*

Hi all!

Is their a fixed protocol that is followed before the match goes inplay- I’ve noticed for horse racing that market is closed for a few seconds before going inplay. I want to stop my signal before it goes inplay and put some bets on bsp at the very last moment. Is there a way to find the “last moment” using seconds_to_start or something?

*Tags: Errors Debugging*

---

## 2023-10-07

**Muhammad Adeel Zahid** - *07:04:52*

[@U4H19D1D2](@U4H19D1D2) Can you please tell why `runner.matches` and `runner.orders` are always empty lists? Do I have to configure some parameters for Flumine to fetch orders and matched bets?

*Tags: General Technical*

---

**liam** - *07:39:55*

Those values are not populated when streaming 

*Tags: General Technical*

---

**Bookworms Inbox** - *12:14:31*

many thanks, is this something you could help with as a small project. We have the Function &amp; Main files and it works up to the point of comparing the Alert teams to that of the teams listed on Betfair. At present it returns wrong team names to the CSV

*Tags: General Technical*

---

## 2023-10-08

**AndyL** - *11:29:13*

Is the streaming subscription limit per connection or per user ?

Eg. is the default 200 across all 10 available connections, or 200 per connection(thus potential 2000 in total) ?

*Tags: General Technical*

---

**Unknown** - *17:05:27*

Thanks for the reply [@U4H19D1D2](@U4H19D1D2). When do they get populated? I have a simple use case where I want to analyze the matched orders for a horse (selection) before placing my own. How do I do it with Flumine?

*Tags: General Technical*

---

**liam** - *19:21:45*

They don’t when streaming, you need to use process_orders or just the blotter in the market object, check out the docs 

*Tags: General Technical*

---

## 2023-10-09

**liam** - *12:24:40*

`process_orders` is triggered on any order update through the stream or every 0.25s if there are live orders.



The [https://betcode-org.github.io/flumine/markets/|blotter](https://betcode-org.github.io/flumine/markets/|blotter) is in the `Market` object and you can use whenever you want

*Tags: Deployment*

---

**Muhammad Adeel Zahid** - *14:10:11*

Ok. Below are my strategy parameters

```strategy = LayStrategy(

    market_filter=streaming_market_filter(

        event_type_ids=["7"], # Horse racing

        country_codes=["GB", 'IE'],

        market_types=["WIN"], # Win Markets

    )

)



framework.add_strategy(strategy)



framework.run()```

and this is my `process_orders` function in `LayStrategy`

```def process_orders(self, market: Market, orders: list) -&gt; None:

    print('process orders')

    if orders:

        print(orders)

    aorders = [o for o in market.blotter.live_orders]

    if aorders:

        print('process order')

        print(aorders)

    for order in orders:

        print(order)```

But not even the first statement is being called (`print('process orders')`. I don't know what is wrong with this code.  Nothing shows up related to orders

*Tags: Deployment, Strategies*

---

**Muhammad Adeel Zahid** - *14:31:37*

No. I am trying to first analyze the orders placed in the market. I was going through [https://github.com/betcode-org/flumine/tree/master/examples|examples](https://github.com/betcode-org/flumine/tree/master/examples|examples) and it appears that `process_orders` will only be called for my own orders. However, I tried to get the overall orders or trades as suggested by [@UEA14GBRR](@UEA14GBRR) in `process_market_book` but did not succeed.

```def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:



    orders = [o for o in market.blotter.live_orders]

    if orders:

        print('Process market book')

        print(orders)

    for runner in market_book.runners:

        r_context = self.get_runner_context(market.market_id, runner.selection_id, runner.handicap)

        trades = [t for t in r_context.trades]

        live_trads = r_context.live_trades

        if trades or live_trads:

            print("Trades = ", trades)

            print("Live Trades = ", live_trads)```

Nothing shows up here either

*Tags: Deployment*

---

**Muhammad Adeel Zahid** - *15:22:00*

Honestly, no. I was just going through the flumine code and seeing if I could access what I needed.

*Tags: General Technical*

---

**foxwood** - *15:28:12*

Run it under debug with some breakpoints and explore the structure and field contents of `market` and `market_book` - it makes a lot more sense when populated with real data than trying to unravel the sources. Step it through and see what happens with different packets as well.

*Tags: Errors Debugging*

---

**Muhammad Adeel Zahid** - *16:05:35*

Ok. I was going through the Betfair streaming API and it appears that it is not possible to get orders as they are placed on the market. However, traded data is available with price and size. Can someone confirm/deny this conjecture, please? And yes, I was able to access the traded volume through `runner.ex.traded_volume` as suggested by [@U4H19D1D2](@U4H19D1D2)

*Tags: General Technical*

---

## 2023-10-10

**Muhammad Adeel Zahid** - *10:44:30*

I was going through Betfair exchange API and they two fields namely `EX_TRADED` and `EX_TRADED_VOL` accessible through fields `trd` and `tv` respectively. But Flumine only exposes `runner.ex.traded_volume` . What does it refer to? `trd` or `tv`? Because, I am interested in `trd` which, according to docs, points to the matched volume at each price. whereas `tv` refers to liquidity in the market available to lay and back at each price.

*Tags: General Technical*

---

**tone** - *11:07:16*

Hi everyone,

A newbie trying to figure things out!

Is anybody aware of some code which will record a market in a similiar format to Betfair's PRO format historic data files?

I've looked at marketrecorder.py in flumine, which intially seemed perfect, but the price data doesn't appear to be stored in time series.

What I'm after is something will look a bit like this:



&lt;&lt; MarketDefinition message &gt;&gt;

&lt;&lt; Price messages with timestamps and runner IDs &gt;&gt;

&lt;&lt; MarketDefinition message with inPlay=true &gt;&gt;

&lt;&lt; inPlay price messages with timestamps and runner IDs &gt;&gt;

&lt;&lt; MarketDefinition message with results &gt;&gt;



Thanks for your time.

*Tags: General Technical*

---

**liam** - *11:22:31*

flumine recorded data === betfair pro data

*Tags: General Technical*

---

**Peter** - *11:25:40*

Firstly welcome.



All the data you currently want, and more, is available in the files generated by the market recorder, but it needs a slight shift in how you think about them.



The way we approach this here is to save those files as raw data. Then run them through Flumine in simulation mode to extract the specifics that we want for the analysis being done at the time and save it in a format conducive to that analysis.



Later we may return, stream the same raw data but extract different parts of it, e.g. trading volumes, or SP estimates, or some metric that we derive from the raw data. We don't assume that what we want now is what we will want in the future, Hence keeping the data in it's raw form allowing for maximum flexibility.

*Tags: Data Quality, Strategies*

---

**Muhammad Adeel Zahid** - *12:08:19*

Thanks [@U4H19D1D2](@U4H19D1D2). I think it should solve my problem. Just for information, how would I access the `tv` field in Flumine. `BaseStrategy` seems to pass `EX_TRADED_VOL` in `marketDataFilter` but there is no object exposing this value

*Tags: Strategies*

---

**Peter** - *13:26:59*

Agreed. Both of those references provide wrappers around Flumine to stream and extract data from the raw files.

*Tags: General Technical*

---

**mzaja** - *18:14:17*

Hi all! I've just released version 1.0.0 of `betfairdatabase` package for those that are interested.  Check out readme for more details.



[https://pypi.org/project/betfairdatabase/1.0.0/](https://pypi.org/project/betfairdatabase/1.0.0/)

*Tags: General Technical*

---

## 2023-10-11

**foxwood** - *00:00:52*

Good stuff and a nice solution to managing the data warehouse - which I keep meaning to do ! Had a quick look and a couple of gotchas from my setup that may also apply to others you might like to consider at some point in the future

1. Rightly or wrongly all my market catalogues are gzipped - I thought that was the standard for the default flumine market recorder used here. The implementation seems to require these to be in unzipped form.

2. It's effectively tied to sqlite. Since my need is sql server via sqlalchemy it would be useful if the sql specific bits (possibly statements as well since there are differing sql dialects) were subsumed into a class on their own. That would allow users to implement / contribute their own flavour of sql.

Is this an open project for others to provide contributions or just one you control ? Don't know enough about how github works to answer that - i still use a 30 year old legacy GUI VCS lol.

*Tags: Getting Started, Data Quality, Deployment*

---

**mzaja** - *17:11:53*

2. I chose sqlite3 because it is in Python's standard library. It would be possible to make the package support different SQL flavours, but I am not entirely sure what the benefit would be. With the current implementation, one can perform a coarse selection of data with an SQL query and then do additional filtering in pure Python.

*Tags: General Technical*

---

## 2023-10-12

**Lee** - *16:25:01*

Yep, looks like the same problem. This can probably be fixed with a cleaner version of the patch i’m running. I’ll raise a PR to fix it but will probably be next week.

*Tags: Errors Debugging*

---

## 2023-10-13

**andres gonzalez** - *13:09:05*

Hello, very new to using the betfair API, at the moment I'm simply trying to create a bot that tracks prices of specific markets I want to and want to to notify me when available back or lay liquidity is what I'm looking for. going through the betfairlightweight GitHub to try and do this step by step. Currently trying to find in the documentation all the use cases of trading.betting on python.

*Tags: Getting Started, Strategies*

---

**Peter** - *13:34:36*

Skip trying to do this with betfairlightweight and start with Flumine. It's a trading framework wrapped around betfairlightweight, and adds many of the tools you'll need when you want to start acting on those triggers.



Take a look at the lowest layer example in the repo to see how this would work.

*Tags: Strategies*

---

**John Foley** - *14:46:25*

really nice tool, thanks! with historical data downloaded from betfair (not self recorded), you dont get the market catalogue files. i guess that means this package will only work if you recorded the data yourself?

*Tags: Data Quality*

---

**andres gonzalez** - *14:51:07*

Thanks Peter appreciate the advice, will have a look at Flumine and hopefully it's easier to navigate!

*Tags: General Technical*

---

## 2023-10-15

**Rishab** - *21:46:27*

Does running all the markets for a sport(say horse racing) in just 1 flumine instance affect the strategy in any way if my strategy is latency sensitive?

*Tags: Performance, Strategies*

---

## 2023-10-16

**liam** - *07:22:53*

Depends, latency is a sliding scale, where are you on the scale?

*Tags: Performance*

---

## 2023-10-17

**rob smith** - *09:36:24*

Hi guys, I'm trying to make sense of some data I've recorded. I am trying to record price data for Aus racing at 1s intervals using streaming. Conflation is set to 1000ms. There are lots of occasions where the gaps in the data are more than 1000ms. Is that to be expected with streaming where there are no changes in the market in the meantime? Thanks

*Tags: General Technical*

---

**tone** - *13:22:30*

Hello again,

Any pointers re how I might record the data in "raw" format? I'm currently attempting to do this using using the below code but the results don't look right for use in replay / simulation mode.

I'm guessing it could be something to do with the stream_class!

Any help would be very welcome.



import time

import logging

import betfairlightweight

from pythonjsonlogger import jsonlogger



from flumine import Flumine, clients

from flumine.strategy import strategy

from flumine.streams.datastream import DataStream

from local_marketrecorder import MarketRecorder

import betfairlightweight



logger = logging.getLogger()

custom_format = "%(asctime) %(levelname) %(message)"

log_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_handler.setFormatter(formatter)

logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



trading = betfairlightweight.APIClient("###", "###", devKey, certs=(###))

client = clients.BetfairClient(trading)



framework = Flumine(client=client)



strategy = MarketRecorder(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["AU"],

        market_types=["WIN"],

        market_ids=["1.219765628"],

    ),

    stream_class=DataStream,

    context={

        "local_dir": "d:/BetFair/data",

        "force_update": True,

        "remove_file": False,

        "remove_gz_file": False,

        "load_market_catalogue": True,

    },

)



framework.add_strategy(strategy)

framework.run()

*Tags: Strategies*

---

**Unknown** - *14:11:25*

I'm using a local copy of MarketRecorder (a bit of an ugly hack in order to get things to compile. I imagine I'll be able to get things working in better fashion when I get more accustomed to using Python). The code is unchanged from the version in the repo.

I've attached a copy of a market I recorded from a couple of minutes before start time until completion. Note there are only 5 mcm messages and 5 mc messages although there are lots of prices and volumes attached to these messages as lists.

I also attach the output from the "lowestlayer" simulation on the repo.

I occurs to be that the recorder might be populating the price list on chronological order and then therefore the simulation would not require timestamps. Is this correct?

*Tags: General Technical*

---

**Unknown** - *17:45:42*

I'm using a delayed key.  Attached is a market that I started recording 5 mins before the start time (in case it's still relevant and the delayed key explains the problem!)

*Tags: General Technical*

---

## 2023-10-19

**Unknown** - *10:24:12*

Hi folks,



I think I may be getting somewhere, but very slowly!

The attached code is attempting to use a MarketRecorder strategy to record a simulated market but I'm running into the attached error in marketrecorder.py.

It seems there is no "id" property in the data object. I've tried hard-coding market_id to be the actual market id but this then creates a some new problems, which I'm happy to go into if anyone thinks necessary.

*Tags: Errors Debugging, Performance, Strategies*

---

**liam** - *10:25:00*

'The attached code is attempting to use a MarketRecorder strategy to record a simulated market'



what?

*Tags: Strategies*

---

**liam** - *10:50:21*

Correct but you don't use a MarketRecorder for this, the recorder is for recording live data into a format that can then be processed by the simulation. You are currently trying to record data you have already recorded which isn't going to work



Have you had a look at any of the docs or [https://github.com/betcode-org/flumine/blob/master/examples/simulate.py|examples](https://github.com/betcode-org/flumine/blob/master/examples/simulate.py|examples)?

*Tags: Deployment*

---

**liam** - *10:55:42*

:+1: if you just copy the LowestLayer strategy and debug through / print stuff you can see how it works/simulates things

*Tags: Errors Debugging, Strategies*

---

**ShaunW** - *12:59:21*

Quick question to people who'll know, football... Are Betfair using Optima for stats info/graphics/scores or an agent at the game? And are suspensions manually managed or automated to some extent?

*Tags: General Technical*

---

**liam** - *15:00:05*

[@U03N4QBJ0TV](@U03N4QBJ0TV) can you try using Lee's [https://github.com/betcode-org/flumine/pull/712|fix](https://github.com/betcode-org/flumine/pull/712|fix) and report back?



```pip install git+[https://github.com/lunswor/flumine.git@handle-clearing-orders-only-once-settled](https://github.com/lunswor/flumine.git@handle-clearing-orders-only-once-settled)```

*Tags: Getting Started, Errors Debugging*

---

## 2023-10-20

**Eddy** - *07:55:53*

Hi everyone, another new-ish user to the betfair API.



I am trying to achieve a couple of things as I play around with the data, such as identifying a list of all market_ids in the upcoming day and beginning to record market data.



I am currently still using the delayed key, but having some troubles obtaining the data I am looking for (the market catalogue for example takes forever to populate). I just wanted to check whether this could be driven by the use of the delayed key and if obtaining the live key may solve the problem.



Thanks in advance!

*Tags: Deployment*

---

**Peter** - *11:06:29*

&lt;whisper&gt;Bit of an informal rule here [@U04JVTVLGMT](@U04JVTVLGMT) that we don't answer questions where the delayed key is being used. By all means use it to check that you can connect to the API. But any data you do or don't get back is pretty much rubbish, and the root of many odd issues that pop up here. Hence the informal rule&lt;/whisper&gt;

*Tags: General Technical*

---

## 2023-10-22

**James** - *12:23:11*

This is how  exposure is calculated in flumine.



[https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L185](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L185)

*Tags: General Technical*

---

**Trex44** - *19:01:55*

Hey Guys. So the TLDR is the fix worked. Thanks very much.



A few more details. I set up another instance and ran the patch on that one. I ran the same strategies at the same stakes on the patched instance as the for the unpatched instance. I couldn't get the install to work using the CLI even though everything seemed to download/install ok so I manually overwrote the workers file using the updated workers file Lee created. It might not have worked via the CLI because I am inexperienced using GitHub so did something wrong.



All the printed results in the the patched instance had the 'profit' column in the csv filled in correctly. The unpatched instance displayed the same error as before with zeros in sometimes present in the profit column.



• I have noticed that sometimes the same line or set of lines will be written twice. This occurs on both instances but not necessarily for the same results on each instance. My guess is its a different bug to the one that has just been fixed. It doesn't bother me as I only upload each unique bet id once to my results database. See below for an example from the patched instance. The same strategy executes on two markets but the results from the first market print again after the second.

`325529200273,strat_x,1.219940406,6846739,e75cea8e-7019-11ee-b135-17589b09afd9,2023-10-21 13:58:25.603658,3.65,3.7,1,1,-1.0,BACK,0.034584,Execution complete,"3.65,3.7,3.7",,"{'runner_id': 6846739}"`



`325538377913,strat_x,1.219940554,15914199,ca20765c-701e-11ee-b135-17589b09afd9,2023-10-21 14:33:24.004443,3.4,3.4,1,1,-1.0,BACK,0.021638,Execution complete,"3.4,3.45,3.45",,"{'runner_id': 15914199}"`



`325529200273,strat_x,1.219940406,6846739,e75cea8e-7019-11ee-b135-17589b09afd9,2023-10-21 13:58:25.603658,3.65,3.7,1,1,-1.0,BACK,0.034584,Execution complete,"3.65,3.7,3.7",,"{'runner_id': 6846739}"`





• A strategy that was active on both instances and fired on both instances over the two day test period fired on one extra market on the patched instance than on the un patched instance. Uncertain why this happened, possible could be strategy related so will need to look into the code to see. 

I will leave the two instances running the next few days to see what is generated. The unpatches instance is one I use at low stakes for my test strategies. I will be upgrading my main instance to the new patch of the back of these results.

*Tags: Getting Started, Errors Debugging, Strategies*

---

**James** - *21:27:49*

[https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L28](https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L28)



Here is the OrderStatus definition.

Note that there are live and complete statuses.



An order is execution_complete when there is nothing left to be matched. That could be because it has all been matched, or some combination of matched, lapsed, cancelled and voided.

*Tags: Deployment*

---

## 2023-10-23

**Jonjonjon** - *00:02:41*

Will you need to consider the `market_exposure` method from that module?



[https://github.com/betcode-org/flumine/pull/517](https://github.com/betcode-org/flumine/pull/517)

*Tags: General Technical*

---

## 2023-10-25

**George** - *15:54:34*

yes, though the problem is I don't necessarily have the market catalogues for these particular markets.

*Tags: General Technical*

---

**Mikkel** - *20:26:26*

Does anyone know how non-runner reductions rate are handled in the Betfair historical data?

*Tags: Data Quality*

---

## 2023-10-26

**Herugrim** - *07:39:26*

[https://www.betfair.com.au/hub/racing/horse-racing/predictions-model/|https://www.betfair.com.au/hub/racing/horse-racing/predictions-model/](https://www.betfair.com.au/hub/racing/horse-racing/predictions-model/|https://www.betfair.com.au/hub/racing/horse-racing/predictions-model/)

*Tags: Strategies*

---

**liam** - *08:27:32*

I imagine Terry wants the logic for getting the rating, didn't the hub once have a walk through on how to do this?

*Tags: General Technical*

---

**Herugrim** - *08:35:08*

He asked for the percentages as displayed in the image which is what I explained. The rated prices are a result of a machine learning model that isn’t publicly available. 

*Tags: Strategies*

---

**Trex44** - *23:38:11*

Hey guys. Is there a way to get the traded ladder (so prices and volumes traded). I think the RunnerBookCache stores it in the traded instance variable but I can't figure out how to access it.

*Tags: General Technical*

---

## 2023-10-27

**NAS** - *02:11:30*

hi. what's the easiest way of obtaining the information regarding all scratches in a race (in particular time since last one) with betfairlightweight?

*Tags: General Technical*

---

**liam** - *06:35:59*

It’s just `runner.ex.traded_volume` don’t go into the private methods / cache, are you using flumine?

*Tags: General Technical*

---

**Terry** - *11:31:15*

Sorry, screenshot from greyhounds prediction models.

*Tags: Strategies*

---

**Lee** - *17:13:47*

sorry, I’ve not been at my computer much the last couple of weeks



• are you running two instances on the same machine? Are you setting different hostnames/customer_strategy_ref in the flumine config? not sure how often you see this but is betfair resettling the market? 

• the worker changed wouldn’t/shouldn’t effect the strategy triggering

*Tags: Strategies*

---

## 2023-10-29

**Dave** - *11:16:29*

Suppose I am running an instance of flumine, with a logging control that dumps my orders in `_process_cleared_orders_meta` by looping over `for order in event.event`. Suppose I run another instance of flumine with the same logging control, that trades on the same market. Will both instances each log orders submitted by both instances, or only their own? Does this depend on whether i'm running those two instances on the same host (and therefore their customer_strategy_refs will be different)?

*Tags: Strategies*

---

**Dave** - *11:17:42*

I'm basically trying to figure out: If run a second flumine instance on another host, do I need to run it with the logging control _as well_ or will my first instance that's running with the logging control take care of the order logging. And how does that differ if I run the second flumine instance on the same host

*Tags: General Technical*

---

**birchy** - *11:48:15*

You'll get ALL orders. Personally, I have a logging control for each strategy instance and filter by strategy name, although it would be more efficient to run a single instance of logging control if you're pushing to a database or whatever.

*Tags: Strategies*

---

## 2023-10-30

**Dave** - *19:29:02*

Due to this thing: [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L149](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L149)

*Tags: General Technical*

---

**birchy** - *19:42:38*

Ah yes...I run multiple strategies per flumine instance, hence the reason I filter by strategy name.

*Tags: Strategies*

---

## 2023-11-01

**Trex44** - *21:10:47*

[@UUCD6P13J](@UUCD6P13J) No the instances were on different EC2 environments. I didn't notice that behaviour again so shut down the test instance and am just continuing with the main one. I am willing to bet the firing of the strat on one instance an not the other is related to the seconds_to_start_parameter in the strategy as its possible for this to be different across two different instances for the same market update due to the EC2's local clock being used in the calculation.  I am very happy with the fix and am pushing ahead, I was just reporting back to you guys to let you know.



Curious, as I haven't used GitHub much before, will the commit that fixed this now become part of the main codebase?

*Tags: Errors Debugging, Deployment, Strategies*

---

**JazzMan** - *22:53:10*

My strategies do it all the time, never had a problem. 

*Tags: General Technical*

---

## 2023-11-02

**birchy** - *13:17:10*

Scratching is a common "trading" strategy

*Tags: Strategies*

---

**liam** - *14:01:03*

Yeah, my misunderstanding / assumption of the question, you would be matching against others money so no issue

*Tags: General Technical*

---

**Peter** - *18:20:45*

Can't see any reason why streaming activity would interfere with manual betting. Also you can have a number of subscriptions for your API key - the number varies by account as more active users can request an increase - but you need at least 2 to be able to subscribe to markets and the order stream and I believe the default is 10.



Can you provide more specifics as to the problem / interference you're experiencing.

*Tags: Strategies*

---

## 2023-11-05

**Trex44** - *17:58:45*

Hey all, is there a way to speed up the back test code [https://github.com/betcode-org/flumine/blob/master/examples/simulate-sportsdata.py|here](https://github.com/betcode-org/flumine/blob/master/examples/simulate-sportsdata.py|here) using  multiprocessing ? I want to speed it up when testing 1000's of markets.

*Tags: Performance*

---

**liam** - *18:00:13*

[https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/](https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/)

*Tags: Performance*

---

## 2023-11-08

**joe taylor** - *07:18:59*

Hey guys! Wanted to use the information from race status api, looked at flumine wasn’t able to find anything regarding it. Is there a way to access it from flumine ?

*Tags: General Technical*

---

**liam** - *08:18:03*

You would use a worker following [https://github.com/betcode-org/flumine/blob/master/examples/workers/inplayservice.py|this](https://github.com/betcode-org/flumine/blob/master/examples/workers/inplayservice.py|this) pattern

*Tags: General Technical*

---

**Brian Sexton** - *12:23:04*

Hi All. Im looking to build a an ML model for tracking in game tennis price movements. I have historcal data from betfair and I want to combine it with pre match stats on the players. The issue im having is putting the 2 datasets together so I can start to train some models and get started. In the  betfair data I have market, selection and event id and also the selection name. Has anyone experience with this and how do you approach getting the data prepared so a model can be trained with various datasets?

*Tags: Strategies*

---

**liam** - *15:10:05*

[https://github.com/betcode-org/betfair/blob/9dfed6a2acd18550a6c2192c8c67bc6b0d2ce27e/betfairlightweight/endpoints/scores.py#L16](https://github.com/betcode-org/betfair/blob/9dfed6a2acd18550a6c2192c8c67bc6b0d2ce27e/betfairlightweight/endpoints/scores.py#L16)

*Tags: General Technical*

---

**Štefan** - *15:37:29*

Just from the table names you can judge my schema and how I normalized database structure. I link datasources by race time and horse names as getting data from 3 different sources.

*Tags: General Technical*

---

**Brian Sexton** - *17:01:47*

Ill get the data i have structured into a database and try to normalize and match them up to see how it works. It should be ok :crossed_fingers:

*Tags: General Technical*

---

## 2023-11-13

**liam** - *09:04:28*

[https://github.com/betcode-org/flumine/blob/master/HISTORY.rst#254-2023-11-13|2.5.4](https://github.com/betcode-org/flumine/blob/master/HISTORY.rst#254-2023-11-13|2.5.4)

*Tags: General Technical*

---

## 2023-11-14

**Justice** - *09:56:04*

[@U028Z5T4AF6](@U028Z5T4AF6) Do you use the `RacingpostRating` in your model? The historical ratings I have scraped are retrospectively revised (collateral handicapping etc.) and so I can't use them for machine learning without leaking data into my model.

*Tags: Strategies*

---

**D C** - *10:37:04*

Assuming you are streaming, If you really want to do this I can't see any other option than you would need to manually detect the first inplay suspension when the race starts and store the market version, then stop recording on subsequent change in the market version (or detection of further suspension). Premature and incorrect setting of market inplay is not unheard of though, so no method will be 100% effective. Personally, if this is purely for recording of data as you say, then I'd just record everything and extract the portions you are interested in later as it would  be easier, and I might find further down the road that I actually DO want that data for some reason.



If you are polling using listMarketBook then depending on your poll interval you always run the risk of missing a small portion of data, or getting some non inplay portion included in your data. I used to do this years ago and it is full of shortcomings, so if you are not streaming, I would strongly suggest you switch.

*Tags: General Technical*

---

**Justice** - *11:57:53*

[@UUE6E1LA1](@UUE6E1LA1) Yes, I am streaming with no conflation set so I can get tick data. I think I will take your suggestion and leave my code as is. I'll filter out anything else later.

*Tags: General Technical*

---

## 2023-11-15

**Ralegh** - *11:02:51*

Are the market catalogues output by market recorder included in historical data? I've been using historical data fine with just market definitions so I'm assuming I can just use the streamed market data files and ignore catalogues?

*Tags: Data Quality*

---

## 2023-11-16

**Chandana Tennakoon** - *07:30:40*

Hello, I am exploring the betfairlightweight library and am very new to this. I cannot find from the in-built documentation complete information about methods and members of the classes in the library. Although I have managed to hazard some guesses from going through sample code what some of these are, is there a place where there is a proper documentation? (e.g. where is market_book.market_definition.market_type or runner.ex.available_to_back is documented)

*Tags: Getting Started*

---

**liam** - *08:31:24*

I have never seen it, bflw doesn't handle it :man-shrugging:

*Tags: General Technical*

---

**Alex A** - *08:32:07*

Looks like bflw does request it by default, just seems to be a bluff on Betfair’s end.

*Tags: General Technical*

---

## 2023-11-17

**Alex A** - *12:52:08*

Would you mind elaborating? Is there something that bflw and I are doing wrong to not receive it?

*Tags: General Technical*

---

**Unknown** - *13:10:08*

Hi I'm new to the `betfairlightweight` Python library. I'm aiming to get a data stream of prices over time for live matches that's reflective of the viz that the Betfair UI shows you (below). Is there a method that will do this for live events, or do I need to use the `Streaming API` throughout the event and subsequently store the results manually?

*Tags: Getting Started, Deployment*

---

**Alex A** - *14:59:38*

Bit strange that they recommend enabling it for performance, as performance while processing the initial snapshot will be orders of magnitude worse than normal with or without segmentation.

*Tags: Performance*

---

## 2023-11-18

**Mo** - *00:10:17*

Yes the API convention is camelCase while betfairlightweight as a Python library follows snake case conventions

*Tags: General Technical*

---

**Chandana Tennakoon** - *08:16:48*

Thank you, that was very helpful!

*Tags: General Technical*

---

**Prime** - *11:35:11*

if you have a streaming json decoder then its not such a big deal. however if, like most people, you download the whole json message then decode it in one go and then handle each market in sequence, you will do much better if the snapshot is segmented because you can start doing your market handling faster, while the rest of the json downloads and decodes on another thread and feeds that process. on betfair's side, they prefer segmented messages because it reduces memory load when assembling very large messages and allows them to send you data faster as the first message will be ready more quickly. generally, most of the time, because this is all startup stuff its not very relevant. on our systems we certainly never bother with it. we talk to betfair via 1/10 Gbit links and initial decoding only happens once before any trading etc kicks in. imo it's all Joe's fault.

*Tags: Performance, Strategies*

---

## 2023-11-19

**db888** - *14:31:40*

So it seems like the `Private API`  rate limits or chokes (perhaps user error). Therefore I feel like recording data from the `Public Streaming API` throughout the course of an event is probably easiest..

*Tags: Errors Debugging*

---

**Trex44** - *22:00:53*

Hey all, is there a way to get the timestamp (e.g. "pt":1534433284382) betfair send with each update via Flumine? I am trying to access it via a strategy.

*Tags: Strategies*

---

## 2023-11-20

**Mo** - *09:25:23*

Perhaps you could be a bit more specific about what you're trying to achieve and why the Betfair API is not appropriate. Once you start considering other APIs you have to start worrying about mapping events, markets and selections

*Tags: General Technical*

---

**liam** - *09:58:17*

[https://ero.betfair.com/www/sports/exchange/readonly/v1/bymarket?_ak=nzIFcwyWhrlwYMrh&amp;alt=json&amp;currencyCode=GBP&amp;locale=en_GB&amp;marketIds=1.221401681&amp;rollupLimit=1&amp;rollupModel=STAKE&amp;types=MARKET_STATE,MARKET_RATES,MARKET_DESCRIPTION,EVENT,RUNNER_DESCRIPTION,RUNNER_STATE,RUNNER_EXCHANGE_PRICES_BEST,RUNNER_METADATA,MARKET_LICENCE,MARKET_LINE_RANGE_INFO,RUNNER_SP|eg](https://ero.betfair.com/www/sports/exchange/readonly/v1/bymarket?_ak=nzIFcwyWhrlwYMrh&amp;alt=json&amp;currencyCode=GBP&amp;locale=en_GB&amp;marketIds=1.221401681&amp;rollupLimit=1&amp;rollupModel=STAKE&amp;types=MARKET_STATE,MARKET_RATES,MARKET_DESCRIPTION,EVENT,RUNNER_DESCRIPTION,RUNNER_STATE,RUNNER_EXCHANGE_PRICES_BEST,RUNNER_METADATA,MARKET_LICENCE,MARKET_LINE_RANGE_INFO,RUNNER_SP|eg)

*Tags: Strategies*

---

## 2023-11-22

**Ralegh** - *14:08:12*

Any advice for finding market trades since last update in flumine?  Currently just storing ex.traded and finding the differences but seems backwards since the stream sends trades as deltas which is what I want

*Tags: General Technical*

---

**liam** - *14:12:47*

`market_book.streaming_update` is the update received from the stream however you can use the simulated middleware to give you an abstraction:



```simulated = market.context["simulated"]

for runner in simulated.values():

    print(runner.traded)```

*Tags: General Technical*

---

## 2023-11-25

**Roy** - *07:27:33*

[@U4H19D1D2](@U4H19D1D2) I get 403 forbidden error while I try to scrape price data vs time from Load runner chart. Any other ways to scrape it? Thanks

*Tags: Errors Debugging*

---

**db888** - *07:35:19*

[@U067MDW0YPK](@U067MDW0YPK) Same issue I was having, despite only attempting to scrape it every few minutes throughout a live event. I've found that the rate limiting is non-deterministic - for some events I can hammer it and get what I want without issues, but in other cases it just fails with a 403 error. Hence why I feel the `Public Streaming API` may be a better option than the `loadRunnerChart` private API. Worth also noting that the priv API endpoint will purge data shortly after a match has ended - so if you wanted to scrape something after the fact, you won't get data via this method.

*Tags: Errors Debugging, Deployment*

---

**Roy** - *12:10:29*

[@U066M1HC42V](@U066M1HC42V) That's right. I was trying it for horse racing and I have used almost all headers but I was getting same 403 error. However, I tried to open URL with PowerBI and it was able to extract the data.

*Tags: Errors Debugging*

---

## 2023-11-26

**Adam** - *00:15:25*

```thoroughbreds_event_filter = {

        "eventTypeIds": ["7"],

        "marketCountries": ["AU"],

        "marketStartTime": {"to": end_of_day_str},

        "marketTypes": ["WIN"],

        "raceTypes": ["Flat"]

    }

market_filter={

            "markets": market_ids,

            'market_types': ['WIN'],

            "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

            "event_processing": True

        },```

```strategy = Strategy(

    market_filter={

        "markets": market_ids,

        'market_types': ['WIN'],

        "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

        "event_processing": True

    },

    market_data_filter=market_data_filter,```

Hi all. I have generated a list of market_ids (e.g `market_ids = [1.170212754, 1.170212755, 1.170212759, …]`) which consists of all thoroughbred markets that start and end in a given day (determined by `thoroughbreds_event_filter`).

The goal is that for each race, the flumine strategy only starts processing events 80 seconds before the start of each race and continues receiving events while the race is in play. My question is, can I use `listener_kwargs` param with multiple `market_ids`? Do I need to set `inplay` to true if I want to monitor events while the race is live, and do I need to set `event_processing` to True? Thank you

*Tags: Deployment, Strategies*

---

**Peter** - *19:04:03*

Use a loose filter to capture all horse racing then use your strategy's `check_market_book` method to filter out the races that you're not interested in, and then the updates prior to the 80 seconds from the start for those you do want. You'll get all updates including inplay, and can access related markets through the `market.event` object.

*Tags: Strategies*

---

## 2023-11-27

**Adrian** - *08:18:30*

Hi Rob

Make sure you've "purchased" all the data you're trying to download. Even for the free data on the basic plan. If there's any missing you'll get an error message.

*Tags: Errors Debugging*

---

## 2023-12-01

**birchy** - *07:40:13*

Using Flumine, how do we access account balance at strategy level? More specifically, if we are using a staking strategy like Kelly, is there a way to set a theoretical bank, e.g. start at £500 and calculate from there?

*Tags: Strategies*

---

**liam** - *08:10:11*

You have `self.clients` however my experience is that many use a fixed bank when using kelly on sports (ie ignore it)

*Tags: Errors Debugging, Strategies*

---

**birchy** - *13:08:12*

Yeah, I've seen `client.account_funds`, but that's populated with the full account balance. I was actually thinking of using a fixed 'bank' due to the fact that it doesn't take long to hit stake market capacity and trying to get £200 matched in one hit is a bad idea. :wink:

*Tags: Errors Debugging*

---

## 2023-12-03

**Adam** - *03:12:22*

Hey all, hoping someone can help me understand the ‘available to back’ price and sizing for runners (horses) in a WIN market.

From a conceptual perspective, I want to understand how the available to back prices equate to traditional book makers odds in order to get a better understanding what I’m looking at in betfair markets. This isn’t for building a strategy, just to help aid my knowledge when analysing betfair market book data.



In a traditional market (e.g. on [http://bet365.com|bet365.com](http://bet365.com|bet365.com) or [http://www.tab.com.au|www.tab.com.au](http://www.tab.com.au|www.tab.com.au) etc) the fixed-odds available in a race is set by the platform and are traditionally BACK odds. In this example on bet365, at a point in time *Horse A is given a 9.7 odds to win*. The same Horse A on Tab sports has 9.8 odds to win.

In Betfair we receive the following EX for Horse A:

```Horse A - EX Details:

 - Available to Back: [{'price': 9.6, 'size': 7.03}, {'price': 9.4, 'size': 18.77}, {'price': 9.2, 'size': 12.29}]

 - Available to Lay: [{'price': 10.5, 'size': 11.79}, {'price': 11, 'size': 18.89}, {'price': 11.5, 'size': 3.6}]

 - Traded Volume: []```

Since betfair operates as a betting exchange, where bettors can bet against each other in a market exchange, we are provided with the market book for that runner. My understanding is that therefore the best available price to back on betfair for Horse A is *9.6* and this is the true reflection of the price provided by the other markets. But this maybe naive because it doesn’t consider the size or volume traded at this price. E.g. since the size is 7.03 is less than half of the size available at price 9.4, maybe is 9.4 a truer representation of the best backed odds for this horse? Do we need to take some average or consider volume or size to find this equivalence?

*Tags: Errors Debugging, Strategies*

---

**James** - *04:42:43*

I tend to think about it as the available price += a couple of ticks unless there is a large volume of money or large gaps there.



It really depends a lot on market dynamics though.



If I have a biiiig gap in prices. IE a back price of $5 and a lay price of $10 the price isn’t really clear. Should you try lay at 5.1 or 7 or 9? it really depends on what you see as value. And just because you can’t get instantly matched, doesn’t mean you couldn’t get matched at on a back bet at $9.5 if you place it with the understanding it might not be matched.



Sometimes you want a quick match because you see a galloper in a harness race so you want to lay it at any price below $50, other times you only want to lay it if someone will take what you think it unders. It;s all pretty dependent on your strategy :slightly_smiling_face:

*Tags: Strategies*

---

**Adam** - *04:54:18*

Yep that makes sense. Thanks James. Super helpful :) I guess big gaps are pretty rare but it’s good to get an understanding of how the market orders stack up 

*Tags: General Technical*

---

**James** - *04:57:11*

and happy to help, theres a lot to learn but thats part of what makes it interesting.

*Tags: General Technical*

---

## 2023-12-07

**Trex44** - *17:46:03*

Hey all, just a general question on the fastest most cost effective way to back test. I currently run back tests on a spot instance on AWS, usually has 256 vCPU's. I use multiprocessing and I usually test on 20,000 horse racing win markets at a time  and save the data to S3 in parquet files, then load up in jupyter. Whilst fast, it still takes me longer than I would like to back test the markets (a single strat takes 10's of minutes). I was wondering if anyone with can suggest how to scale this or if there is something I am doing that is obviously wrong.

*Tags: Deployment*

---

## 2023-12-08

**Mo** - *07:10:00*

Are you doing this in flumine? Have you followed all of the guidance on performance? [https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/](https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/)

*Tags: Performance*

---

## 2023-12-10

**AndyL** - *13:25:41*

I found cprofilev great to profile my strategies, I improve my performance by a factor of about x4

*Tags: Performance*

---

## 2023-12-12

**Sen** - *07:41:14*

Hi All- quick question on best sources of data for UK horse racing. Is there any preferences in this community - I'm looking at the more fundamental side.



Betfair historical data + odds

Timeform 

Tpd (total performance data) 

Any other sources that people think are essential? 



Thanks!

*Tags: Data Quality, Performance*

---

**Justice** - *08:40:33*

Depends what you're doing. RacingPost has fairly accurate data that is very easy to scrape. You can get BSP and in-play high and low from Timeform with a free account, although scraping is very slow- they rate limit you. They have an API but it is seriously expensive. TPD is more for in-running, although I have heard from some in this community that their historical data is doctored to more accurately reflect the position of the horse, and does not reflect the data you receive in real-time- therefore it's useless for modelling

*Tags: Data Quality, Performance, Strategies*

---

**D C** - *08:55:31*

Proform is popular. Apparently you can export the DB which would be useful for anyone doing fundamental modelling. Downside is that it is/was only available for use on Windows (although there is a cheaper sub option to acccess the online racecards only too). This was the case when last I used it which was over a year ago so maybe different now.

*Tags: Strategies*

---

**Justice** - *09:17:59*

[@UUE6E1LA1](@UUE6E1LA1) Yes, you can get access to the Proform DB. If I remember, it installs an old version of SQL Server locally on your machine. You can view the raw database with something like SSMS or DataGrip, export it elsewhere and get it into the format you want

*Tags: Getting Started, Deployment*

---

**Justice** - *13:16:41*

I also use my own IDs. You can match runners within a race using the cloth number, which you can get from the betfair api by asking for the runner metadata.

*Tags: General Technical*

---

## 2023-12-14

**Johnny Boston** - *09:16:52*

Hey, If I am using *`LoggingControl`* should I expect to see a 1:1 match of *`_process_cleared_orders_meta`* orders and *`_process_cleared_orders`* ?



• I am noting that *`_process_cleared_orders_meta`* has 10x the amount of orders data than *`_process_cleared_orders`* 

• by filtering for `order.cleared_order` I approximately, get more of a match, but not that close

What am I missing here? I am using the exact example from *[https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|here](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|here)*

*Tags: General Technical*

---

**Johnny Boston** - *10:16:22*

I use meta to save the strategy name and I am finding i cant id that for all my orders

*Tags: Strategies*

---

**liam** - *10:20:03*

100% they should, yeah whatever you aren't telling me is the problem

*Tags: General Technical*

---

**liam** - *10:25:23*

Search the logs, any patterns on missing orders per strategy/market etc or is it just random

*Tags: Strategies*

---

**liam** - *10:26:18*

flumine version?

*Tags: General Technical*

---

**Johnny Boston** - *10:27:21*

```flumine==2.2.3```

*Tags: General Technical*

---

**Johnny Boston** - *15:22:23*

Well upgrading meant I'm seeing this now...





```{"asctime": "2023-12-14 15:19:58,081", "filename": "worker.py", "name": "flumine.worker", "levelname": "ERROR", "message": "Error in BackgroundWorker poll_market_closure: '1.222524898'", "exc_info": "Traceback (most recent call last):\n File \"/usr/local/lib/python3.10/site-packages/flumine/worker.py\", line 66, in run\n self.function(\n File \"/usr/local/lib/python3.10/site-packages/flumine/worker.py\", line 180, in poll_market_closure\n if _get_cleared_orders(\n File \"/usr/local/lib/python3.10/site-packages/flumine/worker.py\", line 226, in _get_cleared_orders\n market = flumine.markets.markets[market_id]\nKeyError: '1.222524898'", "worker_name": "poll_market_closure", "function": "&lt;function poll_market_closure at 0x7fc4c8e49480&gt;", "context": {}}```

*Tags: Errors Debugging*

---

## 2023-12-15

**Johnny Boston** - *04:02:48*

I wish you would have told me `check_market` doesnt exist in the new Flumine ( `BaseStrategy` ) - cost me a bit :disappointed:

*Tags: Strategies*

---

**Paul** - *18:15:20*

Do they actually differ? I’m not that into dogs but this question surprised me. If the data does not exist, I think you could either create it (lots of decent open source GIS data out there), or normalise it through dog performance at different tracks, maybe?

*Tags: Performance*

---

**Paul** - *18:19:14*

Does your strategy not hold up at say, 1 second after going in-play? Or are you betting into markets that don’t go in play?

*Tags: Strategies*

---

## 2023-12-18

**Peter** - *14:53:25*

[https://betcode-org.github.io/faq/developing-and-testing-strategies/|Betcode FAQ](https://betcode-org.github.io/faq/developing-and-testing-strategies/|Betcode FAQ) (2nd question)

*Tags: General Technical*

---

## 2023-12-19

**liam** - *07:32:16*

Because flumine filters by customerStrategyRef / hostname it won’t pull in manually placed bets without using some middleware (see examples for the startup version)

*Tags: Strategies*

---

**Peter** - *11:12:56*

Flumine will only see its own orders. But you could use betfairlightweight in a work or middleware to inject those that you place directly on the Betfair website. There's [https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|a middleware example that does this](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|a middleware example that does this) to repair the setup when Flumine is re-started that could be adapted to infill your manually-placed bets.

*Tags: Getting Started*

---

## 2023-12-20

**liam** - *08:53:04*

User error 

*Tags: Errors Debugging*

---

**Johnny Boston** - *08:53:46*

```

    def _process_cleared_orders_meta(self, event):

        orders = event.event

        updated_orders = []

        for order in orders:

            if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:

                size = order.order_type.size

            else:

                size = order.order_type.liability



            if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:

                price = None

            else:

                price = order.order_type.price



            order_data = {

                "bet_id": order.bet_id,

                "market_id": order.market_id,

                "selection_id": order.selection_id,

                "trade_id": order.trade.id,

                "date_time_placed": order.responses.date_time_placed,

                "price": price,

                "price_matched": order.average_price_matched,

                "size": size,

                "size_matched": float(order.size_matched),

                "side": order.side,

                "market_type": order.market_type,

                "elapsed_seconds_executable": order.elapsed_seconds_executable,

                "customer_strategy_ref": order.trade.strategy.name,

                "order_status": order.status.value,

                "market_note": order.trade.market_notes,

                "trade_notes": order.trade.notes_str,

                "order_notes": order.notes_str,

                "is_cleared_order": True

                if order.cleared_order

                else False,  # it is either None or True, so need to coerce

                **order.context,

            }

            updated_orders.append(order_data)



        if updated_orders:

            cleared_orders = [o for o in updated_orders if o["is_cleared_order"]]

            [http://logger.info|logger.info](http://logger.info|logger.info)(f"Writing {len(cleared_orders)} cleared orders meta to S3")

            self.write_logs(data=updated_orders, write_table="cleared_orders_meta")```

*Tags: Strategies*

---

## 2023-12-25

**Matthieu Labour** - *17:39:20*

Hi all,



I am looking for guidance to implement properly in Flumine what I have done below in a hacky way. I would be grateful for any help on how to leverage flumine constructs as opposed to the hacks I have in place. Thank you in advance!



The purpose is to run a simulation in tennis with scores and marketdata.



The historical file that is being streamed has both mb and score



Example:

```[...]



"op": "mb", "clk": "APTx4AcAw63/BgDA8+cH", "pt": 1698962123328, "mc": [{"id": "1.220368399", "rc": [{"atb": [[1.55, 133.58]], "id": 51718172}]}]}



{"op": "score", "clk": "", "pt": 1698962127237, "mc": [{"eventTypeId": 2, "eventId": 32753994, "score": {"home": {"name": "Hunter/Mertens", "score": "15", "halfTimeScore": "", "fullTimeScore": "", "penaltiesScore": "", "penaltiesSequence": [], "games": "0", "sets": "0", "gameSequence": [], "isServing": true, "highlight": true, "serviceBreaks": 0}, "away": {"name": "Krawczyk/Schuurs", "score": "15", "halfTimeScore": "", "fullTimeScore": "", "penaltiesScore": "", "penaltiesSequence": [], "games": "1", "sets": "0", "gameSequence": [], "isServing": false, "highlight": false, "serviceBreaks": 0}}, "currentSet": 1, "currentGame": 2, "fullTimeElapsed": {"hour": 0, "min": 0, "sec": 0}}]}



{"op": "mb", "clk": "AJv04AcA+a//BgC29ucH", "pt": 1698962128901, "mc": [{"id": "1.220368399", "rc": [{"trd": [[2.82, 66.15]], "ltp": 2.82, "tv": 916.22, "id": 43352014}, {"atb": [[1.55, 13.23]], "trd": [[1.55, 120.35]], "ltp": 1.55, "tv": 1413.85, "id": 51718172}], "tv": 2330.07}]}



[...]```

And the strategy is the example of the tennis strategy in flumine



```class ExampleStrategy(BaseStrategy):

  def process_market_book(self, market, market_book):

        # process marketBook object

        if "score" in market.context:

            score = market.context["score"]

            print(

                score.match_status,

                score.current_game,

                score.current_set,

                score.current_point,

                score.score.home.score,

                score.score.away.score,

            )```



I have the following hacks in place



In `historicalstream.py`, I combine the lines mb and score lines so I get the score in the mb message

```[...]

    def on_data(self, raw_data: bytes) -> Optional[bool]:

        try:

            data = json.loads(raw_data)

        except ValueError:

            logger.error("value error: %s" % raw_data)

            return



        # remove error handler / operation check



        # skip on_change / on_update as we know it is always an update

        publish_time = data["pt"]

        if data["op"] == "score":

            self.last_score = data

        if data["op"] == "mb" and self.last_score:

            data["mc"][0]["score"] = self.last_score["mc"][0]

            self.last_score = None

        return self.stream._process(data[self.stream._lookup], publish_time)



[...]```



In `simulation.py`, I attach the score to the context of the market



```def _process_market_books(self, event: events.MarketBookEvent) -> None:

[...]

            # process market

            market(market_book)



            if "score" in market_book["streaming_update"]:

                market.context["score"] = Scores(

                    eventId=market_book["streaming_update"]["score"]["eventId"],

                    eventTypeId=market_book["streaming_update"]["score"]["eventTypeId"],

                    currentGame=market_book["streaming_update"]["score"]["currentGame"],

                    currentSet=market_book["streaming_update"]["score"]["currentSet"],

                    fullTimeElapsed={"hour": 0, "min": 0, "sec": 0},

                    score=market_book["streaming_update"]["score"]["score"],

                )

[..]```

*Tags: Errors Debugging, Strategies*

---

## 2023-12-26

**Matthieu Labour** - *18:24:14*

Hello

In the following tennis example [https://github.com/betcode-org/flumine/blob/master/examples/tennisexample.py#L30](https://github.com/betcode-org/flumine/blob/master/examples/tennisexample.py#L30), what is the timestamp for Score and the timestamp for MarketBook that I can use to compare the timeline of those events?

Should it be the Score’s datetime_created or datetime_updated? (They seem to be identical) [https://github.com/betcode-org/betfair/blob/master/betfairlightweight/resources/baseresource.py#L14C28-L14C28](https://github.com/betcode-org/betfair/blob/master/betfairlightweight/resources/baseresource.py#L14C28-L14C28)

and, as far as the MarketBook is concerned, should it be publish_time_epoch, publish_time, _datetime_created (in BaseResource), _datetime_updated (in BaseResource)?

The reason that I am asking is because I want to enable inplay betting within N seconds after the score is established.

Thank you for your help

*Tags: Strategies*

---

## 2024-01-01

**Jonjonjon** - *21:02:58*

I just want to improve my model calibration skills.



I roughly maintained my annual PNL in 2023.



Would like to increase it by 10x this year, which would put me in the [@UBS7QANF3](@UBS7QANF3) league

*Tags: Strategies*

---

**Jonjonjon** - *22:53:57*

What is your strategy for maintaining enthusiasm?

*Tags: Strategies*

---

**PeterLe** - *23:02:16*

more of a reason than a strategy...and that is to test myself against some of the brightest minds. It brings many more benefits that just monetary gains

*Tags: Strategies*

---

## 2024-01-02

**Ricky** - *15:50:09*

Hi everyone! I'm fairly new to the automated betting world and would like to start by getting my hands on some of the historical data that betfair supplies at [https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home). Problem is that I live in Sweden and this service is not accessible for us swedes... Does anyone know an alternate way to get access to historical data? Thanks in advance! :raised_hands:

*Tags: Getting Started, Data Quality, Deployment, Strategies*

---

**Derek C** - *16:00:31*

I can't answer your question, but a lot of us are collecting our own data: [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py)

*Tags: General Technical*

---

## 2024-01-03

**river_shah** - *08:06:11*

Happy new year everyone! '23 saw some good growth within sports betting. My goals for '24 are to keep focusing on more scalable and robust research methods and automating ever more. Also one of the key goals is to find exceptional people to work with. If you are an experienced researcher / trader constrained on technology, capital or market access and could benefit from collaborating, please DM me for an exploratory chat. Focus is sports betting &amp; crypto.

*Tags: Errors Debugging, Strategies*

---

**Trex44** - *11:24:37*

Fair enough but I assumed you would be one of the last to go, had assumed you had good edge and lots of strats given how knowledgable you are with Flumine. If you are worried about that then maybe we all should be! I have considered just focusing on financial markets in light of the affordability checks and what they are likely to do to the liquidity.

*Tags: General Technical*

---

**Mo** - *11:46:13*

1. I have one core strategy and everything else I've tried to expand into hasn't worked. From speaking to other people, I think this is reasonably common

2. I think any of the established players you talk to will tell you this game is getting increasingly harder. How much of that is down to lower liquidity versus increased competition is hard to say. And how much of an impact on liquidity the affordability checks are having is anyone's guess

3. For example, [@U4H19D1D2](@U4H19D1D2)’s shared that his pnl is down 30% for the year. It doesn't take that happening many years in a row for there to be (much) easier ways to make money

4. I can't go into details about the costs but if you think of a number and then multiply it by 100 you might be getting close to the total. Another factor is a lot of them are denominated in foreign currency so the exchange rate is another vector by which I can get be screwed over

5. I've tried to expand into market making and have been getting fucked in the arse by Betfair transaction charges. The lack of support from Betfair to encourage liquidity provision has been really disappointing

*Tags: Strategies*

---

**Trex44** - *12:05:19*

Thanks for the info Mo. Very interesting. I have hardly any edge in horse racing. The strategies have back tested well but have been live for such a short amount of time that I can't get confident enough to put large sums into them yet. They don't require live data feeds but I spend a fair bit on AWS services crunching through the data to find the edge. I picked betfair trading to try my hand at over financial markets back in 2017ish as I thought it would be easier that financial markets :woozy_face:. I did Peter Webb's course about 5 or 6 years ago, met him in person. Lovely guy, but realised I would never have the temperament to sit in front of a screen and do what he does, would much rather automate it but didn't know how then. So it went on the back burner whilst other things took over. Got serious about it again at the start of 2023. Part of the appeal for getting back into it was Flumine, I wanted to learn python and having a project to work on rather than craming leet code is a good way to do it. It has been great for learning python and how AWS works.

*Tags: Deployment, Strategies*

---

**Ricky** - *13:45:14*

I have another question related to gathering data from streams: Is there a way to filter the data stream on a time range? I would like to stream price data for all events of a certain type but only from e.g. a few hours before event start.

*Tags: General Technical*

---

**Trex44** - *17:39:21*

Is there an easy way in flumine to bring up the last traded size for a runner?

*Tags: General Technical*

---

## 2024-01-04

**Tarang Ambaliya** - *08:17:09*

Hello, I have one query,



I am logging in before starting the stream in a long running task.

Will the session token expire during streaming, so i have to explicitly handle that?

*Tags: General Technical*

---

**liam** - *09:34:28*

The token will expire as per the docs however your stream won't be closed, the streaming error handling example will handle this or flumine

*Tags: Errors Debugging*

---

## 2024-01-10

**river_shah** - *10:09:27*

Hi Peter, unfortunately my guess would be as good as any others. Crypto is model driven, short term trading for me.

I am biased long as I believe in the anarcho capitalist, alternate monetary system narrative but the sharpe ratio on that view is pretty miserable

*Tags: Strategies*

---

**Trex44** - *14:00:47*

[@U04NWADNCFR](@U04NWADNCFR) Hey mate, do you know why Athena seems to hate nearly all parquet files? If I use Athena on CSVs its fine but if I use it on the same data saved as parquets it can usually find a way to error and there isn't enough information in the error for me to figure out the issue. For speed and costs I was hoping to save my data as parquets but at the moment can't.

*Tags: Errors Debugging, Performance*

---

**Derek C** - *15:08:37*

most of my Athena data is stored as parquet. Can you share the error?

*Tags: Errors Debugging*

---

## 2024-01-11

**Matthieu Labour** - *02:34:08*

Hello, few questions about connections. How many betfair connections does one flumine instance use?

How many connections does betfair provide per realtime key by default?

Is it easy to get betfair to increase that number of connections?

I believe they provide 200 markets by default per connection and one can ask betfair to increase to 1000 markets per connection.

For Soccer, does someone have experience in the number of connections and markets required? I saw 4800 markets for 336 events in the 5 upcoming days.

*Tags: General Technical*

---

**D C** - *09:21:28*

I think Betfair provide 10 connections by default but you could easily find out how many you have by testing it (I know some people here have 20 - I personally have 10).

I've seen people post here that their request for increase from 200 to 1000 markets were refused. I suspect that things might depend on when you obtained your key. I'd suggest asking Betfair what your limits are (or finding out yourself with suitably chosen market filters), work out if you REALLY do need more and then ask Betfair the question if you do need more.

*Tags: General Technical*

---

**Peter** - *12:17:42*

By default a Flumine instance uses one connection to subscribe to market updates and one to subscribe to order updates. However you can choose not to subscribe to order updates if you don't need them, e.g. when using the market recorder.

*Tags: Data Quality*

---

## 2024-01-14

**Matthieu Labour** - *23:47:21*

Hi Mo, does betfair offer a documented official scores endpoint for tennis? I could not find documentation on it. If not, what sources/services do people use and are folks piping those streams into flumine? Or do folks infer the score from Set and Game and Point markets when they get suspended ?

*Tags: General Technical*

---

## 2024-01-15

**Mo** - *07:57:45*

Yes they do but the documentation appears not to be publicly available. Ask BDP

*Tags: General Technical*

---

**Jonjonjon** - *19:25:13*

I'm only part time, but I could live off my Betfair winnings. A reason I don't is that my model could stop working at any point in time (similar to being able to get fired at any time I guess), and that being employed gives you benefits such as NI contributions (but if you have children, you can claim child benefit and that will give you NI contributions too without having to work).



But in your favour, I think some of the most successful guys here are software devs. I'm a professional-level Python dev (which doesn't mean I'm one of the best, it just means that I'm good enough to convince people to pay me to do it.)



One thing to consider is... What was your worst ever losing day, and how did you feel about it?

*Tags: Deployment, Strategies*

---

**Steve** - *20:40:56*

Hello :wave:  apologies I'm new to this and I'm sure you've probably seen my questions 100 times before, but I was just hoping for a little direction if possible:



1. I'm looking to parse the historic data files for American football data and preferably to analyse in a csv file.  For this I thought I might need to use betfairlightweight rather than flumine? As I'm not looking to do automatic trading atm I thought BFW was the right program for that?

2. If so, I was looking at the [https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py|example historical stream](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py|example historical stream). Given the data file structure is in the format of `{month}/{day}/{marketID}/{...bz2}` I guess I need to traverse the folders and load in each file individually?

3. I've seen people talk about recording prices themselves though the [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|MarketRecorder](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|MarketRecorder) rather than buying historic data after the fact. Do you generally put that on an EC2 or something running 24/7?

Apologies if these are all very basic!

*Tags: Getting Started, Deployment, Strategies*

---

**Jonjonjon** - *21:52:50*

A large budget and exceptional communication skills would be required for buying skills.

*Tags: Errors Debugging*

---

## 2024-01-16

**Tony** - *12:29:16*

hello all i am going to start using the market recorder and have an aws account does any one know if i can get away with a micro ec2 instance or does it need more juice?

*Tags: Data Quality, Deployment*

---

**Jonjonjon** - *12:52:17*

If your strategy is simple, well written and clean, you can even run over 100 of them on a micro instance.

*Tags: Strategies*

---

**PeterLe** - *14:59:38*

I know its a basic noddy question but how do i turn off the Market Stream - 'Latency High' messages?

Would I comment out the line that is printing it, or set the level of the messages so these (info ?) type are missed. Thanks

*Tags: Performance*

---

**ShaunW** - *15:21:38*

.... To account for less than ideal connections you could have the high latency warning threshold calculated on the fly from a recent average + allowable diff? Scope creep tho.

*Tags: Performance*

---

**PeterLe** - *15:22:45*

Well Ive had these on and off intermittently Liam since I started using Flumine (either when running locally or on AWS).

Some days its fine, last 3 or 4 days have been fine for instance..

I usually have the terminal window visible so i can keep my eye on it when doing the day job alongside...ie of any error messages etc..

When the latency message comes through it just streams down the page and I can see other messages

I know its there for a reason but its working and making profits etc

*Tags: Errors Debugging, Performance, Deployment*

---

**PeterLe** - *15:35:15*

I was just thinking...I have two strats running on separate instances (the code/logic for the strat is slightly different on each)

When I get the latency messages I get it on both instances (never on just one or the other), so that would suggest its not the strat code then ?

Could well be the clock timing in some way as you say...

Anyway, if I dont find the problem how can I disable just the latency message Liam?

*Tags: Performance*

---

**Unknown** - *17:41:03*

the pitchforks may get pointed at me for this but - just an opposing view on betfair basic data. because it gets a very bad rap in here and needs some friends



• it is definitely useless if you’re trying to do anything low-latency/inplay/high frequency/purely market data-driven

• if your work (like mine) does not fall into this category, its a totally fine place to start. i found it extremely useful when starting out

• a runner’s last traded price accurate to within 1 minute (available in basic data) is far from useless. could be the thing that makes your purely fundamentals model into something profitable

*Tags: Data Quality, Performance, Strategies*

---

**foxwood** - *18:06:25*

Based on lots of time creep throughout the day on different windows systems, I now always sync time every hour using task scheduler. Helps eliminate that possibility.

*Tags: General Technical*

---

**PeterLe** - *18:15:41*

Thanks [@U02RN7YDRQ9](@U02RN7YDRQ9) the thing is, it sometimes does it when I first start it running, moments after I sync the time. Anyway, I have a Ubuntu system on AWS I might just move my flumine stuff there, simple enough to do 

*Tags: Deployment*

---

## 2024-01-17

**Michael** - *11:26:29*

The other thing about starting out without expecting to earn a living is that you'll probably begin with minimal stakes and be happy with small profits. It's quite easy to make a positive return that way, scaling small profits into large profits is a far harder problem.

*Tags: General Technical*

---

**Unknown** - *20:47:26*

Hi All relatively new, just started reading “Sports Trading On Betfair” appreciate some of you may think these book are useless, but it’s helped me brainstorm a few ideas. 



Anyway the author attached figure 15 which shows for a given horse it’s previous races, including it’s Betfair SP and Inplay high. 



This type of data I would love to scrape/obtain for the horses. Has anyone seen where this exists? The author says the “Timeform/Betfair” website, but I can’t find anything remotely similar. 



If it no longer exists or never did, is my best option to obtain some historic data and try to derive my own BetfairSp/Inplay high? What’s the best way to go about that. 



Appreciate any help :heart: 

*Tags: Strategies*

---

## 2024-01-19

**Derek C** - *12:12:29*

Using the logging control in Flumine is probably the best place to start?

*Tags: General Technical*

---

## 2024-01-21

**liam** - *15:57:59*

streaming_timeout on the market filter / strategy from memory 

*Tags: Performance, Strategies*

---

## 2024-01-22

**AndyL** - *19:54:13*

[@U03FS7KM2NL](@U03FS7KM2NL) sounds like the issue I saw : [https://github.com/betcode-org/flumine/issues/721](https://github.com/betcode-org/flumine/issues/721)

but is now fixed with the latest Flumine version

*Tags: Errors Debugging*

---

**AndyL** - *19:54:50*

was fixed in Flumine 2.5.4

*Tags: Errors Debugging*

---

## 2024-01-23

**Jared King** - *08:17:42*

Hi all, wondering if there is a way to set customerStrategyRef per strategy or is it only instance wide? Currently setting config.customer_strategy_ref = 'my_strategy' just before i place an order. Doesn't seem right.

*Tags: Strategies*

---

**liam** - *08:49:11*

Not really, what you are doing will cause issues, why are you doing this? Flumine is designed to use loggingcontrol to log/store order data

*Tags: General Technical*

---

**Jared King** - *09:12:52*

Hmm my understanding was that you could run multiple strategies eg horseracing strat, greyhound strat, tennis etc. I was using strategyRef as a way for the betfair account to have a ref for my strategy's orders, so when I list cleared orders I can filter by strategyRef. Have I fundamentally misunderstood?

*Tags: Strategies*

---

**Jared King** - *09:16:48*

i create order like this:

```config.customer_strategy_ref = 'HR_main'

trade = Trade(

    market_book.market_id,

    runner.selection_id,

    runner.handicap,

    self,

)

order = trade.create_order(

    side="BACK",

    order_type=MarketOnCloseOrder(self.context["stake"]),

    notes=OrderedDict(strategy='pick_winner'),

)                     

market.place_order(order)```

*Tags: Strategies*

---

**Jared King** - *09:20:27*

my 'setup' is like (eg):

```horseracing_main.py - trading, client and framework created here

greyhounds_main.py

strategies/

   horseracing1.py - BaseStrategy classes

   horseracing2.py

   horseracing3.py

   greyhounds1.py```

*Tags: Getting Started, Strategies*

---

**Jared King** - *09:20:56*

I dont understand how to use loggingcontrol to set customerStrategyRef for each of those strategies. Im sure there is a v good reason, but i would have intuitively set customerStrategyRef in the trade.create_order() function. Thanks for your time, i do love the software.

*Tags: Strategies*

---

**liam** - *09:52:52*

So flumine hacks the `customerStrategyRef` as a filter for which 'instance' makes the bet as opposed to the strategy level (as you want) To handle this the 'flumine' way is to record all of your bets using loggingcontrol and then link them up, an example [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|here](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|here) where you could record the betId and strategy name.



Currently by changing the config you will break the order stream as these orders' wont be received/processed.

*Tags: Strategies*

---

**Jared King** - *11:06:42*

Ahh Thanks Liam, I understand how flumine works now wrt instance/strategy. I will check out your example. Thanks you so much :)

*Tags: Strategies*

---

## 2024-01-24

**Aiyaj Khalani** - *10:43:18*

Hi,



I am using streaming API of betfair with betfairlightweight with live API key. but still the odds which i am getting from streaming API differs from [http://betfair.com|betfair.com](http://betfair.com|betfair.com) and it;s playground. what can be the issue with this?

*Tags: Deployment*

---

**Aiyaj Khalani** - *10:54:27*

its not just about couple of seconds but i am getting completely different results from streaming APi than its showing on betfair website

*Tags: General Technical*

---

**Ralegh** - *11:02:32*

If you’re not logged in on the website it will randomize also, otherwise sounds like an error on your end, maybe wrong market

*Tags: Errors Debugging*

---

## 2024-01-25

**Jared King** - *08:41:42*

Hi All, wondering if someone could help me with the flumine way to close out a bet? eg lay runner at $1.50 and close out all liability when price is $2.50. Currently tracking orders with a separate dict (i know). Should I implement in process_orders? Or use the blotter? One more thing, if I use `get_exposures(self, strategy, lookup: tuple, exclusion=None)` to get strategy/selection exposures, what would the lookup tuple look like?

Many thanks!

*Tags: Strategies*

---

**liam** - *08:58:33*

The flumine way is not to close :wink:



`process_orders` is for processing orders or you can access them via `market.blotter` [https://betcode-org.github.io/flumine/markets/#blotter|docs](https://betcode-org.github.io/flumine/markets/#blotter|docs) 100% use the blotter as its efficient and designed exactly for this



The lookup is `(market_id, selection_id, handicap)`

*Tags: General Technical*

---

## 2024-01-26

**jnik** - *13:40:29*

Hi I have found a winning strategy on greyhounds (around 3 months of data), I've successfully tested this strategy past few days,  but problem is I need to spend 12 hours a day betting and its extremely tiring. So I'd like to automate this.



The information is contained in my own db -&gt; mysql and server is a remote server that runs  ubuntu.



Is their any guide to building this automated bot?

*Tags: Deployment, Strategies*

---

**Mo** - *13:42:01*

Do you know Python?

*Tags: General Technical*

---

**jnik** - *13:43:26*

Yes I know python

*Tags: General Technical*

---

**Mo** - *13:45:55*

I can't comment on that guide. Take a look at [https://github.com/betcode-org/flumine|flumine](https://github.com/betcode-org/flumine|flumine), especially the [https://github.com/betcode-org/flumine/tree/master/examples|examples](https://github.com/betcode-org/flumine/tree/master/examples|examples) and specifically an [https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py|example strategy](https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py|example strategy)

*Tags: Strategies*

---

## 2024-01-27

**Jared King** - *03:35:43*

whats the strategy? ill code it up for you

*Tags: Strategies*

---

**Unknown** - *04:29:56*

I'm trying to track down this section in the betfair api

*Tags: General Technical*

---

**Finn** - *11:08:15*

I don't really know how to use discord but I think this a link that only members can see. Could you find an invite somehow?

*Tags: General Technical*

---

**Jonjonjon** - *12:56:23*

Strangely, some of the psychology and mindset stuff helped me, though I admit that most trading/psychology related material is a scam.

*Tags: Strategies*

---

**Dave** - *15:39:34*

Looking to upgrade to latest flumine version, have been on 1.21.4 for a few years. Noticed a few things changed (renaming Backtest to Simulation), otherwise my backtests are producing the same numbers (which is a massive relief). Any gotchas that come to mind to watch out for, that might not pop up when comparing backtests (e.g. live order entry, maybe logging control behaviour etc)? Thanks in advance. Trying to minimize any disruption in prod...

*Tags: Deployment*

---

## 2024-01-28

**liam** - *08:09:11*

That would explain a lot, I remember the slack having evidence of some professionals but the discord seems to be more of a getting started help forum. They need Misha to come back..

*Tags: Getting Started*

---

**Jonjonjon** - *08:16:40*

Wasn't Misha still at the getting started phase? But with a big starting bankroll? He was kind enough to share his record and it was super volatile.

*Tags: Getting Started*

---

**Jonjonjon** - *08:20:52*

This question probably depends on user. I've got something that generates about £1k per year. Every time I increase the size it starts losing. If I reduce the size it starts winning again.

*Tags: General Technical*

---

**liam** - *08:37:20*

He had the worst personality traits for a gambler, I can only assume the big bankroll didn’t survive 

*Tags: General Technical*

---

**D C** - *10:08:34*

He was a strange one. I remember all that stuff like "my codebase providing the most advanced betting framework out there" was hard to take. Then we had gems like "there is more than one value price for an outcome". Could never get my hear around why someone who had worked for a horse syndicate would start off his solo career with a partner who modelled tennis!! Ended up blocking him on twitter as his ego was writing cheques that his IQ could not deliver. Last I heard he was doing something with GPS and greyhounds.

Honestly surprised he never released an e-book.

Genuinely miss him though. We need more quirky characters!!

*Tags: Deployment, Strategies*

---

**Unknown** - *10:17:04*

1. Download this file: [https://betcode-org.slack.com/files/U4H19D1D2/F06E327EJM8/betcode_slack_export_mar_11_2017_-_jan_16_2024.zip](https://betcode-org.slack.com/files/U4H19D1D2/F06E327EJM8/betcode_slack_export_mar_11_2017_-_jan_16_2024.zip)

2. Install the slack-export-viewer Python package; [https://github.com/hfaran/slack-export-viewer](https://github.com/hfaran/slack-export-viewer)

3. Unzip the export

4. `slack-export-viewer -z ./betcode\ Slack\ export\ Mar\ 11\ 2017\ -\ Jan\ 16\ 2024`

*Tags: Getting Started*

---

**liam** - *14:08:32*

I assume a similar strategy still used, has this been a gradual drop or a big bang when it comes to profit as that is a big difference!

*Tags: Strategies*

---

**JazzMan** - *14:21:41*

Same strategy. Just been improving it over the years by adding in amount matched per second, and spliting it up for different prices

*Tags: Strategies*

---

**JazzMan** - *15:35:14*

I'm 63, so it wasn't a problem for me to retire

*Tags: General Technical*

---

**NAS** - *20:52:14*

I'm running a trade where i want to trade certain selections back and forth up until post time



First tried to implement this logic within process_orders, but seems like that method isn't called on a regular basis (only on betfair order events) so i'm stuck with the code not being run, which is ofc causing problems. idea was to call order.trade for each order and then work with the Trade object if certain conditions were satisfied. Is there a trivial way to have this run every 60s or so using workers?



Then thought the solution would be to move the logic to the process_market_book, but again experiencing problems. Can't retrieve the full Trade object that holds all the orders etc, all I get is a bunch of strings with tradeIDs when calling runner_context.trades. Is there a way to get the Trade Object if in possession of the tradeID?

*Tags: General Technical*

---

## 2024-01-29

**Michael** - *09:50:00*

[@U01S1VB9X9P](@U01S1VB9X9P): To answer your question directly, I would say that  £1 per market is a small profit and is is easy to the point of being trivial. That should be possible with the most basic strategies,. Scaling that to £5 per market does require a bit of nouse, and either some smart analysis or a bit of original thinking, £20-50 per market is quite difficult, but it does make you a proper professional, albeit not the highest rank, and £100 is properly hard. If you can average more than that you're in a very select group.



Obviously it's more complex that that, some types of markets are more friendly than others and it depends what data you have access to and so on, but I wanted just to give you some simple numbers, so that's where I'm putting them.



Winning any amount is great and £1000 a month is terrific if you were previously making zero. When people talk about the difficulty of scaling they really mean that you shouldn't just project multiples of whatever you are winning into the future, and especially that you can't expect your profits to scale with your stakes.



It's worth saying as well that market conditions wax and wayne even if you stay the same or continually improve. Stuff comes along, best ex,  streaming, TPD, Covid,, whatever's next. Sometimes your ship comes in and sometimes it doesn't.

*Tags: General Technical*

---

**Jonjonjon** - *09:58:40*

I don't think making £1 per market would be trivial for someone without a good starting strategy. If done consistently, I'd consider it a great achievement

*Tags: Strategies*

---

**Jonjonjon** - *10:03:27*

If it's from genuine edge, then it represents a starting point for continual improvements, that might lead to the big bucks.



The problem with most educational material out there is just that it's worthless.

*Tags: General Technical*

---

**D C** - *10:26:28*

Yeah the notion of what defines "success" in this regard is probably down to the circumstances of who asks the question. I know when I was employed, that I'd happily take an extra £1K a month tax free on my wages. Wouldn't make you a big fish but if you're making any profit consistently you are doing well IMO

*Tags: General Technical*

---

## 2024-01-30

**Ralegh** - *08:07:04*

Yeah I could do that, and mostly just for debugging to see what orders I have open, I’ve ended up with a duck taped monstrosity but it appears to work

*Tags: Errors Debugging*

---

**liam** - *08:08:58*

if the filter is the same flumine will do this automatically

*Tags: General Technical*

---

**Ralegh** - *08:09:31*

Hmm, alright I need to debug a bit then bc that’s not happening right now

*Tags: Errors Debugging*

---

**liam** - *08:10:19*

logic [https://github.com/betcode-org/flumine/blob/101470b0d2d5c0aecaa68cc76a2057d8b064bb04/flumine/streams/streams.py#L94|here](https://github.com/betcode-org/flumine/blob/101470b0d2d5c0aecaa68cc76a2057d8b064bb04/flumine/streams/streams.py#L94|here), what do your filters look like?

*Tags: General Technical*

---

**Ralegh** - *08:12:49*

I’ve just got

streaming_market_filter([

event_type_ids =[…],

‘market_types=[…],

])

*Tags: General Technical*

---

**Trex44** - *14:25:07*

```QueryFailed: TYPE_MISMATCH: Unable to read parquet data. This is most likely caused by a mismatch between the parquet and metastore schema. You may need to manually clean the data at location '[s3://aws-athena-query-results-630653152022-eu-west-2/tables/3cfa2ebf-9d15-438d-8615-822f68e2af17](s3://aws-athena-query-results-630653152022-eu-west-2/tables/3cfa2ebf-9d15-438d-8615-822f68e2af17)' before retrying. Athena will not delete data in your account.```

Yea they typically look like the above. I don't understand why the metastore schema is different. I generate the schema off of the parquet files using a crawler. If I save the files as csv's I get no problems at all. I crawl the csv's, get the table and then there is no issue.

*Tags: Errors Debugging, Deployment*

---

**Derek C** - *16:43:12*

In the case above it looks like the type of a column has changed. The crawler, I think, doesn't scan every single file but just a sample so it assumes all files are like its samples. I've found this error sometimes disappears if you are not querying the column that is confusing Athena - is the query above selecting all columns or just a subset? does selecting a single column work? If you have a column that in some parquet files is a string and in others numeric then that might explain your error. that could happen where a null field was defined as string but is being treated as floating point somewhere, i.e. nan, in your code.

*Tags: Errors Debugging*

---

**Trex44** - *17:29:56*

Hmm that might be the issue but I did specifically enforce data types on the columns of the pandas dataframe before converting to a parquet so I don't know why the column type would then differ from one parquet file to another.



I think I have found a workaround for now, I saved about 100 of the files as csvs then loaded them up into a data flow in SageMaker and had sagemaker convert them to parquet files. I could then crawl and load these files in a notebook just fine. I am trying it with the whole data set now.  Strangely when I tried this trick with parquet files I got garbage, the data viewer function in the data flow just showed random symbols instead of headers and the data. Yet downloading the same parquet files and converting them to csv worked just fine and showed the data was in tact and readable.  Its all quite odd. The fix will be fine for now but it means I cant just save the days races as parquets and run a crawler on them which is what I really want to do.

*Tags: Errors Debugging, Feature Engineering*

---

**casper** - *19:53:33*

Hey guys, I’ve just started looking at setting up the CricketStream. The _subscribe_to_cricket_matches_() function doesn’t seem to be taking in the usual _initial_clk_ and _clk_ parameters used in the _subscribe_to_markets_/_subscribe_to_orders_ for reconnect. Why is that?

[https://github.com/betcode-org/betfair/blob/master/betfairlightweight/streaming/betfairstream.py#L191](https://github.com/betcode-org/betfair/blob/master/betfairlightweight/streaming/betfairstream.py#L191)

*Tags: General Technical*

---

**Derek C** - *21:53:33*

It's a long time since I wrote the code, but I specifically use this snippet to set the parquet schema deliberately and iirc this is to avoid the pandas/parquet combination messing up my datatypes under certain circumstances such as when a string column is all nulls.



```table = pa.Table.from_pandas(df, schema=parquet_schema,   )```

Using sagemaker, as you say, sounds like a more solid option anyway though.

*Tags: Feature Engineering*

---

## 2024-02-01

**Jorge** - *08:34:53*

Hi guys! Is my understanding correct that in flumine a [https://github.com/betcode-org/flumine/blob/master/flumine/order/ordertype.py#L104C7-L104C25|MarketOnCloseOrder](https://github.com/betcode-org/flumine/blob/master/flumine/order/ordertype.py#L104C7-L104C25|MarketOnCloseOrder) will be simulated/placed at SP price? And a [https://github.com/betcode-org/flumine/blob/master/flumine/order/ordertype.py#L77|LimitOnCloseOrder](https://github.com/betcode-org/flumine/blob/master/flumine/order/ordertype.py#L77|LimitOnCloseOrder) will only be matched if the SP price is better than the one provided? I didn't know LIMIT_ON_CLOSE existed in betfair, that's quite cool:smile:

*Tags: General Technical*

---

**Unknown** - *13:21:23*

Don't know if this is a good place to ask. Does anyone here successfully used VPN to place a bet on the website (the Australia version)? I keep getting geo-blocked errors. I even try to route my internet through one of my VPS, but the errors keep happening.

*Tags: Errors Debugging*

---

**Simon Chan** - *15:43:56*

Thank for the info. I try the same VPN provider on my smartphone and placing bets seems to work

. I use Surfshark with their dedicated IP setup. It looks like my laptop might be part of the problem. I will debug it further tomorrow

*Tags: Getting Started, Errors Debugging*

---

**thambie1** - *17:17:54*

Different IP restrictions for different account geos. Had an IP that was mistakenly showing as from the US which was causing problems for me as a UK account holder

*Tags: General Technical*

---

## 2024-02-02

**Aiyaj Khalani** - *05:06:31*

Hi All,

I have deployed my server in london which is interacting with betfair API. i am calling API for placing bet hosted on my sever. for live events, it seems betfair is taking around 5.5s to place a bet. but since my user is not from the london and its from india, i see extra 1s overhead on sending request to my server. i thought of moving my server to india but then betfair APIs will not be accessible from my server. i was thinking to add VPN in my server to interact with betfair API but again it will add some extra overhead due to VPN proxy. I saw many exchanges place bets around 5.5s, is there any good way to reduce latency in my case?

*Tags: Performance, Deployment*

---

**Aiyaj Khalani** - *07:37:47*

no, i am just thinking about how can we improve latency while placing a bet from my server which is based on india to betfair API

*Tags: Performance, Deployment*

---

**Jorge** - *08:15:23*

Hi guys! In flumine/betfairlightweight, how is the `market_book.runners[0].ex.traded_volume` filled? I thought it contained the traded history in the runner but sometimes I see it empty

*Tags: General Technical*

---

**Simon Chan** - *12:07:55*

What do you mean nuke your cache?  I tried clearing the browser cache and even a different browser to test placing bets, it is still not working. It is weird because I already switched to VPN and can login and navigate the website without any issues. The geo-restriction error only happens when I place a bet.

*Tags: Errors Debugging*

---

**Peter** - *16:43:37*

[@U065R0MLLBB](@U065R0MLLBB) It's not clear to which question you're looking for a solution: obtaining historical data in Sweden, or filtering streams on a time range.



If it's the Swedish thing, we've seen other questions about Betfair-related things happening differently in Sweden, so there are likely to be legal, regulatory or licensing issues that preclude an easy answer.



If it's streaming with a time range - that's not how Betfair streams work. The accepted solution is the one alluded to by [@U04NWADNCFR](@U04NWADNCFR), namely record all the data, persist to a file and then stream the through Flumine setting the [https://betcode-org.github.io/flumine/quickstart/#listener-kwargs|listener kwargs](https://betcode-org.github.io/flumine/quickstart/#listener-kwargs|listener kwargs) to do the filtering for you.

*Tags: Data Quality*

---

**ShaunW** - *17:56:45*

Hi. Try to focus on the specfic things you're investigating. You only need the whole stream for high frequency stuff and that can't really be explored in CSVs, you'd use a backtest for that. But try using a backtest (with no actually betting strategy, just a blitz through the data)  as a means of logging if the data doesn't directly store what you need, or to extract at a lower frequncy, or just in certain conditions.  Also work on just a subset of your data warehouse and then test against the remaining data to safeguard against backfitting and to speed up your analysis.

*Tags: Performance, Strategies*

---

**JL** - *19:14:11*

my bad for being unclear. It's about obtaining historical data from a Swedish account. I understand, I'll contact support and see what they say

*Tags: Data Quality*

---

**Ralegh** - *20:30:08*

Depends what you want them to be better for, if you’re betting then you probably need prices but if you just want an accurate probability model I would use both and stick in a logistic regression 

*Tags: Strategies*

---

**Jonjonjon** - *20:31:29*

For betting. One set is actually the prices. The other set will be some sort of model forecast.



Or maybe I'll be comparing two different sets of modell forecasts

*Tags: Strategies*

---

**JL** - *20:36:01*

this is quite good, compare with BSP:

[https://betfair-datascientists.github.io/modelling/greyhoundModellingPython/#5-evaluate-model-predictions](https://betfair-datascientists.github.io/modelling/greyhoundModellingPython/#5-evaluate-model-predictions)

*Tags: Strategies*

---

## 2024-02-03

**ShaunW** - *00:38:51*

Is it all selections or not.  If a strategy selects runners then the assumption that a randomly seleted sample will have an overall 0ev BSP goes out of the window as it's no longer random.

*Tags: Strategies*

---

**D C** - *10:36:10*

[@U05REBJ21EC](@U05REBJ21EC) how does this approach hold up in situations where you have multi-selections with one winner, but where number of selections is variable across groups (so basically horse race scenario for modelling winner probability). Does this hold up theoretically in such cases or would you have to analyse separately according to field size ?

*Tags: Strategies*

---

**Dave** - *16:29:25*

ROC curve is generally quite good for evaluating classification performance, so you can compare ROC curves of your two sets of estimates.

*Tags: Performance*

---

## 2024-02-04

**Simon Chan** - *13:57:57*

Hi guys, I'm interested in using the betfair Stream API to collect price data. I'm aware that betfair only allows this usage as long as you show some regular betting activity.  Is there anyone doing similar things and can share how to go about it??  My idea is placing bets randomly with small stakes and always immediately hedge out ,but I don't know how many of these "surrogate" bets are enough to satisfy betfair (let's say if I run my collection bot every day)

*Tags: Strategies*

---

**D C** - *14:07:48*

Seen this asked in the past and IIRC there are no fixed thresholds. But when you apply for stream access there used to be a short set of questions you had to answer where you state your expected turnover and things like that and then you'd get a yes or no to access. Not sure if that is still the case now though (it was when I requested it about 4 years ago).

*Tags: Errors Debugging*

---

**Mo** - *14:30:18*

There are no concrete rules that anyone is aware of for how much volume is enough volume. Here’s an example strategy you could run that would probably be better than placing random bets: [https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py|https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py|https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py)

*Tags: Strategies*

---

**Jeff Waters** - *17:32:29*

Hi



Up till now, I've used Flumine for backtesting system ideas. I'm now exploring how to use it for placing bets and creating my own JSON files.



Is it advisable to use multi-processing, or will one process be able to comfortably process multiple streaming markets simultaneously?



If I'm recording JSON files as well as monitoring markets and placing bets in them, is it recommended to have a separate thread or process for each activity? Also, does each market I'm logging JSON for need its own thread?



Thanks in advance.

*Tags: General Technical*

---

**ShaunW** - *19:49:45*

I seperate my data collection and trading [@U013K4VNB6D](@U013K4VNB6D). If you ever need to stop your strategy then the collection is unaffected. Plus a hair of a speed advantage if the trading instance isn't also recording although that can be done asynchronously if you're short of available connections and have to trade and record at the same time. But I'm not using flumine so it could be different in terms of routing one api connection to multiple strategy instances.

*Tags: Performance, Strategies*

---

**liam** - *19:53:03*

As a above, one process is enough to handle multiple strategies (10+ easy), multi processing isn’t helpful when running live 

*Tags: Deployment*

---

## 2024-02-05

**Peter** - *20:45:26*

I wouldn't recommend it. `runner_context` isn't protected and could easily get overwritten. Better would be to store the data in `market.context`.



Personally I'd also consider calculating it in middleware rather than `process_market_book`. This keeps the strategy code clean, makes the result available to all strategies being run on the Flumine instance, and encapsulates the calculation allowing it to be easily re-used elsewhere.

*Tags: Strategies*

---

## 2024-02-06

**Tony** - *14:59:25*

i have had that from alot of folks :joy: doesnt help

*Tags: General Technical*

---

**D C** - *14:59:58*

Well if BF says it's gone through sounds like a problem at the bank end?

*Tags: General Technical*

---

**Jeff Waters** - *17:40:12*

Hi Shaun



I believe that, with back testing, you can separate your data into chunks, and allocate each chunk to a separate process, which speeds up the operation. I believe that each process is run on a separate core, and its memory is independent of the memory used by the rest of the app (though apparently there is still a possibility of race conditions if you're using shared variables, which I don't quite understand).



I would have also expected the stuff in the background to work out how to run code as quickly as possible, but that's not the case in this instance.



As you can probably tell, my knowledge of this stuff is quite vague. For a more technical explanation, see  [https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python](https://stackoverflow.com/questions/3044580/multiprocessing-vs-threading-python)

*Tags: Performance*

---

## 2024-02-08

**Tarang Ambaliya** - *08:10:49*

Hello everyone,



I came across the documentation that default betfair keys are only for personal use and we need commercial keys for commercial purpose. Just wanted to know that are there any limitations on the commercial keys like we have on the default keys (ex: only 10 concurrent streaming connections can be done).



Thank you.

*Tags: General Technical*

---

## 2024-02-09

**mzaja** - *22:41:04*

Can someone explain this behaviour to me? I've created a test strategy which places a single fill or kill order on a single market. It is a dead market with no updates. The order gets placed and then moves to `EXECUTION_COMPLETE` status. Nothing happens afterwards so I would expect no updates. However, Flumine keeps calling `process_orders` like mad for some reason, despite there being no changes in orders. The strategy is initialised with `streaming_timeout=None,` so it should not be down to `snap` being called on cache.

``` Time                    | Source              |   Order count | Order status log

-------------------------+---------------------+---------------+-----------------------------

 2024-02-09 22:35:05,801 | process_market_book |             1 | Pending

 2024-02-09 22:35:05,847 | process_orders      |             1 | Pending

 2024-02-09 22:35:06,009 | process_orders      |             1 | Pending

 2024-02-09 22:35:06,273 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:06,526 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:06,780 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:07,034 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:07,287 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:07,538 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:07,790 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:08,044 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:08,298 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:08,549 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:08,799 | process_orders      |             1 | Pending, Execution complete

 2024-02-09 22:35:09,051 | process_orders      |             1 | Pending, Execution complete```

*Tags: Strategies*

---

## 2024-02-10

**Ben Coleman** - *11:32:48*

Hey guys, I'm wanting to know if there is a way I can filter for markets that start less then 30 mins away using streaming_market_filter for implementation of my strategy using flumine? If not I notice that instead there is an argument in market_filter that allows me to do that and but can this be used when streaming with flumine? Also just generally, is flumine constanty updating the markets that fit the criteria in the market filter or is it just ran every minute or so?

*Tags: Strategies*

---

**Derek C** - *11:57:48*

this goes against the way that streaming works - generally you would subscribe to the markets you are interested in and just ignore the updates until within 30 minutes of start time.

*Tags: General Technical*

---

**D C** - *11:57:56*

The whole point of streaming is for you to pass a filter that is sent one time only - then anything that satisfies it comes through automatically when new markets get created (that also satisfy the filter).

*Tags: General Technical*

---

**Peter** - *13:10:28*

No, you can't use the market_filter for streaming. You have to use the streaming_market_filter. As the previous answers suggest, one-off API calls (market_filter) work in a fundamentally different way to streams (streaming_market_filter).



There is constant addition of new markets and removal of old markets from the stream, but this is done by Betfair rather than Flumine. Flumine's role is to initiate the stream, keep it open and deliver the results (market books or orders, depending on the type of stream) to your strategies.

*Tags: Deployment*

---

## 2024-02-12

**Tony** - *17:59:06*

hey all, i have been thinking about this more recently, how  much data is really needed to create a 'decent' model? I look at football and tennis and how valuable is the last 10 seasons with various changes in each sport e.g. tennis court speed, racquets, fitness, covid etc - I'm a big believer of its the quality of the data rather than the amount. Just interested to see what other people think.

*Tags: Performance, Strategies*

---

**ShaunW** - *18:17:11*

Hi [@U034TBZ5H6F](@U034TBZ5H6F) It depends on the odds of the situation you're looking for but a rule of thumb on sample size is.......:

1. *Calculate Implied Probability*: First, convert the odds into an implied probability. For decimal odds, the formula is Implied Probability=1OddsImplied Probability=Odds1​.

2. *Estimate Variance*: The variance in outcomes is inversely related to the implied probability. For a binary outcome (win/lose), the variance can be approximated by Variance=p×(1−p)Variance=p×(1−p), where pp is the implied probability.

3. *Determine Sample Size*: There's no one-size-fits-all formula for this, but a general rule of thumb is that higher variance requires a larger sample size. A common approach in statistics for estimating sample size for a proportion (which a win probability is) is using the formula: n=Z2×p×(1−p)E2n=E2Z2×p×(1−p)​ where nn is the sample size, ZZ is the Z-score for your desired confidence level (e.g., 1.96 for 95% confidence), pp is the implied probability, and EE is the margin of error you're willing to accept.

4. If in doubt ask chatGPT:smirk:



*Tags: Errors Debugging*

---

**Tony** - *18:19:30*

thanks [@UEA14GBRR](@UEA14GBRR) that is helpful. I've not got much use of chatgpt but might try

*Tags: General Technical*

---

**ShaunW** - *18:25:51*

GPT is a permanent fixture on my right-hand screen.  Productivity should be going through the roof but it comes up with so many avenues to consider it's almost overwhelming.   Best to keep topics to single threads as it takes a while to understand your context.  "tidy my scruff code and put some comments in" is handy too:smirk:.

*Tags: Errors Debugging*

---

**Ralegh** - *19:14:20*

I look at data as proof, you want enough of it to reach some confidence level. The level isn’t some fixed thing, it depends on the complexity model/chance of overfitting and the number of avenues you’ve tried with the data. The more ideas you try the more likely you are to get ‘lucky’ and find something which only made money by accident. Ultimately it’s down to your intuition. 

*Tags: Errors Debugging, Strategies*

---

**Paul** - *21:16:00*

Let’s suppose you have hold out data of x games. Does a model that works on x games produce value? What about 2x? Or 3x? I think it’s always worth thinking about over fitting, especially when the game is evolving. I’d argue some sports evolve faster than others, that gives you opportunities and also presents threats.

*Tags: Strategies*

---

## 2024-02-15

**PeterLe** - *19:32:23*

Quick Question please on the market recorder

I have the TPD Subscription on one key (via betfair)

I have the code to create a file that contains just the TPD data and I have code that creates a file that contains just the normal data



For those of you that have the TPD on your account; When you record, do you create a single file for each race that contains BOTH the normal data and the TPD data that you can back test against?

Thanks in advance

*Tags: Data Quality*

---

**liam** - *19:43:21*

No keep separate so you can simulate using flumine middleware 

*Tags: General Technical*

---

**PeterLe** - *22:48:58*

from flumine.markets.middleware import Middleware ?

*Tags: General Technical*

---

## 2024-02-16

**liam** - *08:11:17*

[https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py|examples](https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py|examples)

*Tags: General Technical*

---

**liam** - *10:45:48*

No flumine does the hard work for you as you have both available, the market object which has the `market.market_book` as it was at time x and the `sports_data` ie the tpd race update, this is how you would develop a TPD strategy as well, no change from simulating to live, its very powerful



```def process_sports_data(self, market, sports_data) -&gt; None:

    # called on each update from sports-data-stream

    print(market, sports_data)```

*Tags: Deployment, Strategies*

---

**Unknown** - *19:51:57*

I think the point of these is to stop your strategy from firing repeatedly due to the same signal on every update to market_book. As a way of throttling order creation?

*Tags: Strategies*

---

## 2024-02-17

**Derek C** - *08:58:39*

I am not sure about your question. Flumine keeps track of which bets are being placed and the total exposure, which you can control with 'max_order_exposure' and 'max_selection_exposure'. There's nothing stopping you from placing multiple bets per second on the same runner if you want to, but these controls help stop you doing it by accident.

*Tags: General Technical*

---

**liam** - *19:25:23*

How have you calculated these returns? Fixed odds or lowest traded?

*Tags: Errors Debugging*

---

## 2024-02-18

**Trex44** - *18:13:24*

Well you can extract the price just before the game starts from BASIC files. Which would be fine for what I am doing as I am not trying to exploit the BSP in any way I just want the prices near or at the off. Only problem is is that the BASIC files are patchy and don't have a lot of  Match_Odd markets for some important matches in them.

*Tags: General Technical*

---

## 2024-02-19

**Aiyaj Khalani** - *09:58:55*

Hey folks,

I was working with betfair stream and wanted to know how will betfairlightweight client will inform application if stream is closed due to session expiry?

*Tags: General Technical*

---

**liam** - *10:07:28*

Betfair won't actually close your stream on session expiry however you should use the error handling [https://github.com/betcode-org/betfair/blob/master/examples/examplestreamingerrhandling.py|example](https://github.com/betcode-org/betfair/blob/master/examples/examplestreamingerrhandling.py|example) or just use flumine and it does it all for you

*Tags: Errors Debugging*

---

**Jonjonjon** - *23:07:41*

I noticed this in my logs:



`"instructionReports": [{"status": "FAILURE", "errorCode": "ERROR_IN_ORDER", "instruction":`



The market was shortly unsuspended.



What's the recommended way to handle this in my code? I would like to attempt to place the order after the market resumes.



For context, this was pre-race.

*Tags: Errors Debugging*

---

**Jonjonjon** - *23:16:50*

The `ERROR_CODE` was `MARKET_SUSPENDED`.

Looking at my historical data file, the market did not get suspended at time of placement.

*Tags: Data Quality, Errors Debugging*

---

## 2024-02-20

**Jared King** - *04:09:27*

Hi All, I run a script before each day's racing to collect 'raceday' data for my models to use. This data is stored in a csv, and a ref to the csv is created in the start function of the strategy. The model is run x seconds before jump in the strategy class to determine bets etc.



I would like to use a background worker to run the raceday data collection every x hours. What is the best practice to do this?

Could I run the collection script, then re-run the strat start function using the worker function? Is there anything else i need to do?



Happy for suggestions on how i can do the whole process betterrerr :slightly_smiling_face:



Update: I have placed the 'get data' code inside the start func and re run start() using the worker. Seems ok?

*Tags: Strategies*

---

**Andrew** - *05:11:45*

I’d put the collection script is a separate schedule process. The output files would be named accordingly. Inside the strategy when each market is ready to process (“few seconds before the jump”) then open the appropriate presaved csv to read your necessary racedata. Alternatively use a database rather than CSV or maybe a preprocessed pickled file for quick loading. But definitely disconnected processes.

*Tags: Strategies*

---

**Mo** - *06:37:25*

[https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py|https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py](https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py|https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py)

*Tags: General Technical*

---

**liam** - *08:38:39*

Yeah this suits a worker, load at startup/hourly/daily and store the data in `strategy.context`  for execution but as above a db (competent) or API (pro)

*Tags: Strategies*

---

**James** - *09:01:16*

Heres a worker I use to do something like  that [@U06DURYFXK7](@U06DURYFXK7)

[https://gitlab.com/-/snippets/3675960](https://gitlab.com/-/snippets/3675960)



I use it to It to filter markets by competitions. I schedule it to run every hour, It fetches the Betfair navigation calls “process_navigation” on any strategy where the method is defined. Each strategy can decide what it needs from nav, and if it doesn’t need nav I just don’t define the process_nav function.

*Tags: Strategies*

---

**Mark Littlewood** - *12:19:12*

Hi All I am new to this chat so looking forward to exploring the content. My background is in Machine Learning modelling for sports. I have also written a series of blog posts on accessing the Betfair API. For those of you interested in no code required sports betting modelling I have created a GUI based approach to ML modelling sports data called MySportsAI. My blog is [https://markatsmartersig.wordpress.com/](https://markatsmartersig.wordpress.com/)

*Tags: Getting Started, Strategies*

---

**Michael** - *12:51:33*

[@U06KP54CEMR](@U06KP54CEMR) I think years and years ago I based my first python based bot on something you published, back in the days of polling. Does that sound right? That was my first contact with python and the first time I stepped away from using third party software. It was super simple, just a big loop of a handful of functions and nothing else. If that is right I think maybe [@U4H19D1D2](@U4H19D1D2) based his early efforts on the same thing. If I've got this right then I've a lot to thank you for.

*Tags: Getting Started*

---

**Michael** - *13:11:06*

[@U016TGY3676](@U016TGY3676) have you had a look at the smartresig _[https://markatsmartersig.wordpress.com/2014/04/24/betfair-api-ng-session-9/|Betfair API-NG Session](https://markatsmartersig.wordpress.com/2014/04/24/betfair-api-ng-session-9/|Betfair API-NG Session)_ sessions? Is that code familiar at all? It'd be interesting to know the history of how everything joins up.

*Tags: General Technical*

---

**birchy** - *13:11:18*

Bloody hell, Fred77 is a blast from the past! We were in regular contact many years ago via a forum called DIYbetfairbots or something similar. We shared a fair bit of knowledge on our web scraping libraries which circumvented the 1 request per second "free API6". His was in PHP, mine was VB6 and then I started using python due to adoption of VPS. God, I feel old now. :grinning:

*Tags: General Technical*

---

**Jared King** - *14:03:51*

[@U01D23DDMTQ](@U01D23DDMTQ) didn't think to use a pickled file (use it for model load, monkey see..) - good suggestion to disconnect. [@U4H19D1D2](@U4H19D1D2) thankyou for your input as always. [@U05C31YKZ1C](@U05C31YKZ1C) you a legend, code err borrowed.

*Tags: Strategies*

---

## 2024-02-21

**Tony** - *12:27:20*

i know this slack is about betcode/flu but does anyone not use the python stuff and maybe written their own in another language ?

*Tags: General Technical*

---

**D C** - *12:33:53*

Yeah there are a few of us who have codebase that is not Python

*Tags: General Technical*

---

**D C** - *12:37:00*

Are you just starting out or already using flumine?

*Tags: General Technical*

---

**Ralegh** - *12:45:46*

I’m rewriting in scala at the moment. I have a habit of blowing things up when they’re in python… Also I use quite a few different APIs and need more granular control over how messages are handled/recorded

*Tags: General Technical*

---

**Ralegh** - *12:47:18*

Just use flumine unless you specifically need other functionality, I only use a subset of features and am using copilot a lot for the rewrite but otherwise it’s a huge time sink

*Tags: Feature Engineering*

---

**ShaunW** - *14:30:28*

PiQ, not taking your own money, not taking money more than once, latency etc. And still it's only part of a system. There's some sort of pre/post trading analytics to build around it too.

*Tags: Performance, Strategies*

---

**Paul** - *14:33:17*

Quite a few people are writing their own stuff, partly because of familiarity with other languages. A lot of those people still seem to use flumine for collecting data because it’s (almost) plug and play, then do some back testing in flumine, then might end up running off and doing stuff elsewhere.

*Tags: General Technical*

---

**Paul** - *14:33:37*

Most of my sport stuff uses betfair lightweight, not flumine. I’m very, very odd though (even my Mum says so).

*Tags: General Technical*

---

**D C** - *14:46:36*

A lot of it depends on motivation and attitudes. I was going to use flumine for simulation but ended up sticking with C++ just so I could reuse lots of my code base. I'm at an age and career point where learning a new language seems like a chore rather than something interesting to do. I readily admit I am seriously jaded and have little enthusiasm for programming anymore and grinding away in my preferred language was the "easier" option.

For someone fresh and enthusiastic though, flumine would be a great start - something that works and has the benefit of many users running it for hours on end and all source there for you to dig deeper if you wanted to.

I REALLY wish that flumine was about when I first started off but all we had back then were some examples on the Betfair API demo code pages. I remember my first ever Betfair bot was written in Perl - simply because the sample Perl bot code available was the best available on the old demo code pages.

*Tags: General Technical*

---

**ShaunW** - *14:57:36*

... But once the code is done then it's no problem. I designed a fully generic trading solution that's driven from my own scripting 'language' and very rarely touch code. It's a rip-off of something I designed in the PAYE job about 20yrs ago, if you don't know what someone wants, give them a generic solution. 



Like DC I'm very much over all that, when I quit work in 2009 I swored I'd never do an IT job ever again.... now look what's happened. :face_with_rolling_eyes:.

*Tags: Strategies*

---

**Peter** - *16:37:52*

I started in PHP as I had a lot of years experience of and expertise in it. Then about 3-4 years ago, I learnt Python as it became clear that it had way better tools for data science and Betfairlightweight specifically for trading, Since then Flumine has arrived to more than justify the initial investment in learning a new language.

*Tags: Strategies*

---

**D C** - *17:17:16*

I definitely feel that knowledge in depth of one language makes learning a new one so much harder because you know what you WANT to happen but you need to get to a certain level of competence in the "new" language before you can even assess whether it is possible or not (eg passing function parameters by non-const reference - is there a Python equivalent to this C/C++ type mechanism??).

Maybe it is just me and the fact that my enthusiasm for coding has evaporated but this was the biggest obstacle for me when I was trying to learn Python (I had all sorts of trouble because of an assumption I made with how to use global variables in Python).

*Tags: General Technical*

---

**Ralegh** - *17:44:01*

Pythons always my goto for research, ideally you should be able to load ML models into other languages at runtime, or use python bindings if available, or at a worst case run python in a separate process and feed targets/forecasts into the trading process. Worst case as in performance but still a very valid approach, anything that latency sensitive can probably be a simpler model anyway

*Tags: Performance, Strategies*

---

**D C** - *17:53:39*

When I was a researcher it was MATLAB that was king and most statisticians I knew were R people.  But now it seems that what I knew as "Applied Statistics" has been rebranded as AI and ML  and the self-proclaimed experts on Twitter advise on Python or R. I did try to use Python for GLMs but whatever I was using just didn't seem able to do what I needed so I stuck with R. R does seem a very ugly language but it does what I need so happy to put up with that.

*Tags: General Technical*

---

**Paul** - *18:48:00*

Use what you can be most productive in, which for a lot of people without coding history in any one language is likely to be flumine. :shrug: 

*Tags: General Technical*

---

**Si** - *18:49:48*

I’m a C# guy at heart, had to learn Java over the last year or two for the day job. Got into this space in the last month or two. Never touched Python before and discovered BF Lightweight, then subsequently this Slack. If you can code well in one language, then ChatGPT or GitHub Pilot is a game changer at picking up another. Use micro prompts, and slowly build out what your doing.  It helped immensely picking up Java and Python.

*Tags: Performance*

---

## 2024-02-22

**liam** - *08:18:07*

flumine is my third attempt at a framework in python, there will probably be a fourth. Using python comes at a cost when it comes to speed but in terms of development speed I am not sure it can be matched

*Tags: Performance*

---

**jp** - *10:07:51*

C# + SQL Server here. Seemed to be a good choice at the time I started, and still works fairy well. If I had started today, I would probably go for flumine, as that takes away the need to write all the basic stuff yourself, and concentrate on strategies.

*Tags: Deployment*

---

## 2024-02-24

**Trex44** - *15:10:59*

Guys does anyone know how the following is possible. I had a strategy misfire and place multiple bets on a single runner (see logs below, strategy is strat_x). It misfired because it thought many of the bets weren't matched (see the size_matched column in the logs attached) so kept placing bets. However the bets were matched and flumine has correctly registered the profit in the profit column in the logs which is odd. I have the following parameters set for the strategy so not sure how it did this.

*Tags: Strategies*

---

**Trex44** - *15:11:07*

```max_selection_exposure=20,

max_order_exposure=20,

max_live_trade_count=2,

context={"stake": 20},```

Been using the strat for months and this has never occurred before. Luckily it won, so I can have a few extra pints later. Note strat_y also fired on the runner and behaved as expected, only placing one bet.

```

bet_id,strategy_name,market_id,selection_id,trade_id,date_time_placed,price,price_matched,size,size_matched,profit,side,elapsed_seconds_executable,order_status,market_note

339328190102,strat_x,1.225197849,42355492,4bb70e45-4e65-4378-adf8-e7eaeaf04249,2024-02-24 14:22:00.240611,3.05,0.0,20,0.0,0.0,BACK,20.14354,Execution complete,"3,3.05,3"

339328318899,strat_y,1.225197849,42355492,e15ba5ef-b32d-425d-a3e4-8c7e9a88a63a,2024-02-24 14:22:20.712415,2.94,0.0,20,0.0,0.0,BACK,20.203602,Execution complete,"2.92,2.94,2.94"

,strat_x,1.225197849,42355492,1565f6b5-d914-418d-bb6b-6529bc9dccc7,2024-02-24 14:22:40.971425,2.8,0.0,20,0.0,0.0,BACK,0.000393,Execution complete,"2.78,2.8,2.78"

339328394435,strat_x,1.225197849,42355492,4c1d4558-64ad-475c-96d1-fff3ea21dff8,2024-02-24 14:22:41.247138,2.8,0.0,20,0.0,36.0,BACK,20.15453,Execution complete,"2.78,2.8,2.8"

339328470581,strat_x,1.225197849,42355492,e75483b8-ea3f-4dc1-af85-c135771fbb7e,2024-02-24 14:23:01.665815,2.8,0.0,20,0.0,36.0,BACK,20.068519,Execution complete,"2.78,2.8,2.78"

339328552148,strat_x,1.225197849,42355492,995c2e62-c080-49c0-a92a-c3ace657498b,2024-02-24 14:23:21.966241,2.88,0.0,20,0.0,37.6,BACK,20.04243,Execution complete,"2.86,2.88,2.86"

339328632974,strat_x,1.225197849,42355492,943480d6-e9e4-48ab-9eae-4916ae78012c,2024-02-24 14:23:42.393435,2.88,0.0,20,0.0,17.86,BACK,20.438296,Execution complete,"2.86,2.88,2.86"

339328660906,strat_y,1.225197849,42355492,79919a27-7708-49c4-a01c-58635c5c0ea6,2024-02-24 14:23:50.201002,2.84,2.86,20,20.0,37.2,BACK,20.193885,Execution complete,"2.86,2.88,2.86"

339328709246,strat_x,1.225197849,42355492,4ab65045-1cd0-42a6-a8e9-d6e9d013263c,2024-02-24 14:24:03.043500,2.84,0.0,20,0.0,36.8,BACK,20.144299,Execution complete,"2.82,2.84,2.84"

339328835977,strat_x,1.225197849,42355492,7daaea34-4351-4fb7-bad3-e2d58ab24584,2024-02-24 14:24:23.507317,2.86,2.86,20,20,37.2,BACK,15.967436,Execution complete,"2.84,2.86,2.84"```



*Tags: Deployment, Strategies*

---

**Peter** - *19:31:18*

Even much used strategies can have subtle edge cases triggered by exceptional real-world circumstances. But tracking them down generally requires forensic examination of the code alongside the logs, neither of which you're providing here.

*Tags: Errors Debugging*

---

**Trex44** - *21:41:51*

These are the only logs I have! I don't actually know how to record any other logs. Regardless of logs or strategies though, I didn't think that it would be possible to have a size matched of zero yet show a profit figure?

*Tags: General Technical*

---

## 2024-02-25

**Andrey Luiz Malheiros** - *12:45:27*

I'm doing exactly as shown in the example you sent to me. I tried using a market ID from a greyhound race that has a closed market and a market ID from a greyhound race that has an open market, but both returned empty results. I have read the Betfair documentation, and the `getRaces` method seems to be paid and works for horse racing. I think this information is available on the following link in the 'Personal Usage' section: [https://developer.betfair.com/en/get-started/#timeform-api|https://developer.betfair.com/en/get-started/#timeform-api](https://developer.betfair.com/en/get-started/#timeform-api|https://developer.betfair.com/en/get-started/#timeform-api)

*Tags: General Technical*

---

## 2024-02-26

**Mo** - *07:58:13*

My apologies, I got confused. I don't use the `get_races` method. I don't even know what this is referring to as it doesn't exist in the betfairlightweight codebase

*Tags: General Technical*

---

**Mo** - *07:59:58*

I've just tried running the above Python code with the following changes:



1. `market_id` set to `1.225270654`

2. Added `app_key=""` argument to `betfairlightweight.APIClient`

With the following output:



```[&lt;RaceCard&gt;]

RaceCard £9000.00 1 Indian Louis, 2 Travail D'orfevre, 3 Nights In Venice

Fia Fuinidh (IRE) Made all in 2m handicap hurdle here last winter and promise when runner-up twice in 2m handicap chases this winter. Faded into fourth back from wind surgery here (2m) 13 days ago and remains to be seen whether this step up in trip suits.

Indian Louis (IRE) Dual point winner who left his qualifying runs over hurdles behind when making a winning chase debut at Musselburgh (2½m) on New Year's Day. Let down by his jumping when only fifth over C&amp;D since but he's probably worth another chance.

Travail D'orfevre (FR) Scored at Carlisle (2m) on his return and good runner-up efforts on all 3 outings since, including C&amp;D. Enters calculations again.

Jolly Nellerie (FR) Fairly useful winning hurdler in France but yet to fire in 4 starts so far for his current trainer, including switched to fences at Newcastle before Christmas. Sports first-time blinkers back from a short break.

Nights In Venice Fair maiden handicap hurdler who took to chasing when fourth of 12 over 3m at Carlisle 3 weeks ago but he didn't deliver much off the bridle (not for first time). Possible the return to shorter,

Ardera Cross (IRE) Nine-time course winner, the latest when accounting for Fia Fuinidh over 2m as the turn of the year. Does need to shrug off a poor run back here since, though.

[{'raceId': '1.2.1240226.1', 'markets': [{'marketId': '1.225270654', 'marketType': 'WIN', 'numberOfWinners': 1}, {'marketId': '1.225270659', 'marketType': 'EACH_WAY', 'numberOfWinners': 2}, {'marketId': '1.225270656', 'marketType': 'PLACE', 'numberOfWinners': 2}, {'marketId': '1.225270658', 'marketType': 'OTHER_PLACE', 'numberOfWinners': 3}, {'marketId': '924.395521980', 'marketType': 'WIN', 'numberOfWinners': 1}, {'marketId': '927.264606400', 'marketType': 'WIN', 'numberOfWinners': 1}], 'distance': 4510, 'startDate': '2024-02-26T14:05:00.000Z', 'raceClassification': {'code': 'H', 'classification': 'Handicap', 'classificationAbbr': 'Hcap', 'displayName': 'Handicap Chase', 'displayNameAbbr': 'HcapCh'}, 'raceTitle': 'New Bet-In-Race With Coral Handicap Chase (Qualifier) (4)', 'raceType': {'key': 'C', 'abbr': 'Ch', 'full': 'Chase'}, 'raceClass': 4, 'course': {'courseId': '1.2', 'name': 'Ayr', 'country': 'Scotland', 'countryCode': 'GB', 'courseType': 'Both', 'timeformCourseCode': 'Ayr', 'surfaceType': 'Turf', 'timezone': 'Europe/London'}, 'going': {'key': 'V', 'abbr': 'Hy', 'full': 'Heavy'}, 'prizeMoney': '£9000.00', 'eligibilityCriteria': {'ageLimitText': '5yo+'}, 'betfairMeetingId': '33052007', 'raceIdExchange': '33052007.1405', 'resultsStatus': 'NoResults'}]

{'raceId': '1.2.1240226.1', 'markets': [{'marketId': '1.225270654', 'marketType': 'WIN', 'numberOfWinners': 1}, {'marketId': '1.225270659', 'marketType': 'EACH_WAY', 'numberOfWinners': 2}, {'marketId': '1.225270656', 'marketType': 'PLACE', 'numberOfWinners': 2}, {'marketId': '1.225270658', 'marketType': 'OTHER_PLACE', 'numberOfWinners': 3}, {'marketId': '924.395521980', 'marketType': 'WIN', 'numberOfWinners': 1}, {'marketId': '927.264606400', 'marketType': 'WIN', 'numberOfWinners': 1}], 'distance': 4510, 'startDate': '2024-02-26T14:05:00.000Z', 'raceClassification': {'code': 'H', 'classification': 'Handicap', 'classificationAbbr': 'Hcap', 'displayName': 'Handicap Chase', 'displayNameAbbr': 'HcapCh'}, 'raceTitle': 'New Bet-In-Race With Coral Handicap Chase (Qualifier) (4)', 'raceType': {'key': 'C', 'abbr': 'Ch', 'full': 'Chase'}, 'raceClass': 4, 'course': {'courseId': '1.2', 'name': 'Ayr', 'country': 'Scotland', 'countryCode': 'GB', 'courseType': 'Both', 'timeformCourseCode': 'Ayr', 'surfaceType': 'Turf', 'timezone': 'Europe/London'}, 'going': {'key': 'V', 'abbr': 'Hy', 'full': 'Heavy'}, 'prizeMoney': '£9000.00', 'eligibilityCriteria': {'ageLimitText': '5yo+'}, 'betfairMeetingId': '33052007', 'raceIdExchange': '33052007.1405', 'resultsStatus': 'NoResults'}```

*Tags: Getting Started, Deployment*

---

**Fab** - *10:01:53*

Hello, I have a question about streaming. Let's say I add 3 strategies to the same Flumine instance, they all use exactly the same streaming_market_filter. For example:



```strategy = ExampleStrategy(

    market_filter=streaming_market_filter(

        event_type_ids=["4339"], # greyhounds

        country_codes=["GB"],

        market_types=["WIN"],

    )

)```

• A) Is my understanding correct that Flumine connects to Betfair WebSocket only once and reuses the same incoming messages for both strategies?

• B) What if I add another strategy to the same Flumine instance, this time for horse racing markets so `event_type_ids=["7"]`: does this establish a new WebSocket connection?

*Tags: Strategies*

---

## 2024-02-29

**Andrey Luiz Malheiros** - *17:23:39*

Hey guys, recently I asked if it was possible to get "Betting Forecast" or "Timeform 1-2-3" from Betfair for greyhound and horse races. Mo helped me with that, and we can use the `race_card.get_race_card` method to obtain that information. However, I'm only getting results for horse races using this method. Is anyone getting "Betting Forecast" results for greyhound races from Betfair?

*Tags: Strategies*

---

## 2024-03-01

**Sen** - *19:56:52*

Hi Guys. So I do a ton of fundamental modelling and predictions and basically no quick in and out strategies. Basically 0 in play betting. I was thinking of trying to improve my execution using the stream API. Does this make sense? Or is it really only designed for the high frequency betting during in-play markets? 



Does anyone have any know how big the latency diff is in terms of millisecodns between the standard API and the stream API is?

*Tags: Performance, Strategies*

---

**liam** - *20:20:38*

I think you should be using the stream regardless tbh, HFT is just as prevalent pre play, it’s the simplicity / scalability that streaming allows which is the big perk 

*Tags: General Technical*

---

## 2024-03-03

**Unknown** - *22:58:59*

Hello, I'm new to simulating bets with Flumine and am having problems with a LAY bet at SP. Here's my code:



```client = clients.SimulatedClient()

framework = FlumineSimulation(client=client)



...



order = trade.create_order(

    side = "LAY",

    order_type = LimitOrder(price=2, size=10, persistence_type="MARKET_ON_CLOSE"),

)

market.place_order(order)```

As you can see my stake is £10. In my test market the runner BSP is 4.00. Let's say the runner wins, I expect to lose £30. But Flumine says I lost £3.29, treating £10 as the liability.



Based on the attached Betfair docs, this would be correct with a *MarketOnCloseOrder*. But not with a *LimitOrder*: Betfair treats the latter's stake as the stake and not liability.



Is my understanding incorrect?

*Tags: Getting Started*

---

## 2024-03-04

**Fab** - *20:50:50*

Thanks for your help Paul and foxwood. My idea was to back a runner a few minutes before the start then lay at BSP, so I wanted back/lay stake must be the same.



I accept that my understanding of BSP laying was incomplete and that type of order cannot be placed with a given stake but only with a given liability.



So, I'll focus on backing only, without closing my "trade" at BSP. Hopefully not many people read this inner thread, given how frowned up "trading" is here. :smile:

*Tags: Strategies*

---

## 2024-03-06

**Prime** - *09:02:00*

mumbai (for example) to london network latency is around 120ms. If this is a manual user thing, I dont think that's particularly relevant. It also doesnt matter where the server is, as long as its "in the path" between the user and London, as the information will still have to travel from their fingers to London at a maximum of lightspeed.

*Tags: Performance, Deployment*

---

**Paul** - *10:13:55*

Trading is definitely not frowned upon here. If you’re referring to the “never green” take, that’s because if you’re doing the work to find value, then put in a +EV bet at kelly stakes, the question has to be what is it you’re actually doing when you green up other than throw away some of that value?

*Tags: Strategies*

---

## 2024-03-08

**ShaunW** - *14:02:51*

Price action doesn't necessarily require an exit strategy [@U012XF5CNPN](@U012XF5CNPN). My interpretation has each decision made on the same criteria. Ie a continual buy/hold/sell loop.

*Tags: Strategies*

---

**Derek C** - *15:19:16*

are you looking for something above and beyond what Flumine simulation does? [https://betcode-org.github.io/flumine/quickstart/#simulation](https://betcode-org.github.io/flumine/quickstart/#simulation)

*Tags: General Technical*

---

**ShaunW** - *17:00:34*

No idea how you'd implement it in Flumine,but that seems like the basic principal.

*Tags: General Technical*

---

## 2024-03-10

**RiskTaker5555** - *06:10:13*

Using Flumine, I can get selection_id but not runner name.

How to get runner name in streaming data ?

*Tags: General Technical*

---

**Unknown** - *09:21:52*

Is there a way to run a simulation of, say, 5 strategies on the same list of markets? In a way that a market gets processed once while all 5 strategies get applied.



Looking at Github example, it makes me think that perhaps each strategy has an independent list of markets, so they would get looped through 5 times, once per strategy.

*Tags: Strategies*

---

**liam** - *10:46:30*

It will do what you want by default, if the market filter matches on each strategy they will ‘share’ the updates 

*Tags: Strategies*

---

**foxwood** - *10:50:58*

Just construct and add all strategies to the framework before calling `framework.run()`

All bets on the market regardless of strategy end up in `market.blotter`

*Tags: Strategies*

---

## 2024-03-11

**Herugrim** - *05:36:14*

Hi,

I'm using the example code here: [https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py)

I'm getting the below error:



```betfairlightweight.exceptions.APIError: SportsAPING/v1.0/listCurrentOrders

Params: {'orderProjection': 'EXECUTION_COMPLETE', 'customerStrategyRefs': ['MELWKS-4535'], 'dateRange': {'from': None, 'to': None}}

Exception: None

Error: {'code': -32099, 'message': 'ANGX-0005', 'data': {'APINGException': {'requestUUID': 'ie2-ang14b-prd-01291035-005af2205e', 'errorCode': 'NO_SESSION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0005', 'data': {'APINGException': {'requestUUID': 'ie2-ang14b-prd-01291035-005af2205e', 'errorCode': 'NO_SESSION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}```

I'm able to use Flumine without the middleware and am only getting the error when calling listCurrentOrders

*Tags: Errors Debugging, Strategies*

---

**Bruno** - *11:33:59*

Hi, I'm having this issue where I think I'm processing the data slower than it's coming in, which leads to a (growing) lag between my code and the market. is there a way to force flumine to discard data above a certain age, or is my only option to use conflation?

*Tags: Performance*

---

**Bruno** - *11:36:17*

I've tried using `streaming_timeout` , but I don't think this is what it's for?

*Tags: General Technical*

---

**liam** - *11:39:33*

Fix your code, use conflation as a last resort 

*Tags: Errors Debugging*

---

**river_shah** - *12:24:41*

Fix your code

*Tags: Errors Debugging*

---

**liam** - *12:24:51*

Just check the latency yourself



```market_book.publish_time or market_book.publish_time_epoch```

But yeah fix your code :joy:

*Tags: Errors Debugging, Performance*

---

## 2024-03-12

**Herugrim** - *00:39:05*

I seem to have fixed this error, just by removing a trading.logout() line

*Tags: Errors Debugging, Strategies*

---

## 2024-03-13

**Unknown** - *09:54:10*

Any thoughts how to handle the Cheltenham race change times gracefully.



Races are delayed 15-25 minutes but neither bookmakers nor betfair can update the off times now. Code is making reference to this:

`market.seconds_to_start` in many places. Any idea how I could patch this to reflect true off time for the 4 Cheltenham races in question

*Tags: General Technical*

---

## 2024-03-15

**Derek C** - *15:14:20*

process_orders is a good place for this logic, see the example: [https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py) which might meet your needs

*Tags: General Technical*

---

**PeterLe** - *16:17:10*

Hi does this help :



```order = trade.create_order(

            side="LAY",

            order_type=LimitOrder(price, stake)

        )

        market.place_order(order)



def process_orders(self, market, orders):

    for order in orders:

        if order.status == OrderStatus.EXECUTABLE:

            if order.elapsed_seconds and order.elapsed_seconds &gt; 1.5:

                market.cancel_order(order)```



*Tags: General Technical*

---

## 2024-03-18

**Testes Squads** - *17:52:39*

Hello everyone and thank you in advance for creating betfairlightweight, it is magnificent!

I would like to know if anyone is interested in giving me the budget for the work to create this function (I wasn't able to create it):



I need a `place_orders()` where I define the maximum value that should be used from my Betfair account, I define the `runner_id` and the function takes the value and invests it in `Back` immediately at the best odds that have values waiting to be matched on the market, if everything had matches and have stake left over, continue going down the ladder and invest at the odds below until there is nothing left of the desired value.

*Tags: General Technical*

---

**Testes Squads** - *17:52:52*

If by chance anyone has this function already created for personal use and would like to share it free of charge, I would also greatly appreciate the help!

*Tags: General Technical*

---

**Testes Squads** - *19:06:24*

Hi [@U05L8PZD2FM](@U05L8PZD2FM) so do I just do it like this below?



```limit_order = betfairlightweight.filters.limit_order(size=xxxxxx.xx, price=1.01)

instruction = betfairlightweight.filters.place_instruction(

    order_type="LIMIT",

    selection_id=45321,

    side="BACK",

    limit_order=limit_order

)

place_orders = trading.betting.place_orders(

    market_id="1.123456789",

    instructions=[instruction]

)```

And I will do this in Tennis in a specific game scenario only in the third set of women's matches, but I will manually define the runner_id and activate the code, it will not run automatically, if it were I could comment in more detail on what I will do.

*Tags: Strategies*

---

**river_shah** - *21:20:53*

I guess, I was being unhelpful. So in the interest of being helpful: Perhaps something is getting lost to me in the description on how you wish to execute, however it seems you are giving up a ton of value. So I would revisit exactly why you are executing the way you are before actually figuring out the technical part

*Tags: General Technical*

---

**Testes Squads** - *21:21:43*

Not to mention [@UGV299K6H](@UGV299K6H) that through Betfair or Geeks Toy I would need to do more steps, change the game page, etc., and there are more chances of human error, so my only mistake would be to write NO instead of YES and vice versa .

*Tags: Errors Debugging*

---

**Michael** - *21:33:43*

Just to re-frame [@U01B8031PM1](@U01B8031PM1)'s question a bit: Do you actively want to clear a few ticks on the ladder, or are you just trying to get your stake fully matched at the best price you can?

*Tags: General Technical*

---

**Testes Squads** - *21:34:15*

[@U01B8031PM1](@U01B8031PM1) There is no specific need, I just don't want to miss an investment opportunity just because I will need to buy 1 or 2 ticks below the current odds, because it will continue to have value. So if there is only 5k for example at odds 2.0 and the total I want to invest is 10k, there is no problem in buying the other 5k at odds 1.98

*Tags: General Technical*

---

**Michael** - *21:43:37*

There's no need to be like that the guy just asked for help.

*Tags: General Technical*

---

**Testes Squads** - *21:45:17*

[@UGV299K6H](@UGV299K6H) I'll take this precaution about 1.01, I've been working for a while but I always ran into difficulties when it came to choosing an odds and ending up with stakes waiting to be matched because the odds dropped by 1 tick or 2 and I had to keep cancelling, so I decided to learn how to program the basic just to get the automation to work, because I had never thought about it and no one had ever told me that betting on some lower odds would correspond to the current odds and so on.

*Tags: Strategies*

---

## 2024-03-19

**Jared King** - *03:59:38*

Hi All, is there a way to limit the number of connections used per strategy?

*Tags: Strategies*

---

**James** - *10:02:55*

Am i able to get market catalogues from anywhere once the market has closed?

The API doesn’t return them, and the Betfair historical data also doesn’t seem to include them.

*Tags: Data Quality*

---

**Mo** - *10:08:21*

Depending on what you need, [https://github.com/mberk/betfairmappings](https://github.com/mberk/betfairmappings) may help although desperately needs a refresh

*Tags: General Technical*

---

**James** - *10:10:08*

I’m just looking for a chunk of recent AU horse catalogues. I’m migrating some strategies from another framework and haven’t recorded any horse data on Flumine for back testing sake. I use the catalogue to get race distance and just want to be able to develop without having to wait for a real race.

*Tags: General Technical*

---

**James** - *10:11:06*

Thanks [@UBS7QANF3](@UBS7QANF3), thats probably enough to help me with the dev/testing cycle. I’ll start collecting them myself from here.

*Tags: General Technical*

---

**James** - *10:55:28*

Actually looking closer it seems its the market_name I need which doesn’t appear to be in your mappings. Thanks though.



I’ll just record some and try again tomorrow with a days worth of data to debug the strat.

*Tags: Errors Debugging*

---

**mon mon** - *10:55:31*

hi all. I am trying to get the current account balance from within a strategy but can't seem to extract it - is there a std way to do this?

*Tags: Strategies*

---

## 2024-03-20

**Mo** - *08:13:55*

OK good, race cards should be available for those



Example use of race card endpoint in betfairlightweight: [https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py](https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py)



You should probably adapt the market catalogue example middleware from flumine unless someone can just share a race card middleware with you: [https://github.com/betcode-org/flumine/blob/master/examples/middleware/marketcatalogue.py](https://github.com/betcode-org/flumine/blob/master/examples/middleware/marketcatalogue.py)



[@U4H19D1D2](@U4H19D1D2) worth adding a race card middleware example?

*Tags: General Technical*

---

**liam** - *08:34:49*

For live you need a worker, here is mine



```import logging

from flumine.utils import chunks

from betfairlightweight import BetfairError

from flumine.events.events import CustomEvent



logger = logging.getLogger(__name__)





def poll_race_card(context: dict, flumine) -&gt; None:

    client = flumine.clients.get_betfair_default()

    trading = client.betting_client

    if trading.race_card.app_key is None:

        trading.race_card.login()

    markets = _get_markets(flumine.markets)

    for market_ids in chunks(markets, 10):

        try:

            race_cards = trading.race_card.get_race_card(market_ids=market_ids)

        except BetfairError as e:

            logger.error(

                "poll_race_card error",

                exc_info=True,

                extra={"trading_function": "get_race_card", "response": e},

            )

            continue

        if race_cards:

            flumine.handler_queue.put(CustomEvent(race_cards, callback))





def _get_markets(markets: list, event_type_id: str = "7") -&gt; list:

    """Restrict to open and eventTypeId"""

    _markets = []

    for market in markets:

        if not market.closed and market.event_type_id == event_type_id:

            _markets.append(market.market_id)

    return _markets





def callback(flumine, event):

    # update market context

    race_cards = event.event

    for race_card in race_cards:

        for market in flumine.markets:

            for race_card_market in race_card.race.markets:

                if market.market_id == race_card_market.market_id:

                    market.context["race_card"] = race_card```



*Tags: Errors Debugging, Deployment, Strategies*

---

## 2024-03-21

**liam** - *08:55:11*

Have you looked at what is available in `market.blotter`?



[https://github.com/betcode-org/flumine/blob/bfaa5def6dc66794f088d9e4fed539c3c3043926/flumine/markets/blotter.py#L161|code](https://github.com/betcode-org/flumine/blob/bfaa5def6dc66794f088d9e4fed539c3c3043926/flumine/markets/blotter.py#L161|code)

*Tags: General Technical*

---

**tone** - *13:55:26*

Thanks Liam. I think selection_exposure(self, strategy, lookup: tuple) might be what I'm looking for, but I'm having a bit of trouble using it. What is the "lookup" param for and how do I pass it?

*Tags: Strategies*

---

**Brøndby IF** - *23:09:29*

Guys, I saw a conversation here a few days ago suggesting limiting the number of ticks below the current odds when using `place_orders()` (strategy where must enter the market immediately and not set a standard odds for everything, but instead define how many ticks would be an acceptable limit). If anyone's interested, I've created (to keep it handy for perhaps a future need) a function that accepts both tick increments and decrements. In case it's useful for anyone, here it is (if by any chance someone has a smarter method than this or if any errors are discovered, please let me know):



```from betfairlightweight import utils



def tick_ladder(current_odds:float, ticks:int) -> float:

    """

    Limit odds based on the reference odds and the number of ticks away.



    Args:

        current_odds (float): The reference odds to be used.

        ticks (int): The number of ticks to be added or subtracted from the reference odds.

                     Positive values move up the ladder, while negative values move down the ladder.



    Returns:

        float: The new odds after applying the ticks adjustment.



    Raises:

        ValueError: If the adjusted index exceeds the limits of the ladder.

    """

    full_ladder = [1.01]

    ticks_list = list(utils.TICK_SIZES.keys())



    for i, pattern in enumerate(ticks_list[:-1]):

        while full_ladder[-1] < ticks_list[i+1]:

            new_value = round(full_ladder[-1] + TICK_SIZES[pattern], 2)

            full_ladder.append(new_value)



    index_current_odds = full_ladder.index(current_odds)

    if index_current_odds + ticks < len(full_ladder):

        return full_ladder[index_current_odds + ticks]

    else:

        raise ValueError("Exceeded ladder limits")```

*Tags: Errors Debugging, Strategies*

---

## 2024-03-22

**liam** - *06:25:33*

Any reason you wouldn't just use flumine's version? It's optimised



```from flumine.utils import price_ticks_away```

*Tags: General Technical*

---

**Brøndby IF** - *14:14:30*

[@U4H19D1D2](@U4H19D1D2) I confess to you that the only reason is that I only read the chat, I didn't see it in the `utils` of `betfairlightweight` and as I was looking for something to pass the time doing something useful and I went to VSCode to write. It didn't occur to me to look at `flumine`. But it was worth it, I managed to clear my mind from work a little! Haha

*Tags: General Technical*

---

**Brøndby IF** - *14:20:07*

And [@U4H19D1D2](@U4H19D1D2) `price_ticks_away` of `flumine` is perfect :clap:

*Tags: General Technical*

---

## 2024-03-24

**Ben Coleman** - *01:14:11*

Hi all, I have a query about the way flumine handles a large running time of process_market_book and process_orders. If one of these functions is in the process running and in the interim another update occurs, does flumine just delay processing the new update and wait for the functions to finish running or does it try to handle both concurrently? Also slightly different question, what is the benefit of separating process_market_book and process_orders if they are only ever called at the same time anyway? At the moment I am placing trades and processing old trades in process_market_book and I'm wondering if it would more more time efficient to utilise process_orders as well.

*Tags: General Technical*

---

**liam** - *10:20:42*

Not sure I understand the question but a strategy would only get orders it has created in process_orders 

*Tags: Strategies*

---

## 2024-03-26

**Brøndby IF** - *14:45:26*

I don't have slack premium so I can't read old messages (only at most 2 months ago) and I believe this topic has been discussed before so I apologize in advance for a repeated question.



Well, could tell me the best way or if there is at least a way to filter the soccer games that have Betfair's "Live Video"?



In addition to the endpoints for collecting markets, I have also searched in `in_play_service.get_scores` and found nothing regarding this event detail.

*Tags: Deployment*

---

**Brøndby IF** - *14:49:33*

I searched for keywords in the entire Betfair documentation and couldn't find anything about this either, I must be missing something.

*Tags: General Technical*

---

**liam** - *15:03:05*

Is it not returned in any of these endpoints as well?



[https://github.com/betcode-org/betfair/blob/master/examples/exampleinplayservice.py](https://github.com/betcode-org/betfair/blob/master/examples/exampleinplayservice.py)

*Tags: General Technical*

---

**Brøndby IF** - *15:12:49*

[@U4H19D1D2](@U4H19D1D2) I tried to analyze both possibilities, but I couldn't find it (I apologize if I'm doing something wrong), see the `vars` of each of them from a live tennis event (`eventId`: 33136177) for example as there is no live football event with Live Video:



get_event_timeline:



```{'elapsed_time': 0.341503381729126, '_datetime_created': datetime.datetime(2024, 3, 26, 15, 5, 32, 960247), '_datetime_updated': datetime.datetime(2024, 3, 26, 15, 5, 32, 960247), '_data': {}, 'event_id': None, 'elapsed_regular_time': None, 'event_type_id': None, 'in_play_match_status': None, 'status': None, 'time_elapsed': None, 'score': None, 'update_detail': []}```

get_scores:



```{'elapsed_time': 0.34694671630859375, '_datetime_created': datetime.datetime(2024, 3, 26, 15, 8, 17, 616935), '_datetime_updated': datetime.datetime(2024, 3, 26, 15, 8, 17, 616935), '_data': {'eventTypeId': 2, 'eventId': 33136177, 'score': {'home': {'name': 'Daniel Rincon', 'score': '0', 'halfTimeScore': '', 'fullTimeScore': '', 'penaltiesScore': '', 'penaltiesSequence': [], 'games': '3', 'sets': '0', 'gameSequence': [], 'isServing': False, 'highlight': False, 'serviceBreaks': 0}, 'away': {'name': 'Carlos Taberner', 'score': '0', 'halfTimeScore': '', 'fullTimeScore': '', 'penaltiesScore': '', 'penaltiesSequence': [], 'games': '3', 'sets': '0', 'gameSequence': [], 'isServing': True, 'highlight': True, 'serviceBreaks': 0}}, 'currentSet': 1, 'currentGame': 7, 'fullTimeElapsed': {'hour': 0, 'min': 0, 'sec': 0}}, 'event_id': 33136177, 'elapsed_regular_time': None, 'elapsed_added_time': None, 'event_type_id': 2, 'match_status': None, 'time_elapsed': None, 'time_elapsed_seconds': None, 'status': None, 'current_day': None, 'current_set': 1, 'description': None, 'match_type': None, 'current_game': 7, 'current_point': None, 'full_time_elapsed': &lt;betfairlightweight.resources.inplayserviceresources.FullTimeElapsed object at 0x0000023DDC97B400&gt;, 'score': &lt;betfairlightweight.resources.inplayserviceresources.Score object at 0x0000023DDC97B010&gt;, 'state_of_ball': None}```

*Tags: Deployment*

---

**JP** - *23:50:19*

Hi all, this is probably a n00b question but I couldnt find the answer in the slack. When I try to run two strategies at once I get an error:

```{"op":"status","id":3002,"statusCode":"FAILURE","errorCode":"MAX_CONNECTION_LIMIT_EXCEEDED","errorMessage":"You have exceeded your max connection limit which is: 2 connection(s).You currently have: 3 active connection(s).","connectionClosed":true,"connectionId":"204-260324233850-3056148"}```

This seems like a crazy low number of concurrent connections. Could this be because it’s a delayed key?

*Tags: Errors Debugging*

---

## 2024-03-27

**Ammar** - *18:22:54*

Has anyone done any work to run flumine on AWS (or similar) - I’d be interested to have control and visibility over it on the go; but not sure how much of a lift it would be …

*Tags: Deployment*

---

**Ammar** - *19:37:49*

how the strategy is doing when running, being able to force a terminate, being able to throw another market in there parametrically

*Tags: Strategies*

---

**Ammar** - *19:45:33*

fundamentally, I can imagine a `flask` process running somewhere wrapping around `flumine` + some other functionality

*Tags: General Technical*

---

**Lee** - *19:58:23*

You can log your bets using [https://betcode-org.github.io/flumine/controls/#logging-controls|logging controls](https://betcode-org.github.io/flumine/controls/#logging-controls|logging controls) to csv/db/api/etc. I certainly wouldn’t be wrapping flumine into a flask app.

Streaming doesn’t work in the way of adding markets on the fly, you have a wide filter which automatically picks up new markets. You decide in the strategy if you want to bet on that market.

As for the kill switch, you can stop flumine with a [https://github.com/betcode-org/flumine/blob/master/examples/workers/terminate.py|TerminationEvent](https://github.com/betcode-org/flumine/blob/master/examples/workers/terminate.py|TerminationEvent) or more simply, just turn off the vm.

*Tags: Strategies*

---

**liam** - *21:19:10*

I use docker on EC2 with portainer as my UI, let someone else do the hard work 

*Tags: Deployment*

---

**liam** - *21:19:48*

Also you will be using streaming so there will be no need to add markets / restart etc 

*Tags: General Technical*

---

**Ammar** - *21:31:32*

&gt;  I use docker on EC2 with portainer as my UI, let someone else do the hard work

Is there a recipe somewhere you’d be comfortable sharing?  Just want to get a sense of how everything gets set up

*Tags: Deployment*

---

**Ammar** - *21:33:32*

I’m currently building my data pipeline and ML model after a few years of eyeballing and manually validating something;  and want it all joined up so the trading is happening based on the latest from the model

*Tags: Strategies*

---

## 2024-03-28

**liam** - *07:22:46*

You are in the right place :+1: 



Tbh not sure there is anything that would help, what experience do you have when it comes to deployment? 

*Tags: Deployment*

---

**Adrian** - *09:11:16*

Hopefully this isn't as dumb of a question as my last. But I need to get runner names from Betfair market catalogue in order to match them up with my data. Is the catalogue only available from live markets? If so, is there any functionality in flumine to record these?

*Tags: Deployment*

---

**liam** - *09:19:16*

Yes, the example market recorder will do it, which I assume you are already using?

*Tags: Data Quality*

---

**Adrian** - *23:13:05*

Now to figure out how to add them to the markets

*Tags: General Technical*

---

**Adrian** - *23:21:24*

I found this. Hopefully can get it to work [https://github.com/betcode-org/flumine/blob/master/examples/middleware/marketcatalogue.py](https://github.com/betcode-org/flumine/blob/master/examples/middleware/marketcatalogue.py)

*Tags: General Technical*

---

## 2024-03-29

**PeterLe** - *17:23:32*

Is it good practice to save your log files?

At the moment my stuff is just sent to the terminal window.

It seems I can do it like so :



`import logging`

`import time`

`from pythonjsonlogger import jsonlogger`



`# Logger setup`

`logger = logging.getLogger()`

`custom_format = "%(asctime)s %(levelname)s %(message)s"`

`formatter = jsonlogger.JsonFormatter(custom_format)`

`formatter.converter = time.gmtime`



`# Adding StreamHandler to output to sys.stderr`

`log_handler_stream = logging.StreamHandler()`

`log_handler_stream.setFormatter(formatter)`

`logger.addHandler(log_handler_stream)`



`# Adding FileHandler to output to a specified file`

`log_file_path = 'path/to/your/logfile.log'`

`log_handler_file = logging.FileHandler(log_file_path)`

`log_handler_file.setFormatter(formatter)`

`logger.addHandler(log_handler_file)`



`logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))`



What happens if you encounter an issue (Eg latency high )which generates lots of messages, is there not a danger you could fill your disk space up quickly with the output etc?

I expect the answer will be yes - save your log files, but what is considered good sys management ? do you clear them out each day etc?

Thanks in advance

*Tags: Getting Started, Performance*

---

**Peter** - *18:04:07*

The answer is, as you suspect, yes. You'll need them if you're need to track down a problem.



How big they are depends on why you're logging. Normally they'd be logging at level ERROR or CRITICAL, but if you're trying to track down an issue, you might need to set them to INFO or even DEBUG, in which case they'd be much larger.



How long to keep them for? It depends on your reason for logging. Normally I clear them out frequently, but I have some that I keep forever as they log data that helps me to develop new signals.

*Tags: Errors Debugging*

---

**PeterLe** - *18:09:55*

Thanks for the reply Peter

That makes sense.

I can see that Im being limited by max_live_trade_count etc (as the window scrolls past!) so wanted to look more in depth at the logs...

Im keen to look at the info to help with signals as you say too

Thanks Again

*Tags: Deployment*

---

**liam** - *19:13:51*

Yeah, just setup cloudwatch and all your problems will be solved. Then you can add something like sentry to tell you when something bad happens (Err/Critical) 

*Tags: Getting Started*

---

## 2024-03-31

**D C** - *17:38:15*

I'd imagine the implied chance of winning from a TPD based model.

*Tags: Strategies*

---

**James Scott** - *21:47:53*

Do you need to buy a TPD license to get a 200ms connection with the Betfair api?

*Tags: General Technical*

---

## 2024-04-01

**Paul** - *08:25:52*

Why would betfair increase latency across all their markets if you don’t have a license for a third party data product for one sport in one country?

*Tags: Performance*

---

**The Marco** - *10:50:55*

Very much trying to wrap my head around lightweight/flumine. Will start with one question, but I have quite a few. Don't want to annoy everyone with a flumine of questions right away, though.



• I can get to competition_id > event_id > market_id using lightweight. 

• When it comes to market_book I can get it via lightweight, like so 

```market_books = trading.betting.list_market_book([mkt_id], price_projection={'priceData': ['EX_BEST_OFFERS']},                    lightweight=True)```

However, following [https://betfair-datascientists.github.io/api/apiPythontutorial/#get-market-books](https://betfair-datascientists.github.io/api/apiPythontutorial/#get-market-books) In [212] (about halfway through the page) suddenly uses dot syntax to access runner_book, though it was not introduced earlier, as far as I can tell. This syntax seems consistent with that in flumine's example strategy and plausibly comes from the MarketBook class inside bettingresources.py in betfairlightweight. I don't really get how to use it though.



Q: how do I get prices (EX_BEST_OFFERS or similar) using this class and dot syntax?

*Tags: Strategies*

---

**liam** - *10:59:06*

If you plan on using flumine is there any reason you don’t want to use the dot syntax? 



But to answer your question just use your debugger to have a look or just print the dict 

*Tags: Errors Debugging*

---

**Aiyaj Khalani** - *13:57:59*

Hi All,



i am frequently getting TEMPORARY_BAN_TOO_MANY_REQUEST error when trying to login using betfair API, however login from betfair UI works fine. doc says that whenever you gets 100 requests per min for login, you're getting banned for 20 mins. but i dont see that much login requests i am making, what could be the issue in this case?

*Tags: Errors Debugging*

---

**The Marco** - *20:23:04*

I have recorded a few markets using the marketrecorder.py script included in the examples. If I understand correctly, the .json file is sort of metadata about the event recorded, the actual data being in the file named after the market_id. The structure seems consistent with the historical Data Feed Spacification here [https://historicdata.betfair.com/Betfair-Historical-Data-Feed-Specification.pdf](https://historicdata.betfair.com/Betfair-Historical-Data-Feed-Specification.pdf)



I would like to start with something very trivial like plotting matched prices for runners over time, backtesting much later. Is there an existing jupyter notebook / python script that I could refer to as an example?

*Tags: Data Quality*

---

## 2024-04-02

**Aiyaj Khalani** - *08:29:52*

would this error be raised only due to Login attempts or is there any other possibility for this error?

*Tags: Errors Debugging*

---

**Aiyaj Khalani** - *08:35:46*

i also tried to reproduce same from making curl for login in my system and exactly after 16-17 successful attempts, i got this temporary ban error. so i am sure that we're not exceeding 100 login requests

*Tags: Errors Debugging*

---

**ShaunW** - *13:54:01*

Using the wrong password would (should!) freeze the account rather than just giving a temporary error, and commercial software should use a client interactive logon with 2FA anyway.  Storing people's passwords and linking accounts definately sounds like one for Neil to look at.

*Tags: Errors Debugging*

---

**Michael** - *14:11:38*

You might want to consider how to give yourself a bit of comfort, just don't do it at the cost of your net profits.

*Tags: General Technical*

---

**Michael** - *14:17:22*

I'm not going to help you more than this so don't dm me about it; but there are some creative (but legitimate) options. If you're not that imaginative then maybe see if bumping your stakes slightly above the optimal increases your volatility without costing you overall. Otherwise you can try churning in a different sport at low margin, just don't do it in your main betting sport. Don't shit where you eat.

*Tags: Strategies*

---

## 2024-04-06

**Peter** - *12:15:23*

Extending [@U02RN7YDRQ9](@U02RN7YDRQ9)’s helpful advice ... my approach is to reduce all horse and greyhound names to lower case, remove any trap numbers, punctuation-like characters and the now lowercase (res) indicators and strip any remaining whitespace from the beginning and end, to create standardised names that match nicely to similarly standardised names from other data sources.

*Tags: General Technical*

---

**Justice** - *20:30:55*

A question for TPD folks on here. How common are missed updates? I've been collecting data for a few months and I've noticed that often the gaps between updates don't reflect the granularity of the data I should be receiving. This happens approx 1/100 updates according to my queries. By far the worst offender is Wolverhampton- there has been a pattern of missing updates every 10s:

```[2024-03-04 19:00:49 WRN] Data loss - received 19:00:48.800 - last received 19:00:48.200

[2024-03-04 19:00:59 WRN] Data loss - received 19:00:58.800 - last received 19:00:58.200

[2024-03-04 19:01:09 WRN] Data loss - received 19:01:08.800 - last received 19:01:08.400

[2024-03-04 19:01:19 WRN] Data loss - received 19:01:18.800 - last received 19:01:18.400

[2024-03-04 19:01:29 WRN] Data loss - received 19:01:28.800 - last received 19:01:28.200```

I am starting to become paranoid there is an issue my end. Is this normal or should I be investigating further?

*Tags: General Technical*

---

**Unknown** - *21:12:19*

I get quite regular dropouts - for many seconds sometimes. I've queried it before with Neil and posted about it here before. GPS stream basically just disappears. I've verified it across multiple instances and the gaps are the same. I assume it is a TPD side issue. Even if it were a bottleneck or error on my side, I should still get the queued messages on the socket buffer - but I just don't get them. Example below:

*Tags: Errors Debugging*

---

## 2024-04-07

**Justice** - *12:53:41*

[@UUE6E1LA1](@UUE6E1LA1) I have to collect data through their approved vendors (Bet Angel) as I've had no success trying to get direct API access and have had no reply with regards to getting sports data access for my Betfair API key

*Tags: General Technical*

---

**liam** - *12:54:18*

What is the latency like?

*Tags: Performance*

---

**Unknown** - *13:15:31*

Yes, the difference between the feed time and the time received at my script. I am guessing it will have to route from the course, to TPD then to 3rd party vendor apps hence the poor latency

*Tags: Performance*

---

**Justice** - *13:24:02*

I would love to just get the raw data directly without having to pay fortunes for commercial terms but oh well. The script runs on Azure, in the the UK South region, so it's not an internet problem

*Tags: General Technical*

---

**Justice** - *13:28:25*

Potentially, though I would have to de-obfuscate the code and I suspect most the latency is from the extra hop to TPD from the course then to the vendor

*Tags: Performance*

---

**Justice** - *13:33:03*

I recall the UDP feed I got 4 or 5 years ago was straight from the course, the latency was more like what [@UUE6E1LA1](@UUE6E1LA1) is reporting

*Tags: Performance*

---

## 2024-04-08

**Trex44** - *10:03:33*

Hey guys can anyone help me resolve the following error.  I run three flumine instances, one for data collection, one for horse racing and one for football. I don't understand how I have hit a connection limit this morning seeing as all I did was reboot the horse racing and football instances and add some new strategies to the football instance before the reboot. I thought each instance of FLumine was one connection? So I don't know how I am close to the Max connection limit of 10.

```

{"asctime": "2024-04-08 08:57:25,093", "levelname": "ERROR", "message": "MarketStream 5008 run error", "exc_info": "Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/flumine/streams/marketstream.py\", line 44, in run\n    self._stream.start()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 67, in start\n    self._read_loop()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 233, in _read_loop\n    self._data(received_data)\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 274, in _data\n    raise ListenerError(self.listener.connection_id, received_data)\nbetfairlightweight.exceptions.ListenerError: connection_id: 107-080424085725-430913, data: {\"op\":\"status\",\"id\":5009,\"statusCode\":\"FAILURE\",\"errorCode\":\"MAX_CONNECTION_LIMIT_EXCEEDED\",\"errorMessage\":\"You have exceeded your max connection limit which is: 10 connection(s).You currently have: 11 active connection(s).\",\"connectionClosed\":true,\"connectionId\":\"107-080424085725-430913\"}"}```

*Tags: Errors Debugging*

---

**Trex44** - *18:22:18*

I am still going to have issues here. I collect data on 4 sports via one instance so thats 8 connections and then run strats on horses and football so that would be 12 connections total. I am not certain why it was letting me do this before today but now I am limited at 10.



 If I run the Football strats on the same instance, with the same filters, as the data collection is there a way to keep an instance of flumine running whilst shutting down some of the strategies running through it? I am thinking I will have to shut down the betting strats whilst keeping the data collection running.

*Tags: Strategies*

---

**Lee** - *18:34:27*

Just to clarify. Flumine will use 1 connection for the order stream and 1 for each market filter. In the case of your instance that’s collecting data it sounds like it would be 5 connections.

If you are not betting on that instance you can disable to order stream on the `BetfairClient`.

Can you combine some sports into the same filter?

*Tags: Strategies*

---

**Trex44** - *18:58:38*

Hmm, that's really odd. I have absolutely no idea how I have reached 10 then. Have to dig about and see if I have left an old EC2 instance running by mistake or something.



I could combine the filters for the market recorders perhaps. They are different filters for each sport at the moment because I wanted to record different countries for some sports and because I use the context on each class instance/sport to direct the files to be saved in different S3 buckets.

*Tags: Data Quality, Deployment*

---

**Trex44** - *19:11:21*

Will also look to disable the order stream on the market recorder instance. Not certain how to do this, but that should at least free up one connection.

*Tags: Data Quality*

---

**foxwood** - *20:56:09*

Put the logs into INFO mode - you'll then see stuff like "Starting OrderStream 1000" in the default flumine setup.

*Tags: Getting Started*

---

## 2024-04-09

**Trex44** - *09:46:17*

So, I think the problem was caused by a Jupyter notebook. I had one of the cells take a snap shot of market data when it was executed. I did have trading.logout() in the same cell but all I can think is this didn't execute properly. When I rebooted my IDE (that the notebook was running in) it seems to have fixed the issue. Will see if crops up again or not.

*Tags: Errors Debugging, Strategies*

---

**Justice** - *18:05:57*

What would be the easiest way to start recording raw data using flumine once a certain condition is met? Should I use a worker to check against the condition then add the market to the framework? Or is there a better way of doing this? Forgive my ignorance, I'm not a native flumine user

*Tags: General Technical*

---

**Justice** - *19:31:30*

Advice taken. Is a custom stream the way to go to integrate my scuffed TPD data collection into flumine?

*Tags: General Technical*

---

## 2024-04-11

**JFP** - *11:45:48*

Just wondering if anyone can point me in the right direction in regards to logging total cleared profit for each strategy once market/orders are cleared.



I currently log cleared orders via _process_cleared_orders_meta() and market profit via _process_cleared_markets(), but would like to add total profit for each strategy.



I can just process the cleared orders in a dataframe and groupby sum profit based on strat name, but I am hoping there is a more Flumine way to tackle this.



Have retrieved cleared orders from betfair via "client.betting_client.betting.list_cleared_orders()" to sum based on strategy name, but strategy refs have been recorded as my device name. So it appears only Flumine knows which orders belong to each strategy.

*Tags: Feature Engineering, Strategies*

---

**JFP** - *21:52:42*

I'm wanting to log cleared profit for each strategy, I have a backtest logging control set for the client which logs net cleared market profit but would like breakdown of net profit for each strategy. Does the blotter get updated once orders are cleared? In the _process_cleared_markets() function can I just call orders from blotter and get profit per strat?

*Tags: Strategies*

---

## 2024-04-14

**Dave** - *15:46:02*

Looking at optimizing some of my market stream subscriptions. Suppose I have two strategies, one trades AU WIN horse markets, and the other trades GB WIN horse markets. If I call add_strategy twice with each of these market filters, presumably that will use two connections.



If I call add_strategy twice but each time with the filter [(AU, GB), 7, WIN] - presumably it will only use 1 connection, but presumably I'd have to add logic to discard AU ticks in my GB strategy and visa versa. Is this correct? Does anyone have any intuition of whether this brings any benefits other than just freeing up a connection?

*Tags: Strategies*

---

**ShaunW** - *18:43:16*

Most of the year they don't overlap, or by much. Schedule them to start up and shutdown seperately as required. But combining your sub into Aus and UK together probably won't make any performance difference  because when one is busy the other isn't.  You'll be checking a tick is one you want anyway regardless of subscription, as you don't assume a data source that's out of your control is as expected, as part of defensive coding good practice.

*Tags: Performance*

---

**Dave** - *18:54:02*

The context is that I want to free up some connections to launch additional markets. But I'm worried that if I "merge streams" in this way, ticks may arrive "slower", (e.g. in this case GB ticks and AU ticks will be interleaved on the same stream rather than receiving them independently on two streams), or maybe even subject to forced conflation. Obviously you're still at the mercy of the GIL + the fact that flumine is single process, but still. I am imaging a scenario where stream throughput is always the same on Betfair's side, so subscribing to GB+AU on one stream may result in suboptimal delivery of ticks vs 2 streams with GB + AU independently. (just using GB + AU for example sake btw, obviously more concerned about markets that tick heavily in paralell).

*Tags: Performance, Deployment*

---

**AndyL** - *19:18:13*

[@U0128E7BEHW](@U0128E7BEHW) this is absolutley what I do, I was consuming all my 10 connections, but I combined my US, GB and AU horse racing under 1 Flumine instance, with 3 add_strategy's, the first thing each strategy check_market_book does is return False if market not for it....

I am now running with 6 connections :slightly_smiling_face:

Just wish I could find a new profitable strategy for the remaining 4 now :man-shrugging: !

*Tags: Strategies*

---

**liam** - *19:18:26*

Are your strategies that latency sensitive?

*Tags: Performance*

---

**Unknown** - *19:56:06*

If you can run win and place together, or match odds, CS and OU as one sub, you can certain run markets that aren't even active at the same time. 



But I run each market/strategy combo in its own parallel process so mixed feeds aren't as much as an issue as high update rates on one.  You just need to move your hoppers to after the sorter if you can't keep up with the input.. :wink:

*Tags: Strategies*

---

**AndyL** - *22:07:12*

on a typical busy Saturday my AWS instance peaks about 4% CPU ....

*Tags: Deployment*

---

## 2024-04-17

**Massimo Abitante** - *15:33:41*

*betfairlightweight/streaming/cache*

*Tags: General Technical*

---

**Massimo Abitante** - *15:37:46*

Hi everybody! Sorry for the dumb question but I'm a beginner.

When I subscribe to the markets stream (subscribe_to_markets) I get the updates regarding prices and sizes on the market.

I need to do some stuff only when prices change. My idea was to create my own "cache" saving the prices so when I get an update I can compare old prices (saved in my cache) with new prices (received from the stream).

Here I've seen this "cache" code that looks like something I could use instead of my own cache.

How can I access that cache? Is it something that betfairlightweight populates automatically or do I need to initialize it somehow?



Thank you very much

*Tags: Getting Started*

---

**liam** - *15:49:01*

What do you mean by 'I need to do some stuff only when prices change` because this is how streaming works

*Tags: General Technical*

---

**liam** - *15:50:23*

You can see the raw update in `market_book.streaming_update`

*Tags: General Technical*

---

**Massimo Abitante** - *15:53:24*

Yes but I get a streaming_update everytime the price or the size changes, I need to check if the price has changed or not. So I was thinking to use that "cache" that i see in betfairlightweight code, instead of creating my own dict where I save the price so I can compare it with the next streaming_update.

Is that cache accessible somehow?

*Tags: General Technical*

---

**Massimo Abitante** - *15:57:24*

It's a dumb question, I know, but I cant find the way to see what there's in that cache

*Tags: General Technical*

---

**Unknown** - *16:22:11*

I think I found why I couldn't understand you [@U4H19D1D2](@U4H19D1D2).

I got confused because I was looking at the prices that I find in the "runners" element of the update (I set lightweight=True so I get the dictionary).

In those fields I get the current market prices so if price has not changed I will find the same price in all the updates I get.

Instead, as you [@U4H19D1D2](@U4H19D1D2) suggested, I have to look at the actual streaming_update. There only delta will be sent.

Got it so I don't actually need any comparison with old prices. If I get a price in batb or batl, that price must have been changed.

Thank you [@U4H19D1D2](@U4H19D1D2).

Thank you [@UQL0QDEKA](@UQL0QDEKA) , I think that comparison you make is not needed. I'll look better into it, anyway.

*Tags: General Technical*

---

**Justice** - *19:21:09*

How does everyone handle converting raw market data into point-in-time snapshots of the market for ingestion into a database? I usually save the market books from the stream directly as they're far easier to work with but I wanted to record raw data this time for redundancy.



For context, I am using flumine raw data stream to capture the data, record to a temp file, then move to a storage account once the market closes. The script that does this is running in a docker container on Azure.

*Tags: General Technical*

---

**liam** - *19:58:32*

I keep all markets in the raw format (always do this) and then process into flat files when I want to do some analysis. It’s then a case of just using pandas, I haven’t found any advantages of putting a database in between. 

*Tags: Feature Engineering*

---

## 2024-04-18

**Justice** - *07:52:42*

That's very fast indeed, is that using flumine to process the raw data?

*Tags: General Technical*

---

**liam** - *08:14:32*

Sorry I skipped a step



&gt; raw data -&gt; flumine or bflw -&gt; flat file -&gt; pandas

The flumine part obviously takes the longest but once in a flat file its quick

*Tags: Feature Engineering*

---

**Massimo Abitante** - *13:49:44*

Guys sorry I come back again but [@U4H19D1D2](@U4H19D1D2) take a look at these:



```{'id': '1.227726521', 'rc': [{'batl': [[0, 1.2, 51.86]], 'id': 15282695}]}

{'id': '1.227726521', 'rc': [{'batl': [[0, 1.2, 30.39]], 'id': 15282695}]}

{'id': '1.227726521', 'rc': [{'batl': [[0, 1.2, 17.24]], 'id': 15282695}]}```

I got these 3 streaming_updates. Only size has changed, no price change.

If I could access the cache that betfairlightweight stores somewhere, I could check the old price. How can I see the content of that cache? Is there a function to access it? I can't find it.

*Tags: General Technical*

---

**JL** - *16:42:04*

is the `process_new_market` function executed for all markets subscribed to when starting flumine? I want to be able to access information for all races, e.g. runners and trackname, that takes place that day when I start flumine

*Tags: General Technical*

---

**Dario Scardina** - *17:43:04*

Hello everyone :)

Is there anyone generous who wants to share a Python snippet to replicate Betfair's cashout function? Not the order placement, but I would like to understand how to obtain that value that Betfair reports on a live market.

Thank you in advance :wink:

*Tags: Deployment*

---

**Massimo Abitante** - *17:53:54*

I want to ignore it but I need to save the old price by myself somewhere, exaclty as [@UQL0QDEKA](@UQL0QDEKA) did, otherwise how do I ignore it if I don't compare it with the old price?

If this old price is already stored somewhere, like in the MarketBookCache or something, it would save me some work + I won't store the same data in two places.

My question is: is there a way to print the content of the MarketBookCache?

*Tags: General Technical*

---

**Ralegh** - *18:24:23*

It’s a linear programming problem, you want to maximize your worst individual selection pnl. I.e maximize the utility function min(profit_on_win(stall 1), profit_on_win(stall 2),… for each stall) and your variables would be back/lay bet sizes on each selection. There’s a min bet size so you need two variables per price - place_bet? as 0 or 1 and bet_size between 1 and level size in the book. Then your final bet is place_bet?*bet_size. You’ll probably have to do some other shenanigans to go deeper in the book or you could just run a few iterations as long as one of your bets is = the size of a level and use the next level down in the next run. Haven’t implemented this myself though and I don’t know how betfairs approach works so take with a grain of salt.

*Tags: General Technical*

---

**Dario Scardina** - *19:31:53*

Hello [@U06RE5C6X2P](@U06RE5C6X2P), sorry maybe I didn't explain myself well. I already know how to obtain the market_profit_and_loss, but those values indicate the "final" outcome, not the current cashout that I could execute on the market. If I back €1 at odds of 1.50, on that selection I have a theoretical profit of €0.50 (minus taxes). But during the live event, I can execute a cashout (the one that Betfair suggests on its website) to achieve an all-green outcome (in the luckiest scenario, hehe :D).

*Tags: Deployment*

---

**Ralegh** - *19:43:34*

You’d need to read up on linear programming- the gist is you’ve got some variables (how much you bet on each selection) and you want to pick the best combination of values for those variables based on some criteria. In this case you want to minimize your exposure (make your worst case profit/loss as high as it can be). You could write a function to brute force this - calculate your new exposure after hedging by betting £0,£0,£0,£0,£1, then £0,£0,£0,£1,£0, etc (for each of 5 selections). But there are known algorithms you can use if you can formulate the problem in a specific way. Once you do that you can use e.g scipy.optimize and it will come up with a solution a lot faster than brute forcing, for complex problems it may not be possible to come up with a closed form formula so linear programming is sort of smart trial and error.

*Tags: Errors Debugging, Strategies*

---

**liam** - *20:21:37*

Open up your debugger and start looking 

*Tags: Errors Debugging*

---

**JL** - *21:45:19*

this one! [@U9JHLMZB4](@U9JHLMZB4)

[https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py#L100](https://github.com/betcode-org/flumine/blob/master/flumine/strategy/strategy.py#L100)

*Tags: Strategies*

---

**Fab** - *22:29:02*

Noob question...



I'm simulating a strategy that places a lay bet then cancels any unmatched size after 30 seconds.



When using a lay stake of £1, the order profit appears in the blotter. When using a lay stake of £3 on exactly the same market, the order profit does not appear at all.



*Question: assuming that a £1 bet is fully matched, shouldn't at least £1 being matched on a £3 bet? Or perhaps the simulator treats £3 as "not matched at all"?*



```# How I run the simulation

framework = FlumineSimulation(client=client)

framework.add_strategy(strategy)

framework.run()



for market in framework.markets:

    for order in market.blotter:

        if order.average_price_matched &gt; 0:

            print(order.profit)```

```# Bet placing

stake = 1

order = trade.create_order(side = "LAY", order_type = LimitOrder(price=lay_odds, stake))

market.place_order(order)```

```# Bet canceling

for order in orders:

    if order.status == OrderStatus.EXECUTABLE:

        if order.size_remaining &gt; 0 and order.elapsed_seconds and order.elapsed_seconds &gt;= 30:

            market.cancel_order(order)```

*Tags: Strategies*

---

**JL** - *22:33:09*

the problem however is that `market.market_catalog` is `None` in that function for some reason

*Tags: General Technical*

---

**JL** - *22:48:46*

yes live. So the information seem to be available when `process_market_catalogue` is called which solved it!

*Tags: Deployment*

---

**Lee** - *23:02:57*

Look at how to setup logging from the examples, you should then see the reason 

*Tags: Getting Started*

---

## 2024-04-19

**Tarang Ambaliya** - *11:18:14*

Hello, Need a help.



I have been facing this issue were my account keeps getting blocked and shows up this error `LoginError: API login: ACCOUNT_PENDING_PASSWORD_CHANGE`. I am using the account on one instance. I got it reset more than 4 times now. But still it is getting blocked on second successful login attempt. Double checked the password is correct and username is also correct along with the app key. Earlier it was working fine with multiple login at multiple places but once i faced this error it is not going away now.



Thanks.

*Tags: Errors Debugging*

---

**Trex44** - *11:29:15*

Hey all, when using a simple market filter such as the one below for filtering by market_ids is there a way to update  all_selections (which is the list of market_ids) without having to restart the flumine instance?

```standard_filter = betfairlightweight.filters.streaming_market_filter(

        market_ids=all_selections

    )```



*Tags: General Technical*

---

**liam** - *12:33:05*

flumine accepts a list for `market_filter`

*Tags: General Technical*

---

## 2024-04-21

**Unknown** - *22:38:34*

On a similar note, I currently have a limit of 200 markets for a stream. I assume this is fixed regardless if I fire up another stream?



It seems lots of people got the limit extended to 1K - is there any criteria I'd need to meet to get this?

*Tags: Errors Debugging*

---

## 2024-04-22

**liam** - *08:16:28*

[https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|This](https://github.com/betcode-org/flumine/blob/master/examples/middleware/orders.py|This) what you are after?

*Tags: General Technical*

---

## 2024-04-23

**Ammar** - *10:37:08*

Streaming is “cheap” - you get one connection established per market filter you pass into the flumine instance (plus one for orders if you want it) 



The market filter can be as wide as you want (subject to number of markets at time of connection - where the default limit is 200 but you can ask BDP to raise that)

*Tags: General Technical*

---

**Aiyaj Khalani** - *10:38:39*

re: But you can filter in check_market_book based on your specific logic if you really need to



so anyway streaming is going to push updates for all the markets we have subscribed too, right?

*Tags: General Technical*

---

**Aiyaj Khalani** - *10:46:07*

re: Streaming is “cheap” - you get one connection established per market filter you pass into the flumine instance (plus one for orders if you want it)



we have requirement of subscribing to around 12k markets on streaming API. do you think it will be still cheaper? i meant having few market subscriptions vs this much wider subscription would result in same latency or load?

*Tags: Performance*

---

**Ammar** - *10:54:22*

If you partition by a market filter when opening flumine, you will get one connection per filter, so number of markets will be lower per stream / connection. 



I have not seen any posts about anyone hit a stream performance issue tho, so it may not be necessary. But my experience w the framework is still quite limited so I may be missing something



Also I believe betfair limits each api key to 10 streaming connections. 



You could try it and see … it may be a case of trial and error to get the right hardware and stream filter setup for your use case 

*Tags: Getting Started, Errors Debugging, Performance*

---

**alter_life** - *16:57:14*

Hi, is it possible to subscribe for score updates for tennis with betfairlightweight/flumine or the only way it is to call In Play Service api?

*Tags: General Technical*

---

## 2024-04-25

**Jorge** - *10:01:56*

Hi guys, I am recently getting a lot of "Order XXX not present in blotter" in my flumine Strategy. What do these mean?

*Tags: Strategies*

---

**Jorge** - *10:10:17*

I didn't restart the framework in this case. Could it be because those orders are sent to the same market by a different flumine Strategy?

*Tags: Strategies*

---

**Jorge** - *10:11:51*

Orders sent by a different flumine instance would never appear in `market.blotter` ?

*Tags: General Technical*

---

**Jorge** - *10:14:55*

I think my Strategy is saving completed orders which are not sent by the Strategy (I know this because the context in those orders is empty, whereas it would be filled for my own orders)

*Tags: Strategies*

---

**Jorge** - *10:16:03*

Which is affecting market.blotter.get_exposures() and making my strategy enter into a loop of replacing the same order again

*Tags: Strategies*

---

**Jorge** - *10:19:46*

Other question tho, if I run 2 flumine instances in separate scripts and they send orders to the same market. Would the market.blotter.get_exposures() be per instance or would it consider the whole market (both instances)?

*Tags: General Technical*

---

**liam** - *10:20:30*

flumine separates using the hostname (customerStrategyRef) which is what is probably causing your issues

*Tags: Strategies*

---

**Unknown** - *10:23:51*

Let’s me rephrase, is it possible to create a score stream (similar to orders stream) rather calling In Play Service api every time I want to have a current score. Any help appreciated.

*Tags: General Technical*

---

**Mo** - *10:28:10*

Mate you wrote the code for it: [https://github.com/betcode-org/betfair/blame/master/betfairlightweight/endpoints/scores.py](https://github.com/betcode-org/betfair/blame/master/betfairlightweight/endpoints/scores.py)

*Tags: General Technical*

---

**Mo** - *10:29:47*

I would say the answer to the original question is "yes" and that it's not the case that the only way is to use the in play service

*Tags: General Technical*

---

**alter_life** - *10:36:24*

I am a bit confused, because betfair historical data also does not contains score, and there are not so many providers who has this data with timestamps (usually it is quite expensive) that makes it almost impossible to backtest the things..

*Tags: Data Quality*

---

**Jorge** - *10:36:57*

I'll set a different customerStrategyRef in each of them to avoid this in the future

*Tags: Strategies*

---

## 2024-04-29

**Tarang Ambaliya** - *07:58:57*

And that is not working for me, it gives 403 errors.

*Tags: Errors Debugging*

---

**Tarang Ambaliya** - *08:39:56*

yeah but that would be too many transactions per second right? And will increase with every other market that I add to the subscription. If anyone has already done and knows about it then it will help me.

*Tags: General Technical*

---

**Tarang Ambaliya** - *08:40:36*

database transaction

*Tags: General Technical*

---

## 2024-04-30

**birchy** - *20:34:51*

Flumine simulation: what's the definition of a "passive" bet? I know it's a price that is not currently available, e.g. back @ ATB + 1 tick, but what impact does the inplay delay have? i.e. let's say the ATB/ATL is 3.2/4.0 and we place a back bet @ 3.6. Bet lands in the live market 1 second later and the prices have changed to 3.6/4.0 and matches instantly. Is that still "passive"? To me it's not, but I'd like some clarification.

*Tags: Deployment*

---

**birchy** - *20:55:35*

Ok, you've answered answered my underlying question. :wink:

*Tags: General Technical*

---

## 2024-05-01

**Dave** - *20:21:03*

If I'd like to override customer_strategy_ref, is it just a matter of setting config.customer_strategy_ref before instantiating Flumine? I'd like to run multiple flumine instances on a single machine, and don't want instances becoming confused because of the same customer_strategy_ref they see in the order stream. (And I'd like to run multiple instances to use the multiple cores on the host in question)

*Tags: Strategies*

---

## 2024-05-02

**Adrian** - *13:38:09*

So i build an app that uses ML to predict winners. My problem is execution. I don't really know whether to take a price or offer a price. I'm struggling to get match on my offers and the taking prices seems to end up in me getting a bad deal. Any pointers?

*Tags: General Technical*

---

**Mo** - *13:39:08*

If you’re not getting value then your model needs improving

*Tags: Strategies*

---

**Adrian** - *13:46:41*

there was a betfair guide floating around somewhere- how to build an ML model for greyhounds. i think he had something like 32% win rate prediction? could he find value with that model? I'm thinking that some of his predictions would be accurate and that sometimes those winners would be mispriced

*Tags: Strategies*

---

**Adrian** - *14:13:45*

the model gives each runner a probability of winning and they all sum to one

*Tags: Strategies*

---

**Adrian** - *14:32:07*

i know everyones going to scream at me again to go buy data or just run a simple strategy but $300 for a month of data is way out of my budget and every single "simple strategy" i have run ever has cost me an arm or leg

*Tags: Strategies*

---

**Johnny** - *16:38:39*

With an untested model, offering prices screams adverse selection risk

*Tags: Strategies*

---

**A** - *19:52:11*

Ah just found this which might be suitable → [https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py](https://github.com/betcode-org/betfair/blob/master/examples/exampleracecard.py)

*Tags: General Technical*

---

**Jonjonjon** - *22:04:01*

How do you know that your model has a positive expectation?

*Tags: Strategies*

---

## 2024-05-03

**Adrian** - *00:05:10*

Making $300 seems like a far off dream. But I'm confident my model has a positive expectation because i can beat bsp with a few tweaks, so being unrealistic i can imagine making thousands! Of course there is a reality called execution to deal with first

*Tags: Strategies*

---

**Bruno** - *13:55:41*

hi, when we create a trade I understand that we need to pass an instance of a strategy class, and then later we can access the orders unique to that strategy using blotter.strategy_orders and similar, however my question is what aspect of the class exactly determines this uniqueness - is it the memory location of the class, the name of the class, the hash of the class, or have i misunderstood the scenario? in short, I want to run multiple instances of flumine and selectively reference orders between the instances, but I'm struggling to work out how I can achieve that

*Tags: Performance, Strategies*

---

**Bruno** - *14:03:41*

when I ran two strategies with the same name, it seemed they could see each others' orders, whereas once I changed the name of the classes this seemed to stop. is this related to strategy.name_hash?

*Tags: Strategies*

---

**Bruno** - *14:36:39*

is this for `blotter.client_orders` or `blotter.strategy_orders`?

*Tags: Strategies*

---

**Bruno** - *14:39:01*

ok cool, so when I had 2 seperate instances of flumine running on the same machinec were their orders visible to each other through blotter.strategy_orders because the class names were the same?

*Tags: Strategies*

---

**Bruno** - *15:11:21*

Hi Liam,



I think you may have misunderstood my question a bit, perhaps I did not make myself very clear.



please consider this MWE



```filter=streaming_market_filter(

        event_type_ids=["7"],

        market_types=["WIN"],

    )



class ML_strategy(BaseStrategy):

    pass

fakestrat=ML_strategy(market_filter=filter)



class BlotterWatcher(BaseStrategy):

    def start(self):

        print('BlotterWatcher started')

    def check_market_book(self, market, market_book):

        # process_market_book only executed if this returns True

        if market_book.status != "CLOSED":

            return True

    def process_market_book(self, market, market_book):

        self_orders=market.blotter.strategy_orders(strategy=self)

        fakestrat_orders=market.blotter.strategy_orders(strategy=fakestrat)

        if self_orders:

            print('Self orders:',self_orders)

        if fakestrat_orders:

            print('Fakestrat orders:',fakestrat_orders)



strategy=BlotterWatcher(market_filter=filter)

framework.add_strategy(strategy)

framework.add_strategy(fakestrat)

framework.run()```

I have a strategy, _also called ML_strategy,_ running _in another process_.



this "blotter watch script" only prints out orders from fakestrat, not from self. contrary to what you said, I believe this is because the strategies share a class name, and the hostname is not the answer to my question. many thanks for your speedy reply and thank you for making an attempt to answer my question.



Many Thanks,



Bruno

*Tags: Performance, Strategies*

---

**liam** - *15:17:29*

Yeah I don’t understand the question tbh 

*Tags: General Technical*

---

**Bruno** - *16:32:58*

self_orders is not defined in the global scope, it is only defined locally.

strategy_selection_orders returns orders when I pass it the fake strategy, but not the blotterwatch strategy. I believe this is because the strategy is also called ML_strategy in the other process.

```from flumine import Flumine, clients, BaseStrategy

import logging

import betfairlightweight

from credentials import username, password, app_key

from default_config import *

from config import *

import os

# from subprocess import Popen

from betfairlightweight.filters import streaming_market_filter



logging.basicConfig(filename='strategy.log', level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO), format='%(asctime)s:%(levelname)s:%(message)s')

trading = betfairlightweight.APIClient(username, password, app_key, "./certs")

client = clients.BetfairClient(trading)



bought_markets = []

# max_markets=1



framework = Flumine(client=client)



class ML_strategy(BaseStrategy):

    pass





class BlotterWatcher(BaseStrategy):

    def start(self):

        print('BlotterWatcher started')

    def check_market_book(self, market, market_book):

        # process_market_book only executed if this returns True

        if market_book.status != "CLOSED":

            return True

    def process_market_book(self, market, market_book):

        self_orders=market.blotter.strategy_orders(strategy=self)

        fakestrat_orders=market.blotter.strategy_orders(strategy=fakestrat)

        if self_orders:

            print('Self orders:',self_orders)

        if fakestrat_orders:

            print('Fakestrat orders:',fakestrat_orders)



filter=streaming_market_filter(

        event_type_ids=["7"],

        market_types=["WIN"],

    )



strategy = BlotterWatcher(market_filter=filter)

fakestrat=ML_strategy(market_filter=filter)

framework.add_strategy(strategy)

framework.add_strategy(fakestrat)

framework.run()```

*Tags: Strategies*

---

**Dave** - *18:43:35*

change customer_strategy_ref in flumine.config and your processes will only see their own orders (because they specify the strategy ref when subscribing to the order stream)

*Tags: Strategies*

---

**Bruno** - *18:44:29*

my goal is for them to be selectively visible. my question is what determines the visibility - I think it’s the (hash of the) supplied strategy class name

*Tags: Strategies*

---

## 2024-05-04

**foxwood** - *09:52:48*

I patch the hash once the BaseStrategy constructor has run and get values like "stratABCxxxxx-139340382301022140". Code at end of MyStrategy.add(...) shown below. This lets you identify the strat in the blotter and leaving the tail in place separates instances of the strat if required. Only works if you use a unique top level class name for each strat.

EDIT: this was my solution when first using flumine. Having now found out about config I'd probably go that route.

```self.ctxname = self.__class__.__name__

self.name_hash = self.__class__.__name__\

                 + "x"*(STRATEGY_NAME_HASH_LENGTH-len(self.__class__.__name__))```

*Tags: Strategies*

---

## 2024-05-06

**Adrian** - *06:28:12*

[@UBS7QANF3](@UBS7QANF3) i use MC to get runner ids, names, rugs/boxes and the race number from the market name. it's not a major issue as i can recreate what i need from other betfair data sources, but would be nice just to have it all in one spot so i could treat my historical data the same way I would a live market

*Tags: Data Quality, Deployment*

---

**liam** - *09:15:37*

The market recorder is designed for this, recording the live data and market catalogue for loading during simulation

*Tags: Data Quality, Deployment*

---

**Lee** - *11:52:54*

[https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/](https://betcode-org.github.io/flumine/performance/|https://betcode-org.github.io/flumine/performance/)

*Tags: Performance*

---

**Adrian** - *11:59:32*

Thanks for that! Didn't know there was a guide on performance. I've tried most of these but will give multiprocessing and limiting control blocked orders a go

*Tags: Performance*

---

**Adrian** - *13:14:17*

Sure, thanks for checking it out

&gt; {"asctime": "2024-05-06 12:11:19,275", "levelname": "INFO", "message": "Removing market 1.227403968", "clients": {"Betfair": {}, "Simulated": {"34jhdr5": {"username": “34jhdr5”, "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x15fcf0950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 6, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;", "&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 7997365952)&gt;"]}

&gt; {"asctime": "2024-05-06 12:11:19,275", "levelname": "INFO", "message": "Completed historical market '/Users/adrian/Projects/betfair/data/historic/PRO/2024/Apr/11/33180336/1.227403968.decompressed'"}

&gt; {"asctime": "2024-05-06 12:11:19,275", "levelname": "INFO", "message": "Starting historical market '/Users/adrian/Projects/betfair/data/historic/PRO/2024/Apr/11/33180336/1.227403983.decompressed'", "market": "/Users/adrian/Projects/betfair/data/historic/PRO/2024/Apr/11/33180336/1.227403983.decompressed"}

&gt; {"asctime": "2024-05-06 12:11:19,275", "levelname": "INFO", "message": "[Register: 7000]: marketSubscription"}

&gt; {"asctime": "2024-05-06 12:11:19,275", "levelname": "INFO", "message": "[MarketStream: 7000]: \"MarketStream\" created"}

&gt; {"asctime": "2024-05-06 12:11:19,279", "levelname": "INFO", "message": "[MarketStream: 7000]: 1.227403983 added, 1 markets in cache"}

&gt; {"asctime": "2024-05-06 12:11:19,280", "levelname": "INFO", "message": "[MarketStream: 7000]: 1.227403983 added, 1 markets in cache"}

&gt; {"asctime": "2024-05-06 12:11:19,280", "levelname": "INFO", "message": "[MarketStream: 7000]: 1.227403983 added, 1 markets in cache"}

&gt; {"asctime": "2024-05-06 12:11:19,280", "levelname": "INFO", "message": "[MarketStream: 7000]: 1.227403983 added, 1 markets in cache"}

&gt; {"asctime": "2024-05-06 12:11:19,289", "levelname": "INFO", "message": "Adding: 1.227403983 to markets"}

&gt; {"asctime": "2024-05-06 12:11:19,290", "levelname": "INFO", "message": "Execution new hour", "current_transaction_count_total": 1, "current_transaction_count": 1, "current_failed_transaction_count": 0, "total_transaction_count": 1, "total_failed_transaction_count": 0, "client": {"username": "41f95cae", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x15fcf0950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}

*Tags: Errors Debugging, Strategies*

---

**Adrian** - *13:22:27*

Ahh ok i looked up cprofilev but I'm not familiar with it

Yes i just ran the same strategy on a single day to generate a whole log (if you still want it) and it was at least 100 times faster than loading the whole month

*Tags: Strategies*

---

**Adrian** - *13:36:55*

Single market:

&gt;        1044 function calls (1041 primitive calls) in 0.001 seconds

&gt; 

&gt;    Ordered by: standard name

&gt; 

&gt;    ncalls  tottime  percall  cumtime  percall filename:lineno(function)

&gt;         8    0.000    0.000    0.000    0.000 &lt;frozen genericpath&gt;:121(_splitext)

&gt;         8    0.000    0.000    0.000    0.000 &lt;frozen posixpath&gt;:117(splitext)

&gt;         8    0.000    0.000    0.000    0.000 &lt;frozen posixpath&gt;:140(basename)

&gt;         8    0.000    0.000    0.000    0.000 &lt;frozen posixpath&gt;:41(_get_sep)

&gt;        24    0.000    0.000    0.000    0.000 &lt;frozen posixpath&gt;:52(normcase)

&gt;         1    0.000    0.000    0.001    0.001 &lt;string&gt;:1(&lt;module&gt;)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1087(flush)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1098(emit)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:123(getLevelName)

&gt;         2    0.000    0.000    0.000    0.000 __init__.py:1319(disable)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1479(info)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1561(findCaller)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1595(makeRecord)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1610(_log)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1636(handle)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:164(&lt;lambda&gt;)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1690(callHandlers)

&gt;         2    0.000    0.000    0.000    0.000 __init__.py:1720(getEffectiveLevel)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1734(isEnabledFor)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:183(dumps)

&gt;        24    0.000    0.000    0.000    0.000 __init__.py:194(_is_internal_frame)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:2223(handle)

&gt;         2    0.000    0.000    0.000    0.000 __init__.py:228(_acquireLock)

&gt;         2    0.000    0.000    0.000    0.000 __init__.py:237(_releaseLock)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:292(__init__)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:368(getMessage)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:606(formatTime)

&gt;        16    0.000    0.000    0.000    0.000 __init__.py:815(filter)

&gt;        16    0.000    0.000    0.000    0.000 __init__.py:922(acquire)

&gt;        16    0.000    0.000    0.000    0.000 __init__.py:929(release)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:942(format)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:965(handle)

&gt;         1    0.000    0.000    0.000    0.000 accountresources.py:17(__init__)

&gt;         5    0.000    0.000    0.000    0.000 baseclient.py:101(transaction_count_total)

&gt;         5    0.000    0.000    0.000    0.000 baseclient.py:120(info)

&gt;         5    0.000    0.000    0.000    0.000 baseclient.py:87(username)

&gt;         5    0.000    0.000    0.000    0.000 baseclient.py:94(current_transaction_count_total)

&gt;         2    0.000    0.000    0.000    0.000 baseexecution.py:156(shutdown)

&gt;         2    0.000    0.000    0.000    0.000 baseflumine.py:124(log_control)

&gt;         1    0.000    0.000    0.000    0.000 baseflumine.py:128(_add_default_workers)

&gt;         1    0.000    0.000    0.000    0.000 baseflumine.py:401(_process_end_flumine)

&gt;         2    0.000    0.000    0.000    0.000 baseflumine.py:404(info)

&gt;         2    0.000    0.000    0.000    0.000 baseflumine.py:412(&lt;listcomp&gt;)

&gt;         1    0.000    0.000    0.000    0.000 baseflumine.py:417(__enter__)

&gt;         1    0.000    0.000    0.000    0.000 baseflumine.py:445(__exit__)

&gt;         1    0.000    0.000    0.000    0.000 baseresource.py:11(__init__)

&gt;         5    0.000    0.000    0.000    0.000 clientcontrols.py:108(current_transaction_count_total)

&gt;         5    0.000    0.000    0.000    0.000 clientcontrols.py:112(transaction_count_total)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:56(login)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:66(logout)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:71(update_account_details)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:76(simulated)

&gt;         2    0.000    0.000    0.000    0.000 clients.py:84(info)

&gt;         2    0.000    0.000    0.000    0.000 clients.py:86(&lt;dictcomp&gt;)

&gt;         6    0.000    0.000    0.000    0.000 clients.py:87(&lt;dictcomp&gt;)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:94(__len__)

&gt;         8    0.000    0.000    0.000    0.000 encoder.py:105(__init__)

&gt;         7    0.000    0.000    0.000    0.000 encoder.py:161(default)

&gt;         8    0.000    0.000    0.000    0.000 encoder.py:183(encode)

&gt;         8    0.000    0.000    0.000    0.000 encoder.py:205(iterencode)

&gt;        11    0.000    0.000    0.000    0.000 enum.py:1255(value)

&gt;        11    0.000    0.000    0.000    0.000 enum.py:193(__get__)

&gt;         2    0.000    0.000    0.000    0.000 events.py:39(__init__)

&gt;         7    0.000    0.000    0.000    0.000 inspect.py:456(istraceback)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:176(add_fields)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:193(_perform_rename_log_fields)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:198(process_log_record)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:205(jsonify_log_record)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:213(serialize_log_record)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:217(format)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:28(merge_record_extra)

&gt;         7    0.000    0.000    0.000    0.000 jsonlogger.py:59(default)

&gt;         4    0.000    0.000    0.000    0.000 markets.py:54(markets)

&gt;         2    0.000    0.000    0.000    0.000 markets.py:58(open_market_ids)

&gt;         2    0.000    0.000    0.000    0.000 markets.py:60(&lt;listcomp&gt;)

&gt;         2    0.000    0.000    0.000    0.000 markets.py:69(__iter__)

&gt;         2    0.000    0.000    0.000    0.000 markets.py:72(__len__)

&gt;         8    0.000    0.000    0.000    0.000 process.py:189(name)

&gt;         8    0.000    0.000    0.000    0.000 process.py:37(current_process)

&gt;         1    0.000    0.000    0.000    0.000 simulatedclient.py:18(login)

&gt;         1    0.000    0.000    0.000    0.000 simulatedclient.py:24(logout)

&gt;         1    0.000    0.000    0.000    0.000 simulatedclient.py:27(update_account_details)

&gt;         1    0.000    0.000    0.001    0.001 simulation.py:29(run)

&gt;         1    0.000    0.000    0.000    0.000 strategy.py:142(finish)

&gt;         1    0.000    0.000    0.000    0.000 strategy.py:286(start)

&gt;         1    0.000    0.000    0.000    0.000 strategy.py:298(finish)

&gt;         2    0.000    0.000    0.000    0.000 strategy.py:306(__iter__)

&gt;         1    0.000    0.000    0.000    0.000 strategy.py:96(start)

&gt;         1    0.000    0.000    0.000    0.000 streams.py:254(start)

&gt;         1    0.000    0.000    0.000    0.000 streams.py:263(stop)

&gt;         4    0.000    0.000    0.000    0.000 streams.py:271(__iter__)

&gt;         2    0.000    0.000    0.000    0.000 thread.py:216(shutdown)

&gt;         2    0.000    0.000    0.000    0.000 threading.py:1125(_wait_for_tstate_lock)

&gt;         8    0.000    0.000    0.000    0.000 threading.py:1152(name)

&gt;         2    0.000    0.000    0.000    0.000 threading.py:1192(is_alive)

&gt;         8    0.000    0.000    0.000    0.000 threading.py:1453(current_thread)

&gt;         2    0.000    0.000    0.000    0.000 threading.py:1501(enumerate)

&gt;         4    0.000    0.000    0.000    0.000 threading.py:575(is_set)

&gt;         2    0.000    0.000    0.000    0.000 threading.py:931(__repr__)

&gt;         1    0.000    0.000    0.000    0.000 utils.py:31(__enter__)

&gt;         1    0.000    0.000    0.000    0.000 utils.py:37(__exit__)

&gt;        16    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}

&gt;         1    0.000    0.000    0.001    0.001 {built-in method builtins.exec}

&gt;        53    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}

&gt;        61    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method builtins.iter}

&gt;       8/5    0.000    0.000    0.000    0.000 {built-in method builtins.len}

&gt;        40    0.000    0.000    0.000    0.000 {built-in method posix.fspath}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method posix.getpid}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method sys._getframe}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method time.gmtime}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method time.strftime}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method time.time}

&gt;         4    0.000    0.000    0.000    0.000 {built-in method utcnow}

&gt;         2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}

&gt;         2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}

&gt;        18    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}

&gt;         2    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}

&gt;         1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}

&gt;         8    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}

&gt;        94    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}

&gt;        25    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}

&gt;         8    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}

&gt;         1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}

&gt;         2    0.000    0.000    0.000    0.000 {method 'put' of '_queue.SimpleQueue' objects}

&gt;        18    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}

&gt;        24    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}

&gt;        37    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}

&gt;        16    0.000    0.000    0.000    0.000 {method 'update' of 'collections.OrderedDict' objects}

&gt;         6    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}

&gt;         8    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}

*Tags: Errors Debugging, Deployment, Strategies*

---

**liam** - *13:57:54*

Looks like some super slow pandas 

*Tags: Feature Engineering, Performance*

---

**Adrian** - *14:00:51*

What is pandas doing that destroys the effieciency so much

*Tags: Feature Engineering*

---

**liam** - *14:02:45*

The problem is when it comes to simulating you are processing millions of updates which then get multiplied by the number of runners, so you can have one line like this which is ok on its own but suddenly becomes hours of CPU 

*Tags: General Technical*

---

**Adrian** - *14:05:10*

Yeah ok. I'll have to rethink how I'm doing this. Probably will have to look into using something you've already built that handles passing info in and out of flumine from external sources. In any case, thank you for your help Liam. Greatly appreciated as always!

*Tags: General Technical*

---

## 2024-05-07

**liam** - *10:24:30*

A few things that stand out



`seconds_to_start` you seem to be calling this a lot? Can you reduce calls?

`betfairlightweight==speed` are you using it as I can't see evidence of the C/Rust libraries

`listener_kwargs` are you using as `check_market_book` seems to be called a lot / is quite slow

`dict.___get___` lots of calls, ideally reduce if possible

`regex` seems slow, have you optimised using compile?

*Tags: Performance*

---

## 2024-05-08

**Unknown** - *02:15:41*

`seconds_to_start` done. moved this solely to listener instead of in `check_market_book`

`betfairlightweight==speed` done. downloaded it using `pip install 'betfairlightweight[speed]'` (needed the quotes to make it work)

`listener_kwargs` done. moved all market selection to `market_filter` instead of `check_market_book`

`dict.___get___` done. made better use of context objects to store selection_ids instead of looking up dictionary every time

`regex` not sure how to compile the code. I'm using pycharm; I thought it was done automatically when you run the code.



Other things: it would be great to implement the jupyterloggingcontrol with multiprocessing, but not sure if that's possible? I can get multiprocessing working without logging, but ideally I would like to be able to analyse my results as well. Any ideas?

*Tags: Getting Started, Performance*

---

**rmwesley** - *20:02:18*

First up, I am definitely *not* asking anyone what their strategies are. Plus, this may well have been asked before, but my Slack search history only goes back a few months.



I do OK-ish at a horse racing system based on form reading. All the data crunching and data presentation is automated which takes away any slog, but it is still manually intensive to evaluate more than a couple of races per evening. Therefore, I'd like to have a crack at something more systematized in flumine.



I'm struggling where to start in terms of system approach, so was wondering if people gravitated towards something in particular using flumine, e.g.



• purely statistical, e.g. bet the 2nd fav in every flat AW Class 3 race on the 3rd Sunday in May

• technical analysis, e.g. look for head-and-shoulders patterns, candlestick patterns etc. I'm guessing "no" for this, seeing some of the previous comments :-)

• backing / laying into previous support / resistance levels

Or are people generally still doing "stuff" outside flumine, e.g. compiling a tissue then using flumine to back runners outright if value odds can be found, or to arb a horse's odds if they think it will come in.



I guess it is all of the above, but it feels as though flumine's sweet spot is high-volume, low stake, low volatility systems (with high and low being subjective).



If I have committed a mortal sin even asking this, please delete it Liam.

*Tags: General Technical*

---

**liam** - *21:25:10*

Flumine can do whatever you want, not sure I really understand where you are with your ‘system’? Do you have something which is profitable or are you looking for something which is fully automated? 

*Tags: General Technical*

---

**rmwesley** - *21:42:13*

I have a [non-flumine] system which is labour intensive and only allows me to evaluate a couple of races per day. If I increase my stakes it gets a bit volatile. Therefore, I wanted to change tack and give flumine a crack. My existing system won't port to flumine, so was trying to figure out something entirely new with a more systematic, codified approach. I'm OK coding in python, building ML models etc. but was wondering if this approach bore fruit for others. Or if it didn't, what type of approach did. Hopefully that makes sense.

*Tags: Strategies*

---

**birchy** - *22:07:35*

[@U04M35GKUMP](@U04M35GKUMP) So having briefly read this thread, it seems to me that you have 2 things going on: 1. You want to automate your routine of choosing bets to place, and 2. You want to automate placement of said bets? Flumine will handle the latter, but you'll probably need to supply it with your bet selections. Basically, if your bets are chosen by using market data or external 'live' data feeds, then you can do all of that in Flumine. If your strategy is not time sensitive, i.e. you preprocess some third party sources, then (to me) that's a separate project where you create a list of bets and execute them using Flumine.

*Tags: Deployment, Strategies*

---

## 2024-05-09

**rmwesley** - *06:44:02*

I think I'm explaining myself badly, apologies :joy: I'm not interested in porting my current system to flumine. I'd like to start from scratch and build something completely new. I was hoping to short circuit a few years of trial-and-error by starting to back-test systems using a general approach which was battle proven in flumine, e.g. either what I call a statistical approach (e.g. purely using historic race data to optimise a target variable like ROI), or a classic technical analysis approach (e.g. finding head-and-shoulders patterns), or a price action approach (betting / laying into support/resistance levels etc.), or something else?



Thanks if you have had the strength to read this far.

*Tags: Errors Debugging, Strategies*

---

**liam** - *10:28:58*

Not sure I understand the logic in not building on a already proven profitable strategy, anything can be automated

*Tags: Strategies*

---

**birchy** - *11:41:51*

I may be going senile...can a pending order be cancelled? I've tested in Flumine simulation and can't cancel a pending order because it doesn't get a bet id until it turns executable. Async = False. Not tested live, but I swear that I've cancelled pending orders in the past? The basic scenario is:

a) I place an inplay bet

b) order is pending during the delay

c) I realise that I placed a 'bad' bet

d) I want to cancel the bet before it becomes executable

*Tags: Deployment*

---

## 2024-05-10

**Sarjil Patel** - *11:13:29*

I have been integrating betfair APIs in my application, but while fetching Events I have been getting few events who are basically a competition. For example there is competition with name "ICC World Cup", I am also getting an event with name "ICC World Cup". Another example is I have been getting event with Set1 in Tennis which is basically not a conventional event and I want to avoid that showing in my UI, how can I manage that. So basically I want to avoid showing events which are not conventional events like ICC World Cup, Set 1 etc. Can you please help me with how can I manage this and avoid getting events such as Set1, Set2, ICC World Cup etc.

*Tags: General Technical*

---

**Ammar** - *19:30:57*

ec2 users … out of interest which AMI are you using?

*Tags: Deployment*

---

**liam** - *19:59:48*

AWS Linux 2 

*Tags: Deployment*

---

## 2024-05-11

**Ralegh** - *05:06:47*

Generally compute is gonna be the biggest cost, s3 is pretty cheap especially for betfair data, I know someone storing tradfi futures data (magnitudes larger) and I think their storage (with intelligent tiering) was about $500-1000/month, their total aws bill was $8000/month ish (mostly spot ec2)

*Tags: Deployment*

---

**Ammar** - *08:57:55*

Our entire AWS spend at work 8-10k / month for a pretty sophisticated setup… there’s probably some fat in what they’re doing there

*Tags: Getting Started, Deployment*

---

**Unknown** - *18:28:33*

Yes [@U016TGY3676](@U016TGY3676) your use case scenario is quite strange but you can use heartbeat to cancel all unmatched bets.



In my app I can execute such script, so user can be sure if connection to betfair api is lost, all his unmatched bets are cancelled. He can use it in similar scenario you describe, so in panic when placing wrong bet he can click on Terminate script button, but minimal heartbeat you can use is 10 seconds. If betfair set it to 1 second it could be used in your scenario, so your placed bets would be cancelled in around 1 second after it is materialized on betfair server.

*Tags: Deployment*

---

**Štefan** - *19:11:01*

It depends on what is common sense in Betfair staff who implemented heartbeat. I would make it so that after the heartbeat is lost, bets in queue should be refused, not only bets materialized in their bets system, I believe in the database when a bet is assigned bet id.

*Tags: General Technical*

---

**birchy** - *21:04:52*

_"you can use heartbeat to cancel all unmatched bets"_



We're all well aware of this API feature. The question is...can PENDING bets be cancelled? To clarify, a pending bet is not yet executable.

*Tags: Feature Engineering*

---

**birchy** - *21:07:30*

+1 for Debian (or Ubuntu). I run mine on AWS Lightsail. Much less complicated for an old fart like me to understand.

*Tags: Deployment*

---

## 2024-05-12

**Štefan** - *11:14:40*

The question is what you want to achieve, if cancelling bets which would at normal state would remained unmatched after  bet placement delay, then yes no heartbeat at registered time would lead to unmatched bet/s cancelation.

*Tags: General Technical*

---

## 2024-05-19

**Unknown** - *12:31:23*

Hello everyone, in the "API-NG Demonstration Tool" I tried on all endpoints and I couldn't find any that have this differentiation between games that are "Qualifying" just as there is this differentiation in the website menu. I need to filter only for games that are not of this type, could you help me with a way to achieve this result?



→ I use _*betfairlightweight*_

*Tags: General Technical*

---

## 2024-05-22

**Andrew** - *00:00:21*

I regularly see posts around this question. My understanding is the market filter has a parameter of time to start. If so, wouldn’t that be the best control? How many people are betting on markets hours before start?

*Tags: Strategies*

---

**Dave** - *20:59:04*

For myself, I saw degradation in my tick to trade time as soon as I merged enough streams. It makes sense given the GIL + flumine, if an update arrives a micro after another, but both are tradeable, then the reaction to the second is going to be double your tick to trade inevitably. IMO if you go this route, then you should broadcast the data internally and then scale your trading processes as necessary. Of course the situation is different if you have your own stack that does true multithreading.

*Tags: Strategies*

---

**Dave** - *21:00:36*

But whether there is limited bandwidth per stream on Betfair's q is probably a question for them I reckon.

*Tags: General Technical*

---

## 2024-05-23

**George** - *11:23:38*

I'm having a small issue with `remove_bet_from_runner_book` in betfairutil.

In simulation, the runner-book is a `RunnerBook` object, but the "price-size" objects contained within it are all `dict` objects. The function expects each price-size to be a `PriceSize` object and so it throws an `AttributeError`.



Am I doing something wrong here?

*Tags: Errors Debugging*

---

**Mo** - *11:26:16*

Create an issue on GitHub and I’ll fix it 

*Tags: Errors Debugging*

---

**liam** - *11:44:48*

It’s a flumine specific optimisation 

*Tags: General Technical*

---

**Lee** - *15:21:00*

you’re free to write your own simulation middleware, flumine makes things easy swap things around

*Tags: General Technical*

---

## 2024-05-24

**Andrew** - *00:21:07*

If I’ve got that right, if writing simulation code to later use in production then don’t rely on contents of runner book for placed orders, but instead maintain record of orders placed and then later matched or cancelled?

*Tags: Deployment*

---

**Mo** - *09:23:45*

[@UCQB6S222](@UCQB6S222) should be fixed in the next release branch if you'd like to test it

*Tags: Errors Debugging*

---

**George** - *13:55:52*

Just looking at the code for that function again...



The function doesn't appear to actually change the "size" if runner_book is a `RunnerBook` object?



It seems to just leave the size exactly as it was, without actually removing the size of the bet in question?



Does that make sense or am I going loopy?

*Tags: General Technical*

---

**Mo** - *14:28:47*

If you're looking at the _original_ code before I fixed it today then yes I believe there was the bug you describe

*Tags: Errors Debugging*

---

## 2024-05-26

**foxwood** - *16:42:29*

Agree with [@U01PJ5YMFBJ](@U01PJ5YMFBJ) - "will I see my order in the market book on the next loop" i think the answer is yes and no. Simulation attempts to copy real world and adds in a processing delay for orders to reflect posting to  exchange and bet confirmation. So, if the next data packet is a long time coming then the order may well be there on next `check_market_book()` but on a busy market it may take a couple of loops. There are a number of controllable latencies for simulation in the config file [https://betcode-org.github.io/flumine/advanced/#place_latency](https://betcode-org.github.io/flumine/advanced/#place_latency)

*Tags: Performance*

---

## 2024-05-27

**George** - *10:20:04*

Thanks guys. I was not really interested in latency - I guess my question should have been:



"Does the simulation EVER modify the runner-book to include my newly-placed order?"



My `process_orders()` doesn't do anything currently - I'll look and see whether that was the problem.



Could I ask [@UUCD6P13J](@UUCD6P13J) to clarify also? As I am now super confused!! Thank you all

*Tags: Performance*

---

**Lee** - *10:23:09*

You can look at the simulation middleware to see how it works. It’s also not an easy problem to solve especially when your strategy participated in the market

*Tags: Strategies*

---

**AndyL** - *13:24:48*

[@UCQB6S222](@UCQB6S222) so not sure, but I suspect we're talking cross purpose... I think what you're asking based on the fact you say you don't use "proces_orders()", is does "process_market_book() include newly-placed orders?". Answer, "No", don't think it contains any orders.....

• `process_market_book()` Processes market book updates, called on every update that is received

    ◦ Do all your market book stuff here

• `process_orders()` Process list of Order objects for strategy and Market

    ◦ Do all your order stuff here

*Tags: Strategies*

---

**foxwood** - *14:24:26*

Understand the question now ! No - your order does not go into the visible market book. I was talking about time of visibility of an order status change in market.blotter. Sorry for time waste !

*Tags: General Technical*

---

## 2024-05-28

**Ben** - *12:39:01*

can anyone help me get started with betconnect

*Tags: General Technical*

---

## 2024-05-29

**George** - *12:18:13*

When setting up CI on GitLab I am encountering a lot of warnings about Pydantic via Betconnect. They look like this:



```../../usr/local/lib/python3.9/site-packages/betconnect/resources/betting.py:92

  /usr/local/lib/python3.9/site-packages/betconnect/resources/betting.py:92: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at [https://errors.pydantic.dev/2.6/migration/](https://errors.pydantic.dev/2.6/migration/)```

Does anyone know why this is happening? Am I using the wrong version of something?

*Tags: Errors Debugging, Strategies*

---

**liam** - *12:45:10*

flumine version?

*Tags: General Technical*

---

**liam** - *12:46:00*

actually won't matter, you can upgrade the betconnect repo and it should fix it

*Tags: Errors Debugging*

---

**liam** - *12:48:49*

I don't think so, if you look at that error message it doesn't align with the code in 0.2.1

*Tags: Errors Debugging*

---

**George** - *12:50:45*

OK got it. The order of the pip install matters. Flumine uninstalls Betconnect 0.2.1 and enforces 0.1.7

*Tags: Getting Started*

---

**liam** - *12:53:23*

its currently pinned to 0.1.7 because I haven't updated/released a new version of flumine yet, so yes?

*Tags: General Technical*

---

## 2024-05-30

**liam** - *10:59:30*

flumine 2.6.1 now available

*Tags: General Technical*

---

## 2024-05-31

**George** - *14:19:18*

In Flumine, when an order changes its status, there is a very helpful log print here: [https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/order/order.py#L113](https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/order/order.py#L113) which prints out exactly what happened. I would like to capture that output in my logging.



However, when I use a `LoggingControl` to handle my logging, I am losing those updates. When I use `_process_order` it only tells me about new orders, and it does not cover status changes.



Is there a way to get those order status updates via the `LoggingControl`? If not, am I thinking about this in the wrong way?

*Tags: General Technical*

---

**liam** - *14:23:23*

I think you might be confusing python `logging` with flumine `loggingcontrol`  however when designing flumine I decided not to make an order status update pass through loggingcontrol due to latency and race conditions.



I was worried it might slow things down but you could easily create your own order class that does do this but it does open up a few issues regarding race conditions on the status etc.

*Tags: Performance*

---

**George** - *14:30:05*

OK, thanks. I'm not concerned about race conditions on the status. Latency would concern me a bit but I'm happy to give it a go.



However I'm not immediately sure how I would create an order class that would naturally get its status updates passed through the `LoggingControl` .



To do that, I think an order status update would have to be a `CURRENT_ORDERS_EVENT` and I can't immediately see how to implement that...

*Tags: Performance*

---

**George** - *15:00:43*

it would involve recreating the log to get these updates in chronological order with the rest of the log items, ideally.



another thought - can the problem be solved by adding something into this function:

[https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/order/process.py#L79](https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/order/process.py#L79) ?

*Tags: General Technical*

---

**George** - *20:27:19*

I've implemented the blotter-comparison solution, and I think I'm coming up against the race-condition problem. I think the order changes its status before the logging-control gets a chance to write it to the file.

BUT - this would only be a problem in simulation I think?! Is that right?

*Tags: General Technical*

---

**George** - *21:20:49*

Not sure about that... actually, in simulation, the race condition with order status happens when using the normal logging controls, it's nothing to do with what I've added here.



When I comment out the code I've added, I still see "new" orders being written to the log with size_cancelled > 0 and size_remaining == 0.



That wouldn't happen in live trading because of the real-life latency involved.

*Tags: Performance, Deployment, Strategies*

---

## 2024-06-01

**George** - *08:15:25*

I might do some digging and ask a separate question at some point as it's not really related - thanks very much for your help on this!

*Tags: General Technical*

---

**George** - *10:34:46*

That's definitely a good idea. The other option would be to set the flumine default to be simply sending everything that could possibly be relevant to the log-control, and then let the user decide what they want to actually print to the log via their log-control code.

*Tags: General Technical*

---

## 2024-06-03

**Ralegh** - *09:11:50*

Try smarkets if you aren’t market making, matchbook if you’re not latency sensitive, and *maybe* betdaq if you’re doing slowish market making since their prices are quotes a bit wider but their api is not great. Betconnect is sort of a wild card I would do that if similar to matchbook you are getting value without needing fast order placement or maybe you’re trying to hedge something when there’s not much liquidity on betfair

*Tags: Performance*

---

**ShaunW** - *13:05:58*

All the alternatives do is to dilute the already small pool of money; and too much emphasis is placed on attracting 'pro' money rather than punters we can do business with. The day one of these starts advertising to punters at the same level as Ladbrokes or PP is the day I might start to take them seriously.  Without punters it's just us expending time effort and money to fight with each other, to create the illusion of activity in order to justify their exit strategy price.

*Tags: Strategies*

---

**Ben** - *23:00:31*

Anyone used matchbook api before, need abit of help

*Tags: General Technical*

---

## 2024-06-08

**PeterLe** - *06:32:31*

Hi [@U053KV09FKP](@U053KV09FKP) Well it was more 'tongue in cheek' due to the selection name :grinning:, but to answer your question, it depends.

if your relying on the price action as a trigger, you will get beat very often, even with  faster code than Python (but not always)

Alternatively you could place the bets in anticipation and cancel when they no longer seem viable

Its a conundrum

*Tags: General Technical*

---

## 2024-06-09

**Adrian** - *12:04:50*

I butchered the jupyterloggingcontrol so it would run after framework.run. I don’t know how to make it work on a live strategy

*Tags: Deployment, Strategies*

---

**Adrian** - *23:55:23*

Ok, so thanks for the suggestions. I tried using the backtestloggingcontrol example. A couple of things about the orders.txt output:

• It only recorded one of my bets in the first market when I had two

• It seems like `_process_cleared_markets` isn't running at all as I am not seeing any of those 'extra' fields appear

The logs are recording these things just fine, however, so it's just an issue with how the example is writing to orders.txt

Here is the code I'm running. the logging control is unchanged from the example. Any ideas?

```# Logger setup

logger = logging.getLogger()

custom_format = "%(asctime)s %(levelname) %(message)s"

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

# Adding StreamHandler to output to sys.stderr

log_handler_stream = logging.StreamHandler()

log_handler_stream.setFormatter(formatter)

logger.addHandler(log_handler_stream)

# Adding FileHandler to output to a specified file

log_file_path = '/Users/adrian/Data/logs/betting.log'

log_handler_file = logging.FileHandler(log_file_path)

log_handler_file.setFormatter(formatter)

logger.addHandler(log_handler_file)

logger.setLevel(logging.INFO)

...

control = backtestloggingcontrol.BacktestLoggingControl()

framework.add_logging_control(control)

framework.run()```

Here is the output in the log file (correct and complete):

```{"asctime": "2024-06-09 11:24:35,566", "levelname": "INFO", "message": "Market level cleared", "taskName": null, "market_id": "1.229732924", "profit": -3.01, "bet_count": 2}

{"asctime": "2024-06-09 11:24:35,567", "levelname": "INFO", "message": "Cleared market", "taskName": null, "market_id": "1.229732924", "bet_count": 2, "profit": -3.01, "commission": 0.0}```

And here is the output from the backtestlogging control's orders.txt (incomplete):

```bet_id,strategy_name,market_id,selection_id,trade_id,date_time_placed,price,price_matched,size,size_matched,profit,side,elapsed_seconds_executable,order_status,market_note,trade_notes,order_notes

351219139942,take_bsp,1.229732924,9541871,8902a6e5-5745-4fbf-85ab-f68b02de316d,2024-06-09 11:21:00.429400,3.0,2.761065519504482,2.0,2,-2.0,BACK,54.478439,Execution complete,"2.68,2.74,2.68",,"2.507662997872342,2.0"```

*Tags: Getting Started, Strategies*

---

## 2024-06-10

**Ben Coleman** - *08:10:04*

Hi Guys,

I have been testing a new strategy using historical data simulation on Flumine and I have noticed that quite often some of my simulated bets aren't getting matched. I am only ever trying to trade at the best available price, and I am just wondering what kind of algorithm the Flumine simulation uses to decipher whether you would have been matched or how much you would of been matched. My logic on any given update takes at most 4-5ms.

*Tags: Data Quality, Strategies*

---

**Ben Coleman** - *10:21:19*

I don't think they should anyways be matched I understand that you are always going to have some bets that don't get matched at all. I'm just wondering how the flumine simulation decides which ones get matched and which don't

*Tags: General Technical*

---

**liam** - *10:23:02*

Its all in the [https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/simulation/simulatedorder.py#L64|code](https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/simulation/simulatedorder.py#L64|code)

*Tags: General Technical*

---

**ShaunW** - *13:54:52*

You might respond in 4 or 5ms but physics and the matching engine at BF means that your order won't be applied until after a placement latency of approx 120-150ms (actually iro 50-200ms) A lot happens in that time and what you're aiming for is often long gone, especially bearing in mind that the api msg you're responding to is subject to an arrival delay too.

*Tags: Performance*

---

**George** - *16:05:34*

Hope you don't mind if I follow up on this - I noticed that the `deepcopy` is adding a lot of latency to the extent that it might not be viable to use `remove_bet_from_runner_book` in the process_market_book.

Is there a way of achieving the same result without using `deepcopy`?

*Tags: Performance*

---

## 2024-06-11

**Ben Coleman** - *02:10:47*

Yeah ok interesting. Is that placement latency something that Betfair intentionally imposes on all incoming orders or is it just a reflection of the time taken for Betfair to process your order and any other requests that were ahead in the queue?

*Tags: Performance*

---

**liam** - *05:25:55*

Placement/cancel/replace latency is impacted by your network latency and the matching cycle but on some markets you will have a bet delay that is imposed by betfair, inplay racing / tennis / football etc.

*Tags: Performance*

---

**George** - *14:30:58*

If a runner is removed pre-race and the prices of all previously matched orders are therefore reduced, does the `market.blotter` automatically update itself accordingly? If not, is there a way to do this, or how do others solve that problem?

*Tags: General Technical*

---

**liam** - *14:35:58*

simulation logic is [https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/markets/middleware.py#L86|here](https://github.com/betcode-org/flumine/blob/0241aca36f751bf1e35c5f8d7242735d2f1a5687/flumine/markets/middleware.py#L86|here)

*Tags: General Technical*

---

**George** - *14:39:36*

so in live trading, after a runner is removed, the `blotter._live_orders` and `blotter._strategy___orders` etc will all be lists of orders with the new (lower) prices?

*Tags: Deployment, Strategies*

---

## 2024-06-17

**foxwood** - *10:42:45*

Given that python assignment defaults to "by reference" is there any reason that I shouldn't keep my own list of orders by runner eg `my_runnerX.orders.append(o)` Some quick tests with debug shows that it seems to work ok for simple limit bets ie the blotter orders are == my "copies" at `process_closed_market` Wondering if flumine might recreate the order at any time eg for cancel, modify price/size etc - if so that would break this approach

*Tags: Errors Debugging*

---

**liam** - *11:21:31*

You mean like the optimised helper functions?



`market.blotter.strategy_selection_orders(_, _)`

*Tags: Strategies*

---

**foxwood** - *11:42:12*

Now exploring the wonders of `defaultdict()` - I do like some of the tricks that python has built in but yearn for the solid bedrock of C++ with a compiler and a syntax checker that doesn't let typos through

*Tags: General Technical*

---

## 2024-06-18

**Jorge** - *09:08:28*

Hey, I'm trying to understand how Flumine Simulations work for BSP bets vs Real betting. Is it true that in the simulation we assume the whole stake is filled at the BSP price? But in reality the price would be different because there is a limited amount of stake available at the BSP price?

*Tags: Strategies*

---

## 2024-06-20

**Adrian** - *00:23:11*

Sorry to keep bringing this up, but any ideas why commission wouldn't re recorded in orders.txt from the default backtestloggingcontrol? Do I need to add something extra into my strategy e.g. into the process_closed_markets function?

*Tags: Strategies*

---

**liam** - *05:22:59*

Flumine doesn’t log / store or calculate commission 

*Tags: General Technical*

---

## 2024-06-23

**Jared King** - *12:39:19*

Hi All, is there a flumine way to trigger a bet after a bet is executed? eg trigger a lay after the back is executed. Thinking checking order.trade.status_log in process_orders, then saving the bet_id, or using max_live_trade_count

*Tags: Deployment*

---

## 2024-06-24

**Jared King** - *09:50:15*

Thanks [@U4H19D1D2](@U4H19D1D2). Hows this look

```    def process_orders(self, market: Market, orders: list) -&gt; None:

        orders = market.blotter.strategy_orders(self)

        for order in orders:

            if order.execution_complete:

                if order.id not in self.order_trigger:                    

                    side = [http://order.info['info']['side']|order.info['info']['side']](http://order.info['info']['side']|order.info['info']['side'])

                    opp = {'LAY': 'BACK', 'BACK': 'LAY'}[side]

                    selection_id = [http://order.info['selection_id']|order.info['selection_id']](http://order.info['selection_id']|order.info['selection_id'])

                    handicap = [http://order.info['handicap']|order.info['handicap']](http://order.info['handicap']|order.info['handicap'])

                    size = [http://order.info['order_type']['size']|order.info['order_type']['size']](http://order.info['order_type']['size']|order.info['order_type']['size'])

                    print(f'Order trigger {opp}, {selection_id}, {size}')

                    trade = Trade(market.market_id,selection_id, handicap, self)

                    new_order = trade.create_order(side=opp, order_type=MarketOnCloseOrder(size)) 

                    self.order_trigger[order.id] = order

                    market.place_order(new_order)

        return```

*Tags: Strategies*

---

## 2024-06-25

**Ben Coleman** - *02:48:10*

Hi Guys,

I am wanting to access the market catalogue within the process_closed_market function within a flumine strategy class to gather a few bits of information about the markets I have bet on including the market base rate and the market name. I tried calling market.market_catalogue however it just returned None. I have managed to call this successfully before in other strategies however it doesn't seem to be working for me now. Is there anything you need to specify to be able to gather the market catalogue information?

Also, unrelated question, is a bet directly into the SP considered a 'live bet' in the context of setting the argument max_live_trade_count when calling your class, I assume not?

Cheers

*Tags: Deployment, Strategies*

---

## 2024-06-26

**PeterLe** - *11:19:49*

Yes you would use that as well DC ie :

```print("Loading environment variables...")

load_dotenv('details.env')



myacc = os.getenv('USER_SUB1')

if not myacc:

    raise ValueError("Account name not set in environment variables")



mypass = os.getenv('PASS_SUB1')

if not mypass:

    raise ValueError("Password not set in environment variables")



app_key = os.getenv('APP_KEY_SUB1')

if not app_key:

    raise ValueError("App key not set in environment variables")



print("Prompting for 2FA...")

two_fa = input("Please enter your 6-digit Google Authenticator code for Sub1: ")



mypass += two_fa

certs_path = r"C:\certs"



print("Creating API client...")

trading = betfairlightweight.APIClient(myacc, mypass, app_key, certs=certs_path)



print("API Client created, proceeding ...")```



*Tags: Errors Debugging, Strategies*

---

**Derek C** - *13:59:15*

you could put the credentials in AWS secrets manager if you want to avoid storing on the server in case the server is compromised. (assuming you are using AWS EC2)

*Tags: Deployment*

---

**PeterLe** - *14:05:23*

Thanks Derek Im using Lightsail and I dont think you can use secrets with that only EC2

I feel more at ease now that Lee has explained it

*Tags: Deployment*

---

**PeterLe** - *14:12:09*

Thanks Foxwood, yes I have control over the exposed ports, Ill have a think after the help thanks all

*Tags: General Technical*

---

**D C** - *14:28:39*

Jeez this is making me feel very blase about my server security settings. My stuff runs as compiled binaries so everything required is embedded within the binary and that has assuaged my paranoia about account credentials (until now). But my server configuration doesn't differ that much from the default (AWS EC2 Ubuntu). Certainly not doing anything with port access control. How big a concern "should" this be for me?

*Tags: Deployment*

---

**D C** - *14:30:46*

[@U02RN7YDRQ9](@U02RN7YDRQ9) would the password not still be visible through a call to 'ps' (assuming passed as a CL parameter) or via a shell history file? Sorry if that is a stupid question but sysadmin and security are not my forte.

*Tags: General Technical*

---

**Derek C** - *15:21:34*

I would expect to be able to retrieve the password from a compiled binary, unless you've made an effort to obfuscate it. EC2 servers can be protected from external access using security groups.

*Tags: Deployment*

---

**foxwood** - *15:21:56*

@DC as in Peter's script ask for the info within the startup of your program. The entry will still appear on the screen if scrolled back so input should be without echo or take steps to erase after entry. You just don't want to leave it lying around imho. If, as you say, you pass it as a startup argument then it will be in the ps list / python console history. If you PM me the compiled binary you mentioned earlier then I'll run it under debug which will make it easier for me to see all the literal strings in the program or if encrypted single step through until it appears in plain text. Secrets such as pwd etc and keys should never be stored in the same place.

*Tags: Errors Debugging*

---

**foxwood** - *16:08:23*

Doesn't matter how much you obfuscate things you still have to assemble the final plain text at some point. If all the info is in the binary it's hackable - no question.

*Tags: General Technical*

---

## 2024-06-27

**Adrian** - *08:53:36*

Does anyone here run their flumine/bflw sessions while logged out of ssh, e.g. using tmux, nohup or some some kind of daemon? Or do you simply stay logged in

*Tags: General Technical*

---

**Adrian** - *09:14:08*

I'll get around to using a portainer edge agent eventually, if that's any better. For now i just want my flumine to stay alive

*Tags: Deployment*

---

**Adrian** - *09:26:03*

i'm guessing the rest of you use AWS session manager or something

*Tags: Deployment*

---

**Adrian** - *10:11:04*

Is `sighup` needed for tmux? I get a bad feeling that flumine is hanging after i detach and close the console

*Tags: General Technical*

---

## 2024-06-28

**Paul** - *07:46:13*

:face_palm: If you’re doing `docker run` from a terminal via ssh and haven’t learned how to spin up an image/server to start something on boot (hint: Dockerfile is a fundamental concept of Docker), what the hell is the rest of your code like? Come on. Basics. You don’t need to be a pro sys admin to nail simple stuff… 

*Tags: Deployment*

---

**Paul** - *08:09:50*

Note: I’m sat in a Californian garden after an evening of drinking my Dad’s whisky collection for a few hours. Sorry if I’m a bit harsh. I am, however, slightly amazed that people don’t know how to get a unix server to start a script on boot, but hope to make gazillions on 0.1% edges that may or may not exist and competing with literal billionaires…

*Tags: Deployment*

---

**Unknown** - *10:43:09*

my edge is my time. the skills follow along gradually as i go. i'm doing alright. this is my edge vs bsp. everything i do is a learning curve tech wise. but i dont mind asking stupid questions because i will overcome every hurdle

*Tags: General Technical*

---

## 2024-07-02

**Sen** - *09:57:01*

Mostly taking - which helps but because there's often thin liquidity large sizes just can't get filled

*Tags: General Technical*

---

**liam** - *15:36:33*

It's a complete fucking joke tbh, almost 3 years this has been in 'production'

*Tags: Deployment*

---

**liam** - *16:19:40*

Yeah, I imagine there will be an optimum, trial &amp; error / a/b testing / randomness can get you there

*Tags: Errors Debugging*

---

**D C** - *18:24:52*

Is this a recent problem or  long term? Just wondering if it's related to my recent server reboot as I am recording the cricket score feed on said server.

*Tags: Deployment*

---

**AndyL** - *18:40:59*

[@U02S5LNK291](@U02S5LNK291) how long have you been running this strategy? As im surprised you have found a long term profitable edge on dogs with that size bet, but kudos if you have 

*Tags: Strategies*

---

**Sen** - *18:42:56*

Just to clarify - my average bet size is way smaller. There have been on occasion with very short odds combined with a large edge where a large lay bet made profitable and bankroll sense. 



But yes average sizes are way lower. I've been running strategies since Janaury 2023. I had a pretty good 2023... 2024 has naaaaat been good for me!

*Tags: General Technical*

---

**Sen** - *19:09:54*

And of course... I'm now just not getting filled anywhere near enough- so I've started to question everything. Back to the drawing board. Lots of things to test

*Tags: General Technical*

---

**liam** - *19:53:46*

Recent, started today at 11:45ish, apparently now fixed 

*Tags: Errors Debugging*

---

## 2024-07-03

**liam** - *08:21:28*

Looks like either it was never fixed or it started happening again today at about 1am

*Tags: Errors Debugging*

---

**Jonjonjon** - *09:58:32*

If 2024 has not been good why are you increasing size? My models haven't done well on dogs either so I've been scaling back. My pnl seems to stabilise when I scale back, but I'm trading much smaller size than you.

*Tags: Strategies*

---

**Sen** - *15:18:48*

I've been scaling up since end of 2023. It started with an amazing Dec. Then a really tough Q1/Q2. I want to improve many parts... the model and execution. I think that not getting fully filled in every bet is having a negative impact on my pnl

*Tags: Strategies*

---

## 2024-07-04

**Gabriel Mocan** - *01:43:00*

Hey there folks,



Quick question: I’ve purchased some historical data from Betfair, but I’m not sure how to handle the currency. My local currency is BRL, and apparently the currency from historical data comes in pounds. What’s the best approach to convert these values as close as they would be at the time of the event?

*Tags: Data Quality*

---

**Gabriel Mocan** - *12:51:35*

[@U9JHLMZB4](@U9JHLMZB4) I agree, but let me tell you the whole story.



I’m collecting live data since Aug/23. My system is pretty much validated as far as Monte Carlo allows me to validate, and I’m already live trading for about 45 days in that strategy.



Still, I would like to add more data to the backtest to make it more robust. So I’ve purchased a couple months prior to Aug/23. The thing is, my data is collected in BRL, and the strategy parameters were also designed in BRL (most important being the market volume). To make the backtest cohesive, I would need to convert the historical data to BRL before uploading it to my database.

*Tags: Data Quality, Deployment, Strategies*

---

**D C** - *13:54:53*

But surely simulation/backtest errors/assumptions effects of your own money on the market etc. would dwarf tiny fluctations in currency? No backtest can be flawless. I might understand if you were trading using some crypto shitcoin that can change 50% in an hour, but surely no fiat currency fluctuates in any significant sense over such small timeframes? I mean obviously your time is your own to do with what you will, but I'd have thought there are better ways to spend time?

*Tags: Errors Debugging, Strategies*

---

**D C** - *14:02:54*

I really don't know. Given you tend to buy historical data  per month, I'd probably just use the monthly average for the data you've purchased. But probably not a good idea to listen to me - I am not very thorough when it comes to back testing and simulation.

*Tags: Data Quality*

---

**birchy** - *17:26:22*

I guess you'd have to find a history of exchange rates. Maybe via a Forex history or similar? AI chatbots may help if you need to do it manually. Or maybe a website cache such as wayback machine.

*Tags: General Technical*

---

**birchy** - *17:37:11*

Having just seen the other thread on this subject, you could setup a simple scraper for yahoo finance and adjust the timestamps in the URL to match the betfair trading times. I'd probably go for hourly timestamps and save them all to a database or lookup table.

*Tags: Getting Started, Strategies*

---

## 2024-07-05

**Rob** - *18:13:25*

Sorry if this is posted somewhere, but if I get bored of waiting for flumine to record data, can I purchase and use the [https://historicdata.betfair.com/#/home|pro data from Betfair](https://historicdata.betfair.com/#/home|pro data from Betfair) with Flumine?

*Tags: General Technical*

---

## 2024-07-06

**birchy** - *10:06:57*

I'm in the process of tidying up my code and working on some new features. I know it's not 'best practice', but in Flumine, would adding my own object like this `market.abc = myobject()` be safe? Or should I just use `market.context`?

*Tags: Feature Engineering*

---

**liam** - *10:13:55*

Adding an attribute to a class that isn’t declared in the init is just bad practice / doesn’t help readability. If you are going to be using this object in everything then you should look at creating your own Market class but it’s up to you tbh.



I added context as a ‘safe’ place to store stuff and it keeps the Market object clean 

*Tags: General Technical*

---

**birchy** - *10:35:37*

Yeah, I agree on it's bad practice. I'm tracking previous market books and traditionally I've just done `market.context['xmbs'].append(market_book)` and then process as required. Some of the values are static once calculated, so I'm currently working on a 'MiniBook' class and will likely just set an instance in `market.context`  with a view to adding it to middleware in the future. If my strategy works. :grinning:

*Tags: Strategies*

---

## 2024-07-07

**Rob** - *11:26:06*

I'm looking to migrate my home rolled code to flumine, and so far everything seems to work well. I'll try to limit the beginner questions as much as I can, but wanted to check how people handle storing runner data to be able to use data like the last traded price over time, or traded volume in the last x minutes.



As far as I can tell, flumine doesn't offer anything like this out of the box?



Assuming I haven't missed anything, then I realise there are lots of options, including:



• a database, like postgres (or perhaps something like timescaledb if I can make the patterns fit), which is what I do with  my own code

• store as much data as I need in memory in Python, maybe using pandas/polars, or perhaps something like DuckDB?

• doing some heroics with the streaming market data in flumine to calculate what I need on the fly from the data that flumine already has?

Just checking if there's a consensus, or if I've missed other options?

*Tags: Getting Started, Feature Engineering, Performance*

---

**Rob** - *11:31:28*

I previously dumped everything in to a database because that was also where I stored all the data for long term analytics/model builds etc, but at least at first glance, the simulated backtesting that Flumine offers means I expect I can use that, along with the logging that dumps data for each trade

*Tags: Strategies*

---

**Jonjonjon** - *15:16:18*

I'd advise against using Pandas for this as it could be too slow.



I use Numba to store the data in linked lists.

*Tags: Feature Engineering, Performance*

---

## 2024-07-08

**river_shah** - *19:21:03*

I use tenacity to help with inevitable download failures. Have LLM of choice help with filling in the rest of the code per your use case

```from tenacity import retry, stop_after_attempt

....

@retry(stop=stop_after_attempt(3))

def download_file(self, client: APIClient, file):

....

client.historic.download_file(

        file_path=file,

        store_directory=os.path.dirname(dest_path))```



*Tags: General Technical*

---

## 2024-07-10

**liam** - *11:50:54*

IMHO paper trading is for testing your code against live data, no advantage over just simulating using historical data

*Tags: Data Quality, Deployment, Strategies*

---

**Gabriel Mocan** - *18:46:10*

A question regarding white labels…



Does anyone has an idea how a white label exchange can have so much more liquidity than Betfair itself?



A white label exchange is totally isolated from Betfair’s exchange? Odds are always the pretty much the same but volume in WL is far greater. For example, in greyhounds races float around 50-100K BRL while in white label (Bolsa de Aposta) we pretty much always have above 1M BRL per race.

*Tags: General Technical*

---

**birchy** - *20:33:01*

I take a slightly different approach and use market.context for stashing time series, etc, order.notes for signals and a custom [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|LoggingControl](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|LoggingControl) to push it all out to CSV for later analysis

*Tags: General Technical*

---

## 2024-07-11

**river_shah** - *09:34:15*

Does `size_matched` get reduced or set to 0 entirely if bet voided as VAR events occur?



```market.blotter.strategy_selection_orders(strategy, selection_id)

b_exposure = 0

.... 

for order in orders:

    if order.side == "BACK":

        b_exposure += order.average_price_matched```

*Tags: Strategies*

---

## 2024-07-12

**Dave** - *19:55:52*

in flumine simulation, if I submit an order of size x at price y, and then look at the market book, is it synthetically added to it? or do simulation orders not show up in the market book?

*Tags: General Technical*

---

**liam** - *20:04:47*

Not simple to simulate this / not sure how helpful it would be 

*Tags: General Technical*

---

## 2024-07-13

**George** - *20:07:19*

Years ago I implemented a simulator that did all the adding and removal of orders. It required making a lot of assumptions but I felt it was useful, worth the effort, and that I ended up with a reliable system.



Whether it would be better than what flumine does - well I can't say either way as I haven't looked into what logic flumine uses to replicate the matching engine - but it was definitely pretty good.

*Tags: General Technical*

---

**George** - *20:13:05*

It doesn't help in illiquid markets that's true. Though illiquid markets are rubbish anyway. I felt it was a good investment of time and I relied heavily on it in a previous life. I'd encourage you to look into it if you have the time.

*Tags: General Technical*

---

## 2024-07-17

**liam** - *10:01:36*

InPlay Greyhounds please Betfair



[https://totalperformancedata2-my.sharepoint.com/personal/tayla_buck_totalperformancedata_com/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Ftayla%5Fbuck%5Ftotalperformancedata%5Fcom%2FDocuments%2FAttachments%2FTPD%20Q2%20Review%2Epdf&amp;parent=%2Fpersonal%2Ftayla%5Fbuck%5Ftotalperformancedata%5Fcom%2FDocuments%2FAttachments&amp;ga=1|TPD 2024 Q2](https://totalperformancedata2-my.sharepoint.com/personal/tayla_buck_totalperformancedata_com/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Ftayla%5Fbuck%5Ftotalperformancedata%5Fcom%2FDocuments%2FAttachments%2FTPD%20Q2%20Review%2Epdf&amp;parent=%2Fpersonal%2Ftayla%5Fbuck%5Ftotalperformancedata%5Fcom%2FDocuments%2FAttachments&amp;ga=1|TPD 2024 Q2)

*Tags: Performance*

---

**liam** - *13:01:56*

He is missed, I wonder if he ever recovered from going live for the first time with his 'model'

*Tags: Getting Started, Deployment, Strategies*

---

**Jonjonjon** - *19:57:05*

But the goal is to make money rather than be good at tech. Before I started using Flumine I was running stuff on a Godaddy webhosting account.

*Tags: General Technical*

---

**jackofallspades** - *20:41:42*

Thanks, [@UBS7QANF3](@UBS7QANF3). I know about this repo of yours and checked it before posting my question. I need more recent mappings (football season 2023/24).

*Tags: General Technical*

---

**Michael** - *22:12:27*

Yeah you did - but in case anyone missed it here's the summary:



1: You've got to have value.



2: Don't "green up", it breaches 1.



3: If your bot does good with small stakes but crashes when you increase your stakes you can fix it by reducing your stakes again.



That's basically it.

*Tags: Errors Debugging*

---

## 2024-07-18

**Mo** - *10:11:01*

Updated, any problems let me know

*Tags: General Technical*

---

**Jonjonjon** - *18:28:32*

For hypothesis tests...



They are basically tests to check whether or not your returns are likely to be due to blind luck or not.



For example, if you played 10 blackjack hands, you would win money about 45% of the time. But that doesn't mean you should play more blackjack if you happen to win as it just means you got lucky.



If you played a similar game where the chances of winning are unknown, a hypothesis test would help you estimate whether or not the odds of the game are favourable to you or not.



I've seen a lot of professionals experience short terms gains and think it's an exciting positive sign, when the reality is that the results are just luck.

*Tags: General Technical*

---

## 2024-07-19

**Rob** - *13:20:01*

For json, you can just output them as [https://stackoverflow.com/questions/11875770/how-can-i-overcome-datetime-datetime-not-json-serializable|strings](https://stackoverflow.com/questions/11875770/how-can-i-overcome-datetime-datetime-not-json-serializable|strings). Or turn them in to [https://stackoverflow.com/questions/19801727/convert-datetime-to-unix-timestamp-and-convert-it-back-in-python|unix timestamp](https://stackoverflow.com/questions/19801727/convert-datetime-to-unix-timestamp-and-convert-it-back-in-python|unix timestamp).

*Tags: General Technical*

---

**Jonjonjon** - *17:50:37*

How would you derive things like match rate from the Flumine data?

*Tags: General Technical*

---

**Paul** - *18:07:34*

If I put £100 up (make or take, arguably doesn’t matter), and £20 gets matched and £80 is cancelled because it lapses before it matches and the market goes to state where it can’t match (event start, market closes), that’s a 20% match. Iterate for all bets. I’m not sure how understanding queue dynamics helps to that number, it’s a trailing metric. Disagree?

*Tags: General Technical*

---

**Paul** - *18:11:53*

If you hold umnmatched and the price moves away from you, you have hit some EV (you think), but could not exploit for the £ you wanted. Now what? Move price? Wait? How urgent this question is could be determined by match rate. Big EV bets at 10% might not pay at the rate of slightly smaller EV at 95%…

*Tags: General Technical*

---

**AndyL** - *18:13:25*

good question Paul

*Tags: General Technical*

---

**Paul** - *20:40:36*

My gut says follow the Kelly trail

*Tags: Strategies*

---

**AndyL** - *21:43:41*

Thanks for a great answer [@UGV299K6H](@UGV299K6H) 

So I have one strategy that does work and makes me about £2k a month, that’s my background earner albeit not huge.. but I actually enjoy the challenge of digging but it can be very frustrating at times! This thread has spawned a new idea/area, and I think I will try your idea of pushing it… so cheers for your direction 

*Tags: Strategies*

---

**Jonjonjon** - *22:08:31*

I like Michael's advice of sticking to something. I wish I'd done it in the past. However, a difficulty with that is working out how to dig deeper. It's not always clear how to take the correct step forwards

*Tags: General Technical*

---

## 2024-07-22

**A** - *21:18:10*

Hey [@U4H19D1D2](@U4H19D1D2) - I’ve been trying to figure out how you’d done this with middleware? From [https://github.com/betcode-org/flumine/blob/master/flumine/markets/middleware.py#L26|middleware.py](https://github.com/betcode-org/flumine/blob/master/flumine/markets/middleware.py#L26|middleware.py) it looks like we haven’t got the hooks for all the data (market books, market catalogues etc).

*Tags: General Technical*

---

**birchy** - *21:47:10*

I do mine at strategy level in process_market_book but it would be nicer in middleware. I usually just save the complete market_book and then do stuff like get time series based on the publish_time_epoch, etc. From memory, middleware __call__() gets called on every update. [@U4H19D1D2](@U4H19D1D2)?

*Tags: Performance, Strategies*

---

## 2024-07-23

**Adrian** - *09:26:28*

What do you want me to say? If my posts are still there you can read them for yourself! I can’t tell you if they deleted anything or not. What I was writing about last was possible reasons why Michelle Obama’s odds are still relatively low, which someone posed the question.

*Tags: General Technical*

---

**Adrian** - *10:21:05*

[@UQL0QDEKA](@UQL0QDEKA) no I don’t think so. One account, I had had 4 instances of flumine running on two machines at home (gave up on the VPS). One strategy for market recorder, two for live strategies and one for temp/testing. No warnings from the API about connection limits or anything. I don’t my betting patterns were anything weird, one of strategies was placing back and lay bets before the off, the other was placing them after the off.

Why I’m sure it was to do with the political discord is, it was the only place I’ve been posting for the last couple of days, and I was removed instantly from the discord server the second my account was closed. 

*Tags: Data Quality, Deployment, Strategies*

---

**Mo** - *10:44:47*

&gt; Obama had no problem committing genocide in Syria, Libya, Yemen etc. what is one extra death to him?

This is a real banger

*Tags: General Technical*

---

## 2024-07-24

**Jared King** - *00:48:57*

no problem #nevergreen amirite

*Tags: General Technical*

---

**Jonjonjon** - *08:34:48*

Thanks [@UUE6E1LA1](@UUE6E1LA1). Yes I recall reading a paper suggesting that the price is related to the matched volume (similar to how tote is priced), but could not make it work for me. Maybe I should have dug deeper!

*Tags: General Technical*

---

## 2024-07-25

**Jimmy** - *10:40:34*

*If* someone wanted to go about this, what would be the best way using Flumine?

*Tags: General Technical*

---

**Jimmy** - *10:43:14*

Yeah, I get an order can be created — was more intrigued how you might handle “order pairs” and sending one in when another is matched (as per the original question)



Unless I’m misunderstanding the original question!

*Tags: General Technical*

---

**liam** - *11:13:22*

A strategy can have many trades and a trade can have many orders, so makes sense to store 'trade' state within `trade.context`

*Tags: Strategies*

---

**jhaa** - *13:47:08*

I just got the documentation today

*Tags: General Technical*

---

## 2024-07-26

**JFP** - *12:21:05*

Logging level INFO vs CRITICAL



Does logging level affect strategy processing and bet placement, or is logging running in a separate thread?

*Tags: Strategies*

---

## 2024-07-27

**thambie1** - *15:08:06*

[@UPMUFSGCR](@UPMUFSGCR) In strategy research you have endless roads you can go down. How do you choose one? How far down that road do you go? You've spent over a month, hit a bunch of dead ends, is it time to call it quits? What will you learn upon failure (the most likely outcome) by going down that road? I've tried working with other people, this is what I've found to be the lacking skill.

*Tags: Strategies*

---

**AndyL** - *18:45:47*

We know people on here who have mastered it so model pricing must be possible even now

It’s probably so simple but til you find it it seems impossible!

*Tags: Strategies*

---

**Misha** - *20:13:32*

Just started testing again with a better version of the same model, and integrating an ML model, but will be more circumspect this time :wink:

*Tags: Strategies*

---

**Misha** - *20:25:21*

Last thing: the original model I have actually does work, but with high variance. After 1.5 years there was not enough data to know. Now I have 4 years of data and it seems to be pretty consistent over 4 years. In terms of sizing we were a bit naive - due to the first 6 months all showing significant profits. Increased sizes made it a wild ride. So back to data collection, a full-time job as software development manager for a betting agency who started up in the US, and now doing the greyhounds stuff full-time, among other things

*Tags: Strategies*

---

**AndyL** - *21:08:38*

Interesting you say that Michael, I do suspect i'm one of those missfits that finds it very hard, despite pushing and working on this an awful lot, I struggle to find anything new apart from my one basic strategy.

My ideas and coding gets very very complex, and I do suspect that's not right, [http://ie.it|ie.it](http://ie.it|ie.it)'s actually simpler... although that may sound odd! Like my "book" price for 3.0 horses we discussed earlier in the thread, i'm currently working on a combination of 5 aspects that i've constructed, just seems overly complex!!

*Tags: Strategies*

---

**AndyL** - *21:16:06*

current performance rubbish! i'm going to try and swap round a couple of the params, do others do that? ie.your convinced a certain param indicates a lower price, but maybe it means higher.......?

*Tags: Performance*

---

**Michael** - *22:02:54*

I'm curious as to why you stopped betting completely rather than just scaling back your stakes? That would have left you with much better data and probably some cash. Of all the problems you can have over scaling is the easiest to fix.

*Tags: Errors Debugging, Strategies*

---

**Misha** - *22:16:33*

The stuff I developed was only supposed to be a test until my partner developed a real ML model. Except it seemed to work. So we ran with it. Scaled up too quickly, but until you actually do it you won't really learn

*Tags: Strategies*

---

**Michael** - *22:35:28*

Yeah. I guess it might have been helpful if people had warned you before you did it.

*Tags: General Technical*

---

**Misha** - *22:43:15*

I ran the back-testing before we restarted. Now mindful that the worst period the model has is 150 days without going forward (nearly 3 months), here is the graph for 4 years at the scale we are currently betting:

*Tags: Strategies*

---

**Misha** - *22:55:01*

For this model, it's very painful

*Tags: Strategies*

---

**Michael** - *22:56:41*

_"Nope - wouldn't have gone through the same learning process"_



Maybe my irony parser is broken, but I can't help thinking you mean this.

*Tags: General Technical*

---

**Misha** - *22:57:48*

Maybe you don't tackle problems like I do. It's worked well over a 35 year career where I have developed numerous systems that people said wouldn't work

*Tags: General Technical*

---

**Misha** - *23:02:26*

I gained a huge amount from the first attempt. Massive amounts of detailed point and price data; a model that does work; bet execution tests; learnt about the variance of this model; have a system that was robust enough to place around 5 million bets without ever once going rogue. That's a lot of wins right there

*Tags: Strategies*

---

**D C** - *23:10:51*

Serious question now: when you went from 270K bank to 130K bank in the space of a week (fortnight?) were you honestly not tempted to pull the plug and take a deeper look at what was going on?

*Tags: General Technical*

---

**Misha** - *23:17:26*

We are working on a proper ML model, but progress has been a lot slower than I would like. But that's not my job :wink:

*Tags: Performance, Strategies*

---

**Misha** - *23:20:05*

You have to remember that almost by accident I have something that works consistently over years, that doesn't use any new data beyond 2019, and doesn't even take into account the players. If someone told me they had that model, I would have said that they were lying

*Tags: Strategies*

---

**Misha** - *23:25:41*

My biggest client has been working on the same type of model for decades

*Tags: Strategies*

---

**Misha** - *23:26:41*

I think that they had been working on a football model for over 2 years that had not gone live

*Tags: Deployment, Strategies*

---

**Michael** - *23:27:38*

_"My biggest client has been working on the same type of model for decades"_



Is he gradually makin it better bit by bit? That would be smart.

*Tags: Strategies*

---

**Misha** - *23:27:55*

See comment about football model :wink:

*Tags: Strategies*

---

## 2024-07-28

**Misha** - *23:36:26*

This month, after a restart, we are on a "flyer" with low stakes. +15% POT after commission on 130K bets. Average stake about $0.30. Model averages +4% over 4 years, but would not expect to get quite that due to commission (bet execution manages to knock of some of the commission, but not all, at this stage)

*Tags: Strategies*

---

## 2024-07-29

**Michael** - *08:42:10*

Yeah. I think the best thing is probably just to take your history of wins and losses (at the market level) and run through the calculation for your own case.



In all honesty I would previously have answered your original question with the opinion that the lowest comm rate is always best because of the 50/50 calculation but I'm hesitating from disagreeing with [@U01DCR5PXDY](@U01DCR5PXDY) because I know that he's smart so maybe I'm wrong.



If you do work through your actual case I'd be interested to see the results.

*Tags: General Technical*

---

**Michael** - *09:26:09*

Surely you can just take your betting data and do some Pandas Fu?

*Tags: Feature Engineering, Strategies*

---

**D C** - *09:26:52*

Pandas Fu?

*Tags: Feature Engineering*

---

**Michael** - *09:27:09*

Pandas + kung fu.

*Tags: Feature Engineering*

---

**Michael** - *09:27:38*

I just meant use pandas, groupby market etc.

*Tags: Feature Engineering*

---

**D C** - *09:28:43*

Oh python pandas. I don't use pandas. Truth be told I only use python to insert my bot logs into a DB. But yeah I am sure I can do something that requires not much effort

*Tags: Feature Engineering*

---

**Michael** - *09:29:51*

Have you given it a go? The whole pandas, numpy and jupyter labs/notebooks is excellent for analysis.

*Tags: Feature Engineering*

---

**D C** - *09:32:28*

I haven't tried it to be honest. I use R for my modelling, C++ for production bots and node.js for stream data acquisition. It is something I need to get on top of but in truth my enthusiasm for programming is at a career low right now. I'll need to kick myself up the backside and give it a try

*Tags: Deployment, Strategies*

---

**D C** - *09:34:22*

I do use python but not in the jupyter way - more in the way I used to use things like Perl. Just as standalone scripts that do something useful (file IO, DB insertion etc.)

*Tags: General Technical*

---

**Mo** - *10:08:00*

Pandas Fu equivalent to tidyverse Fu [@UUE6E1LA1](@UUE6E1LA1)

*Tags: Feature Engineering*

---

**A** - *10:47:32*

Great thread. I’m currently following a similar path to you Andy - brute forcing things until I find something consistently profitable. I’m playing in the greyhounds market at the moment. Current strategy seems to be breaking even to slightly profitable (based on 2-3 weeks). Need to bite the bullet and implement simulation and backtesting as my feedback loop is so slow in my current workflow (essentially AB testing the same strategy with different thresholds running live)

*Tags: Performance, Deployment, Strategies*

---

**liam** - *10:53:35*

[@U01FJN59B52](@U01FJN59B52) I thought you where a flumine user, what is stopping you simulating?

*Tags: General Technical*

---

**A** - *10:55:54*

I am yes. Haven’t got the data yet, and general impatience :joy: 



Thought I’d learn the ropes a bit first before jumping in “properly”.



Currently adding in a market recorder like the one in the examples.

*Tags: Data Quality*

---

**AndyL** - *20:04:04*

yeah, in fact whatever tweaks I make to my model, January seems up, and March down, which most likely means my model is just "random" !

*Tags: Strategies*

---

**AndyL** - *20:11:54*

I have BSP, just got to join in pandas

*Tags: Feature Engineering*

---

**Unknown** - *20:20:02*

[@U4H19D1D2](@U4H19D1D2) 7mins , my Pandas is improving :slightly_smiling_face:

*Tags: Feature Engineering*

---

**AndyL** - *20:58:29*

I am yes, can Flumine stream from "compressed" ? i've always un-compressed them when needed

*Tags: General Technical*

---

**liam** - *21:03:30*

Yes it can, I store compressed and take the performance hit

*Tags: Performance*

---

## 2024-07-30

**Unknown** - *00:00:48*

So some good points learner by me tonight:

1. You can run Flumine sumulation from compressed data, I can now simulate a whole year at once, nice one [@U4H19D1D2](@U4H19D1D2) 

2. EV_vs_BSP is great to visualize actual startegy performance

3. You need a decent size dataset to evaluate a strategy, ie.1 year! as you can see below this "fitted" strategy can perform seemingly well for a few months in a row, but in the end of the day although +ve, it is highly likely just random!!

 1 year backtest from compressed .gz data:

*Tags: Performance, Strategies*

---

**Michael** - *08:20:20*

I actually don't agree that you need SO much data. The number of markets you need depends on how much edge you have, what prices you're betting at and what you're trying to get from the data.



If you have more edge you need fewer markets to prove it, if you're betting at middleish prices you need fewer markets before your results converge and if you're content to know roughly whether a strategy is profitable rather than exactly quantifying your edge then that needs fewer markets.



There's also a question of recency, when you're stacking up a year or more of data you can risk market conditions changing.



For my own part I'm usually content to analyse a few thousand markets, even fewer if I'm working on something quite specific.



When TPD came along I went live with &lt; 100 and a hunch - as I recall [@U4H19D1D2](@U4H19D1D2) did the same? When I first started out I had no data at all, went live with nothing and just analysed the results.

*Tags: Deployment, Strategies*

---

**liam** - *08:25:25*

So it looks like you have a good setup for simulating strategies however you need to be careful not to use this for signal creation i.e overfitting on historical data



[@UGV299K6H](@UGV299K6H) I would agree when it comes to inplay and/or strategies with a high edge but with pre race I have always found you need a lot of data.



With TPD I built something off the test data betfair had from Monday and went live on Tuesday thinking the edge would last a few weeks at best :joy: 

*Tags: Getting Started, Data Quality, Deployment*

---

**AndyL** - *09:30:55*

[@UPMUFSGCR](@UPMUFSGCR) ev vs bsp gives you an idea if your pre race strategy is beating BSP overall, a cum plot is a good way if visualizing it

Basically you need to stash the reconciled BSP after inplay, I actually just do it in the closed_market callback, then in your analysis/pandas simply merge it with your matched bet data in order to plot…

*Tags: Feature Engineering, Strategies*

---

**Jonjonjon** - *09:32:30*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) so if your model is perfect, should your EV vs BSP plot be a straight line?

*Tags: Strategies*

---

**Michael** - *09:35:02*

I think part of the reason that others found it harder is that by the time they started there were already established players, we didn't have that problem. Also; the first licenses were all given to successful IR players so we were well positioned to use that knowledge. Coming into TPD without that background is a much bigger challenge.

*Tags: General Technical*

---

**liam** - *09:54:05*

I can say that it has resulted in me going down paths that have found alpha but I have yet to use ML in what I would call production / live 

*Tags: Deployment*

---

**D C** - *10:22:30*

I quit my last job in October 2021 so coming up to 3 years. I stopped keeping up with changes to C++ language standard at C++17 (and don't use the vast majority of that). And I shudder to think of that kind of corporate/management bullshit frameworks that are now the norm. Plus I am 49 so not much scope for employment when I'd be up against new grads with a login to the latest trendy LLM code manufacturer and some idea how to get good results out of it.

I'd probably try my hand at A-level tutoring if I ever had to go back to work. I couldn't go back to academic research because I'm about 15 years out from that field. There is always work about regardless, but I'd rather not have to go back to the good old days of unloading lorries for a living.

*Tags: General Technical*

---

**Alex A** - *11:45:25*

A little more humility would also help you have more productive discussions here.

*Tags: General Technical*

---

**liam** - *12:36:34*

I would call it TPD refinement as things have become more competitive. A few years ago it was such an edge that you could be very lazy but now parts of my strategies / models are falling into negative EV particularly in areas where the margins were already thin. 



A lot of my recent work has been to try and quantify where the value actually is so I can better price the market.

*Tags: Strategies*

---

## 2024-07-31

**Misha** - *15:04:16*

On that one I'm very confident. It's nearly 15 years in production, constantly upgraded, and has been used by one of the biggest bettors in Australia, and one of the biggest in the world. Turnover through my software has been over $1B. It's what I do for a living (professional software developer), and last 15 years full-time in sports betting. I can compare what I have to all the other systems used by my clients over that time. So I'm pretty sure on that. My modelling is crap (not my area). Software development is what i do on the other hand

*Tags: Deployment, Strategies*

---

**liam** - *15:07:45*

I would put money on bflw / flumine having processed more 

*Tags: General Technical*

---

**D C** - *15:27:02*

Today:                                             _"My modelling is crap (not my area)"_



Few days ago in another thead:   _"I have no bias in my prices - the market does though"_

*Tags: Strategies*

---

**Paul** - *20:59:06*

As someone who is far away from this point, I’ll just remind you to see the benefits. There’s a reason it’s seen as those outside as a bit of an aspirational club. Regardless of any offsetting/minimisation strategy, I’d encourage you to see those chunk of cash going out as motivating. They are a way of keeping track of your success.

*Tags: Strategies*

---

**Paul** - *21:01:15*

My tax + NI bill as a PAYE employee as per my P60 is in upper 5 figures. I can moan about it, but as my mate pointed out to me when I bitched about it the other day, it means I’m making quite a lot more. Given where I started as a working class broke kid whose school mates disproportionately ended up in prison, it’s a point where I can give myself some kudos. Raise a glass to every deduction: it’s a celebration in some minor way. :) I know this doesn’t help, just a thought.

*Tags: General Technical*

---

**D C** - *21:02:32*

Not sure that I'd say that doesn't help [@U012XF5CNPN](@U012XF5CNPN) - certainly trying to reframe my perspective is something I am trying to do anyway.

*Tags: General Technical*

---

**Jonjonjon** - *21:05:48*

[@UUE6E1LA1](@UUE6E1LA1) to be honest I can't really remember. I've been on the 50% rate for perhaps around 5 years. Fortunately when it kicked in I'd made some improvements (i.e. started using Flumine), so it didn't reduce my net take home profit for the year.

*Tags: General Technical*

---

**Gooat** - *22:02:36*

Have you considered modeling some tennis?

*Tags: Strategies*

---

**Jonjonjon** - *22:04:26*

The thing with Betfair is that if you are lucky enough to have a winning strategy then return on investment is very high.



I'd like to move to financial trading, but don't have a baseline model to work from and don't even know how/where to get clean/reliable data.



The top dogs here are probably making 100x annualised returns (non compounded) without drawdown. I don't think that's possible in traditional finance, though perhaps it is in crypto but I'm not involved in that so can't say.



As a result, I think many people are stuck with betfair

*Tags: Strategies*

---

## 2024-08-01

**ShaunW** - *13:07:57*

Similar humble start [@UUE6E1LA1](@UUE6E1LA1), my old man always used to say he wanted to be the guy who paid the most tax. Took me a while to understand that. 



PC seems bad but compared to almost anything else that's legal we have mercifully low fixed overheads. No buildings, no wages, no accountant, no insurance, no stock etc etc. Throw in complete freedom about when and how hard to work..... Most would kill for the deal we get. If people don't like it, they should try a proper job and see how that goes.

*Tags: Errors Debugging*

---

**Jonjonjon** - *16:46:41*

[@UUE6E1LA1](@UUE6E1LA1) Could you turn that question into a poll.

*Tags: General Technical*

---

**Jonjonjon** - *20:50:01*

But if their model suggested they'd get value from certain 1,000 lays, then why not?

*Tags: Strategies*

---

## 2024-08-02

**thambie1** - *21:43:13*

Absolutely had losses :joy:. If laying at 1000 was all I did, it would be a problematic graph because of the low sample size. Include all the trades at other prices and it's fine

*Tags: General Technical*

---

## 2024-08-03

**D C** - *09:42:24*

That is fair [@UQL0QDEKA](@UQL0QDEKA). I think I do need to revisit my understanding here. I know that my own backtesting codebase does not deal with the subtleties of XM - does the flumine framework ?

*Tags: General Technical*

---

**Jonjonjon** - *14:03:04*

Thanks for posting here [@U9UJH4P1P](@U9UJH4P1P). Out of interest

• do you trade your own account?

• Is Betfair actively trying to improve the exchange performance?

*Tags: Performance*

---

**Justice** - *20:14:45*

I'm curious as to what types of models people on here are using and why.



• Linear  (e.g logistic regression)

• Bagging (e.g random forest)

• Boosting (lightgbm, xgboost etc.)

• Deep learning (pytorch, tensorflow etc.)

• No model (!)

*Tags: Strategies*

---

**D C** - *20:57:22*

You can get a performance bump there if you just do total_vol/vol #freealpha :grin:

*Tags: Performance*

---

**Michael** - *21:58:19*

So I'm going to admit that I don't do "proper" regression. I just plot my features, look at the plots, rub my face, think about it, make up something plausible and try it.



Every once in a while I get all anxious that I should have done it properly, so I make a big effort and do xgboost or whatever, then I test that against my "eyeball" criteria and either the eyeball wins or the difference is trivial.



My general experience is that there are huge gains to be found when I find a new feature but overly stressing on the model doesn't yield much.

*Tags: Feature Engineering, Strategies*

---

**D C** - *22:25:32*

I personally use certain methods of good old fashioned classical inference.

As for why:

1. I don't like using any methods that I DON'T understand from a theoretical standpoint. This rules out most ML at this point in time (even though things like logistic regression seem to be labelled as ML nowadays, to me it's just a GLM).

2. I like to use models where parameters are evaluated and known as part of the model fitting process (I don't like black boxes). This allows some kind of common sense validation and quantification of parameter effects.

3. I use C++ so I need to be able to evaluate models in that context - not by plugging data into some black box evaluation package in python or R. This reason is wrapped up with 2 to a certain extent.

It might make me a bit of a dinosaur, but I'm a firm believer that statistics can be (and is) abused and is most useful when you understand what it's doing. Never been a fan of throwing a shitload of data into a black box and using the output despite having no idea what is going on under the surface.

*Tags: Strategies*

---

**Alex A** - *23:36:59*

I use C++ too, many of the machine learning packages are written in a systems programming language and expose a C API, you don’t have to use Python or R.

*Tags: General Technical*

---

**John Foley** - *23:38:23*

generally go straight to xgboost for any modelling on tabular data

*Tags: Strategies*

---

## 2024-08-04

**Mo** - *07:48:31*

None of these. I use sport specific probabilistic models such as Markov chains for tennis or running time distributions for horse racing

*Tags: Strategies*

---

**John Foley** - *09:34:02*

interested to hear what people have done to tackle latency with model inference (ML or otherwise). struggle with this myself, usually end up just making fewer .predict calls than i would like to

*Tags: Performance, Strategies*

---

**Justice** - *10:09:43*

[@UBS7QANF3](@UBS7QANF3) Very interesting. Is there a particular reason why you use such models? Do the idiosyncrasies of specific sports make them well suited?

*Tags: Strategies*

---

**Justice** - *10:10:48*

[@U02GGCCLTKM](@U02GGCCLTKM) I have used Xgboost before, I know it is very popular. What do you like about it? I always struggle with overfitting when using gradient boosting models

*Tags: Strategies*

---

**Mo** - *10:13:06*

This is obviously very much a philosophical argument but conceptually by using such a probabilistic model you are bypassing a large amount of learning that a generic machine learning model has to do to understand the dynamics of the sport



Also means your model is not a black box and you can directly relate the model parameters to observable factors

*Tags: Strategies*

---

**John Foley** - *10:17:32*

a gradient boosting model requires very few (if any) assumptions about distributions etc. handles outliers and strange distributions well. handles complicated interactions between features and handles missing data. feature selection is somewhat "built-in." performance is comparable to deep learning algos for simple tabular data

*Tags: Feature Engineering, Performance, Strategies*

---

**AndyL** - *10:19:34*

Do people’s models use input parameters based on the live event or historical as well? Eg. how fast the horse ran in the last 5 races??

*Tags: Deployment, Strategies*

---

**D C** - *12:37:19*

Thanks Joe - so in an XM enabled market, using the same example of 2 users putting orders in (almost) simultaneously, what happens if someone places a back bet of £2 on selection A @ 60.0 and the other places a back bet of £50 on selection B @ 1.01 ? Can you explain the flow of events (sorry for all the question, I just really want to nail down my understanding based on your prior posts) ?

*Tags: General Technical*

---

**liam** - *12:50:59*

So the real question is, how much does the XM algorithm make for betfair? 

*Tags: General Technical*

---

**Rob** - *14:03:03*

gradient boosting, because it repeatedly wins the online competitions (like kaggle) for the types of problem I'm trying to solve. Although I understand from all the posts here that everyone's approach is different.

*Tags: General Technical*

---

**Jonjonjon** - *15:19:34*

I prefer no model.

*Tags: Strategies*

---

**Jonjonjon** - *15:32:59*

To be more accurate I use a non-machine learning based model. More similar to black scholes than regression stuff.

*Tags: Strategies*

---

**Jonjonjon** - *15:34:35*

The thing with the models above is that they often won't make much difference. Or if the model makes a difference and you have a good understanding of your features, you can transform them so that even linear regression works.

*Tags: Feature Engineering, Strategies*

---

**Jonjonjon** - *15:37:14*

A problem I've seen with ML models and sports betting is that a lot of highly qualified people with PhDs from top institutions can't even work out what makes a good strategy or not.

*Tags: Strategies*

---

**Jonjonjon** - *15:37:45*

Actually, it's not really a problem as those people probably have a lots of money and might be giving some of us our edge

*Tags: General Technical*

---

**AndyL** - *23:08:56*

For inplay horses does Flumine seconds_executable start from after the inplay 1 second delay?

*Tags: General Technical*

---

**AndyL** - *23:16:58*

So say I have the matched volume for a selection, also the available queued, maybe infer recent match rate, and I put together a model price for a selection as:

3.0/(vol/totvol + queued/totqueued + matchrate/totmatchrate)

What’s wrong with that as a model? No ML in sight! Just plain raw values….

???

*Tags: Strategies*

---

## 2024-08-05

**Michael** - *06:36:39*

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) There's nothing wrong with it if it works. Does it work?



There are different styles of player. It's a question of what your strengths are and some extent one of aesthetics.

*Tags: General Technical*

---

**Jonjonjon** - *20:34:43*

Is bad tech a problem if it makes things equally hard for all?

*Tags: General Technical*

---

**Jonjonjon** - *20:35:40*

Money can be made from bad and inconsistent APIs that others don't know how to use. Obscure and poorly explained rules help as well.

*Tags: General Technical*

---

**Jonjonjon** - *20:39:01*

[@U05N9773A23](@U05N9773A23) isn't there already a wrapper for Betdaq? Similar to betfairlightweight?

*Tags: General Technical*

---

## 2024-08-06

**foxwood** - *10:52:24*

Too early in the morning for me - can't see what's wrong ? a) didn't think there was anything like a "palpable error" on an exchange b) Edna's (sic) comment is an ambiguous Irish typo that could mean "paid", "pallpd" or "pulled". I'd take it as meaning the original £10 back bet at 88 will get pulled by the bettor. What am I missing ?

*Tags: Errors Debugging*

---

**D C** - *10:55:47*

As if the behaviour of offering prices in that particular manner is somehow underhand and against the spirit of Betdaq. I mean it is designed to catch someone's error but fuck it - it's a cruel world and the burned hand is the best lesson against playing with matches. I'm all for a Darwinian approach but seems Betdaq want to protect "regular" users.

*Tags: Errors Debugging*

---

**foxwood** - *11:10:53*

[@UUE6E1LA1](@UUE6E1LA1) thanks for mentioning branches - now know how to use Twit better :upside_down_face:

*Tags: General Technical*

---

**איתמר קינן** - *13:10:26*

Hey guys

I'm currently working on a model for getting some football odds (over under/corners/money line)

I tried some scraping jobs on bet365 but it seems too hard/impossible

I'm looking for a sharp local bookies with fastly updated odds that is possible to scrape/has a good api

I don't mind paying some $ if necessary

*Tags: Strategies*

---

**Jonjonjon** - *13:37:58*

That type of strategy paid for the deposit on my flat many years ago. Please don't criticise it.

*Tags: Strategies*

---

**K H** - *13:38:52*

Betfair pro data is here [https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home)

Not sure if there's a reason you'd prefer a 'sharp local bookie' over betfair odds (maybe poor spreads further out from the event?). But if you were going to scrape Bet365, you can just gather betfair data using flumine and pay nothing at all instead

*Tags: General Technical*

---

**K H** - *13:51:05*

[https://betcode-org.github.io/flumine/quickstart/](https://betcode-org.github.io/flumine/quickstart/)

[https://betcode-org.github.io/betfair/](https://betcode-org.github.io/betfair/)

[https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py)



The second link is an example market recorder flumine provides. But in general you should be able to accomplish what you're trying to do using flumine/betfair-lightweight as a basis (the first two links are to their documentation)

*Tags: Data Quality*

---

## 2024-08-07

**Gooat** - *18:02:07*

Nice. Model or mix of price and flow? I struggled to avoid averse selection when testing it.

*Tags: Strategies*

---

**liam** - *18:08:13*

model

*Tags: Strategies*

---

**Sen** - *18:12:02*

Hey guys - I wondered if anyone has a list of markets showcasing traded volume per event &amp; pre-event. 



Effectively I'm looking for a list like this:

For 2024:

Horse racing: 100k per race 

Greyhounds UK: 15k per race

Football League 2: 10k per game

Sport X: 5k per event 

Sport Y: 5k per event 

And so on.



Much appreciated... or even if you would be able to tell me where to look that could also help.

*Tags: General Technical*

---

**Dave** - *19:49:29*

Suppose I have n Back bets, at various prices of various sizes. I want to calculate my average price - do I vwap or do I compute my total liability and potential winnings and derive the AVG px from that? Flumine simulation computes vwap for average_price_matched on simulated order from what I can see - any reasoning for that?

*Tags: General Technical*

---

## 2024-08-08

**liam** - *05:56:59*

flumine matches what the site gives you / what you would win which makes sense to me?

*Tags: General Technical*

---

## 2024-08-09

**D C** - *23:23:15*

In general, if you have been running a strategy for some time that experiences fairly steady match rates, then suddenly notice a quite severe drop off in that match rate (despite no changes to the strategy) what are the most likely causes, and what can you do to try to confirm the reason?

I can pretty much only think of 3 reasons:

1. Drop off in volumes in general

2. Running a new strategy in parallel that interferes with the existing strategy

3. Someone else competing for prices

I've been running this strategy that is profitable (nothing special but worth doing) that is offering prices (both sides) inplay on US markets for minimum stakes and I've basically seen match rates halved from start of June (as compared with January to May). Profit is down and placing a lot more bets overall as a consequence of so many now being unmatched.

Any advice on where to start looking for a reason? Or best to just forget it?

*Tags: Strategies*

---

## 2024-08-10

**Peter** - *10:27:48*

Assuming that you're not doing 2 - you'd have told us wouldn't you? Then it would have to be 1 or 2. So the question becomes what's happened to the traded volume in the markets? If that's gone down it would suggest 1, but if it's remained constant then it suggests that some of the matched bets are now going elsewhere, i.e. 3.

*Tags: General Technical*

---

**K H** - *13:49:32*

If you don't already, I'd recommend tracking as much as possible around the matching process - total volume matched, volume matched on your selections, liquidity per second around the time you place your bets, the amount of money waiting at the prices you bet on etc.



If you haven't run the strategy for more than a year, one of the causes may be seasonality - although imo the exact immediate cause can only be known by tracking enough data to tell you what's happening around your matching process

*Tags: Strategies*

---

**liam** - *14:10:00*

I wouldn't call this 'normal', based on your previous issues how confident are you that you haven't changed anything? Latency on your tick to trade? etc.

*Tags: Performance*

---

**D C** - *14:51:28*

Yeah I've enough years as a dev to know that the most likely source of error is the bloke working the keyboard.

*Tags: Errors Debugging*

---

**ShaunW** - *14:53:24*

.... Wish mine was user error, the source, config and server setup haven't been changed since July 2022.:thinking_face:

*Tags: Getting Started, Errors Debugging, Deployment*

---

**ShaunW** - *15:29:42*

Not.    But that explains why it's been boom time for my mixed doubles ping pong strategy. :grin:  Wimbledon and big footy can divert some money but Olympic markets aren't big enough to cause a ripple. 



There's usually a silver lining, an enforced revisit isn't necessarily bad thing.

*Tags: Strategies*

---

**D C** - *15:45:53*

I'm not sure that it is possible to determine really. I mean how do you assign a probability to someone else developing a similar strategy. The US spread is wide - this is a 100% offer price strat so someone could easily be offering better than I am. In all honesty [@UGV299K6H](@UGV299K6H) I would not know where to start in regards to putting a number on it.

*Tags: Strategies*

---

**D C** - *15:59:02*

It's possible but I would not know where to start with it to be honest. If I could assume that it isn't a mistake at my end, AND the market volumes have not dropped off then the most likely explanation would be someone else putting up better prices than mine. Something I will be able to scrutinise after looking at the first 2 points. It's the nature of the drop though - it is like an overnight step change which is good evidence that something I've done (but can't identify) is the cause of the problem

*Tags: General Technical*

---

**ShaunW** - *16:11:38*

Tricky one, backtesting variations (like asking for less margin)  is unreliable in markets you were present in. But it's impractical to step away for long enough to get a big enough clean sample without you in it..  I end up lowering the latency just enough for test 'me'  to beat what live me did but then that's at best only an indicative bodge.  What happened is knowable, what you should have done instead and into the future might not be.

*Tags: Performance, Deployment*

---

**Dave** - *18:23:37*

Backtests are good - if your backtest for May + June look similar and look good, while your real performance for May matches backtest but June doesn't then likely you've broken something (i.e. you got a bug). If your backtest for June shows similar degradation then it suggests your alpha has degraded (either because some participants have scaled down, someone competitive has entereed etc). It might be a matter of recalibrating some params, or it might be a matter of resting the strategy and backtesting again in a few weeks to see if the breakage was transient or not.

*Tags: Errors Debugging, Performance, Strategies*

---

## 2024-08-11

**Sen** - *10:44:17*

I'm using advanced data - and yes it's got its flaws. Is the pro data actually just higher granularity but the same data quality? or is it higher granularity + higher quality data?

*Tags: Data Quality, Deployment*

---

**Mo** - *11:59:55*

[@UGV299K6H](@UGV299K6H), I have a somewhat similar situation at the moment where I have a suspicion a code change has adversely affected performance. I’m using a Kolmogorov-Smirnov test to compare the distribution of market returns pre- and post-change. It’s definitely not the only way to do this but it’s one way to do it

*Tags: Performance*

---

**D C** - *12:09:52*

Yeah did that [@UBS7QANF3](@UBS7QANF3) . I added a new strategy that does not overlap with the "problem" strategy 3 days prior to the match rate drop. Turns out that the NEW one was doing nicely over those 3 days and then ALSO suffers from the match rate drop. Now looks like a competitor is the most likely cause.

*Tags: Strategies*

---

**liam** - *12:30:25*

[@U05L8PZD2FM](@U05L8PZD2FM) might be able to explain how the data is actually created but with BASIC/ADV you are getting data that has been grouped ('rolled up') and then streaming has been replicated, thus it is completely different to what you see when betting live or simulating with real data. Obviously it depends on the frequency you are betting as to whether this has an impact or not but if you are concerned about betting movements over a 1s interval you need real data.

*Tags: Deployment, Strategies*

---

**Joe** - *16:01:55*

Sorry I don't know the details other than it is generated from the stream so materialised, rolled up and sampled as you suggest [@U4H19D1D2](@U4H19D1D2) , probably of no use to anyone that is price action based, probably usable if you want to test your fundamental pre-off models against the market.

*Tags: Strategies*

---

**Sen** - *17:02:36*

Interesting- so I'm only running pre-off strategies. But I'm also interestingly finding that modelled vs. Actual is varying quite a bit. So trying to solve this- and this very much might be one of the reasons

*Tags: Strategies*

---

**D C** - *17:08:06*

If you are talking about pure API-NG (as opposed to flumine/bflw wrapper) from memory, I think you'll need listMarketBook and add "SP_PROJECTION" (or maybe "SP_TRADED") to your PriceProjection field. Take a look at the BF API documentation pages for listMarketBook operation though - that will put you right if what I've said above is wrong.

*Tags: Performance*

---

**Paul** - *23:49:15*

Things that spring to mind others didn’t mention 



1. If you’re making rather than taking, and your prices are more than 1-2 pips away from each other (back/lay spread on US racing can be very broad), someone new could be doing the same strategy “inside” your prices. Can you go back and check what volume was matched at a price other than your own?

2. Your hosting setup has changed. Data centres get new links, retire old ones, your VPS could now have a noisy neighbour sucking bandwidth, and so on, and so on. Doesn’t take much to add 10ms latency onto a box. Even worse if you’re running from home (residential broadband especially over OpenReach makes no guarantees about anything, ever)

3. You were betting against another bot, you’ve taken their money, they’re gone.

4. Variance. Law of large numbers says you’ll have bad runs now and again and sounds like this has been running for a long time. I know you asked how to check it’s not chance alone, but one easy check is to wait another few days where the odds become smaller and smaller

*Tags: Getting Started, Performance, Strategies*

---

## 2024-08-12

**Sen** - *09:12:24*

Is there a way to find all marketIds for races yesterday? I know how to get when betfair releases data (but it's with a 5/6 day lag)

*Tags: General Technical*

---

**D C** - *09:32:09*

About 25% for me but it's hard work as the inplay volumes are so low compared with UK. Also get a lot more GPS problems on the US side. On the plus side there are a LOT more races. Seems that there is just no interest until the end of the race either.

*Tags: General Technical*

---

**Sen** - *09:37:25*

Mostly because the 5 day lag doesn't have a meaningful effect on 95% of features for my fundamental model. Then I just get the data from betfair 5 days later for modelling purposes. But given my fears about my data quality... I'm now very much thinking about recording and storing

*Tags: Data Quality, Feature Engineering, Strategies*

---

## 2024-08-19

**James Scott** - *16:09:25*

How many Betfair api connections can you have?

*Tags: General Technical*

---

**James Scott** - *16:16:18*

Thanks, I’m using streaming and I had 6 connections and one of them stopped working, I thought it may have been the number of connections.

*Tags: General Technical*

---

## 2024-08-24

**Štefan** - *17:30:37*

Betfair api is back, but almost no activity on markets

*Tags: General Technical*

---

**Unknown** - *17:34:22*

[@UUE6E1LA1](@UUE6E1LA1) They do not report that on status page, so have problem there

*Tags: General Technical*

---

## 2024-08-28

**ShaunW** - *12:22:26*

I think that in-running seasonality [@U03FS7KM2NL](@U03FS7KM2NL) is due to the longer distances.   



For what it's worth Imo the reasons for the decline are manyfold, innovators (like TPD) have been more interested in wooing us than generating new customers, lack of interest in the affected sports, greater competition for the leisure £, a decade+ of wage/inflation disparity, lack of imagination in the way betting is presented, probably others too.



The question is, will the decline bottom out before it becomes unsustainable.

*Tags: Strategies*

---

**D C** - *14:09:13*

Yeah that makes sense [@U4H19D1D2](@U4H19D1D2). Coinbase introduced a questionnaire that UK customers had to complete in order to carry on using it (based on some FCA recommendation) but it basically asks if you realise that your money can vanish overnight due to the exit scams about. A lot easier than submitting wage slips and bank statements to a bookie. I know more people who have money in shitcoins than I do placing any kind of bet.

*Tags: General Technical*

---

**D C** - *14:11:06*

There is a huge anti-gambling lobby [@UEA14GBRR](@UEA14GBRR) - basically ex-punters onto a good grift who took a bit of a beating at the bookie and cant take responsibility for their actions and now blame the bookies. I agree that the sector could to a LOT more for problem gamblers but these people (including academics) are onto a good thing with their funding so they are not going to disappear anytime soon.

*Tags: General Technical*

---

**D C** - *14:12:25*

Biggest problem is that they refuse to distinguish between RNG shit like online slots and sports betting.

*Tags: Strategies*

---

**D C** - *14:16:53*

Speaking from personal experience, slots really ARE addictive. I've never had a problem with sports betting. But high street bookies only really make money from FOBTs now - 4 per store limit so they open up multiple branches in the same town. Absolute filth and I agree they should be put in the bin.

*Tags: Strategies*

---

**ShaunW** - *15:07:09*

You should be able to spend/gamble your own money freely, and I think most of us here do. It's when they're spending other people's that the problems start hence the funds source check.

*Tags: General Technical*

---

**Justice** - *15:07:59*

[@UEA14GBRR](@UEA14GBRR) Agreed, problem is with people nowadays, it's always someone else's fault

*Tags: General Technical*

---

## 2024-08-29

**Ralegh** - *07:33:13*

Forex is a no go, crypto is a maybe if you have actual trading/quant experience, but there’s always a chance you build out a model and get kicked off because of regulation changes. You have to think what specific edge you might have, tower dominate cross exchange arbs and general HFT crypto, there’s probably money to be made on hours to days horizon but would need experience doing that or something similar. If you somehow have access to an exchange that firms can’t get access too (for whatever reason) then that could be an edge. Otherwise it’s tough.

*Tags: Strategies*

---

**tone** - *10:58:18*

Hi, would someone be able to give me a steer re how to handle the Betfair TPD stream? Any code examples would be really helpful!

*Tags: General Technical*

---

**D C** - *11:09:58*

Have Betfair not given you any documentation for it?

*Tags: General Technical*

---

**tone** - *11:15:57*

Python. I've been using Flumine

*Tags: General Technical*

---

**liam** - *11:18:24*

flumine/examples

*Tags: General Technical*

---

**liam** - *12:54:44*

Yeah I keep meaning to remove it but its just betconnect



[https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py](https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py)

*Tags: General Technical*

---

## 2024-08-30

**ShaunW** - *15:36:53*

.... Amending lays to adjust for liability/stake at the revised sp estimate, whether you amend back stakes depends on if you're using a fixed stake or fixed return.  But now I'm teaching granny to suck eggs I guess :wink:

*Tags: Errors Debugging*

---

**liam** - *15:37:55*

Huh, isn't the problem knowing who the favourite is?

*Tags: General Technical*

---

## 2024-09-02

**tone** - *10:54:38*

I have managed to save the TPD stream to a file but now I wish to replay it in simulation mode. I have set sports_data_filter=["raceSubscription"] but cannot determine how to get the strategy to associate the saved betfair market with the saved TPD stream - or does this somehow happen automatically?

*Tags: Strategies*

---

**tone** - *11:34:02*

Here's the DEBUG level logs: {"asctime": "2024-09-02 10:31:36,488", "levelname": "DEBUG", "message": "Starting new HTTPS connection (1): [http://identitysso-cert.betfair.com:443%22}|identitysso-cert.betfair.com:443"}](http://identitysso-cert.betfair.com:443%22}|identitysso-cert.betfair.com:443"})

{"asctime": "2024-09-02 10:31:36,707", "levelname": "DEBUG", "message": "[https://identitysso-cert.betfair.com:443](https://identitysso-cert.betfair.com:443) \"POST /api/certlogin HTTP/11\" 200 87"}

{"asctime": "2024-09-02 10:31:36,710", "levelname": "INFO", "message": "Client added", "username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": null, "transaction_count_total": null, "trading_controls": [], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2024-09-02 10:31:36,710", "levelname": "INFO", "message": "Adding market middleware &lt;flumine.markets.middleware.SimulatedMiddleware object at 0x00000191605B7460&gt;"}

{"asctime": "2024-09-02 10:31:36,711", "levelname": "INFO", "message": "Adding client control MAX_TRANSACTION_COUNT"}

{"asctime": "2024-09-02 10:31:36,711", "levelname": "INFO", "message": "Adding trading control ORDER_VALIDATION"}

{"asctime": "2024-09-02 10:31:36,711", "levelname": "INFO", "message": "Adding trading control MARKET_VALIDATION"}

{"asctime": "2024-09-02 10:31:36,711", "levelname": "INFO", "message": "Adding trading control STRATEGY_EXPOSURE"}

{"asctime": "2024-09-02 10:31:36,712", "levelname": "INFO", "message": "Adding strategy ExampleStrategy"}

{"asctime": "2024-09-02 10:31:36,712", "levelname": "DEBUG", "message": "{'uri': 'D:/test/1.232378340', 'mode': 'r', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd':

True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}"}

{"asctime": "2024-09-02 10:31:36,717", "levelname": "INFO", "message": "Creating new HistoricalStream (1000) for strategy ExampleStrategy", "strategy": "ExampleStrategy", "stream_id": 1000, "market_filter": "D:/test/1.232378340", "event_id": "33534412", "event_processing": false}

{"asctime": "2024-09-02 10:31:36,717", "levelname": "INFO", "message": "Starting flumine", "clients": {"Betfair": {}, "Simulated": {"15986142": {"username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x00000191605B7490&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 0, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 1200)&gt;"]}

{"asctime": "2024-09-02 10:31:36,718", "levelname": "INFO", "message": "Client login", "username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x00000191605B7490&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2024-09-02 10:31:36,719", "levelname": "INFO", "message": "Client update account details", "username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x00000191605B7490&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2024-09-02 10:31:36,719", "levelname": "INFO", "message": "Starting historical market 'D:/test/1.232378340'", "market": "D:/test/1.232378340"}

{"asctime": "2024-09-02 10:31:36,719", "levelname": "INFO", "message": "[Register: 1000]: marketSubscription"}

{"asctime": "2024-09-02 10:31:36,720", "levelname": "INFO", "message": "[MarketStream: 1000]: \"MarketStream\" created"}

{"asctime": "2024-09-02 10:31:36,720", "levelname": "DEBUG", "message": "{'uri': 'D:/test/1.232378340', 'mode': 'r', 'buffering': -1, 'encoding': None, 'errors': None, 'newline': None, 'closefd':

True, 'opener': None, 'compression': 'infer_from_extension', 'transport_params': None}"}

{"asctime": "2024-09-02 10:31:36,730", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.232378340 added, 1 markets in cache"}

{"asctime": "2024-09-02 10:31:36,732", "levelname": "INFO", "message": "Adding: 1.232378340 to markets"}

{"asctime": "2024-09-02 10:31:39,061", "levelname": "INFO", "message": "Market 1.232378340 closed", "market_id": "1.232378340", "event_id": "33534412", "event_type_id": "7", "event_name": null, "market_type": "WIN", "market_start_datetime": "2024-08-30 13:30:00", "country_code": "GB", "venue": "Ffos Las", "race_type": "Flat", "orders_cleared": [], "market_cleared": [], "closed": true}

{"asctime": "2024-09-02 10:31:39,062", "levelname": "INFO", "message": "Market cleared", "market_id": "1.232378340", "order_count": 0, "clients": {"Betfair": {}, "Simulated": {"15986142": {"username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x00000191605B7490&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 1200)&gt;"]}

{"asctime": "2024-09-02 10:31:39,063", "levelname": "INFO", "message": "Market level cleared", "market_id": "1.232378340", "profit": 0, "bet_count": 0}

{"asctime": "2024-09-02 10:31:39,063", "levelname": "INFO", "message": "Market closed", "market_id": "1.232378340", "clients": {"Betfair": {}, "Simulated": {"15986142": {"username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x00000191605B7490&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 1200)&gt;"]}

{"asctime": "2024-09-02 10:31:39,064", "levelname": "INFO", "message": "Removing market 1.232378340", "clients": {"Betfair": {}, "Simulated": {"15986142": {"username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x00000191605B7490&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 1200)&gt;"]}

{"asctime": "2024-09-02 10:31:39,065", "levelname": "INFO", "message": "Completed historical market 'D:/test/1.232378340'"}

{"asctime": "2024-09-02 10:31:39,065", "levelname": "INFO", "message": "Shutting down Execution (SimulatedExecution)"}

{"asctime": "2024-09-02 10:31:39,065", "levelname": "INFO", "message": "Shutting down Execution (BetfairExecution)"}

{"asctime": "2024-09-02 10:31:39,066", "levelname": "INFO", "message": "Client logout", "username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total":

0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x00000191605B7490&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2024-09-02 10:31:39,066", "levelname": "INFO", "message": "Exiting flumine", "clients": {"Betfair": {}, "Simulated": {"15986142": {"username": "15986142", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x00000191605B7490&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 1200)&gt;"]}

{"asctime": "2024-09-02 10:31:39,067", "levelname": "INFO", "message": "Simulation complete"}

*Tags: Errors Debugging, Strategies*

---

**tone** - *11:42:20*

I thought you needed client = clients.SimulatedClient() &amp; framework = FlumineSimulation(client=client) to run simulation?

*Tags: General Technical*

---

**tone** - *14:52:28*

OK, I'm back. Bloodied but unbowed! So, after using the correct example, I feel I might finally be getting somewhere.

However I'm now struggling with this error:

    framework.run()

  File "d:\sports_trading\source_code\projects\trading\lib\site-packages\flumine\simulation\simulation.py", line 97, in run

    self._process_market_books(

  File "d:\sports_trading\source_code\projects\trading\lib\site-packages\flumine\simulation\simulation.py", line 124, in _process_market_books

    market = self._add_market(market_id, market_book)

  File "d:\sports_trading\source_code\projects\trading\lib\site-packages\flumine\baseflumine.py", line 216, in _add_market

    middleware.add_market(market)

  File "d:\sports_trading\source_code\projects\trading\lib\site-packages\flumine\markets\middleware.py", line 338, in add_market

    self._next = next(self._gen)

  File "d:\sports_trading\source_code\projects\trading\lib\site-packages\flumine\streams\historicalstream.py", line 239, in _read_loop

    if listener_on_data(update):

  File "d:\sports_trading\source_code\projects\trading\lib\site-packages\flumine\streams\historicalstream.py", line 225, in on_data

    return self.stream._process(data[self.stream._lookup], publish_time)

KeyError: 'rc'



The raw_data parameter in HistoricListener.on_data(..) is set to: '{"op":"mcm","clk":"-8312644392346355926","pt":1725022823949,"mc":[{"id":"[tel:335344121330|33534412.1330](tel:335344121330|33534412.1330)","mid":"1.232378340","rpc":{"ft":1725022823900,"g":"","st":0,"rt":0,"spd":0,"prg":1609.3,"ord":[],"J":[]}}]}\n'

This seems, to me at least, like plausible TPD data except of course that there is no "rc" key!

Do I need to do some sort of transformation of key values?

*Tags: Errors Debugging, Strategies*

---

**liam** - *15:08:37*

How have you recorded this TPD data? I am guessing you changed some things in the marketrecorder as opposed to following the [https://betcode-org.github.io/flumine/sportsdata/#race-recorder|docs](https://betcode-org.github.io/flumine/sportsdata/#race-recorder|docs)

*Tags: General Technical*

---

**tone** - *15:14:54*

This is the code: strategyMR = MarketRecorder(

            name="Market Recorder TPD",

            market_filter=betfairlightweight.filters.streaming_market_filter(

                market_ids=tpd_market_ids,

            ),

            stream_class=RaceDataStream,

            context={

                "local_dir": utils.configs.marketDataPathToday,

                "force_update": True,

                "remove_file": False,

                "remove_gz_file": False,

                "load_market_catalogue": True,

                "recorder_id": "",

            },

        )

        framework.add_strategy(strategyMR)

*Tags: Data Quality, Strategies*

---

## 2024-09-04

**Sam G** - *19:33:20*

Hi [!here](!here) i have been a relatively quiet watcher on this slack workspace - and want to dive in more and learn about the ins and outs of sports betting coding. I have been a python programmer for over 12 years, and some C++ and Java before that ( was in high frequency trading in finance sector). I have a property tech startup that I am winding down (due to business profitablity slowing down) and I want to go back to my love of quant + programming in sports betting, an area that I have slowly picked up some interest in. I also learnt Rust for last 1.5 years, used it on my own startup (which really didnt need Rust but used it to learn) and want to see if I can use Rust knowledge in bet-code



I also helped start a crypto trading firm (so have some relatively newer experience in programming for excahnge trading - around 2018-2020 - was working part time with them)



I learnt a bit of the betfair api - but not much,and want to dive in.



*questions:*

1. What are the best resources to learn production level (or near production level) sports betting. Are there good videos or paid training material that explains a lot of things in one place? 

2. Is someone looking for a partner to do it with - perhaps someone who knows more about the sports betting side of things, and some programming experience - and I can help them with my knowledge in programming ?

Will be good to help others and learn in the process



Thanks again

*Tags: Performance, Deployment, Strategies*

---

**Sam G** - *21:11:25*

Great question. I'm tired of running operations.. It drains me out. Depending on electricians and housekeepers to clean and fix properties ( else I lose money) , ensuring that inventory clerk comes in time. Running an operational business that depends on lots of other human beings is stressful



You might say: why don't you go do algo trading in financial markets. ... It's just not possible for an individual. You need big money at least £10million . Colocation near exchange etc



Hence sports trading. Hope that makes sense. Combines my love for trading and programming

*Tags: Errors Debugging, Strategies*

---

**James Scott** - *23:44:14*

I’ll tell you straight, there is very little money to be made in Betfair api trading these day. If you look at amount of work you would have to put in, and the amount of money you would make, you would be better off stacking shelves at Tesco.

*Tags: Strategies*

---

## 2024-09-05

**Michael** - *08:15:43*

_"there is very little money to be made in Betfair api trading these day........you would be better off stacking shelves at Tesco"_



Speak for yourself. As a general statement this is nonsense.

*Tags: Strategies*

---

**ShaunW** - *12:59:55*

Try it as a sidehussle and if you match your salary for a year or two then maybe think about full time. You'll need to consider other factors such as this not being recognised as income and the affect on your credit status, esp if you want to remortgage your properties.



It's not easy though, knowing how to code won't make you a quant gambler anymore than buying a pencil will make you an architect. The people who make good money have usually been doing it for a decade or more. There's exceptions but they're exceptional people.

*Tags: Errors Debugging*

---

**Rob** - *19:04:15*

I'm just about ready to use flumine for live betting for the first time (rather than simulation).



Based on what I've read here, I know I'm doing more heavy computation that others. I'm happy with the speed of simulation but that's using 10 cores, and I'm not clear if flumine can use >1 core when running live? If a `process_market_book` call is still running when the next update arrives, what happens?



I'll try it tomorrow night so I guess I can also find out that way :slightly_smiling_face:

*Tags: Getting Started, Performance, Deployment, Strategies*

---

**Rob** - *19:39:20*

Obviously what I want to avoid!



I can't see any way that flumine is using >1 core, so if that's the case, and my code is too slow, I  guess I can offload the data processing/model scoring to a locally hosted API which can use all the cores, and have the flumine `process_market_book` code call that.



Anyone else do that, or anything similar?

*Tags: Performance, Strategies*

---

**liam** - *19:41:59*

Yeah this is where a worker can be used to call an api and inject predictions into context etc.



However first question is do you need every update or can you conflate?



Paper trade it first as that will tell you how bad things could be without risking any money. 

*Tags: General Technical*

---

**Rob** - *19:45:55*

Thanks.



My current homemade codebase conflates to 1000ms, but on moving to flumine, I also want to record the data for other uses later, and had assumed that if I use conflate its reflected in the recorded data too?



But I'll certainly start that way - thanks for the tip on paper trading.

*Tags: Strategies*

---

**liam** - *20:01:19*

Yeah that is the problem with conflation, you need to record with it to simulate.

*Tags: General Technical*

---

**Jonjonjon** - *20:57:44*

&gt; If you want an easy life and you're able to make it work betting still a good option

A problem is that most people won't be able to make it work.



The chances of getting very consistent but small returns is probably higher than in many other areas though. I don't think anyone but the top 0.1% is likely get get 7 figures a year from this.



Crypto probably has a higher chance of the big bucks, but it's a lot harder to get started than sports betting.

*Tags: Strategies*

---

## 2024-09-06

**James** - *05:15:07*

Is it possible to set a "fill or kill" condition when placing an order using Flumine?

*Tags: General Technical*

---

**Sam G** - *18:26:46*

Is there a “bible” or “course” (i know the word course is cliched) that I can do to get started, I am not newbie into such systems design - i know what is bid offer, spreads, how matching engine works etc - but want to understand specifics of this market and maybe someone has a course? Or maybe someone does a consultancy for a fee



I like the gym trainer analogy. I can learn from youtube and lot of resources on how to lift weights best, but i will lose say 12 months before I start to figure out what works for me. Having a good gym trainer helps me short circuit and get to a good level faster (for a fee of course), Is there anyone i can approach?

*Tags: General Technical*

---

**Mo** - *18:29:49*

Clone [https://github.com/betcode-org/flumine|flumine](https://github.com/betcode-org/flumine|flumine) and start running the examples

*Tags: General Technical*

---

## 2024-09-09

**D C** - *22:16:06*

I'm trying to make more efficient use of my available streaming connections. My limit is 1000. If I subscribe and at time of subscription is not kicked out because the limit is currently acceptable, will that connection persist even if the number of markets later exceeds that limit? I'm sure this has been asked before but I can't remember the outcome. Currently trying to pull cricket and tennis match odds matkets but it's currently just over the 1000 so need to know if it's worth waiting and trying again intermittently or to split into 2 connections.

*Tags: General Technical*

---

**Lee** - *22:21:00*

The limit is at time of subscription. The problem will be if you get disconnected / reconnecting again during busy times.

*Tags: General Technical*

---

## 2024-09-10

**D C** - *10:17:54*

It's ok - it was the problem in my other thread that was the issue. Market filter with a typo resulted in getting a coarse resolution and loads of unwanted shite

*Tags: General Technical*

---

## 2024-09-12

**D C** - *13:58:11*

Intellectual challenge? Lack of tax? Ease of access (to a bookie anyway)? Lower starting bankroll (as [@UPMUFSGCR](@UPMUFSGCR) points out). No financial fees to cough up on losing bets/trades (I know jack shit about financials but you have to pay fees per trade/stock purchase don't you?).

*Tags: General Technical*

---

**river_shah** - *14:01:46*

I have been living under a rock. Thanks for the kind words [@U4H19D1D2](@U4H19D1D2)



sports betting teaches at an accelerated pace. best proving grounds for advanced modelling techniques.



I was speaking with a friend who tests gene editing on zebra fish. You get answers back in a couple of weeks. It would take years on primates



You get feedback from sports markets in days / weeks. In financials you can hold terribly wrong views for years and hold onto crap ideas without something smacking you back in line (the central bank funded asset bubbles don't help either).

*Tags: Strategies*

---

**Ankit Aggarwal** - *15:54:58*

Hello everyone, I have recently joined the exchange and while researching the sample code and other references I happen to come across Flumine and this community. My sincere appreciation to the creator and contributors of flumine its made life a lot easier compared to the API docs and sample code of the exchange. I am an engineer with coding experience at a software company and trading is my passion/escape. Could anyone help me with the keys for live API access? Any suggestions on how to go about it.

*Tags: Deployment, Strategies*

---

## 2024-09-13

**Justice** - *15:55:14*

How are people structuring their codebases for model development and backtesting? I'm trying to formalize my approach a bit as I have several strategies I'm reasonably happy with but they're all just strewn around in notebooks and it's becoming hard to keep track of the experiments I'm running. Python is not my primary language and I've only ever used it for ML/data, so I'm always a bit lost when it comes to this

*Tags: Strategies*

---

**Paul** - *19:56:02*

I’m crazy in that I will use parameters in file names. So for example if I have a cricket model that trades 3 ticks off the last traded it could live in cricket_inplay_3t.py and so I can quickly create multiple variants with a file cp to a new name. That’s a very simplistic version of what I’m actually doing but broadly right.

*Tags: Deployment, Strategies*

---

**Justice** - *20:06:39*

[@U02RN7YDRQ9](@U02RN7YDRQ9) Interesting approach. I personally can't stand python on Windows so I use WSL. Git is definitely a must, I've learnt that. So many times I've made changes then forget how to revert back to what I had before

*Tags: General Technical*

---

**Justice** - *20:08:05*

[@U012XF5CNPN](@U012XF5CNPN) Is that just your backtesting/strategy logic? Or do you literally have the entire process in a single file including data processing, cleaning, feature creation, model training etc.

*Tags: Feature Engineering, Strategies*

---

**foxwood** - *20:19:25*

[@U05N9773A23](@U05N9773A23) PyCharm runs the scripts for test/live itself in a tool window with interactive debug etc. They do a free community edition that does all of that stuff - can't remember why I upgraded to Pro - think it was just for the sql/db stuff which is not as good imho as ssms/redgate I discovered. Worth a look unless you're a dedicated 'xer - I gave all that up together with the wonders of vi a long long time ago.

*Tags: Errors Debugging, Deployment*

---

**foxwood** - *20:22:21*

[@U012XF5CNPN](@U012XF5CNPN) clever approach to tweaking params with a fairly fixed framework :slightly_smiling_face:

*Tags: Errors Debugging*

---

**liam** - *21:43:52*

Is there an answer to this? 



My notebooks are a complete mess, I come back from lunch and it takes me a good 10 minutes to work out what the hell I was doing. However my simulation / production code is tidy, this is where having flumine run the same code simulation / live is one of my favourite parts. I don’t think Jupiter / analysis ever can be clean/tidy 

*Tags: Deployment*

---

## 2024-09-14

**Justice** - *08:39:04*

[@UUE6E1LA1](@UUE6E1LA1) For production I use C#, which is my preferred language. Python is just objectively better for machine learning and working with data imo

*Tags: Deployment*

---

**Justice** - *08:52:36*

[@UUE6E1LA1](@UUE6E1LA1) To sum up my current workflow: data analysis, visualization, model development, backtesting all done in Python (backtesting done directly in a polars dataframe, not with flumine). All this is just strewn around in various notebooks with no real thought. Export trained model via ONNX to C# codebase for production use

*Tags: Feature Engineering, Deployment, Strategies*

---

**Rob** - *13:14:17*

I'd second mlflow, as much for experiment tracking as for holding models.



As much code as possible is shared between strategies, and I structure the code so the same strategy class is used for live/backtesting.



I use notebooks for analysis only.

*Tags: Deployment, Strategies*

---

## 2024-09-15

**Johnnb** - *09:32:26*

I recently separated my market recorder and strategy code and this has caused the market recorder to start generating enormous logs full of warnings like these :



```{"asctime": "2024-09-14 18:04:50,227", "levelname": "WARNING", "message": "Order 361076840419 not present in blotter", "bet_id": "361076840419", "market_id": "1.232946144", "customer_strategy_ref": "ip-172-26-2-232", "customer_order_ref": "fbcd952b00c57-139456298400765211", "client_username": "xxxxxxx"}

{"asctime": "2024-09-14 18:04:50,227", "levelname": "WARNING", "message": "Strategy not available to create order 139456298400765211", "bet_id": "361076840419", "market_id": "1.232946144", "customer_strategy_ref": "ip-172-26-2-232", "customer_order_ref": "fbcd952b00c57-139456298400765211", "strategy_name_hash": "fbcd952b00c57"}```

Can I safely suppress these with a logging filter or do they indicate that I'm doing something wrong? All the recorder's connections are specified as DataStream which I thought meant that they wouldn't receive any order info?



```racing_recorder = S3MarketRecorder(

    name="RACING_RECORDER",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB","IE","FR"],

        market_types=["WIN"],

    ),

    stream_class=DataStream,

    context={

        "local_dir": local_dir,

        "force_update": False,

        "remove_file": True,

        "remove_gz_file": True,

        "recorder_id": recorder_id,

        "bucket":"xxxxxx",

    },

)```

*Tags: Data Quality, Strategies*

---

**foxwood** - *09:44:10*

That looks ok - must have left some strategy stuff in - the example works out of the box - look at that [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py)

*Tags: Strategies*

---

**liam** - *18:29:29*

Who knows with windows but flumine uses the hostname to separate orders (strategies) so you will get this when running multiple flumine instances on the same host 

*Tags: General Technical*

---

## 2024-09-16

**Jemima Cecil** - *15:54:56*

Anyone have any issues with the Betfair API today?

*Tags: General Technical*

---

## 2024-09-17

**Shahid Tariq** - *06:21:05*

Hi Team,

We have integrated Betfair market streaming and noticed that, at times, the streaming stops sending updates even though the connection remains active. Has anyone else encountered a similar issue?

*Tags: General Technical*

---

**Shahid Tariq** - *07:42:02*

I am using betfairlightweight library for streaming, here is my subscription code:

```await stream.subscribe_to_markets(

            market_filter=streaming_market_filter(

                event_type_ids=[1, 2, 4, 7, 4339], betting_types=["ODDS"]

            ),

            market_data_filter=streaming_market_data_filter(

                fields=["EX_MARKET_DEF", "EX_BEST_OFFERS_DISP", "EX_TRADED_VOL"],

                ladder_levels=3,

            ),

            conflate_ms=BetfairStreamConstants.CONFLATE_MS,

            initial_clk=listener.initial_clk,

            clk=listener.clk,

        )```

*Tags: Strategies*

---

**liam** - *07:51:29*

Any warning logs or errors? 

*Tags: Errors Debugging*

---

**Shahid Tariq** - *08:23:38*

Actually, I have the whole thing wrapped in a while True loop. If any case my streaming gets stopped while loop will iterate again and create a new connection.

*Tags: General Technical*

---

**D C** - *14:40:57*

Hence the question. Say I bet on 10% rate region markets - does that full extra 8% commission on top of my base go towards my PC offsetting or does it get swallowed up somewhere else and I only get offset by the 2%.

*Tags: General Technical*

---

## 2024-09-21

**TT** - *13:04:23*

How do people typically go about logging/monitoring latency? In particular the latency between betfair's publish time and the clock time when the update is received?



I can think of a couple of ways of doing it:

1. In the market recorder you could log it on each update [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py#L69|here](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py#L69|here). (Might be a bit overkill for each update but it would be complete.)

2. Have a background worker log the latency metric to Cloudwatch Metrics/Prometheus etc every x seconds.

Its not something I've been recording but if you want to optimise it (by moving aws region/vps etc) then its important to start tracking it.

*Tags: Data Quality, Performance, Deployment*

---

**tone** - *16:12:45*

If you take this approach, then it may be necessary to remove the new timestamp before using bflw to parse the files

*Tags: General Technical*

---

**TT** - *18:17:39*

Yeah I did think about that - from what I can tell I think it will get ignored by bflw/flumine backtests so would need to patch it to make use of it.



But even outside of those tools it might just be worth logging to keep an eye on it over time.

*Tags: General Technical*

---

## 2024-09-24

**Qais Qadri** - *11:17:47*

Hello team. got a question.

Working on betfair data for horse racing, we see in runners metadata, COLOURS_FILENAME field which is the race icon for each runner. I was wondering what is the right way of using this image, serve from betfair directly (eg `[http://content.betfair.com/feeds_images/Horses/SilkColours/c20240923-Canberra|content.betfair.com/feeds_images/Horses/SilkColours/c20240923-Canberra](http://content.betfair.com/feeds_images/Horses/SilkColours/c20240923-Canberra|content.betfair.com/feeds_images/Horses/SilkColours/c20240923-Canberra) (AUS)-H/1686208.png`) or fetch the image and serve ourselves? pls suggest here.

*Tags: General Technical*

---

## 2024-09-25

**AndyL** - *20:54:57*

Hi, I am trying to setup a new AWS Ubuntu 24.04 VM, and have all the python bits and pieces setup, and my certs folder, but am getting the following SSL error. I'm sure i've had this before, but can't remember how I resolved it, any ideas anyone? I'm thinking maybe an OpenSSL problem?

```HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by SSLError(SSLError(524297, '[SSL] PEM lib (_ssl.c:3895)'```

*Tags: Getting Started, Errors Debugging, Deployment*

---

**AndyL** - *21:04:14*

probably AWS VM os difference...

maybe best to run in docker and avoid all this mess!

*Tags: Deployment*

---

**D C** - *23:17:08*

How old are your certs? I remember having to do an openssl hack ages ago because my certs were generated with an older encryption and I had to lower the security setting in the openssl configs (or something like that). I ended up generating newer certs in the end and the problem went away. I think the cause was something to do with a newer version of openssl.

Might not be related to your issue but thought I'd mention anyway as I've had SSL cert issues on fresh installs before.

*Tags: Getting Started*

---

## 2024-10-02

**casper** - *15:24:54*

[@U4H19D1D2](@U4H19D1D2) are the cricket feed issues/errors purely because of issues on Betfair side or issues from Decimal also?

*Tags: Errors Debugging*

---

## 2024-10-03

**liam** - *08:39:08*

bflw and flumine now fixed

*Tags: Errors Debugging*

---

**liam** - *13:22:50*

Its the same, [https://github.com/betcode-org/flumine/blob/cd5b228a412dc6e55d55051ca2be98ce5a0152b5/flumine/markets/middleware.py#L86|flumine code](https://github.com/betcode-org/flumine/blob/cd5b228a412dc6e55d55051ca2be98ce5a0152b5/flumine/markets/middleware.py#L86|flumine code)

*Tags: General Technical*

---

**Newbie99** - *13:33:01*

I get this post upgrade:



```  File "/home/ec2-user/trading/env/projects/startup.py", line 208, in startup

    framework.run()

  File "/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/flumine.py", line 27, in run

    with self:

  File "/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/baseflumine.py", line 450, in __enter__

    self.strategies.start(self)

  File "/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/strategy/strategy.py", line 279, in start

    s.start(flumine)

TypeError: start() takes 1 positional argument but 2 were given```



*Tags: Errors Debugging, Deployment, Strategies*

---

**liam** - *13:37:50*

Umm all I can do is repeat what I have said



&gt; Its the same, [https://github.com/betcode-org/flumine/blob/cd5b228a412dc6e55d55051ca2be98ce5a0152b5/flumine/markets/middleware.py#L86|flumine code](https://github.com/betcode-org/flumine/blob/cd5b228a412dc6e55d55051ca2be98ce5a0152b5/flumine/markets/middleware.py#L86|flumine code)

*Tags: General Technical*

---

**liam** - *13:40:18*

Yeah that was changed last month, it had a depreciation warning for about 6 months



[https://github.com/betcode-org/flumine/blob/cd5b228a412dc6e55d55051ca2be98ce5a0152b5/flumine/strategy/strategy.py#L95](https://github.com/betcode-org/flumine/blob/cd5b228a412dc6e55d55051ca2be98ce5a0152b5/flumine/strategy/strategy.py#L95)

*Tags: Strategies*

---

**Newbie99** - *15:14:00*

Sorry [@U4H19D1D2](@U4H19D1D2), I'm not really understanding the error, the affected line is:



```framework.run()```

What should the syntax be, maybe I'm misunderstanding the error, but I thought this section relates to the Flumine package rather than my code, so I'm a bit confused:



```  File "/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/strategy/strategy.py", line 279, in start

    s.start(flumine)

TypeError: start() takes 1 positional argument but 2 were given ```

*Tags: Errors Debugging, Deployment, Strategies*

---

**liam** - *15:14:32*

its the bit you aren't showing me, what does your strategy `start` look like?

*Tags: Strategies*

---

**Newbie99** - *15:16:45*

```'''     def start(self):

        # subscribe to streams

        print("starting strategy ", self.name) '''```

*Tags: Strategies*

---

**Newbie99** - *15:17:05*

(I commented it out after seeing your message thinking that would work, but the error persisted)

*Tags: Errors Debugging*

---

**liam** - *15:17:26*

yeah so you need to update to `def start(self, flumine)`

*Tags: General Technical*

---

**Newbie99** - *15:18:06*

For my understanding, if its commented out though, why would it not just use start from the BaseStrategy?

*Tags: Strategies*

---

**liam** - *15:19:46*

how confident are you there isn't another strategy causing an issue?

*Tags: Strategies*

---

**D C** - *21:03:06*

I've not looked at it in depth. Went 4 races with no bets placed. Logs said bet placement blocked because of "GPS errors" but this covers a variety of sins. I'll have to dig deeper for specifics. Restarted once - no solution. Restarted again - all good.

*Tags: Errors Debugging*

---

## 2024-10-04

**Lee** - *19:24:26*

[@UNW8Q88EL](@UNW8Q88EL) I have a connection ID if that helps investigate

*Tags: General Technical*

---

**D C** - *19:46:42*

Yeah I just checked my own. I do similar, it's just for a "dead" connection after startup until a few mins before first race (at least that's the fix that I put in my stuff probably not optimal but was sufficient).

*Tags: Errors Debugging*

---

## 2024-10-05

**liam** - *07:34:37*

I was going to just kill every x seconds if no live data coming through, might be able to create something this afternoon 

*Tags: Deployment*

---

**liam** - *08:16:47*

I am going to modify the patch to just restart the stream after 5 minutes on a heartbeat, ie no data coming through the stream, it used to check the cache (`len(self._caches) == 0`) but obviously something else is going on, I haven't heard from [@UNW8Q88EL](@UNW8Q88EL)



```def _on_heartbeat(self, data: dict) -&gt; None:

    self._update_clk(data)



    # reset stream if no data and started over 5 minutes ago

    seconds_since_start = (

        datetime.datetime.utcnow() - self.time_created

    ).total_seconds()

    if seconds_since_start &gt; 300:

        markets = len(self._caches)

        logger.warning(

            "RaceStream no data, restarting",

            extra={"markets": markets, "seconds_since_start": seconds_since_start},

        )

        raise exceptions.ListenerError("", "")```

*Tags: Errors Debugging*

---

## 2024-10-07

**liam** - *10:49:39*

Neil is on holiday till the 10th, I have messaged bdp so will keep this thread updated if they reply, my money is that they have released something for the protocol fix.



However the bflw patch seems to be working for me

*Tags: Errors Debugging*

---

**liam** - *14:20:04*

How do you have this setup? ie. are you using tenacity or another way to restart on error?

*Tags: Getting Started, Errors Debugging*

---

**Michael** - *14:29:28*

I don't do anything to handle the socket timing out, does that raise an exception?

*Tags: Errors Debugging*

---

**Michael** - *14:32:23*

I have no idea then. All I can say is that I've had everything running this way since that last bunch of problems around June 2023 and I don't think it's failed even once until now.

*Tags: Errors Debugging*

---

**liam** - *14:34:12*

hmm without seeing some debug logs its very tricky to debug

*Tags: Errors Debugging*

---

**liam** - *14:41:26*

ok but based on what you have told me it would be theoretically impossible for the stream not to raise an error if the latest patch was used

*Tags: Errors Debugging*

---

**liam** - *16:01:35*

&gt; Hi Liam,

&gt; I have been informed that a fix was released this morning.

&gt; Kind Regards,

&gt; Karl

*Tags: Errors Debugging*

---

**Brøndby IF** - *20:45:53*

I didn't even put this page online, it only works together with my Python locally, keeping the graphs according to a CSV file that I have registered the investments that are currently placed on the market.

*Tags: General Technical*

---

**D C** - *21:04:55*

I don't use Python myself much but is there really no charting package available? What about ggplot or matplotlib? Is it important to emulate the charts precisely in the way Betfair do?

*Tags: General Technical*

---

**Brøndby IF** - *21:08:18*

[@UUE6E1LA1](@UUE6E1LA1) Yes, there are several packages to help create graphs, but when collecting the values ​​and trying to reproduce the value loads in them, my graphs look very different. It's just a lack of knowledge to be able to combine the API values ​​and generate the graphs. I intend to try again in the future, *including sending the finished code here so that in the future whoever needs it already has it*, but at the moment I'm stuck due to a lack of knowledge.

*Tags: General Technical*

---

**Brøndby IF** - *21:19:28*

Hi [@U4H19D1D2](@U4H19D1D2) I didn't know about the existence of the graph api, I tried to recreate the charts using the runner's `totalMatched`. I'm searching here in the documentation about the graph api, but I'm not finding it. Could you help me? (I'm sorry if I might be confusing things)

*Tags: General Technical*

---

**Brøndby IF** - *21:23:09*

Oh perfect [@U4H19D1D2](@U4H19D1D2), thank you very much for the information, it's more or less how we found the information regarding the stream of each event that we talked about a few months ago with the intention of adding such information to betfairlightweight!

*Tags: General Technical*

---

## 2024-10-08

**Brøndby IF** - *20:55:22*

Hey guys, unfortunately the method I was using, even though I was passing the correct headers for my API access, has now started to generate a Cloudflare block that returns a direct error or an HTML page with the message "Please wait"



So it's possible that the Graph API doesn't allow collecting a request every 1 second when we want to request from multiple markets.



I don't think I will continue creating this code. I don't want any risk of having my account blocked.

*Tags: Errors Debugging*

---

## 2024-10-10

**Brøndby IF** - *12:52:24*

Hi [@U4H19D1D2](@U4H19D1D2) I don't know if I really pushed it too far, because the test I did was with 6 simultaneous charts, but I'm analyzing whether it could be the fault of some kind of escape, since there is a script trigger for each of them, with the intention of being asynchronous. I managed to solve the memory accumulation, by the way.



I'm also thinking about generating the requests with Python and sending the JSON data to be processed by HTML instead of using JS for this, perhaps it will provide more security and stability.



But if I do notice that there is such a limitation, I will use the graph api to learn and reproduce it using the official API.

*Tags: Performance*

---

**liam** - *14:44:09*

I think we have some issues to fix for 3.12 first :sweat_smile:

*Tags: Errors Debugging*

---

## 2024-10-11

**Tom** - *06:34:28*

[@UBS7QANF3](@UBS7QANF3) I've had this question for a while and tried to understand the math a bit better to do it myself but am failing miserably - do you know what it would take to get a shins type method to increase overround proportionately (rather than normalise to 1, set it from 1.05 to say 1.10)

*Tags: General Technical*

---

**Jorge** - *13:00:08*

Hi guys! I'm using the in_play_service to determine when the half-time starts and ends for Soccer games. The event_status can be "KickOff", "FirstHalfEnd", "SecondHalfKickOff". For most of the cases it seems to be working well. But in many cases it stays in `Event minute=45. Event status=KickOff`  for too long even when the half-time started, and then jumps directly to "SecondHalfKickOff". Anyone is succesfully using the in_play_service for something similar? Or what are other alternatives?



```bflw_client = betfairlightweight.APIClient('a', 'a', app_key='a')

while True:

    timeline = bflw_client.in_play_service.get_event_timeline(event_id=bot.market.event_id)

    minute = timeline.elapsed_regular_time

    status = timeline.in_play_match_status

    log_info(f'Event minute={minute}. Event status={status}')```

*Tags: General Technical*

---

**Jorge** - *14:37:10*

I got a model that bets in the half-time (I want to avoid betting when the ball is running) and want to be able to bet as soon as possible

*Tags: Strategies*

---

## 2024-10-14

**lorenzo** - *06:42:18*

you might be looking for [https://github.com/betcode-org/betfair/blob/d21e277755de34d13ed10715c5eb6eee532158d7/betfairlightweight/resources/bettingresources.py#L384|RunnerBookEx](https://github.com/betcode-org/betfair/blob/d21e277755de34d13ed10715c5eb6eee532158d7/betfairlightweight/resources/bettingresources.py#L384|RunnerBookEx) instead of the SP ? As far as I understand the SP has only the MarketOrders (related to the Starting Price)

*Tags: Strategies*

---

**Phydeaux** - *08:50:28*

Free historical data on Betfair will have the full market name I think. Failing that, Timeform API?

*Tags: Data Quality*

---

**Gabriel Mocan** - *15:52:18*

[@U05C31YKZ1C](@U05C31YKZ1C) the free historical data will have this information.

*Tags: Data Quality*

---

## 2024-10-16

**Unknown** - *08:02:45*

Hi Team

For Races, why isnt the Lay bets getting placed via Betfair API and even API tool, getting PERMISSION_DENIED error code. (Attaching screenshot)



The documentation explains the error: `Business rules do not allow order to be placed. You are either attempting to place the order using a Delayed Application Key or from a restricted jurisdiction (i.e. USA)`  and from [https://support.developer.betfair.com/hc/en-us/articles/360015530311-Why-am-I-receiving-the-error-PERMISSION-DENIED-ACCESS-DENIED-when-attempting-to-place-a-bet|https://support.developer.betfair.com/hc/en-us/articles/360015530311-Why-am-I-receiving-the[…]ON-DENIED-ACCESS-DENIED-when-attempting-to-place-a-bet](https://support.developer.betfair.com/hc/en-us/articles/360015530311-Why-am-I-receiving-the-error-PERMISSION-DENIED-ACCESS-DENIED-when-attempting-to-place-a-bet|https://support.developer.betfair.com/hc/en-us/articles/360015530311-Why-am-I-receiving-the[…]ON-DENIED-ACCESS-DENIED-when-attempting-to-place-a-bet) restricted Jurisdiction. But neither is the case. I am connected from London.



What Am I missing here?

*Tags: Errors Debugging*

---

## 2024-10-18

**Jonas JN** - *15:14:41*

Hi guys, I'm using betfairlightweight to stream historical data, do you know if it's possible to get the event score for each odd movement?



I would really like to obtain this information along with the movement of the odds, I am trying to link the data with another API to obtain statistics and other information. Could you also tell me if there is a fixed id for teams, besides the selectionId?



Thanks!

*Tags: Data Quality, Errors Debugging*

---

**Lee** - *15:21:39*

This might help you for the mappings [https://github.com/mberk/betfairmappings|https://github.com/mberk/betfairmappings](https://github.com/mberk/betfairmappings|https://github.com/mberk/betfairmappings)

*Tags: General Technical*

---

**Jonas JN** - *19:13:23*

Thanks, I took a look but I don't think it helps much.



We are trying to create an environment to simulate entries in past games, as if it were a player that you can fast forward and rewind the time of the match, so we would need to add more information in addition to the odds, the most essential thing would be the score at that moment in which the user is .

*Tags: General Technical*

---

**Jonas JN** - *19:20:11*

We started collecting data at a frequency of minute per minute to obtain the movement of statistics and odds, we found an unofficial API called BetsAPI, they have Betfair events mapped with their events, so it is possible to obtain statistics from their events and join betfair odds easily. However, it is an unofficial API and the data is often messed up.



So we are looking for a more consistent solution, and for the past data that we have not collected, we are trying to use Betfair's historical data, but we would need more than just the odds...

*Tags: Data Quality*

---

## 2024-10-19

**John** - *14:28:57*

What is the correct way to call the listMarketCatalogue method with a known market_id   My code is:

```# Create the request parameters

listMarketCatalogue_req = {

    "filter": {

        "market_ids": [event_id],

        "marketProjection": ["MARKET_START_TIME", "RUNNER_METADATA", "RUNNER_DESCRIPTION", "EVENT_TYPE", "EVENT"]

    },

    "id": 1

}



# Call the listMarketCatalogue method

response = client.betting.list_market_catalogue(listMarketCatalogue_req)```

where client is betfairlightweight.APIClient(username, password, app_key)

*Tags: Strategies*

---

**John** - *14:40:55*

many thanks. and handling the response in python. I have:  data = json.loads(response.text), but it throws an error

*Tags: Errors Debugging*

---

**Mo** - *14:41:36*

Eh? This is `betfairlightweight` not `requests`

*Tags: General Technical*

---

**John** - *14:47:24*

This the error. Not sure how to fix: AttributeError: 'list' object has no attribute 'text'

*Tags: Errors Debugging*

---

**John** - *15:10:10*

Thanks for your help. Is there a link to a full guide on using BetfairLightweight? I've seen a basic version.

*Tags: General Technical*

---

**Mo** - *15:22:05*

1. Look at the [https://github.com/betcode-org/betfair/tree/master/|source code](https://github.com/betcode-org/betfair/tree/master/|source code), especially [https://github.com/betcode-org/betfair/tree/master/examples|the examples](https://github.com/betcode-org/betfair/tree/master/examples|the examples) and [https://github.com/betcode-org/betfair/blob/master/betfairlightweight/resources/bettingresources.py|the resources](https://github.com/betcode-org/betfair/blob/master/betfairlightweight/resources/bettingresources.py|the resources) 

2. Familiarise yourself with the [https://betfair-developer-docs.atlassian.net/wiki/spaces/1smk3cen4v3lu3yomq5qye0ni/pages/2687473/Reference+Guide|Betfair API documentation](https://betfair-developer-docs.atlassian.net/wiki/spaces/1smk3cen4v3lu3yomq5qye0ni/pages/2687473/Reference+Guide|Betfair API documentation)

3. Install and use a decent IDE like [https://www.jetbrains.com/pycharm/|PyCharm](https://www.jetbrains.com/pycharm/|PyCharm) that will allow you to debug which will let you step through code line by line and examine the Python objects involved

*Tags: Getting Started, Errors Debugging, Strategies*

---

## 2024-10-20

**Jhonny** - *15:00:54*

Hi all, I'm new here, so pardon me if this is a noob question.

Is there a way to get a live market ndjson stream without having to to pay 300gbp for the api?

I need to test my strat before making such a big commitment. Thanks



side note: wow, I didn't know there was this big of a bf coders community; sadly, I've already needlessly reimplemented some stuff in a different prog lang.

*Tags: Deployment*

---

**Michael** - *15:13:13*

Welcome to the Slack. Yes the fee can usually be waived, [@U4H19D1D2](@U4H19D1D2) will probably help you out whenever he logs back in.

*Tags: General Technical*

---

## 2024-10-22

**liam** - *12:57:59*

Yeah it's assignment error that I am interested in however I don't see how often it would be triggered as I believe the operators can actually reassign during loading

*Tags: Errors Debugging*

---

**D C** - *12:59:57*

If it can be reassigned, then presumably that would have to be present until it is resolved (within every RPC). How is this coded? Is it a bitmask because presumably these are not mutually exclusive errors

*Tags: Errors Debugging*

---

**Ralegh** - *17:08:13*

I have the same problem, on the plus side taking a bet at BSP is cheap in EV terms so is just manifesting as higher variance 

*Tags: General Technical*

---

## 2024-10-23

**liam** - *10:20:15*

`pip install flumine`

*Tags: Getting Started*

---

## 2024-10-24

**AndyL** - *20:06:47*

[@U01MPC0GUK1](@U01MPC0GUK1) thanks, you're spot on this works:

```market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["1"],

        country_codes=["GB", None],

        market_types=["MATCH_ODDS"],

    )```

*Tags: General Technical*

---

## 2024-10-26

**Trex44** - *16:33:35*

Any football traders out there have a good recommendation for a live and historical data provider? Preferably one with UTC time stamps for events?

*Tags: Data Quality, Deployment*

---

## 2024-10-27

**Jhonny** - *12:39:04*

Hi everyone,

How long does it take to get approval for the live API key?

I bought historical data for this month, hoping to use it in conjunction with the api (since it's not allowed to simply listen to markets without betting); However, I haven't gotten any update about my live api key; Not even an acknowledgement of my request. The month is as good as gone

*Tags: Data Quality, Deployment, Strategies*

---

## 2024-10-28

**James** - *10:39:50*

Thanks [@U07AK2APF2B](@U07AK2APF2B) I never realised there was a difference between the live data and the betfair historical data. Thanks.

*Tags: Data Quality, Deployment*

---

**D C** - *14:30:46*

When talking about BSP as being a good benchmark to use when calculating your expective EV, do people who accept this as reality consider it to be the case across all markets or just horse win markets? Same question again - for all countries or just UK and IRE ?

*Tags: General Technical*

---

**D C** - *14:35:47*

I did mean BSP specifically. I suppose the same might apply to bookies closing lines but you've got the margin removal to consider then but this was not really the focus of my question. Specifically what motivated the question is the money traded - football matches for a lot more money than horses, which in turn match for a lot more than dog races. But are BSP considered the benchmark regardless of volumes traded?

*Tags: General Technical*

---

## 2024-10-30

**AndyL** - *20:59:12*

[@U07G3MG1X6U](@U07G3MG1X6U) put it on your xmas list :-)

It’s fun but hard this game but Flumine takes a huge amount of the hastle out, so just get going and enjoy it but don’t expect to make too much



*Tags: General Technical*

---

**ian mcneill** - *23:24:30*

Hi all, is there a consensus on the infrastructure to use, for development and then production - DigitalOcean, AWS, Lightsail etc.?

*Tags: Deployment*

---

## 2024-10-31

**liam** - *07:33:34*

The majority use AWS however I would use what ever you have experience in

*Tags: Deployment*

---

**James** - *07:50:50*

AWS Ireland tends to be best for latency from what I’ve seen. 

*Tags: Performance, Deployment*

---

**liam** - *09:07:51*

FYI flumine users / [@UUCD6P13J](@UUCD6P13J) there is a bug with this patch when used on long running instances. The issue is the way flumine uses `stream_id` to differentiate between different streams, this starts at 1000 and +=1000 for every new stream, this becomes an issue when you have a stream which has stopped/started more than 1000 times as it starts to conflict with the next stream.



Adding a digit in flumine would be an easy fix but doesn't really solve the issue, `stream_id` is used throughout the framework for subscribing to strategies/closing markets etc so it can't easily be changed to a uuid :thinking_face:

*Tags: Errors Debugging*

---

**Paul** - *10:12:30*

If using AWS, you're still left with choices. I've been thinking about writing up some stuff on this, but it's complicated because I work for AWS, and if I discuss or write anything in this space I have to a) make it clear I'm doing so in a personal capacity, not as an employee and b) still get it checked by PR, Legal, and "bar raised" by a bunch of solution architects.



Putting all that aside, except please, accept this is a personal recommendation, not an official one: Fargate, DynamoDB and Aurora all allow you to forget about a lot of BS you probably don't want to spend time thinking about. Archiving to S3 is :+1:, but querying it with Athena for most people is :money_with_wings: compared to getting it into a DB. Consider Compute Savings Plans and Reserved Instances to bring down costs significantly. Also, if you want to ignore that and go simple, it's OK to throw up EC2 instances for a while and work it out or even look at Lightsail and App Runner if you want to, and then optimise later.

*Tags: Deployment*

---

**James** - *10:28:25*

Following on from Paul’s recommendation, I work with AWS in my day job extensively but don’t have the same limitations he does. Lightsail and Fargate serverless containers, and s3 for storage. Along with using a neon Postgres or digital ocean Postgres (for no cold start) is what I use for my market catalogues and betting records. Most of your record keeping can happen in background threads and can afford to be a little slower. 



Lightsail and neon are a bit more batteries included and a good place to start, and less knobs go turn. Fargate and a dedicated DB can let you squeeze some more out as you get deeper into it. 



This setup means my costs are low. I run my sims on my own hardware, even if slow it doesn’t cost me anything except electricity :relaxed: 

*Tags: Getting Started, Performance, Deployment, Strategies*

---

**Jhonny** - *15:54:00*

Hi everyone, I keep getting NOT_AUTHORIZED error when trying to use the esa api. I haven't used it in about 2 months; my live key just got accepted and it's my first time trying it out:



```:event {:op connection, :connectionId 206-311024154911-133459}

:event {:op status, :statusCode FAILURE, :errorCode NOT_AUTHORIZED, :errorMessage Connection is not authenticated: MarketSubscriptionMessage{marketFilter=MarketFilter{marketIds=[1.235241223], bspMarket=null, bettingTypes=null, eventTypeIds=null, eventIds=null, turnInPlayEnabled=null, marketTypes=null, venues=null, countryCodes=null, raceTypes=null}, marketDataFilter=com.betfair.platform.exchange.stream.api.domain.market.MarketDataFilter@70734c3a, initialClk='null', clk='null', conflateMs=null, heartbeatMs=null}, :connectionClosed true, :connectionId 206-311024154911-133459}

:stream-closed```

Last I tested with the delayed key, it worked. However, both live and delay keys are currently not working. Would appreciate anyone's help, thanks

*Tags: Getting Started, Errors Debugging, Deployment, Strategies*

---

## 2024-11-01

**Phydeaux** - *03:32:12*

I have a "home lab" for other reasons, with NAS and a decent amount of compute so I've not deployed to the cloud. I am rural so have a good amount of solar and batteries, so even the electricity is free. The capex was relatively chunky, but built for other reasons, so the marginal cost to use it for betting / analysis was basically zero apart from my time.



Network latency isn't a concern for my strategy, but if it was I'd do a little testing to find which GCP/AWS/other zone is "closest".

*Tags: Performance, Deployment, Strategies*

---

**liam** - *13:15:00*

subscribe to them all and forget about it



[https://betcode-org.github.io/betfair/streaming/](https://betcode-org.github.io/betfair/streaming/)

*Tags: General Technical*

---

**liam** - *21:33:03*

No idea, I imagine internal services that handle the feed. Latest update is some comms on Monday regarding releasing the UDP fix however if you ask me I think they need to work out the cause of the current deterioration

*Tags: Errors Debugging*

---

**Unknown** - *21:36:56*

OK thanks. I just assumed these were acronyms of some server side tech I'd never heard of. So you heard they'd released a UDP fix Monday? Doesn't look like it's helped much. My logs are shocking from yesterday (should add that these times include a dropout that contains the inplay suspension event).

*Tags: Errors Debugging, Deployment*

---

**liam** - *21:38:49*

No



&gt; We'll have an update on Monday regarding the timing of the UDP fix that the team has previously been working on.

*Tags: Errors Debugging*

---

## 2024-11-02

**Daniel** - *10:49:08*

Hi all, tyvm for the work that's been done on the Python API layer



I was using it and I was going to make some modifications for myself, just wanting to guage whether these might be wanted upstream?

*Tags: General Technical*

---

**Daniel** - *10:50:37*

The other was to add the relevant enums from the Betting API into a Pythonic representation, i.e.



[https://github.com/nunnsy/betfair/pull/2/files](https://github.com/nunnsy/betfair/pull/2/files)



[https://betfair-developer-docs.atlassian.net/wiki/spaces/1smk3cen4v3lu3yomq5qye0ni/pages/2687455/Betting+Enums](https://betfair-developer-docs.atlassian.net/wiki/spaces/1smk3cen4v3lu3yomq5qye0ni/pages/2687455/Betting+Enums)

*Tags: Strategies*

---

**liam** - *12:44:25*

Yeah same here, I don’t think the other providers go through betfair, I want to get on a call with these devs as I just don’t believe they have a handle on the problem 

*Tags: General Technical*

---

## 2024-11-04

**liam** - *10:07:34*

Np with the first, I have speed/latency concerns with the second, Enums are slow, would need to test/profile 

*Tags: Performance*

---

**Daniel** - *14:18:28*

Ah fair - creation of their objects upon reading the serialised data could be a decent impact on latency

*Tags: Performance*

---

**Daniel** - *14:26:32*

Not sure how pedantic you are with docstrings and associated optional types, as I haven't touched those



[https://github.com/betcode-org/betfair/pull/589](https://github.com/betcode-org/betfair/pull/589)

*Tags: General Technical*

---

## 2024-11-13

**D C** - *15:20:19*

Strangely enough if all you want is sectional timings, I've just received an email in my inbox from proform (I'm a previous subscriber) about some deal with TPD (and others) to get the sectional timings as part of the profotm database. Seems like they are looking for an initial group of subs who are willing to pay for a year upfront (about £600). I realise this is totally different to a full GPS history, but thought I'd mention it anyway.

*Tags: General Technical*

---

## 2024-11-15

**Michael** - *14:05:47*

Well we were having tonnes of problems recently and now we're not. Or I'm not anyway.

*Tags: General Technical*

---

**D C** - *16:30:51*

Yesterday I was clearly connected to a different source/server for my bots. One was almost flawless and the other very bitty (but still better than the recent problems - worst dropout was 45 seconds I think). It still mystifies me why but I'll take it over the car crash of last week

*Tags: Deployment*

---

## 2024-11-16

**Johnny Boston** - *20:55:53*

[https://www.britishhorseracing.com/racing/results/fixture-results/result/#!/2024/1763/37019/0/|https://www.britishhorseracing.com/racing/results/fixture-results/result/#!/2024/1763/37019/0/](https://www.britishhorseracing.com/racing/results/fixture-results/result/#!/2024/1763/37019/0/|https://www.britishhorseracing.com/racing/results/fixture-results/result/#!/2024/1763/37019/0/)

*Tags: Errors Debugging*

---

## 2024-11-17

**Nita Suos** - *10:40:25*

Hi, can someone please explain this with flumine?  With the examples, I see the strategies are added, then the framework is run.  Can the framework be run in it's own thread, and then strategies added to it, and if so, could someone please offer a sample code?  Right now I initialize like so:

```trading = betfairlightweight.APIClient(my_username, my_password, app_key=my_app_key, certs=certs_path)

client = clients.BetfairClient(trading, interactive_login=True)

framework = Flumine(client=client)

framework_thread = threading.Thread(target=run_framework)

framework_thread.start()```

The framework is now running in it's own thread, and I would like to run a strategy to print the X next upcoming horse races.  Can this be done or is it better to use betfairlightweight?

*Tags: Strategies*

---

**Mo** - *10:42:43*

If literally all you want to do is print the upcoming horse races then use `betfairlightweight`. If that's just a first step to developing a full blown strategy then ultimately that strategy will need to be implemented in `flumine` rather than `betfairlightweight`

*Tags: Strategies*

---

**Nita Suos** - *10:47:25*

Thanks Mo.  With adding strategies to Flumine (imagine having a number of different ones being selectable within a gui), would the framework be run, and add strategies/remove strategies from the framework?

*Tags: General Technical*

---

**Mo** - *10:52:26*

You would need to add them all at the start before calling `run`. If you want to dynamically enable or disable them then you could always control this via boolean flags in the strategy that are checked in, for example, `check_market_book`

*Tags: Strategies*

---

**Nita Suos** - *23:28:40*

Can I please clarify this: so if I add a strategy or strategies to the framework and run the framework, at this point it is not possible to add any more strategies, workers etc and for those strategies added they can't be removed unless I shutdown and restart the application, use flags, or is there a solution to this that others use. Thanks.

*Tags: Strategies*

---

## 2024-11-18

**Nita Suos** - *06:15:00*

I am using betfairlightweight streaming for events using market_id and event_id, Can someone tell me how to retrieve the LTP and Volumes for each runner through the stream?  They are always None with the below code.  thanks

```def stream_data():

    global trading



    # Create queue and listener

    output_queue = queue.Queue()

    listener = betfairlightweight.StreamListener(output_queue=output_queue)

    stream = trading.streaming.create_stream(listener=listener)

    current_event_ids = []

    current_market_ids = []

    market_id_to_name = {} 

    runner_name_cache = {}  



    while True:

        event_ids = gd.Betfair.focusEventId

        market_ids = gd.Betfair.focusMarketId



        # Skip processing if IDs are invalid or None

        if not event_ids or not market_ids:

            continue



        if not isinstance(event_ids, list):

            event_ids = [event_ids]

        if not isinstance(market_ids, list):

            market_ids = [market_ids]

        if set(event_ids) != set(current_event_ids) or set(market_ids) != set(current_market_ids):

            current_event_ids = event_ids

            current_market_ids = market_ids

            stream.stop()

            market_catalogue = trading.betting.list_market_catalogue(

                filter=filters.market_filter(

                    event_ids=[gd.Betfair.focusEventId],

                    market_ids=[gd.Betfair.focusMarketId]

                ),

                market_projection=[

                    "MARKET_START_TIME",

                    "RUNNER_DESCRIPTION",

                ],

                max_results=20, lightweight=True,

            )

            print("PP")

            print(market_catalogue)

            for market in market_catalogue:

                market_id = market["marketId"]

                runner_name_cache[market_id] = {

                    runner["selectionId"]: runner["runnerName"]

                    for runner in market["runners"]

                }

            market_id_to_name = {

                market["marketId"]: market["marketName"]

                for market in market_catalogue

            }

            market_filter = streaming_market_filter(

                event_ids=current_event_ids,

                market_ids=current_market_ids,

            )

            market_data_filter = streaming_market_data_filter(

                fields=["EX_BEST_OFFERS", "EX_MARKET_DEF"],

                ladder_levels=10

            )

            stream.subscribe_to_markets(

                market_filter=market_filter,

                market_data_filter=market_data_filter,

                conflate_ms=1000,

            )

            threading.Thread(target=stream.start, daemon=True).start()





        while not output_queue.empty():



            market_books = output_queue.get()

            for market_book in market_books:



                print(vars(market_book))

                print("-" * 50)  

                market_data = {

                    "marketId1": market_book.market_id,

                    "marketName": market_id_to_name.get(market_book.market_id, "Unknown Market"),  # Add marketName

                    "isMarketDataDelayed": market_book.is_market_data_delayed,

                    "status": market_book.status,

                    "betDelay": market_book.bet_delay,

                    "bspReconciled": market_book.bsp_reconciled,

                    "complete": market_book.complete,

                    "inplay": market_book.inplay,

                    "numberOfWinners": market_book.number_of_winners,

                    "numberOfRunners": market_book.number_of_runners,

                    "numberOfActiveRunners": market_book.number_of_active_runners,

                    "lastMatchTime": market_book.last_match_time,

                    "totalMatched": market_book.total_matched,

                    "totalAvailable": market_book.total_available,

                    "crossMatching": market_book.cross_matching,

                    "runnersVoidable": market_book.runners_voidable,

                    "version": market_book.version,

                    "runners": []

                }



                for runner in market_book.runners:

                    print(vars(runner))

                    print("-" * 50)

                runner_data = {

                        "selectionId": runner.selection_id,

                        "runnerName": runner_name_cache.get(market_book.market_id, {}).get(runner.selection_id,

                                                                                           "Unknown Runner"),



                        "lastPriceTraded": runner.last_price_traded,

                        "totalMatched": runner.total_matched,

                        "ex": {

                            "availableToBack": runner.ex.available_to_back,

                            "availableToLay": runner.ex.available_to_lay

                        },

                        "tradedVolume": runner.ex.traded_volume

                    }

                    market_data["runners"].append(runner_data)

                gd.Betfair.EventMarketCatalogue1 = [market_data]```



*Tags: Strategies*

---

**Mo** - *07:36:44*

They are `None` because you haven't requested them:



```market_data_filter = streaming_market_data_filter(

    fields=["EX_BEST_OFFERS", "EX_MARKET_DEF"],

    ladder_levels=10

)```

*Tags: General Technical*

---

**JC** - *08:32:38*

Hi, I've just updated my S3MarketRecorder and am having INVALID_SESSION_INFORMATION issues with also saving market catalogue via poll_market_catalogue and the tennis scores endpoint. The streaming files are still being recorded successfully though. What's the recommended way to record the catalogues and scores without getting logged out using flumine? Cheers

*Tags: General Technical*

---

**D C** - *08:51:19*

No problem here

*Tags: General Technical*

---

**Unknown** - *08:51:24*

Error 500

*Tags: Errors Debugging*

---

**Paul** - *08:58:42*

I was debugging something on the train this morning (via mosh + tmux + nvim and some test scripts on my iPhone, AMA... :zany_face:), and was hitting some weird errors and 400s for a while. Seems the errors I was worried about were not mine: API and mobile app access now back, but definitely feels like that was an ~outage~ unannounced maintenance window...

*Tags: Errors Debugging*

---

**Gooat** - *08:59:53*

That is peak performance Paul

*Tags: Performance*

---

**Mo** - *11:39:35*

This is a hot take but maybe you should try reading the API documentation?

*Tags: General Technical*

---

**liam** - *13:10:13*

Hmm, if things are down it will 95% time tell you which services are down. I tried to automate a watcher on the page but you also get a lot of FP’s which is very frustrating so tbh it’s not that helpful other than confirming what you already know 

*Tags: General Technical*

---

## 2024-11-20

**Nita Suos** - *07:51:00*

Just wondering what others do regarding conflate_time with streaming.  I am tossing up between stopping and restarting a stream, or have a few steams with different conflate times and dynamically update the markets within them.

*Tags: General Technical*

---

**D C** - *08:26:46*

I'm not the person whose opinion matters but I find it hard to consider that NOT using conflation would be considered excessive data usage. If you've got concerns, email BDP about it. I can only think it might be a problem if you are not placing any bets (but that would be a problem for most API usage anyway).

*Tags: General Technical*

---

**liam** - *09:20:04*

I have no idea what problem you are trying to solve here

*Tags: General Technical*

---

**Unknown** - *22:23:38*

[@U4H19D1D2](@U4H19D1D2)  I'm in the process of converting my program to use streaming as opposed to REST API calls.  I thought about going with Flumine as well ( I use your betfairlightweight) but will drop that idea for now because I have a number of systems I select to edit, stop, start and remove within the GUI, I'm unsure how to work around this requirement with Flumine which seems to require strategies added before running the framework.

*Tags: General Technical*

---

## 2024-11-21

**liam** - *08:15:40*

Yeah flumine isn't really designed for this although you could be clever and change things via a config/api however if you have the logic / correct abstractions already I imagine converting to streaming isn't that tricky

*Tags: General Technical*

---

**D C** - *09:07:20*

It's definitely worth the effort to convert to streaming [@U05ULP711T7](@U05ULP711T7). Wish I had done so a long time before I eventually did (especially converting to using order stream as opposed to polling listCurrentOrders).

*Tags: General Technical*

---

## 2024-11-23

**Elie Couttet** - *12:42:38*

Hi, I'm new here! :wave:

A few months ago, I was playing around with the Betfair API (the free delayed one for now) and was managing to send some POST requests and getting a response from javascript code in a Gsheet.

I've been fighting with the same code since yesterday and keep getting the same 403 response... I have tried to dumb it down as much as I could with still no success. Can one of you put me in the right direction?



Here is the simplified version of my code. I have checked that the appKey and sessionId work on the Betting Api Demo Tool. I'd be grateful for any support!



const url = "[https://api.betfair.com/exchange/betting/json-rpc/v1](https://api.betfair.com/exchange/betting/json-rpc/v1)";



  var header = { 'X-Application' : appkey, 'X-Authentication' : ssid ,'content-type' : 'application/json'}

  var jsonrpc_req={"jsonrpc": "2.0", "method": "SportsAPING/v1.0/listEventTypes", "params": {"filter":{ }}, "id": 1}



  const options = {

    method: 'POST',

    muteHttpExceptions: false,

    headers: header,

    data: jsonrpc_req

  };





    // Send POST request to the API

    var response = UrlFetchApp.fetch(url, options);

*Tags: Errors Debugging, Strategies*

---

**Phydeaux** - *14:23:52*

What is the full error you’re receiving?

*Tags: Errors Debugging*

---

**Elie Couttet** - *14:24:31*

Exception: Request failed for [https://api.betfair.com](https://api.betfair.com) returned code 403. Truncated server response: [!DOCTYPE html](!DOCTYPE html)

&lt;!--[if lt IE 7]&gt; &lt;html class="no-js ie6 oldie" lang="en-US"&gt; &lt;![endif]--&gt;

&lt;!--[if IE 7]&gt;    &lt;html class="no-js ie7 oldie" lang="en... (use muteHttpExceptions option to examine full response)

jsonRestExample@

*Tags: Errors Debugging, Deployment*

---

**Phydeaux** - *14:28:11*

[@U08233FRWCA](@U08233FRWCA) do you have the non-truncated error message?

*Tags: Errors Debugging*

---

**D C** - *14:30:12*

Ah ok - then I won't be able to help. I use nodejs for some stuff and I will dump my http options below just on the offchance it might help you

*Tags: General Technical*

---

**Phydeaux** - *14:31:33*

[@U08233FRWCA](@U08233FRWCA) the text of the error rather than the blunt status code will point you/us in the right direction… hard to help further without it.

*Tags: Errors Debugging*

---

**D C** - *14:31:35*

No, the norm here is definitely Python

*Tags: General Technical*

---

**D C** - *14:32:10*

If you want to get into this stuff, I would use flumine or betfairlightweight as there are a lot of people who will be able to help. Not many use nodejs (as far as I know)

*Tags: General Technical*

---

## 2024-11-24

**Elie Couttet** - *14:27:06*

It does recognize my encoded username though, as if I change it, it gives another error (CERT_AUTH_REQUIRED)

*Tags: Errors Debugging*

---

**Josh** - *17:05:52*

Hi all, noob here. Still able to bypass the £299 fee for live API Key (with the help from Liam/Slack group) or is it a case of paying the fine? 



Cheers



*Tags: Deployment*

---

**Elie Couttet** - *17:36:15*

I don't know much about that stuff tbh, I've been doing a lot of try and error.

*Tags: Errors Debugging*

---

**Elie Couttet** - *17:39:41*

I have tried putting everything in plain text with no avail.

Apparently there is an email address from betfair for support with API. Any chance they'd be able to help?

*Tags: General Technical*

---

**Mo** - *17:40:02*

What error do you get when you use your plain username and password?

*Tags: Errors Debugging*

---

**Mo** - *17:46:20*

Fine, so what error do you get when you put the URL encoded password into Postman?

*Tags: Errors Debugging*

---

**Jonjonjon** - *19:35:02*

[@UUE6E1LA1](@UUE6E1LA1) I wouldn't be surprised it most people who paid it haven't made it back. It requires a lot of good fortune to find a working strategy.

*Tags: Strategies*

---

## 2024-11-25

**thambie1** - *06:46:58*

I expect the fee is intentional friction. They've probably had a problem of freeloading API users popping up with new accounts every time one gets shutdown. This would also explain why they're willing to let known users 'vouch' for someone new and forgo the fee

*Tags: General Technical*

---

**liam** - *08:19:49*

Yeah as above, introduced when they realised they could start charging for historical data. I think its a lazy way to do it, I basically used the API to learn python and had there been a charge I probably wouldn't have even started

*Tags: Data Quality*

---

## 2024-11-26

**Jhonny** - *17:50:04*

problem solved tho. I had hard-coded filter for GB markets some weeks back, and forgot to remove it

*Tags: General Technical*

---

## 2024-11-27

**PeterLe** - *17:42:08*

I'm current using Flumine 2.4.1 and have been reluctant to upgrade it as its been rock solid...

I see the current version is 2.6.8



As I've not upgraded before (and as i still consider myself a beginner to Python :slightly_smiling_face: )...Is it a relatively straight forward?

Thanks

*Tags: Getting Started*

---

**liam** - *18:56:55*

as above, my most valuable tests are a simulation on a set market/strategies and then confirm orders/profit etc.



Changes listed [https://github.com/betcode-org/flumine/blob/master/HISTORY.rst|here](https://github.com/betcode-org/flumine/blob/master/HISTORY.rst|here)

*Tags: General Technical*

---

## 2024-11-28

**birchy** - *11:16:05*

Does flumine have a flag for a change of market version (i.e. withdrawals, etc)? I'm currently saving the selection IDs to the market context and checking if they change but it's not ideal. I guess I could monitor market.market_version instead?

*Tags: General Technical*

---

**birchy** - *14:26:02*

My production setup is currently 2.6.0 and I upgraded that from 2.4.x and had no issues, so can't see it being any different for you.

*Tags: Getting Started, Deployment*

---

## 2024-12-02

**JFP** - *10:34:50*

VPS noob question. Setting up a VPS for the first time to run my betting strats and have a question regarding security. There is plenty of info available on how to harden for external threats but how do you protect your data from the VPS provider? Does anyone here take extra steps to hide their setup when running on a VPS vs on their desktop?

*Tags: Getting Started, Strategies*

---

**Alex A** - *10:47:23*

It probably would be possible for the right employee to do that, as AWS say

```We do not access, use, or share customer data without your agreement, except as required to prevent fraud and abuse, or to comply with law, as described in our Customer Agreement.```

You could try encrypting all your software and data on the VPS, but at some point you will have to run your program, and someone who controls that VM or the underlying machine would be able to access that. Personally I make no effort to protect anything I have running on any VPS from the provider, and am not worried in the least about it. I’d guess you’re more likely to have your personal desktop stolen or drive cloned by someone who knows you have a betting strategy, which is also not something I go to any effort to stop.

*Tags: Deployment, Strategies*

---

## 2024-12-03

**Mo** - *10:07:18*

Does flumine simulate the BSP auction?

*Tags: General Technical*

---

**liam** - *12:42:49*

The latter, flumine doesn't try to estimate market impact on BSP or limit orders

*Tags: General Technical*

---

**Mo** - *14:12:30*

I know this has been asked in the past but how can I use the strategy_ref without breaking flumine functionality? Should I be using the customer_order_ref instead?

*Tags: Strategies*

---

## 2024-12-04

**Jorge** - *06:49:55*

If I am only running 1 strategy in the same flumine instance then I use:

```from flumine import config

config.customer_strategy_ref = "alpha_1"```

*Tags: Strategies*

---

**Jorge** - *06:56:09*

If running more than 1 strategy in the same flumine instance, it is not possible to separate every strategy using the customer_strategy_ref. So I use [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py#L56C26-L56C39|order.trade.strategy in the cleared_orders.txt](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py#L56C26-L56C39|order.trade.strategy in the cleared_orders.txt)

*Tags: Strategies*

---

**James** - *07:49:49*

What is behind the decision to not allow us to use our own strategy refs [@U4H19D1D2](@U4H19D1D2)? 

*Tags: Strategies*

---

**liam** - *08:06:48*

hmm this comes up a lot, its routed in the fact I run a lot of instances/strategies on the same markets and I wanted an easy way to separate orders/positions etc. per instance rather than per strategy



As [@U0155J92A7Q](@U0155J92A7Q) mentions, best practice is to always log the strategy data with the betId's and any context etc. so you should never need to use the betfair customerStrategyRef.



To answer Mo's question, flumine also uses [https://github.com/betcode-org/flumine/blob/a9cd71befc6062b52ce65ca695b50a56a2e81344/flumine/order/order.py#L269|customerOrderRef](https://github.com/betcode-org/flumine/blob/a9cd71befc6062b52ce65ca695b50a56a2e81344/flumine/order/order.py#L269|customerOrderRef) however it has a hash of the strategy name



```    @property

    def customer_order_ref(self) -&gt; str:

        return "%s%s%s" % (self.trade.strategy.name_hash, self.sep, self.id)```

*Tags: Strategies*

---

**Mo** - *08:13:11*

Right so if I wanted to separate scraped cleared orders by strategy I can use the first token in the `customerOrderRef`

*Tags: Strategies*

---

**Mo** - *17:18:23*

Should be fixed now

*Tags: Errors Debugging*

---

## 2024-12-06

**WilliamR** - *16:11:33*

Hi [#C4HL6EZTQ|general](#C4HL6EZTQ|general) is there a way to simply place each way bets based on WIN market prices or do I have to create a whole new BaseStrategy class so I get the EACH_WAY market book?

*Tags: Strategies*

---

## 2024-12-12

**Mo** - *13:46:17*

Can anyone share a flumine pattern for cancelling an order X seconds after going live if it hasn't been matched?

*Tags: Deployment*

---

**liam** - *13:47:02*

eexxxaaammmmpplllleeesssss



[https://github.com/betcode-org/flumine/blob/a9cd71befc6062b52ce65ca695b50a56a2e81344/examples/strategies/lowestlayer.py#L65](https://github.com/betcode-org/flumine/blob/a9cd71befc6062b52ce65ca695b50a56a2e81344/examples/strategies/lowestlayer.py#L65)

*Tags: General Technical*

---

**liam** - *13:48:09*

nah, if flumine has live orders it snaps the order stream every 0.25?s

*Tags: Deployment*

---

**Mo** - *13:54:48*

Any plans to improve flumine typing, for example



```    def process_orders(self, market: Market, orders: list) -> None:

        # process list of Order objects for strategy and Market

        return```

subscripting the list type for orders

*Tags: Strategies*

---

## 2024-12-13

**liam** - *08:43:32*

[https://github.com/betcode-org/flumine/blob/master/tests/test_integration.py](https://github.com/betcode-org/flumine/blob/master/tests/test_integration.py)

*Tags: General Technical*

---

## 2024-12-15

**PeterLe** - *19:56:56*

Upgrade went through very smoothly..compared previous backtest results (on one strategy) and then again on new version, identical thanks chaps

*Tags: Strategies*

---

## 2024-12-18

**D C** - *15:23:19*

Lots of links to premium charge documentation now appear to be dead.

*Tags: General Technical*

---

**Jonjonjon** - *19:21:39*

Is there any documentation on how this is going to affect "My Betfair Rewards"? The page appears to be offline now

*Tags: General Technical*

---

**Dave** - *19:51:48*

Someone mentioned they wanted maker-taker differentiation, it actually wouldn't help. Give makers a discount = markets get tighter = lower margins and increased competition for making, while hoping volume picks up..

*Tags: General Technical*

---

## 2024-12-19

**Paul** - *14:41:33*

Everyone needs counter-parties. Liquidity is dripping away, so long-term they need more smart money talking at each other with different takes on how to be smart (there's likely more than one or two ways to make money in highly volatile 102%/98% offer/bid spreads). By saying "OK, let's gear everything around BA Pro manual traders" - and that's what I'm trying to understand, if _*that*_ is the main audience they're working backwards from - they're taking quite a punchy move on how to get liquidity back into markets.

*Tags: General Technical*

---

**Jonjonjon** - *14:42:48*

Yes next six months could be very good or very bad. I hope my models don't blow up.

*Tags: Strategies*

---

## 2025-01-01

**Mo** - *19:13:33*

Happy new year. As I’ve already mentioned to some of you, towards the end of last year I started working on some of my own strategies for the first time. Those of you who are familiar with my background and current circumstances will know that prior to this point, all of my betting activity has been part of syndicates that I’ve been a founding member of



My completely ludicrous goal for this year is for my take home pay from my own strategies to meet or exceed my take home pay from my existing syndicate strategies. I’m going to be documenting this process by providing a weekly update on the Slack comparing (axis free thank you very much) equity curves for the two, in addition to a running commentary on strategy developments and upcoming plans 

*Tags: Getting Started, Strategies*

---

**Josh** - *19:18:17*

To deploy my first strategy!

*Tags: Deployment, Strategies*

---

**Paul** - *22:56:39*

I’ve had a profit every day since the 26th Dec until today, which is my longest run in a while. I want to try and figure out how to get consistently daily profits and then scale them gently. I’ll be doing a lot of blogging on the way too.

*Tags: General Technical*

---

## 2025-01-02

**Ankit Aggarwal** - *12:09:45*

Happy New Year! My goal is to deploy my first strategy!!

*Tags: Deployment, Strategies*

---

**Ankit Aggarwal** - *12:10:37*

Amateur question, does the exchange double count all Trade Quantities, so basically divide by 2?

*Tags: General Technical*

---

**Mo** - *13:56:18*

[@U02GGCCLTKM](@U02GGCCLTKM) - I won't be reducing any time allocated to the syndicates as I have very modest "contractual" weekly hours to work for them which leaves plenty of time for my own strategies. For one of the syndicates, we went through a quite painful process several years ago now to get to the point where we could agree on terms to allow us to work on outside projects. For the other, we've struggled for a couple of years to make any kind of meaningful profit and the other members are my brother and best friend so they are understanding of my desire to branch out on my own, and I'm much more willing to dedicate more hours to working than they are. If more details would be helpful for your situation feel free to DM me

*Tags: General Technical*

---

**Brøndby IF** - *15:46:33*

Guys, another topic:



Could someone who has an account with [http://betfair.com|betfair.com](http://betfair.com|betfair.com) please confirm the volume traded in `currency_code="GBP"` specifically for this runner:



"event_id": "33907483"

"market_id": "1.237786775"

"selection_id": 23278850



→ Betfair in Brazil now has a specific website for our country (.[http://bet.br|bet.br](http://bet.br|bet.br)) and I need to confirm if the volume traded that I am receiving through the API is general or if it has been limited to only the volume traded in Brazil. Thank you very much for your help!

*Tags: General Technical*

---

## 2025-01-03

**Tim** - *14:42:14*

Happy New year everyone! -- Extremely new to the group but have found it immensely useful so far.



I have a background working for market making firms in options + crypto space, interested in seeing how well this can translate into market making on a betting exchange. 



Hoping that with sound market making logic, I can create a profitable strategy that won't need a strong edge in terms of valuation.

*Tags: Getting Started, Strategies*

---

**Jonjonjon** - *22:58:39*

I trade the greyhounds from the UK. My matched volumes and performance haven't changed that much since Dec 30th.

*Tags: Performance*

---

## 2025-01-06

**Jhonny** - *15:29:14*

Hi everyone, 2 questions please:

1. Are all parts of a segmented message guaranteed to be sent? e.g. if SEG_START arrives immediately I switch subscription, is there a guarantee that SEG_END would still eventually be sent before/after the start of the new subscription?

2. Where can I find sample of segmented messages?

*Tags: General Technical*

---

## 2025-01-07

**Alex A** - *00:39:17*

I’ve asked on here before and some people have said they can happen on the initial snapshot, but I still haven’t seen it, and processing the snapshot faster is very unlikely to noticeably improve my betting performance.

*Tags: Performance, Strategies*

---

**birchy** - *13:29:23*

Inspired by [@UBS7QANF3](@UBS7QANF3)'s thread, I thought I'd better try to calculate my Sharpe Ratio, which I've never done before. Can someone please confirm if this is correct? It's in a jupyter notebook where my dataframe is populated from flumine bet logs.







`def calc_sharpe(df, period='D'):

    # Create a copy of the dataframe

    d = df.copy()`



    `# Ensure datetime index

    if not isinstance(d.index, pd.DatetimeIndex):

        d.set_index('date_time_created', inplace=True)`



    `# Calculate liability and ROI

    d['liability'] = d['size_matched'] * (d['price_matched'] - 1)`



    `# Filter for matched bets

    df_matched = d[d['liability'] &gt; 0]`



    `# Calculate period returns (weighted by liability)

    period_profits = df_matched.groupby(pd.Grouper(freq=period))['profit'].sum()

    period_liabilities = df_matched.groupby(pd.Grouper(freq=period))['liability'].sum()

    period_returns = period_profits / period_liabilities`



    `# Filter out null periods

    period_returns = period_returns[period_returns.index.notnull()]`



    `# Calculate metrics

    avg_return = period_returns.mean()

    volatility = period_returns.std()

    sharpe_ratio = np.sqrt(len(period_returns)) * (avg_return / volatility) if volatility != 0 else 0`



    `# Calculate summary statistics

    total_bets = len(df_matched)

    total_profit = round(df_matched['profit'].sum(), 2)

    total_liability = round(df_matched['liability'].sum(), 2)

    overall_roi = total_profit / total_liability if total_liability &gt; 0 else 0`



    `return {

        'sharpe_ratio': sharpe_ratio,

        'average_return': avg_return,

        'volatility': volatility,

        'periods_analyzed': len(period_returns),

        'total_matched_bets': total_bets,

        'total_profit': total_profit,

        'total_liability': total_liability,

        'overall_roi': overall_roi

    }`



`sr = calc_sharpe(df, period='D')

for k, v in sr.items():

    print(f'{k}: {v}')`



*Tags: Feature Engineering*

---

**birchy** - *14:20:41*

It feels wrong to me as well. This is a strategy that has been running for 4 years and I know it's low ROI but it just plods along so I've left it alone.

*Tags: Strategies*

---

**Unknown** - *20:58:39*

That's reassuring, although I already know it's a profitable strategy. Not life changing amounts, but being consistently &gt; 0 is inspiring. Although I've become very complacent. :thinking_face:

*Tags: Strategies*

---

## 2025-01-10

**A** - *08:57:18*

My plans are to pick this up again after a little break following the birth of my second child, hopefully by the end of this year I’ll have my first profitable strategy running in production.



I’m probably going to stick my ML based approaches on ice, and move back to trying to gain a deeper understanding of the markets through fundamentals and regression.

*Tags: Deployment, Strategies*

---

**A** - *09:28:03*

Thanks jonjonjon - I totally lost any understanding I had about the workings of my code, and with that I lost motivation (especially as the models weren’t spitting out anything of any use!). So going back to basics and gathering the momentum again.



Looking forward to finally finding something with an edge. I have some pretty cool reporting and alerting set up in a prod environment, but all it’s done so far is notify me of my own financial demise :rolling_on_the_floor_laughing:

*Tags: Strategies*

---

## 2025-01-13

**Zana Chen** - *14:31:13*

Can't see my Premium Charges this morning, anyone else having this problem&gt;

*Tags: General Technical*

---

**Peter** - *19:58:22*

The nice new Expert Fee dashboard has now disappeared in my account. I rather liked what it was telling me, but now I'm wondering whether it was too good to be true and they're away fixing it!

*Tags: Errors Debugging*

---

## 2025-01-15

**Gooat** - *20:12:33*

Playing around with data types can help slightly with size but what's the worse case total size right now plus x years?

*Tags: General Technical*

---

## 2025-01-16

**Jhonny** - *08:25:18*

Noted [@U4H19D1D2](@U4H19D1D2)! I guess it's better to start simple regardless of perceived complexity of the problem domain. Thanks

*Tags: General Technical*

---

**D C** - *09:37:03*

FWIW [@U07SPFFN010](@U07SPFFN010) I've found in the past that when I feel that I am getting nowhere in terms of "cracking" the real problem at hand, I focus on something else that I CAN do in a vain attempt to make myself feel less stupid. I'd kid myself that an unnecessary rewrite or prettification or "optimisation" of code was a good thing to do just because psychologically it made me feel I had achieved something (when in real terms, I had not). I'm still a bit guilty of it at times but I've learned to recognise it for what it is now and use it to change direction. For some it might be a good thing, but I now take a complete break and step back when I'm getting nowhere on problem of the month/week/day - but we all work in different ways. Managing the disappointment/failure/isolation of doing this kind of thing is important to recognise though.



Not saying that this is what you are doing here BTW - for all I know you might love this kind of problem in which case dig in.

*Tags: General Technical*

---

## 2025-01-17

**D C** - *07:48:03*

Has it become less valuable that you will consider dropping your direct from TPD feed subscription? What's your basis for coming to this conclusion? I've not noticed any drop in profit since it was made available in BA (although I did worry that it would cause problems). Have you any metrics so suggest the ARC track inplay markets are tightening up recently?

*Tags: General Technical*

---

**Justice** - *09:06:43*

[@UUE6E1LA1](@UUE6E1LA1)  As soon as I'm not making sufficient money I will try to renegotiate the rate or cancel. I need to shift my focus away from horse racing and find an edge in another sport, most likely cricket as I have strong domain knowledge- this is one of my goals this year. 



I've not noticed anything in particular but I'm very paranoid about alpha decay... Especially if my strategy is on a drawdown. I currently have no means to identify when this is occurring other than the fact I'd be losing money :joy: Should really work on this.



If I was already getting the data from those other RMG tracks I would be really pissed off, especially if I was paying big money. I wasn't, so this is a win for me as I can turn over more money. Though I'm not comfortable applying my models to new courses without back history

*Tags: Strategies*

---

**D C** - *09:16:17*

i find this really interesting - it doesn't seem that long ago (although it probably was) that you were saying that GPS was rubbish and was too error prone to yield any edge! Now you are paying TPD for a direct sub - so you are going to have a latency advantage over any of us getting it from Betfair and a presumably massive latency advantage over Gruss/BA/Betmover integrated GPS users. And yet you are worried about alpha decay ???? I share your view on the need for diversification though.

*Tags: Errors Debugging, Performance*

---

**Justice** - *09:23:14*

[@UUE6E1LA1](@UUE6E1LA1) I'm just muddying the signal :joy::joy: Though I do maintain that the inaccuracy is hugely frustrating. Whenever my model is wrong and I get picked off, I'd say 95% of the time it's due to an inaccuracy in the data. I'm curious as to whether the data from the other courses will be using the same technology and whether it will be any more accurate. I can imagine there is a very small latency advantage but I don't know if this is significant. I also found I can subscribe to duplicate feeds for redundancy so dropouts are very minimal

*Tags: Performance, Strategies*

---

**D C** - *09:28:42*

You must have been filling your boots nicely then when the BF feed went to shit at the end of last year!!! I'd say you are getting a decent latency advantage over us on the BF feed if you are getting it pushed directly over UDP from TPD direct. And massive advantage to the OTS ladder trader boys what with all the overheads that a graphical desktop app will have. I think that feed that TPD provide is limited to 200ms refresh rate too when last I looked although admittedly it comes with some nice extras like course par times.

*Tags: Performance*

---

**Ralegh** - *09:39:10*

Any idea what the overhead for betfair vs tpd direct is? Would be surprised if it’s taking more than 50ms to go through their system plus latency across open internet 

*Tags: Performance*

---

**Justice** - *09:46:08*

[@UUE6E1LA1](@UUE6E1LA1) imo trading manually with the data defeats the point. This is where I went wrong when I first got the data back in 2019/20. It was a big missed opportunity for me but I was too narrow minded to see past the inaccuracies and didn't have the data skills that I have now. I also then got access to a drone feed when very few others were getting them. This was very easy money so I shelved the TPD data. The drone edge dropped off a cliff after about 6 months as others started to catch on. I would've made considerably more money had I persisted with the data.



[@U03FS7KM2NL](@U03FS7KM2NL) Honestly I don't know how to time it accurately since it relies on the clocks on the gps device being synced with my client. I doubt it's more than 50ms quicker than the Betfair feed

*Tags: Strategies*

---

## 2025-01-21

**D C** - *18:04:48*

I saw that earlier today but I could not fathom what he was talking about in regards to  "regressing to the market" and "65% market 35% model blends".

*Tags: Strategies*

---

**liam** - *18:22:44*

Its something I struggle with, for example inplay you can find value and get stuck in, few seconds later and the price has moved from 5-&gt;15, in my case it is very likely I have been the cause of that movement (ideally it wasn't)



So we now have a new price but it is no longer 'exogenous' because it has very likely been impacted by your own model. You can trick yourself/model into believing it has +ev causing a circular loop to death.

*Tags: Strategies*

---

**D C** - *18:38:09*

So because you don't know the impact your money has had, and because your model is price action driven. you have less certainty that your subsequent model prices are "legit" ?

*Tags: Strategies*

---

**liam** - *18:58:49*

Doesn't have to be price action, a fundamental model can/would use the market price for features / filtering etc.

*Tags: Feature Engineering, Strategies*

---

**D C** - *19:02:34*

So is this what he means by "X% market (100-X)% model blends" ?? Some kind of weighting of a model price with the current market price?

*Tags: Strategies*

---

**liam** - *19:06:09*

Yes, reducing reliance on the model, not sure why but it's not spoken much on here or elsewhere but I know a few pros/syndicates use the market price as a valuable source of info/feature, even if its at the final stage inbetween model/execution

*Tags: Feature Engineering, Strategies*

---

## 2025-01-22

**Justice** - *09:36:32*

Interesting thread and discussion. None of my models have any idea about the market price. Adding this information to my model causes it to become highly correlated to the market and dilutes the other signals/features I'm using. I find this has a large adverse affect on my turnover and therefore profit. Perhaps I need a more sophisticated approach rather than just using it as another feature

*Tags: Feature Engineering, Strategies*

---

**D C** - *09:43:52*

Yeah I've found that for inplay too [@U05N9773A23](@U05N9773A23) when combining price and GPS into a model - the price just dominates. I think using market price makes more sense for pre-off as pre-off prices are only reflective of current market sentiment, whereas inplay prices are more reflective of the true probability (excluding things like high volatility periods and information latency etc.)

*Tags: Performance, Strategies*

---

## 2025-01-24

**tone** - *08:56:07*

Is anyone else getting this error this morning?betfairlightweight.exceptions.APIError: SportsAPING/v1.0/listMarketCatalogue

Params: {'filter': {'eventTypeIds': ['7'], 'marketCountries': ['GB', 'US', 'AE'], 'marketTypeCodes': ['WIN'], 'marketStartTime': {'to': '2025-01-25T05:53:40Z'}}, 'marketProjection': ['RUNNER_DESCRIPTION', 'RUNNER_METADATA', 'COMPETITION', 'EVENT', 'EVENT_TYPE', 'MARKET_DESCRIPTION', 'MARKET_START_TIME'], 'maxResults': 150}

Exception: None

Error: {'code': -32099, 'message': 'ANGX-0001', 'data': {'APINGException': {'requestUUID': 'ie2-ang13a-prd-11260945-00632461b2', 'errorCode': 'TOO_MUCH_DATA', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}

Full Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0001', 'data': {'APINGException': {'requestUUID': 'ie2-ang13a-prd-11260945-00632461b2', 'errorCode': 'TOO_MUCH_DATA', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}

*Tags: Errors Debugging*

---

**tone** - *10:11:51*

No, it has been there for ever. The problem seems to have occurred because hitherto the filter has returned too few results to breach the threshold. I've removed RUNNER_METADATA as it wasn't necessary and it's now returning all the markets without error. Thanks for your help.

*Tags: Errors Debugging*

---

## 2025-01-25

**Unknown** - *00:11:39*

Well, after 20+ years, I never expected this. Have had 2 accounts for the same period. Changed the email address on my other one (which is still active) earlier this evening. Nothing untoward going on, so I can only assume an over zealous CS agent? My accounts have been linked internally for many years, which I've discussed with [@UNW8Q88EL](@UNW8Q88EL) in the past and he said it wasn't an issue as I wasn't trying to hide anything. Bit annoying TBH. Hope it gets resolved...

*Tags: General Technical*

---


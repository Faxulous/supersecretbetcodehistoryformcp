# Deployment - Community Knowledge

*1996 relevant conversations from across all channels*

---

## 2025-01-26 08:28:15 - strategies channel

**Unknown**

Big week this week and it all came out of the meetup on Tuesday



I had an idea for a new strategy while chatting to [@U01PJ5YMFBJ](@U01PJ5YMFBJ) and while it took a few days to go from idea to implementation (both o1 and DeepSeek made a complete hash of it), the backtest looks great - see first attached plot. I've live tested it on a couple of races and am looking to deploy it more widely today. This is now `Strategy3`



Also at the meetup, [@U05SRUKGYCC](@U05SRUKGYCC) made some comments that gave me some perspective on `Strategy2`. It helped cement my feeling that it's not contrarian enough and I need to revert to the original vision of the strategy. I've started work on a third iteration of it with promising signs. I had really hoped I was going to be able to report today that I'd gone live with _both_ `Strategy2` and `Strategy3` this week but sadly not quite there yet



Existing strategies got absolutely crushed this week and that unfortunately coincided with it being my turn to run them. My already considerable shortfall is now even larger. Although they've been running for years, variance like this is still stressful when you're personally making the losses even if it all gets accounted for. However, these emotional factors are easier to handle being part of a group that can provide perspective and shared experience



New strategies flat on the week so far after a bad day yesterday (my naturally pessimistic disposition thinks there's still plenty of time for them to get further in the hole for the week) but the losses seem in line with variance and I'm not at all concerned (yet). I did increase stakes on `Strategy0` on Monday and it's probably past time I increased stakes on `Strategy1` as well



Plan for the coming week is to get `Strategy3` as widely deployed as possible - I'll need to check backtest performance on US/rest of world horses, and see what I can do about testing it on greyhounds as well. I have a feeling the strategy should work well inplay but backtests look awful. I'm actually specifically interested in getting it working for inplay football but that probably needs a fair bit more work than just tuning some parameters



Development-wise the main focus will be trying to finally get `Strategy2` over the line

---

## 2025-01-19 12:59:23 - strategies channel

**Unknown**

It's been a frustrating week trying to get `Strategy2` to the point where I'd be comfortable redeploying it



On Monday I started by increasing the exposure limit on `Strategy1` for in play horses to bring it in line with what I've used for backtesting. I haven't spent much time using backtests to gauge how much I can scale everything up but that's perhaps something to start thinking about



I wasn't planning on doing this but I decided on Monday to also compare `Strategy0`'s live bets to the backtest and discovered some big problems with my execution which meant I'd missed out on - very roughly - about 15,000 bets over the past three months. This was largely down to handling the start of races and using the `marketStartTime`. I've completely overhauled the logic and it appears to have fixed things. I made a separate tweak that should also help reduce the number of missing bets but, checking that now, it's been somewhat less successful



Right, back to `Strategy2`. I've definitely made significant improvements to the model over the past week but none of the changes have translated into a meaningful shift in backtest performance. Intuitively, where the model is strongest it must just be agreeing with the market. However, I did have a small breakthrough last night with the filters I'm applying to try to avoid adverse selection and I think the strategy is just about at the stage where I'm happy to start live testing it again today. I don't think the backtest performance is quite good enough but the thing that convinced me it's worth live testing is that I can run it against many more markets than I have scraped prices for - i.e. all of the `OTHER_PLACE` markets. As in my last update, I can use BSP to quickly assess whether it's actually finding value on those markets. The breakthrough I'm referring to should apply to pretty much any taker strategy so I'm excited to apply it to other strategies



So the plan for the rest of the day is to manually run the strategy against today's UK racing and I also still owe my syndicates another couple of hours work for the week. Looking to next week, I should fit `Strategy2`'s model to Australian racing and backtest it again to see how that's looking. I do have one other big model improvement on my to-do list that I'll action and a couple of suggestions from o1 that I think are worth exploring but I haven't thought about what's next beyond that. I'll review my list of strategy ideas and have another chat with o1



A couple of general findings from this week:



1. It really helps to step away from the computer. All of my big breakthroughs in thinking came from taking a shower or cooking dinner or walking the dog

2. o1 continues to be indispensable. It's at least as good as if I had a junior quant working for me

Update on performance:



1. Got rotated back into syndicate strategies yesterday. We've been undertaking a large reconciliation exercise, the end result of which is that I'm quite a bit behind of my share so I imagine I'll be in rotation for a bit now

2. New strategies had a bad couple of days midweek and have been absolutely crushing it since then

New strategies net profit up to 2.3% of existing strategies net profit!

---

## 2025-01-13 16:37:25 - random channel

**D C**

Definitely some here use containers. I just build my application binaries locally, upload to AWS target and run using tmux. My current use case doesn't require anything more complicated than that.

---

## 2025-01-12 11:40:47 - random channel

**D C**

I personally see no value in sticking with an older LTS (although I don't use flumine) but if that wors for you then do it. When setting up new instances I just go with the latest LTS version then install whatever is needed on top of that and tweak any things that needed tweaking and go from there. You might get a few SSL issues that need resolving (I think I did when moving up from 20.04) due to having "older" BF cert files but I'd say it's worth sticking with the latest if you are setting up something new. I should be honest though and say that other than an initial update on a fresh install, once things are running I don't run updates as that requires restart usually.

In general I like to run the same OS on AWS as I am doing my dev work on and that's always going to be latest LTS ubuntu. Might review that if AWS start offering other linux flavours in future though.

---

## 2025-01-12 10:42:01 - random channel

**PeterLe**

Morning

I'm going to set up a new Ubuntu instance on AWS

I currently have a 20.04 instance but see that there are later version available

Is 24.04 LTS the one to go for (I assume its fine with Flumine etc)?

Thanks

---

## 2025-01-10 11:24:01 - strategies channel

**Unknown**

I haven't really settled on a definitive schedule for publishing these but I figured I'd probably be too busy this weekend so I'm getting this week's update in early



It's been a hard and frustrating week grinding away on `Strategy2`



First up, as mentioned in my [https://betcode-org.slack.com/archives/CTPL3R3FU/p1736081164005719|last post](https://betcode-org.slack.com/archives/CTPL3R3FU/p1736081164005719|last post), I increased stakes on `Strategy0` and `Strategy1` on Monday (2025-01-06). My plan is for these to increase in line with the notional bankroll I have set aside for this collection of personal strategies, and for this bankroll to grow organically. If I find a strategy that I'm extremely bullish on and think it could benefit from an injection of capital then I will obviously do so but that's not (yet?) the case for `Strategy0` or `Strategy1`.



So, `Strategy2`. I've been alternating between thinking I've cracked it and thinking all I have is a horribly overfit hot mess. I started off very quickly making progress in overhauling the model and staking which:



1. Permitted trading on all runners, not just the favourite

2. Widened the number of markets that could potentially be traded

3. Takes current position into account, hopefully resulting in smoother returns

The progress was such that on Wednesday (2025-01-08), I switched off "V1" and deployed "V2". I observed the bets for V2 in a few races and noticed it wasn't doing a great job of beating BSP. Small sample size obviously but I decided to switch it off and dig into the backtests a bit more



First up I confirmed in the backtest that bets which beat BSP do great and bets which don't beat BSP are bad. So it's not like my model is so good that it can identify times BSP is wrong (at least in aggregate)



This strategy and, to a lesser extent, `Strategy1`, has a variety of filters applied to the trading signals. Things like requiring a minimum expected ROI (EROI) to bet, but also not exceeding a maximum EROI. Also some traded volume and other market "quality" indicators like overround/spread in an attempt to restrict to situations where confidence in the model predictions is likely to be highest



Initially I had optimised these filters using backtest realised returns but based on my increased understanding of the importance of beating BSP for this strategy, I went back and included expected value calculations in the optimisation process. This suggested that I'd overfit many of these filters and so I went back and re-optimised them using expected value with the result that several of the filters were discarded in their entirety. The backtest performance although arguably now more likely to reflect future returns looked significantly less impressive by the end of this process



For both backtesting and the live trading that's been done with the strategy, such that it is, I've been focusing on UK horse racing. However, I'm very keen to get it working in other jurisdictions and also on greyhounds. For obvious reasons:



1. The more markets it can trade, the quicker live feedback can be had

2. More markets traded = smoother returns

3. If it can widely scaled then the per bet performance doesn't need to be amazing

I do have some problems with lack of data, however:



1. I haven't been scraping greyhound markets for very long

2. I only realised this week I haven't been scraping horse racing `OTHER_PLACE` markets (now corrected)

3. No doubt there are other market types I'm still not scraping that I am ultimately interested in (e.g. Each Way, AvB, Without favourite, all exactas/quinellas/forecasts/reverse forecasts/perfectas/superfectas/trifectas/etc. and all aliases thereof)

This lack of market data to backtest against is one reason why I'm so keen to nail down the model building process and to rely less heavily on the market based filters. If I can be confident that I'm building a market beating model in cases where I do have the market data, then I am more inclined to cautiously deploy the model to other markets and rely on live results for feedback



I mentioned in my last update that as part of this strategy I wanted to look at extending both Shin's method and Kelly staking to handle markets with multiple winners. I've had some generic insights into these that I think people may find useful:



1. The R package [https://cran.r-project.org/web/packages/implied/index.html|implied](https://cran.r-project.org/web/packages/implied/index.html|implied) already implements Shin's method for multiple winners. However, it simply finds a value of `z` numerically using the existing formula. I don't believe this is correct as the starting assumptions under which Shin derived his model are not applicable for multiple winners. Nevertheless, this approach may be practically useful if not conceptually sound

2. A recently developed alternative to Shin is [https://github.com/gotoConversion/goto_conversion|GOTO (Gambling Odds To Outcome) Conversion](https://github.com/gotoConversion/goto_conversion|GOTO (Gambling Odds To Outcome) Conversion). This method apparently has serious chops, having been used in several gold medal winning solutions on Kaggle. It appears to be able to handle cases with multiple winners trivially. However, I observed some issues using the method in practice. Firstly, the code does not seem to be very well written, requiring me to fork the repo and make some changes. Secondly, the method produced negative probabilities for the BSP on a completely unremarkable horse racing WIN market. Also worth noting that most if not all of the motivating analysis supporting the method focuses on markets with few winners like football and therefore there's much weaker evidence that it would outperform Shin in horse racing. The repo is not configured for me to create a GitHub issue to discuss these concerns with the author so I have reached out to him on LinkedIn but have yet to receive a response

3. ChatGPT, o1 specifically, has been extremely useful for strategic discussions about developing `Strategy2`. In my opinion, a product like it is becoming increasingly essential if you work by yourself

4. I'm no longer planning on extending Kelly staking to handle market with multiple winners but while looking at it I noticed that the way I've always done fractional Kelly staking is wrong. This has implications for my existing strategies and it's gratifying that looking at new markets in new ways can lead to widely applicable fresh insights in this way

Update on performance with reference to the attached plots:



1. I got rotated back into the syndicate strategies for a day, made a load of money and got rotated out

2. New strategies took a few days to dig themselves out of the hole but they've done it

3. Looks like `Strategy1` in play is really carrying water

New strategies net profit up to 0.77% (from 0.4%) of net profit of existing strategies so everything is moving in the right direction!



Plan for the coming week is to continue to grind away on `Strategy2`, mainly focusing on the model, and hopefully getting it to the point where it can be redeployed to UK racing. Next step after that will be getting it deployed to Australian racing then US racing then perhaps rest of world if it seems worthwhile, followed by greyhounds. I haven't thought about what's next after `Strategy2` but last count I have ideas for 11 other strategies on my list. Other syndicate members will be back to work on Monday so I'll be back to working on existing strategies next week as well

---

## 2025-01-10 08:57:18 - general channel

**A**

My plans are to pick this up again after a little break following the birth of my second child, hopefully by the end of this year I’ll have my first profitable strategy running in production.



I’m probably going to stick my ML based approaches on ice, and move back to trying to gain a deeper understanding of the markets through fundamentals and regression.

---

## 2025-01-06 18:46:42 - strategies channel

**Mo**

[@U05REBJ21EC](@U05REBJ21EC) misinterpreted your original question but yeah I would backtest the events that were live traded and compare bets placed between the two. Or just not give a shit and wait for the results to play out

---

## 2025-01-06 10:26:42 - issues channel

**D C**

This has been discussed in the past but does anyone use a market AND order subscription within the same streaming connection? I am basically at my max connection limit (10) and looking for ways to cut down. I know people use things like redis so there is a "common" store of updated order information and thus only using a single connection for all order updates, but this is not a great option for me right now. I do some things that most would consider stupid (like having separate bots to record stream data - the same data that production strategy bots also request) but I prefer this approach and won't be changing that.

Basically it seems to me that the quickest and easiest method will be make my market stream connections also subscribe to order change messages but this has always felt a little bit "dirty".

Does anyone actually do this (seem to remember last time the most common approach was single stream each for market and order data)?

---

## 2025-01-06 09:21:20 - random channel

**Paul**

Not relevant to your question, but there may also be glitches in the machine for that course. Weird markets last night at Sam Houston. I was watching them live. There would be huge delays (fine, BAU, US racing often delays as they wait for the betting pools to fill up), and the markets would turn to treacle (nothing matching), and then the market would be voided. And then again on the next and on the next. I assumed adverse weather meant a cancelled meeting but noted the races had actually run. Settled down by about 22:00 UTC.

---

## 2025-01-05 15:48:19 - strategies channel

**AndyL**

Nice one Mo, My other post is inrunning horses model, you’re giving me some added motivation now, although I’m not running live!!

---

## 2025-01-05 12:46:04 - strategies channel

**Unknown**

As promised, here is the first update on my yearly goal for my new strategies to equal or exceed my take home profit from existing strategies



The first attached plot shows cumulative net profit. Some comments:



1. The existing strategies have very high costs associated with them. Therefore I am using 37.5% of gross profit as net profit

2. In contrast, the new strategies have very low costs associated with them, therefore I am using 59% of gross profit where, hopefully obviously, this is almost entirely the 40% premium char - ahem, expert fee

3. The new strategies have made approximately 0.4% of the net profit of the existing strategies to date. LFG!

4. Since 2nd January I'm ahead of my syndicate share so haven't _personally_ been trading the existing strategies hence the flat line

The second attached plot shows the cumulative net profit only for the new strategies. Some comments:



1. As you can see, they had a bit of a rough time yesterday

2. I was planning on increasing the stakes but this has given me some pause. Note that I'm just being a pussy, the correct thing is still to increase the stakes. It's arguably sensible to wait until Monday anyway to minimise the impact on live trading from restarts

The third attached plot shows the relative profit for the new strategies. Some comments:



1. `Strategy2` got deployed yesterday and only had two bets, one of which accounts for almost all of the loss for that strategy. The price was fine (according to BSP) but the staking was a bit too high for a newly deployed strategy. Lesson learned

2. `Strategy1` is a generic strategy that I'm keen to also deploy to football but it looks like this will require some elbow grease as I'll need to optimise some market (sport) dependent parameters

3. `Strategy0` trades all horse races regardless of country; `Strategy1` and `Strategy2` only do GB racing for now. I'd like to expand them to other countries but that's fairly low down the priority list

My focus for now will be `Strategy2`:



• `Strategy2` bets into markets that have multiple winners; think forecasts/exactas/trifectas/superfectas/quinellas etc.

• I'm doing some foundational work to both my shin ([https://github.com/mberk/shin](https://github.com/mberk/shin)) and kelly ([https://github.com/mberk/kelly](https://github.com/mberk/kelly)) packages to allow for markets with more than 1 winner. Everyone will be able to take advantage of these

• `Strategy2` currently only bets on the favourite. Being able to bet on all runners is the next step

Some general comments:



1. Knowing that I "had to" share this update was excellent motivation, as hoped. Without it, I might have dragged my heels over getting `Strategy2` deployed

2. Hopefully you will agree I've been remarkably open with this update but will also understand why certain details have been held back. You're welcome to ask my anything at all - whether I can give a straight forward answer is another matter. If you know some of the details from things I've previously told you in confidence then obviously please refrain from including these in your questions. If in doubt you can always DM me

3. My overall philosophy with these strategies is this: I'm hoovering up pennies. Hopefully there will be enough pennies that they will add up to something meaningful. There are inefficiencies every-fucking-where. Not necessarily massively profitable inefficiencies but it is free money being left on the table and the more of these I can hit as soon as possible the better my money printing machine will become. Having chunky profitable strategies to begin with helps as this encourages doing more volume

Fire away...

---

## 2025-01-02 12:09:45 - general channel

**Ankit Aggarwal**

Happy New Year! My goal is to deploy my first strategy!!

---

## 2025-01-01 23:54:51 - issues channel

**Brøndby IF**

[@U07AK2APF2B](@U07AK2APF2B) I used to do the same thing as you, creating a new one whenever another one reached the limit and generated the error, but honestly I get a bit anxious seeing several sessions started in the account history there at Betfair. So I decided to unify the session for all projects and created an exclusive trigger in one of the codes for keep-alive, so several days go by and a single session appears in the history, even helping me to monitor a possible account hack or something like that.

---

## 2025-01-01 19:18:17 - general channel

**Josh**

To deploy my first strategy!

---

## 2024-12-31 10:11:08 - random channel

**liam**

This is cool, I currently use bitbar for todays pnl/exposure/ec2 instances etc. But I could imagine this showing far more although not sure it would help focus 



[https://x.com/usetrmnl|https://x.com/usetrmnl](https://x.com/usetrmnl|https://x.com/usetrmnl)

---

## 2024-12-28 10:11:53 - random channel

**AndyL**

Thanks [@UUE6E1LA1](@UUE6E1LA1) yeah see what you mean 

Football im leaving as is at the moment as I don’t think I have enough data yet, ive just got one live football strategy i am leaving running for the moment 

---

## 2024-12-27 21:56:32 - random channel

**AndyL**

blimey, and I sniff at £40 a month on AWS servers !

---

## 2024-12-24 15:15:31 - strategies channel

**AndyL**

My live returns for this strategy over the year match the backtest roughly 

---

## 2024-12-24 09:58:52 - strategies channel

**foxwood**

Quite dramatic. Since you're offering I'd look for things that might have changed suddenly that could impact your signal(s). Latency, timing, server upgrade, software upgrade - faster/slower performance, cross matching are a few. Left field thought is that whoever you were feeding from pre-off went wholly inplay 6 months ago and affected you on both counts.

---

## 2024-12-16 08:12:24 - random channel

**Unknown**

Morning, its an interesting way to think about things. I know [@U4H19D1D2](@U4H19D1D2) posted this before and been meaning to test one of my strats (live production)

Id be interested to gain your opinion Liam. (this is in play 2seconds forward)

Ill need to check the scale of the EV, but it looks like im not placing many -ev. (On the cricket image you posted on the Ba forum it could be seen that the -ev bets had a minus PL so i assume you worked to strike these out?)

---

## 2024-12-14 17:08:57 - random channel

**Unknown**

[@UPMUFSGCR](@UPMUFSGCR) I get your point, feeling exactly the same at the moment, this is a new football strategy 3 month "backtest", went live 3 days ago and then starts going off a cliff!!

---

## 2024-12-12 21:03:31 - issues channel

**ian mcneill**

Hi - apologies noob noob trying to reboot attempts at this. I have a fail when trying to install flumine  as output below shows. I will read line by line and try to resolve myself but if anyone can ident my idiocity easily and point me in the right direction, that's great - I'll keep this window open :wink:



Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)

Building wheels for collected packages: pydantic-core

  Building wheel for pydantic-core (pyproject.toml) ... error

  error: subprocess-exited-with-error



  × Building wheel for pydantic-core (pyproject.toml) did not run successfully.

  │ exit code: 1

  ╰─&gt; [115 lines of output]

      Running `maturin pep517 build-wheel -i /Users/ian/miniconda3/envs/fex/bin/python --compatibility off`

      :package: Including license file "/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/LICENSE"

      :tropical_drink: Building a mixed python/rust project

      :link: Found pyo3 bindings

      :snake: Found CPython 3.13 at /Users/ian/miniconda3/envs/fex/bin/python

      :satellite_antenna: Using build options features, bindings from pyproject.toml

      :computer: Using `MACOSX_DEPLOYMENT_TARGET=11.0` for aarch64-apple-darwin by default

         Compiling autocfg v1.1.0

         Compiling proc-macro2 v1.0.76

         Compiling unicode-ident v1.0.10

         Compiling target-lexicon v0.12.9

         Compiling python3-dll-a v0.2.9

         Compiling libc v0.2.147

         Compiling once_cell v1.18.0

         Compiling version_check v0.9.4

         Compiling cfg-if v1.0.0

         Compiling static_assertions v1.1.0

         Compiling heck v0.4.1

         Compiling zerocopy v0.7.32

         Compiling parking_lot_core v0.9.8

         Compiling rustversion v1.0.13

         Compiling scopeguard v1.1.0

         Compiling lexical-util v0.8.5

         Compiling allocator-api2 v0.2.16

         Compiling smallvec v1.11.2

         Compiling tinyvec_macros v0.1.1

         Compiling serde v1.0.195

         Compiling tinyvec v1.6.0

         Compiling memchr v2.6.3

         Compiling equivalent v1.0.1

         Compiling unicode-bidi v0.3.13

         Compiling regex-syntax v0.8.2

         Compiling ahash v0.8.7

         Compiling num-traits v0.2.16

         Compiling num-integer v0.1.45

         Compiling lock_api v0.4.10

         Compiling memoffset v0.9.0

         Compiling num-bigint v0.4.4

         Compiling lexical-write-integer v0.8.5

         Compiling lexical-parse-integer v0.8.6

         Compiling unicode-normalization v0.1.22

         Compiling lexical-write-float v0.8.5

         Compiling lexical-parse-float v0.8.5

         Compiling aho-corasick v1.0.2

         Compiling indoc v2.0.4

         Compiling serde_json v1.0.109

         Compiling percent-encoding v2.3.1

         Compiling unindent v0.2.3

         Compiling form_urlencoded v1.2.1

         Compiling regex-automata v0.4.3

         Compiling lexical-core v0.8.5

         Compiling idna v0.5.0

         Compiling ryu v1.0.14

         Compiling itoa v1.0.8

         Compiling uuid v1.6.1

         Compiling base64 v0.21.7

         Compiling url v2.5.0

         Compiling regex v1.10.2

         Compiling quote v1.0.35

         Compiling syn v2.0.48

         Compiling pyo3-build-config v0.20.2

         Compiling getrandom v0.2.10

         Compiling parking_lot v0.12.1

         Compiling hashbrown v0.14.3

         Compiling pyo3-ffi v0.20.2

         Compiling pyo3 v0.20.2

         Compiling pydantic-core v2.16.3 (/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571)

         Compiling indexmap v2.0.0

      error: failed to run custom build command for `pydantic-core v2.16.3 (/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571)`



      Caused by:

        process didn't exit successfully: `/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/target/release/build/pydantic-core-39a2e9245b501e5a/build-script-build` (exit status: 101)

        --- stdout

        cargo:rustc-cfg=Py_3_6

        cargo:rustc-cfg=Py_3_7

        cargo:rustc-cfg=Py_3_8

        cargo:rustc-cfg=Py_3_9

        cargo:rustc-cfg=Py_3_10

        cargo:rustc-cfg=Py_3_11

        cargo:rustc-cfg=Py_3_12

        cargo:rustc-cfg=Py_3_13

        cargo:rerun-if-changed=python/pydantic_core/core_schema.py

        cargo:rerun-if-changed=generate_self_schema.py



        --- stderr

        Traceback (most recent call last):

          File "/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/generate_self_schema.py", line 192, in eval_forward_ref

            return type_._evaluate(core_schema.__dict__, None, set())

                   ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

        TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'



        During handling of the above exception, another exception occurred:



        Traceback (most recent call last):

          File "/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/generate_self_schema.py", line 240, in &lt;module&gt;

            main()

            ~~~~^^

          File "/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/generate_self_schema.py", line 210, in main

            value = get_schema(s, definitions)

          File "/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/generate_self_schema.py", line 54, in get_schema

            return type_dict_schema(obj, definitions)

          File "/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/generate_self_schema.py", line 152, in type_dict_schema

            field_type = eval_forward_ref(field_type)

          File "/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/generate_self_schema.py", line 195, in eval_forward_ref

            return type_._evaluate(core_schema.__dict__, None)

                   ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^

        TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'

        thread 'main' panicked at [http://build.rs:29:9|build.rs:29:9](http://build.rs:29:9|build.rs:29:9):

        generate_self_schema.py failed with exit status: 1

        note: run with `RUST_BACKTRACE=1` environment variable to display a backtrace

      warning: build failed, waiting for other jobs to finish...

      :boom: maturin failed

        Caused by: Failed to build a native library through cargo

        Caused by: Cargo build finished with "exit status: 101": `env -u CARGO MACOSX_DEPLOYMENT_TARGET="11.0" PYO3_ENVIRONMENT_SIGNATURE="cpython-3.13-64bit" PYO3_PYTHON="/Users/ian/miniconda3/envs/fex/bin/python" PYTHON_SYS_EXECUTABLE="/Users/ian/miniconda3/envs/fex/bin/python" "cargo" "rustc" "--features" "pyo3/extension-module" "--message-format" "json-render-diagnostics" "--manifest-path" "/private/var/folders/b6/6bn7gvx54lv2p4kjxcdz2s0h0000gn/T/pip-install-9p34jo51/pydantic-core_97eb83b395414c3f9cf1c80056b9a571/Cargo.toml" "--release" "--lib" "--crate-type" "cdylib" "--" "-C" "link-arg=-undefined" "-C" "link-arg=dynamic_lookup" "-C" "link-args=-Wl,-install_name,@rpath/pydantic_core._pydantic_core.cpython-313-darwin.so"`

      Error: command ['maturin', 'pep517', 'build-wheel', '-i', '/Users/ian/miniconda3/envs/fex/bin/python', '--compatibility', 'off'] returned non-zero exit status 1

      [end of output]



  note: This error originates from a subprocess, and is likely not a problem with pip.

  ERROR: Failed building wheel for pydantic-core

Failed to build pydantic-core

ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (pydantic-core)

---

## 2024-12-12 13:48:09 - general channel

**liam**

nah, if flumine has live orders it snaps the order stream every 0.25?s

---

## 2024-12-12 13:46:17 - general channel

**Mo**

Can anyone share a flumine pattern for cancelling an order X seconds after going live if it hasn't been matched?

---

## 2024-12-02 10:47:23 - general channel

**Alex A**

It probably would be possible for the right employee to do that, as AWS say

```We do not access, use, or share customer data without your agreement, except as required to prevent fraud and abuse, or to comply with law, as described in our Customer Agreement.```

You could try encrypting all your software and data on the VPS, but at some point you will have to run your program, and someone who controls that VM or the underlying machine would be able to access that. Personally I make no effort to protect anything I have running on any VPS from the provider, and am not worried in the least about it. I’d guess you’re more likely to have your personal desktop stolen or drive cloned by someone who knows you have a betting strategy, which is also not something I go to any effort to stop.

---

## 2024-11-30 10:48:07 - strategies channel

**Mo**

Second related question: what is your workflow for going from idea to live trading strategy? Mine is usually:



1. Get told about an idea ([@U012XF5CNPN](@U012XF5CNPN) leaking alpha) or reading about one (in a book that [@UPMUFSGCR](@UPMUFSGCR) has dismissed out of hand)

2. Validate idea by doing some basic analysis on summary data eg BSP data

3. Confirm idea looks good in a backtest, mainly looking at the equity curve (is it up and to the right, how do the drawdowns and other variance characteristics look)

4. Validate backtest by live trading small stakes and comparing live trading results to backtests on the same events

5. Scale up stakes in line with live trading performance



---

## 2024-11-30 10:20:51 - strategies channel

**Mo**

Related question: how many hours of work is it normally for you to go from idea to profitable live trading strategy?

---

## 2024-11-28 14:26:02 - general channel

**birchy**

My production setup is currently 2.6.0 and I upgraded that from 2.4.x and had no issues, so can't see it being any different for you.

---

## 2024-11-27 15:58:23 - issues channel

**liam**

yeah or



```if market.blotter.strategy_selection_orders(self, selection_id, 0, order_status=LIVE_STATUS):

    # cook```

---

## 2024-11-27 15:48:32 - issues channel

**Mo**

Something like



```if any(

    order.status in LIVE_STATUS or order.size_matched &gt; 0

    for order in market.blotter.strategy_selection_orders(

        self, selection_id, 0

    )

):

    # Do cool shit```

?

---

## 2024-11-27 15:18:47 - issues channel

**Mo**

In flumine, what's the canonical way to check if a runner has either a live order or a filled order?

---

## 2024-11-27 08:34:43 - issues channel

**Tom**

Exactly that's why I think its the betfair API/Server

---

## 2024-11-27 08:31:20 - issues channel

**liam**

whats a server/API rule? The logs will tell you if your code is the problem re the latency warnings

---

## 2024-11-26 10:40:35 - strategies channel

**Unknown**

I do not know if that applies to your workflow, but I implemented scripting to my backtest/analyze app, allowing me on live data or on specifically selected market/s to make live coding and testing. You can see that on the screenshot above, that Console view where I can type code and execute it.

---

## 2024-11-25 11:15:00 - strategies channel

**Joe**

[@UBS7QANF3](@UBS7QANF3) To answer 1-5 I have an IO front end with a little dsl that can extract data from the compressed json stream files, fast enough like [2min per year of global racing, but this is largely because there is minimal impedance mismatch between IO and compute and memory in my implementation, it is using very fast local storage and lots of cores for parallel decompression, this is the same point that <@US2RWCWKY](2min per year of global racing, but this is largely because there is minimal impedance mismatch between IO and compute and memory in my implementation, it is using very fast local storage and lots of cores for parallel decompression, this is the same point that <@US2RWCWKY) was making.



To back test / train / ML that same IO front end is used but to fill memory structures whose contents are 'implied' by the strategy being trained and shape/order optimised for the hardware (e.g. gpu structures will be totally different shape to cpu). In other words it only keeps the answers to the questions it is likely to need during training and puts them in the memory location that will guarantee the lowest latency during training given the expected order those questions will be asked in. This is the data equivalent to dead code removal during compilation followed by compile time optimisation. The training process then only iterates over these memory structures at high frequency. To do this and ensure that the 'logic' being trained will behave identically when run in production on live streams requires everything be written against interfaces that are implemented multiple times. This is similar to what [@U012XF5CNPN](@U012XF5CNPN) alluded to in point 3. It could also be thought of as a very application specific cache.



This would be quite a departure from how Flumine works where there is no real change between live and back testing other than inserting the simulated matcher.

---

## 2024-11-24 17:05:52 - general channel

**Josh**

Hi all, noob here. Still able to bypass the £299 fee for live API Key (with the help from Liam/Slack group) or is it a case of paying the fine? 



Cheers



---

## 2024-11-23 20:33:14 - strategies channel

**Jhonny**

I'm new to this, but I store live esa and historical data in xtdb. previously, it was just in an binary format, which meant I had to read the whole file to query the market. xtdb time slices are pretty much instant, and allows me to play with many markets at once. drawback is it takes quite the amount of disk space

---

## 2024-11-23 14:24:31 - general channel

**Elie Couttet**

Exception: Request failed for [https://api.betfair.com](https://api.betfair.com) returned code 403. Truncated server response: [!DOCTYPE html](!DOCTYPE html)

&lt;!--[if lt IE 7]&gt; &lt;html class="no-js ie6 oldie" lang="en-US"&gt; &lt;![endif]--&gt;

&lt;!--[if IE 7]&gt;    &lt;html class="no-js ie7 oldie" lang="en... (use muteHttpExceptions option to examine full response)

jsonRestExample@

---

## 2024-11-23 09:32:42 - random channel

**James**

What health checks have you built into your Flumine containers/deployments?

---

## 2024-11-16 23:55:25 - random channel

**James**

Yeah I write my code locally on mac, and run my logic testing debugging locally. For my sims I use docker compose and run it in a container that mirrors my deployed.

---

## 2024-11-16 07:49:19 - random channel

**Jonjonjon**

Do you use docker to build a python env that is consistent with your deployment? I've been trying to read into that.

---

## 2024-11-16 00:26:44 - random channel

**James**

I just use python debian.



```FROM python:3.11-bookworm```

I was running Alpine for slimmer images and faster deployments but I found the convenience of debian just worked better for me.

---

## 2024-11-15 16:30:51 - general channel

**D C**

Yesterday I was clearly connected to a different source/server for my bots. One was almost flawless and the other very bitty (but still better than the recent problems - worst dropout was 45 seconds I think). It still mystifies me why but I'll take it over the car crash of last week

---

## 2024-11-07 09:46:00 - random channel

**Derek C**

Going long oil after a Trump win is an interesting strategy. I would have gone the opposite way, based on likely increases in US production and Trump’s previous involvement with the Abraham accords. Time will tell.

---

## 2024-11-05 06:22:40 - issues channel

**anomaly**

Hi all, quick question regarding betfairlightweight and getting market clarifications for the historical stream. For the live api when i query the APIClient object i have to specify the market_projection, like as follows



```        trading_client.login()

        market_catalogues= trading_client.betting.list_market_catalogue(

            ...

            market_projection=[

                'MARKET_START_TIME',

                'RUNNER_DESCRIPTION',  

                'MARKET_DESCRIPTION', 

            ],

            ...

            )```

and everything works as expected and i can access the clarifications.



Is it possible to also get the market catalogues for the historical streaming (like in examples/examplestreaminghistorical.py). I am only able to retrieve the marketbook right now and it's not clear how I would go about accessing the market catalogues (specifically, clarifications)



cheers!

---

## 2024-11-04 10:48:18 - issues channel

**Lee**

Testing upgrades in production, nice

---

## 2024-11-01 21:36:56 - general channel

**Unknown**

OK thanks. I just assumed these were acronyms of some server side tech I'd never heard of. So you heard they'd released a UDP fix Monday? Doesn't look like it's helped much. My logs are shocking from yesterday (should add that these times include a dropout that contains the inplay suspension event).

---

## 2024-11-01 03:32:12 - general channel

**Phydeaux**

I have a "home lab" for other reasons, with NAS and a decent amount of compute so I've not deployed to the cloud. I am rural so have a good amount of solar and batteries, so even the electricity is free. The capex was relatively chunky, but built for other reasons, so the marginal cost to use it for betting / analysis was basically zero apart from my time.



Network latency isn't a concern for my strategy, but if it was I'd do a little testing to find which GCP/AWS/other zone is "closest".

---

## 2024-10-31 15:54:00 - general channel

**Jhonny**

Hi everyone, I keep getting NOT_AUTHORIZED error when trying to use the esa api. I haven't used it in about 2 months; my live key just got accepted and it's my first time trying it out:



```:event {:op connection, :connectionId 206-311024154911-133459}

:event {:op status, :statusCode FAILURE, :errorCode NOT_AUTHORIZED, :errorMessage Connection is not authenticated: MarketSubscriptionMessage{marketFilter=MarketFilter{marketIds=[1.235241223], bspMarket=null, bettingTypes=null, eventTypeIds=null, eventIds=null, turnInPlayEnabled=null, marketTypes=null, venues=null, countryCodes=null, raceTypes=null}, marketDataFilter=com.betfair.platform.exchange.stream.api.domain.market.MarketDataFilter@70734c3a, initialClk='null', clk='null', conflateMs=null, heartbeatMs=null}, :connectionClosed true, :connectionId 206-311024154911-133459}

:stream-closed```

Last I tested with the delayed key, it worked. However, both live and delay keys are currently not working. Would appreciate anyone's help, thanks

---

## 2024-10-31 10:28:25 - general channel

**James**

Following on from Paul’s recommendation, I work with AWS in my day job extensively but don’t have the same limitations he does. Lightsail and Fargate serverless containers, and s3 for storage. Along with using a neon Postgres or digital ocean Postgres (for no cold start) is what I use for my market catalogues and betting records. Most of your record keeping can happen in background threads and can afford to be a little slower. 



Lightsail and neon are a bit more batteries included and a good place to start, and less knobs go turn. Fargate and a dedicated DB can let you squeeze some more out as you get deeper into it. 



This setup means my costs are low. I run my sims on my own hardware, even if slow it doesn’t cost me anything except electricity :relaxed: 

---

## 2024-10-31 10:12:30 - general channel

**Paul**

If using AWS, you're still left with choices. I've been thinking about writing up some stuff on this, but it's complicated because I work for AWS, and if I discuss or write anything in this space I have to a) make it clear I'm doing so in a personal capacity, not as an employee and b) still get it checked by PR, Legal, and "bar raised" by a bunch of solution architects.



Putting all that aside, except please, accept this is a personal recommendation, not an official one: Fargate, DynamoDB and Aurora all allow you to forget about a lot of BS you probably don't want to spend time thinking about. Archiving to S3 is :+1:, but querying it with Athena for most people is :money_with_wings: compared to getting it into a DB. Consider Compute Savings Plans and Reserved Instances to bring down costs significantly. Also, if you want to ignore that and go simple, it's OK to throw up EC2 instances for a while and work it out or even look at Lightsail and App Runner if you want to, and then optimise later.

---

## 2024-10-31 07:50:50 - general channel

**James**

AWS Ireland tends to be best for latency from what I’ve seen. 

---

## 2024-10-31 07:33:34 - general channel

**liam**

The majority use AWS however I would use what ever you have experience in

---

## 2024-10-30 23:24:30 - general channel

**ian mcneill**

Hi all, is there a consensus on the infrastructure to use, for development and then production - DigitalOcean, AWS, Lightsail etc.?

---

## 2024-10-30 09:37:05 - strategies channel

**Unknown**

```if runner_context.invested or runner_context.live_trades:

   [http://logger.info|logger.info](http://logger.info|logger.info)(f"{runner_name} has Live Trades!")

   continue ```

this code avoids logging an error. 12:02:18,536:INFO:Gloze has Live Trades!

---

## 2024-10-30 09:25:03 - strategies channel

**Terry**

```GREYS_2 = MyModel_LAY(

    market_filter=marketfilter,

    max_order_exposure=50,

    max_trade_count=1,

    max_live_trade_count=1, 

    max_selection_exposure=50,

    conflate_ms=1500,

    name="greys_LAY",

    

)```

---

## 2024-10-30 09:11:58 - strategies channel

**liam**

What are your strategy settings? (and logs will tell you)



```strategy = LowestLayer(

    market_filter={"markets": markets},

    context={"stake": 2},

    max_live_trade_count=2

)```

---

## 2024-10-30 09:10:49 - strategies channel

**Terry**

Can anyone offer any advice on this one?



i am using this to check if any orders exist before placing and order:



runner_context = self.get_runner_context(market.market_id, SEL, 0)



```# Check if there are any active investments or trades for the runner

if runner_context.invested or runner_context.live_trades:

    [http://logger.info|logger.info](http://logger.info|logger.info)(f"{runner_name} has Live Trades!")

    continue  # Skip placing an order if there is already an active investment or trade



else:

    SIDE = "LAY"

    order = trade.create_order(

    side=SIDE, 

    order_type=LimitOrder(

        size=LSTK,

        price=LPC,

        time_in_force='FILL_OR_KILL'

    )



    market.place_order(order)```

however if the order is killed, then flumine still thinks and order is placed and doesnt fire off another. I am sure theres a better way to achieve this!

---

## 2024-10-29 16:40:01 - issues channel

**liam**

Why is this a problem / why are you mixing closed markets with live?

---

## 2024-10-28 10:39:50 - general channel

**James**

Thanks [@U07AK2APF2B](@U07AK2APF2B) I never realised there was a difference between the live data and the betfair historical data. Thanks.

---

## 2024-10-27 12:39:04 - general channel

**Jhonny**

Hi everyone,

How long does it take to get approval for the live API key?

I bought historical data for this month, hoping to use it in conjunction with the api (since it's not allowed to simply listen to markets without betting); However, I haven't gotten any update about my live api key; Not even an acknowledgement of my request. The month is as good as gone

---

## 2024-10-26 16:33:35 - general channel

**Trex44**

Any football traders out there have a good recommendation for a live and historical data provider? Preferably one with UTC time stamps for events?

---

## 2024-10-25 08:39:16 - strategies channel

**liam**

From the AUS team



&gt; NEW CONTENT   

&gt; Just in time for the weekend, Parts 1 &amp; 2 of our new series "How To Build A Soccer Bot" are now live on the Automation Hub! 

&gt; Part 1: [https://betfair-datascientists.github.io/modelling/howToBuildASoccerBotPartI/](https://betfair-datascientists.github.io/modelling/howToBuildASoccerBotPartI/) 

&gt; Part 2: [https://betfair-datascientists.github.io/modelling/howToBuildASoccerBotPartII/](https://betfair-datascientists.github.io/modelling/howToBuildASoccerBotPartII/)

---

## 2024-10-24 13:40:36 - issues channel

**jp**

Good question, I was looking at the PT field for price updates, and comparing with the local time at my server

---

## 2024-10-21 21:19:41 - issues channel

**NAS**

```betfairlightweight.exceptions.SocketError: [Connect: 1002]: Connection closed by server```

```2024-10-21 13:05:15,865 - flumine.streams.marketstream - INFO - Starting MarketStream 2001

2024-10-21 13:05:15,866 - betfairlightweight.streaming.listener - INFO - [Register: 2002]: Unique id updated on listener and stream

2024-10-21 13:05:15,985 - flumine.streams.marketstream - ERROR - MarketStream 2001 run error```

```2024-10-21 13:05:17,057 - flumine.streams.orderstream - INFO - Starting OrderStream 1001

2024-10-21 13:05:17,057 - betfairlightweight.streaming.listener - INFO - [Register: 1002]: orderSubscription

2024-10-21 13:05:17,057 - betfairlightweight.streaming.listener - WARNING - [Listener: 1002]: stream already registered, replacing data

2024-10-21 13:05:17,057 - betfairlightweight.streaming.stream - INFO - [OrderStream: 1002]: "OrderStream" created

2024-10-21 13:05:17,166 - flumine.streams.orderstream - ERROR - OrderStream 1001 run error```

idk if i've seen them before, but got a lot of them today? someone else? connections available.

---

## 2024-10-20 15:00:54 - general channel

**Jhonny**

Hi all, I'm new here, so pardon me if this is a noob question.

Is there a way to get a live market ndjson stream without having to to pay 300gbp for the api?

I need to test my strat before making such a big commitment. Thanks



side note: wow, I didn't know there was this big of a bf coders community; sadly, I've already needlessly reimplemented some stuff in a different prog lang.

---

## 2024-10-17 11:32:30 - issues channel

**NT**

I have, I'm just not the greatest coder and need some time to figure out how to use it appropriately in my algorithm. Lack of coding skill is honestly probably the root cause of my keep alive issues too. Thankfully, other than the keep alive, the code is running quite well and generating profit.



It is definitely on my to do list to learn how to use flumine to get a stream reading of prices to a dataframe for use on my algorithm. It would save me having to call the BFLW API on a loop.

---

## 2024-10-17 09:38:59 - issues channel

**NT**

Thanks. I've struggled to get the detail I wanted from those docs, but I'll give a close review along with the BFLW wrapper. Hopefully this solves my keep alive function issues!

---

## 2024-10-17 09:21:58 - issues channel

**NT**

Rookie question, is there a limit to how many times you can call trading.keep_alive()?



I am trying to keep my session alive to both: 1) read price data, and 2) make trades.



My strategy for doing this has been to: 1) call the keep alive endpoint and 2) use trading.keep_alive().



I'm pretty new to the betfairlightweights library, so sorry if I've missed something obvious on where to find this information!

---

## 2024-10-14 10:33:11 - strategies channel

**liam**

Weird as it lets me search it but then the link doesn't work



&gt; 

&gt; 

&gt; *liam*  [3:24 PM]

&gt; Does anyone know how much Decimal charge for the Cricket feed?

&gt; 49 replies

&gt; 

&gt; 

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [3:55 PM]

&gt; I'm on the hunt for a decent feed too!

&gt; 

&gt; 

&gt; 

&gt; *D C*  [4:02 PM]

&gt; Is it just a score feed? Is Betfair's not up to scratch?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [4:05 PM]

&gt; No comment

&gt; 

&gt; 

&gt; 

&gt; *D C*  [4:06 PM]

&gt; Understood

&gt; 

&gt; 

&gt; 

&gt; *liam*  [9:10 AM]

&gt; I have sent an email, will share the price if they do however I fear it might be out of my budget

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [9:56 AM]

&gt; I can give you a personal introduction if you don’t get anywhere

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [11:22 AM]

&gt; I think the main issue, not just for cricket, is Betfair's inability to use the widely accepted team or players names.

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [3:15 PM]

&gt; And the inability to match names strikes again :expressionless:

&gt; 

&gt; 

&gt; 

&gt; *liam*  [3:36 PM]

&gt; It's a complete fucking joke tbh, almost 3 years this has been in 'production'

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [3:48 PM]

&gt; you would have thought, it being the same team causing the issue as yesterday, they might have pre-empted it .....

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [3:49 PM]

&gt; what issues are you seeing [@U4H19D1D2](@U4H19D1D2)?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [3:51 PM]

&gt; The usual ongoing mapping issues and there is some sort of bug with the Lancashire game which is pushing out about a million updates per second on certain connections, seems like they just don't care

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [6:11 PM]

&gt; Wow, looks like this is bringing down my entire scraping infrastructure

&gt; 

&gt; 

&gt; 

&gt; *liam*  [6:19 PM]

&gt; Try restarting, doesn’t seem to be on every connection

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [6:21 PM]

&gt; Better but the damage has already been done

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [6:22 PM]

&gt; nice to know the cricket feed could take down all my recording

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [6:23 PM]

&gt; Well your mileage may vary depending on how much disk space your box has and whether you have appropriate alerts set up

&gt; 

&gt; 

&gt; 

&gt; *D C*  [6:24 PM]

&gt; Is this a recent problem or  long term? Just wondering if it's related to my recent server reboot as I am recording the cricket score feed on said server.

&gt; 

&gt; 

&gt; 

&gt; *liam*  [7:53 PM]

&gt; Recent, started today at 11:45ish, apparently now fixed

&gt; 

&gt; 

&gt; 

&gt; *liam*  [8:21 AM]

&gt; Looks like either it was never fixed or it started happening again today at about 1am

&gt; 

&gt; [8:22 AM]

&gt; 

&gt; This file is hidden because it was uploaded more than 90 days ago. [https://betcode-org.slack.com/plans?feature=unlimited_messages|Upgrade to a paid subscription](https://betcode-org.slack.com/plans?feature=unlimited_messages|Upgrade to a paid subscription) to view.

&gt; 

&gt; 

&gt; 

&gt; 

&gt; *Robert*  [12:10 PM]

&gt; My machine must have been smoking, I wonder what's going on... (edited)

&gt; 

&gt; 

&gt; 

&gt; *liam*  [12:29 PM]

&gt; eventide issues apparently

&gt; 

&gt; [http://docs.eventide-project.org/](http://docs.eventide-project.org/)

&gt; 

&gt; 

&gt; *[http://docs.eventide-project.org|docs.eventide-project.org](http://docs.eventide-project.org|docs.eventide-project.org)*

&gt; *[http://docs.eventide-project.org/|Eventide](http://docs.eventide-project.org/|Eventide)*

&gt; Pub/Sub, Event Sourcing, Evented Microservices

&gt; 

&gt; 

&gt; 

&gt; *Gooat*  [6:48 PM]

&gt; Any ball park price numbers come back?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [7:03 PM]

&gt; Nope

&gt; 

&gt; 

&gt; 

&gt; *liam*  [11:29 AM]

&gt; For those interested

&gt; 

&gt;&gt; You are more than welcome to come direct but we do have a minimum fee for direct customers that may be prohibitive to you of £2k per month and £150 per game.

&gt; 

&gt; 

&gt; 

&gt; *D C*  [11:46 AM]

&gt; 2K a month AND £150 a game. No idea if that's cheap or expensive to be fair

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [11:48 AM]

&gt; Expensive compared to other sports but there’s easily enough liquidity in a match to extract that much EV

&gt; 

&gt; 

&gt; 

&gt; *liam*  [12:18 PM]

&gt; I read that as £150 a game, minimum 2k a month.

&gt; 

&gt; Update on the betfair feed is that I have continued hounding Neil to the point they have made some changes to the 'team' lots of politics/stupid code freezes but things have improved

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [12:20 PM]

&gt; I noticed last month there was a big jump in quantity of matches with the feed. Up around 100 matches.

&gt; 

&gt; [12:20 PM]

&gt; Is that seasonal or from the improvements?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [12:21 PM]

&gt; Both but we are now down to missing about 15% of the matches due to mapping, there where times last year where it was around 50%

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [12:22 PM]

&gt; Big improvement

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [2:03 PM]

&gt; How do you work out which markets are missing the feed?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [2:09 PM]

&gt; nightly script to reconcile between market/feed data

&gt; 

&gt; 

&gt; 

&gt; *Lee*  [2:12 PM]

&gt; are we expecting every cricket match to have the feed?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [2:13 PM]

&gt; pretty much, expect sub ~3% to not have a feed depending on what betfair put up

&gt; 

&gt; 

&gt; 

&gt; *Gooat*  [5:38 PM]

&gt; Has anyone levelled up to big stakes from the feed?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [5:58 PM]

&gt; I am up to a few k exposure per market

&gt; 

&gt; 

&gt; 

&gt; *Gooat*  [6:02 PM]

&gt; Nice. Model or mix of price and flow? I struggled to avoid averse selection when testing it.

&gt; 

&gt; 

&gt; 

&gt; *liam*  [6:08 PM]

&gt; model

&gt; 

&gt; 

&gt; 

&gt; *Gooat*  [9:28 PM]

&gt; How did that game go?

&gt; 

&gt; 

&gt; 

&gt; *Sen*  [2:32 PM]

&gt; On this note - [@U4H19D1D2](@U4H19D1D2) how do you know Decimal is the best data provider in the cricket space? Something you've been able to test? (edited)

&gt; 

&gt; 

&gt; 

&gt; *liam*  [2:36 PM]

&gt; I don't, its just the one betfair provide

&gt; 

&gt; 

&gt; 

&gt; *Mo*  [2:37 PM]

&gt; I can say that the people behind it have been involved in cricket data for at least a decade

&gt; 

&gt; 

&gt; 

&gt; *Sen*  [3:15 PM]

&gt; Another option is sportmonks - they look pretty decent too

&gt; 

&gt; 

&gt; 

&gt; *liam*  [3:15 PM]

&gt; Yeah, considerably cheaper

&gt; 

&gt; 

&gt; 

&gt; *D C*  [4:31 PM]

&gt; Has the Betfair cricket feed not improved ?

&gt; 

&gt; 

&gt; 

&gt; *liam*  [7:23 PM]

&gt; Yes it has but it’s been hard work (for myself and Neil)



---

## 2024-10-05 07:34:37 - general channel

**liam**

I was going to just kill every x seconds if no live data coming through, might be able to create something this afternoon 

---

## 2024-10-03 18:56:22 - issues channel

**Matthew Lawrence**

Hi, I'm trying to login to betfair api. i have previously been using the interactive login. when I created a docker and access all files and parameters from amazon AWS, this login stops working and gives me ssl error. I'm a bit confused by this as i believed interactive login didnt require certs and ssl. i have since tried using the non interactive login but no matter what i get this error: Exception: HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:3900)'))). Any help or advice would be greatly appreciated.

---

## 2024-10-03 15:14:00 - general channel

**Newbie99**

Sorry [@U4H19D1D2](@U4H19D1D2), I'm not really understanding the error, the affected line is:



```framework.run()```

What should the syntax be, maybe I'm misunderstanding the error, but I thought this section relates to the Flumine package rather than my code, so I'm a bit confused:



```  File "/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/strategy/strategy.py", line 279, in start

    s.start(flumine)

TypeError: start() takes 1 positional argument but 2 were given ```

---

## 2024-10-03 13:33:01 - general channel

**Newbie99**

I get this post upgrade:



```  File "/home/ec2-user/trading/env/projects/startup.py", line 208, in startup

    framework.run()

  File "/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/flumine.py", line 27, in run

    with self:

  File "/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/baseflumine.py", line 450, in __enter__

    self.strategies.start(self)

  File "/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/strategy/strategy.py", line 279, in start

    s.start(flumine)

TypeError: start() takes 1 positional argument but 2 were given```



---

## 2024-09-25 21:04:14 - general channel

**AndyL**

probably AWS VM os difference...

maybe best to run in docker and avoid all this mess!

---

## 2024-09-25 20:54:57 - general channel

**AndyL**

Hi, I am trying to setup a new AWS Ubuntu 24.04 VM, and have all the python bits and pieces setup, and my certs folder, but am getting the following SSL error. I'm sure i've had this before, but can't remember how I resolved it, any ideas anyone? I'm thinking maybe an OpenSSL problem?

```HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by SSLError(SSLError(524297, '[SSL] PEM lib (_ssl.c:3895)'```

---

## 2024-09-21 18:20:36 - random channel

**TT**

If you are using flumine and aws could you just store them in ssm parameter store and poll it periodically with a worker?

---

## 2024-09-21 13:04:23 - general channel

**TT**

How do people typically go about logging/monitoring latency? In particular the latency between betfair's publish time and the clock time when the update is received?



I can think of a couple of ways of doing it:

1. In the market recorder you could log it on each update [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py#L69|here](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py#L69|here). (Might be a bit overkill for each update but it would be complete.)

2. Have a background worker log the latency metric to Cloudwatch Metrics/Prometheus etc every x seconds.

Its not something I've been recording but if you want to optimise it (by moving aws region/vps etc) then its important to start tracking it.

---

## 2024-09-18 15:09:48 - issues channel

**sartux**

good morning, I'm back to programming with betfairlightweight, but I wanted to ask you if the API policies have changed because I can no longer download the live odds

---

## 2024-09-16 02:41:42 - issues channel

**Herugrim**

Question: If a Keep Alive is used and connections are not closed correctly, will they remain active until the Keep Alive duration expires? (And therefore increase the risk of MAX_CONNECTION_LIMIT_EXCEEDED errors? And if so, is there a function to end all active sessions?

---

## 2024-09-14 13:14:17 - general channel

**Rob**

I'd second mlflow, as much for experiment tracking as for holding models.



As much code as possible is shared between strategies, and I structure the code so the same strategy class is used for live/backtesting.



I use notebooks for analysis only.

---

## 2024-09-14 08:52:36 - general channel

**Justice**

[@UUE6E1LA1](@UUE6E1LA1) To sum up my current workflow: data analysis, visualization, model development, backtesting all done in Python (backtesting done directly in a polars dataframe, not with flumine). All this is just strewn around in various notebooks with no real thought. Export trained model via ONNX to C# codebase for production use

---

## 2024-09-14 08:39:04 - general channel

**Justice**

[@UUE6E1LA1](@UUE6E1LA1) For production I use C#, which is my preferred language. Python is just objectively better for machine learning and working with data imo

---

## 2024-09-13 21:43:52 - general channel

**liam**

Is there an answer to this? 



My notebooks are a complete mess, I come back from lunch and it takes me a good 10 minutes to work out what the hell I was doing. However my simulation / production code is tidy, this is where having flumine run the same code simulation / live is one of my favourite parts. I don’t think Jupiter / analysis ever can be clean/tidy 

---

## 2024-09-13 20:19:25 - general channel

**foxwood**

[@U05N9773A23](@U05N9773A23) PyCharm runs the scripts for test/live itself in a tool window with interactive debug etc. They do a free community edition that does all of that stuff - can't remember why I upgraded to Pro - think it was just for the sql/db stuff which is not as good imho as ssms/redgate I discovered. Worth a look unless you're a dedicated 'xer - I gave all that up together with the wonders of vi a long long time ago.

---

## 2024-09-13 19:56:02 - general channel

**Paul**

I’m crazy in that I will use parameters in file names. So for example if I have a cricket model that trades 3 ticks off the last traded it could live in cricket_inplay_3t.py and so I can quickly create multiple variants with a file cp to a new name. That’s a very simplistic version of what I’m actually doing but broadly right.

---

## 2024-09-12 18:48:45 - strategies channel

**Justice**

[@U4H19D1D2](@U4H19D1D2) Yeh I just have level stake fixed liability at the moment. I used to use flumine. I have my own trading framework written in C# for production code. And I do backtesting using a custom Rust plugin for polars dataframes

---

## 2024-09-12 15:54:58 - general channel

**Ankit Aggarwal**

Hello everyone, I have recently joined the exchange and while researching the sample code and other references I happen to come across Flumine and this community. My sincere appreciation to the creator and contributors of flumine its made life a lot easier compared to the API docs and sample code of the exchange. I am an engineer with coding experience at a software company and trading is my passion/escape. Could anyone help me with the keys for live API access? Any suggestions on how to go about it.

---

## 2024-09-05 19:04:15 - general channel

**Rob**

I'm just about ready to use flumine for live betting for the first time (rather than simulation).



Based on what I've read here, I know I'm doing more heavy computation that others. I'm happy with the speed of simulation but that's using 10 cores, and I'm not clear if flumine can use >1 core when running live? If a `process_market_book` call is still running when the next update arrives, what happens?



I'll try it tomorrow night so I guess I can also find out that way :slightly_smiling_face:

---

## 2024-09-04 19:33:20 - general channel

**Sam G**

Hi [!here](!here) i have been a relatively quiet watcher on this slack workspace - and want to dive in more and learn about the ins and outs of sports betting coding. I have been a python programmer for over 12 years, and some C++ and Java before that ( was in high frequency trading in finance sector). I have a property tech startup that I am winding down (due to business profitablity slowing down) and I want to go back to my love of quant + programming in sports betting, an area that I have slowly picked up some interest in. I also learnt Rust for last 1.5 years, used it on my own startup (which really didnt need Rust but used it to learn) and want to see if I can use Rust knowledge in bet-code



I also helped start a crypto trading firm (so have some relatively newer experience in programming for excahnge trading - around 2018-2020 - was working part time with them)



I learnt a bit of the betfair api - but not much,and want to dive in.



*questions:*

1. What are the best resources to learn production level (or near production level) sports betting. Are there good videos or paid training material that explains a lot of things in one place? 

2. Is someone looking for a partner to do it with - perhaps someone who knows more about the sports betting side of things, and some programming experience - and I can help them with my knowledge in programming ?

Will be good to help others and learn in the process



Thanks again

---

## 2024-09-04 19:09:38 - strategies channel

**A**

I think I’ve been following a similar path to you [@U01PJ5YMFBJ](@U01PJ5YMFBJ) - exploring various signals using purely price/volume/queued data, and drawing blanks when it comes to finding any positive expected value. I even had a little foray into machine learning, but could never find anything that would spit out EV of anything slightly above 0.



Frustrating, as I feel I have everything else nicely set up (production envs, reporting, data, alerting, post analysis charts, etc) - just missing the value part (retrospectively, I know I shouldn’t have invested so much time into the ‘fun’ stuff before finding an edge…).



I don’t think I’m ready to give up just yet, but I’m starting to feel disheartened - feeling like I’m just not smart or creative enough to compete.



You mentioned something along the lines of going back and tuning your “already profitable algorithm” - I’m curious on how that’s any different from what you’ve been trying already? Congrats on finding profit anyway - more than I’ve got so far :rolling_on_the_floor_laughing:

---

## 2024-08-28 20:53:34 - random channel

**birchy**

We're in very different leagues. I still run simulation and model training on a $20 Lightsail instance. And only yesterday, I coded up a burst capacity monitor using boto3 to put my process to sleep when I've hammered the server and need to wait for some more credits to build up. :grinning:

---

## 2024-08-25 09:45:28 - random channel

**Ralegh**

Bit of hassle first time but worth it, there’s a few awkward things with AWS like you need to add a security group to enable ssh and I think you need an IAM thing set up for S3 

---

## 2024-08-24 22:09:31 - random channel

**Ralegh**

Easiest way is use a large EC2 spot instance, you can  100+ cores and 1TB of ram for 1-2£/hour

---

## 2024-08-24 15:16:34 - random channel

**thambie1**

Fitting your models once for production doesn't sound that expensive. Doing it over and over again while iterating on your models may be. Find a way to iterate on your models without training all of them fully. Can you train and analyze just 10% of your models?

---

## 2024-08-18 12:44:45 - strategies channel

**Peter**

[@U016TGY3676](@U016TGY3676) If you're using the same paginator object, then they should be the same. But if you're creating a new paginator object because the session is interrupted, then there's no guarantee.



However, there may be a better way of handling this anyway. I use the S3 inventory feature to get a daily list of objects (or subsets with a key prefix) dumped into a bucket in parquet format (csv format also available). It gives you the same data but without the the need for iterating over a bonkers number of pages.



There are some limitations:

• The list is available no more frequently than daily (weekly or month are also available), but at a time determined by AWS, you have no control over this.

• Even when it arrives the data is typically 12 or more hours out-of-date, I suspect because they run the report against cached data.

But unless you need bang up-to-date data (which would be difficult anyway with buckets containing 1mn plus objects and the most recent at the end of the pagination), these probably aren't big issues. They haven't been for me.

---

## 2024-08-11 12:30:25 - general channel

**liam**

[@U05L8PZD2FM](@U05L8PZD2FM) might be able to explain how the data is actually created but with BASIC/ADV you are getting data that has been grouped ('rolled up') and then streaming has been replicated, thus it is completely different to what you see when betting live or simulating with real data. Obviously it depends on the frequency you are betting as to whether this has an impact or not but if you are concerned about betting movements over a 1s interval you need real data.

---

## 2024-08-11 10:44:17 - general channel

**Sen**

I'm using advanced data - and yes it's got its flaws. Is the pro data actually just higher granularity but the same data quality? or is it higher granularity + higher quality data?

---

## 2024-08-10 16:11:38 - general channel

**ShaunW**

Tricky one, backtesting variations (like asking for less margin)  is unreliable in markets you were present in. But it's impractical to step away for long enough to get a big enough clean sample without you in it..  I end up lowering the latency just enough for test 'me'  to beat what live me did but then that's at best only an indicative bodge.  What happened is knowable, what you should have done instead and into the future might not be.

---

## 2024-08-10 14:53:24 - general channel

**ShaunW**

.... Wish mine was user error, the source, config and server setup haven't been changed since July 2022.:thinking_face:

---

## 2024-08-08 10:35:14 - strategies channel

**Mo**

I have two different setups, both relatively old school. For the first there is an EC2 instance running 24/7 that processes new data on an hourly basis using a cron job. That “new data” is a combination of data scraped from APIs into an RDS Postgres database and log files uploaded to S3 from trading servers



For the second it’s a one off daily pipeline, also running on an EC2 box as a cron job. AWS EventBridge used to bring the box up and down so it’s only running for the duration of the pipeline. Database this time is Aurora Serverless for, in principle, cheaper database costs. In practice, regularly get fucked in the arse by IOPS

---

## 2024-08-06 20:16:02 - strategies channel

**thambie1**

I have one medium size machine, where I do all production work. Strategy development lives elsewhere. If haven't/don't want to host your flumine instances with other production jobs on the same machine.. I can see why you'd go serverless

---

## 2024-08-06 13:27:04 - strategies channel

**birchy**

Yeah, anything you can derive from the market data we receive. There's a million ways to calculate signals from combinations of available & matched prices and volumes. I'd also add that I spent far too long trying to find strategies using just simulation and there's no substitute for live betting, which, ironically, was my mindset several years ago before I discovered flumine. For example, most of my strategies are taking prices and the only way to prove that is by running live.

---

## 2024-08-06 13:15:57 - strategies channel

**birchy**

With horse racing the pre play liquidity has dropped off a lot but there's a significant amount matched inplay, sometimes more than was matched at the off time. I've been hacking around with modelling recently and have had some success with a basic LR model that has about 30 features that are all market data based, but there are 3 or 4 features/signals that are particularly stronger than the others. Engineering features is a trivial exercise but finding good ones is much harder. I took the approach of using lots of features and then dropping the shit ones until I had something production ready. I'm not really a fan of the 'black box' approach but it seems to understand the market nuances better than I do. The bottom line is that it's bringing in some £'s, so I'm slowly convincing myself that it's a much easier route to take.

---

## 2024-08-06 06:59:07 - strategies channel

**liam**

Agree with above however racing has the advantage of a lot of opportunities, you can spend an afternoon testing / debugging on live markets. This is invaluable when starting out, sports like cricket really slow your progression down due to the smaller number of markets. 

---

## 2024-08-04 10:59:51 - issues channel

**Tom**

Yeah live key; been running the script for quite a while without this problem. It's Flumine

---

## 2024-08-04 10:19:34 - general channel

**AndyL**

Do people’s models use input parameters based on the live event or historical as well? Eg. how fast the horse ran in the last 5 races??

---

## 2024-08-04 09:29:36 - issues channel

**liam**

Live key? XM? What are you using for this? Bflw?

---

## 2024-08-04 07:08:15 - issues channel

**Unknown**

Hi guys; I've noticed lately that my marketbooks are not being updated - I'm not sure what the problem is exactly, but in the log file the price is 11.5 where the actual price is 15.5.  It's the whole book, and it happened the other day as well. On a reboot of the program it's done the same thing again. Log files are showing old prices, with the book just not being updated.  What do I need to check to analyse the problem / what should I be on the lookout for?



I am running a beta version of my script to add untested strategy on the same ec2 instance, so there are multiple scripts pulling the data with the same address or whatever. I don't think there are any memory problems because it's happened again after I've checked that.



The prices on the other script running look fine - different strategy names if it helps.

---

## 2024-07-31 15:04:16 - general channel

**Misha**

On that one I'm very confident. It's nearly 15 years in production, constantly upgraded, and has been used by one of the biggest bettors in Australia, and one of the biggest in the world. Turnover through my software has been over $1B. It's what I do for a living (professional software developer), and last 15 years full-time in sports betting. I can compare what I have to all the other systems used by my clients over that time. So I'm pretty sure on that. My modelling is crap (not my area). Software development is what i do on the other hand

---

## 2024-07-30 09:54:05 - general channel

**liam**

I can say that it has resulted in me going down paths that have found alpha but I have yet to use ML in what I would call production / live 

---

## 2024-07-30 08:25:25 - general channel

**liam**

So it looks like you have a good setup for simulating strategies however you need to be careful not to use this for signal creation i.e overfitting on historical data



[@UGV299K6H](@UGV299K6H) I would agree when it comes to inplay and/or strategies with a high edge but with pre race I have always found you need a lot of data.



With TPD I built something off the test data betfair had from Monday and went live on Tuesday thinking the edge would last a few weeks at best :joy: 

---

## 2024-07-30 08:20:20 - general channel

**Michael**

I actually don't agree that you need SO much data. The number of markets you need depends on how much edge you have, what prices you're betting at and what you're trying to get from the data.



If you have more edge you need fewer markets to prove it, if you're betting at middleish prices you need fewer markets before your results converge and if you're content to know roughly whether a strategy is profitable rather than exactly quantifying your edge then that needs fewer markets.



There's also a question of recency, when you're stacking up a year or more of data you can risk market conditions changing.



For my own part I'm usually content to analyse a few thousand markets, even fewer if I'm working on something quite specific.



When TPD came along I went live with &lt; 100 and a hunch - as I recall [@U4H19D1D2](@U4H19D1D2) did the same? When I first started out I had no data at all, went live with nothing and just analysed the results.

---

## 2024-07-29 10:47:32 - general channel

**A**

Great thread. I’m currently following a similar path to you Andy - brute forcing things until I find something consistently profitable. I’m playing in the greyhounds market at the moment. Current strategy seems to be breaking even to slightly profitable (based on 2-3 weeks). Need to bite the bullet and implement simulation and backtesting as my feedback loop is so slow in my current workflow (essentially AB testing the same strategy with different thresholds running live)

---

## 2024-07-29 10:20:53 - strategies channel

**liam**

This is all theoretical though based on backtesting / simulation, IMO not sure you can state facts on liquidity constraints until you have proved a strategy live, anyone can create a graph like that

---

## 2024-07-29 09:32:28 - general channel

**D C**

I haven't tried it to be honest. I use R for my modelling, C++ for production bots and node.js for stream data acquisition. It is something I need to get on top of but in truth my enthusiasm for programming is at a career low right now. I'll need to kick myself up the backside and give it a try

---

## 2024-07-27 23:26:41 - general channel

**Misha**

I think that they had been working on a football model for over 2 years that had not gone live

---

## 2024-07-26 12:47:33 - strategies channel

**Bert Raven**

you don't know the true probabilities but you should have a reasonable estimate of what your edge is through backtesting and live realized, in which case I simply scale the kelly ratio by realized (or backtested) ev/ model forecasted ev

---

## 2024-07-23 10:52:39 - issues channel

**Jared King**

rgr that. fixed your code:

```runner_context = self.get_runner_context(market_id, selection_id, handicap)

if runner_context.live_trade_count == 0:

    lose money```

---

## 2024-07-23 10:38:29 - issues channel

**liam**

Sounds like you just want to use a trade / multiple orders and limit using `max_live_trade_count` on the strategy

---

## 2024-07-23 10:21:05 - general channel

**Adrian**

[@UQL0QDEKA](@UQL0QDEKA) no I don’t think so. One account, I had had 4 instances of flumine running on two machines at home (gave up on the VPS). One strategy for market recorder, two for live strategies and one for temp/testing. No warnings from the API about connection limits or anything. I don’t my betting patterns were anything weird, one of strategies was placing back and lay bets before the off, the other was placing them after the off.

Why I’m sure it was to do with the political discord is, it was the only place I’ve been posting for the last couple of days, and I was removed instantly from the discord server the second my account was closed. 

---

## 2024-07-19 11:18:13 - strategies channel

**A**

That’s cool, I’ve toyed with the idea of remote control with a little flask app. I have one currently as a separate app that delivers data written from my flumine strats for further analysis.



I’ve also got Telegram reporting going which is extremely distracting but quite fun.



I’m also yet to be profitable, so probably should have focussed on that first :laughing: 

---

## 2024-07-17 13:01:56 - general channel

**liam**

He is missed, I wonder if he ever recovered from going live for the first time with his 'model'

---

## 2024-07-11 20:11:26 - strategies channel

**birchy**

Don't fret [@UPMUFSGCR](@UPMUFSGCR). I run flumine simulation on a $20 AWS Lightsail instance to generate 'features' CSV's and then train models on Google Colab. My strategies are unsophisticated, so 5K markets takes an hour or so to run and model takes &lt; 10 minutes to train with ~30 features. I've set a limit of 5k markets so I can get results fairly quickly and that's working for me. I take the view that if a strategy needs &gt; 5k markets to make a profit, then it's not viable in the real world.

---

## 2024-07-10 11:50:54 - general channel

**liam**

IMHO paper trading is for testing your code against live data, no advantage over just simulating using historical data

---

## 2024-07-06 09:30:25 - issues channel

**Unknown**

i got it fixed, after synching ntp the server time still wasn't adjusted until i rebooted the VPS. Strange that it doesn't fix itself until reboot. Even docker gave wrong times after reboot too, so that threw me off. Anyway, thanks for everyone's help. It's a long pain in the arse but it's finally fixed

---

## 2024-07-05 16:27:10 - issues channel

**birchy**

Lesson 1: if it ain't broke, don't fix it.

Lesson 2: make sure you have NTP running on your server

---

## 2024-07-05 10:57:16 - random channel

**liam**

I would advise creating a new flumine instance for data collection anyway as it would be something you start and never stop but a flumine instance running live strategies is likely to need the odd start/stop/restart/update

---

## 2024-07-05 09:02:33 - issues channel

**liam**

Agreed, but why do you think the UK server should have higher latency?

---

## 2024-07-05 08:57:29 - issues channel

**Unknown**

here is the exact same setup on England server. this is probably what an AWS setup looks like

---

## 2024-07-04 12:51:35 - general channel

**Gabriel Mocan**

[@U9JHLMZB4](@U9JHLMZB4) I agree, but let me tell you the whole story.



I’m collecting live data since Aug/23. My system is pretty much validated as far as Monte Carlo allows me to validate, and I’m already live trading for about 45 days in that strategy.



Still, I would like to add more data to the backtest to make it more robust. So I’ve purchased a couple months prior to Aug/23. The thing is, my data is collected in BRL, and the strategy parameters were also designed in BRL (most important being the market volume). To make the backtest cohesive, I would need to convert the historical data to BRL before uploading it to my database.

---

## 2024-07-02 18:24:52 - general channel

**D C**

Is this a recent problem or  long term? Just wondering if it's related to my recent server reboot as I am recording the cricket score feed on said server.

---

## 2024-07-02 15:36:33 - general channel

**liam**

It's a complete fucking joke tbh, almost 3 years this has been in 'production'

---

## 2024-07-02 08:11:53 - random channel

**D C**

Just logged into one of my AWS servers to find that it must have rebooted overnight. Bots on the server have been running for about 18 months uninterrupted. How common is it for AWS stuff to just reset like that? Memory and CPU and disk usage were all very low. It's no big deal I guess, but I've never seen Linux just reboot like that for no (obvious) reason.

---

## 2024-07-02 04:06:35 - issues channel

**Adrian**

[@UUCD6P13J](@UUCD6P13J) no nothing related to that bet at all, which is why I omitted it, just hundreds of max_live_trade count >= 1 strategy violation messages while fumine tries to place a second bet

---

## 2024-07-01 15:15:53 - issues channel

**liam**

Can you confirm the flumine version you run locally and the one on the server? Would ideally need some (minimal) code to replicate this as that will be where the problem is

---

## 2024-06-29 10:02:39 - issues channel

**Joe**

When did the problem start, did it exactly coincide with you moving the server? Could just be coincidence say if someone else has started taking the bets you were taking.

---

## 2024-06-29 09:08:22 - issues channel

**foxwood**

Given the bet cancellation is 8 minutes later could be some code running at suspension / close of market - although no idea what type of markets you are on ? If sure it is not your code doing / causing the cancel then try comparing the server/home code and package versions. Losses due to improved latency quite possible - now getting matched on bad value bets that were just too late from Aus so not being matched from home but are now.

---

## 2024-06-29 06:22:35 - issues channel

**Adrian**

the problem's not as bad on my UK server. Maybe the IE server is so fast it's breaking Flumine? :sweat_smile:

---

## 2024-06-29 05:57:18 - issues channel

**Adrian**

This log for one of the bets. If I'm reading correctly, it's cancelling the matched, execution complete bet? How is that possible

```{"asctime": "2024-06-29 00:26:34,373", "levelname": "INFO", "message": "execute_place", "taskName": null, "trading_function": "place", "elapsed_time": 0.31791043281555176, "response": {"customerRef": "cfca0e5e66bb42aca58660e4d5d04156", "status": "SUCCESS", "marketId": "1.230213698", "instructionReports": [{"status": "SUCCESS", "instruction": {"selectionId": 63123802, "handicap": 0.0, "limitOrder": {"size": 0.9, "price": 6.0, "persistenceType": "LAPSE"}, "customerOrderRef": "10486de05a131-139389135940498137", "orderType": "LIMIT", "side": "LAY"}, "betId": "352983020564", "placedDate": "2024-06-29T00:26:34.000Z", "averagePriceMatched": 6.0, "sizeMatched": 0.9, "orderStatus": "EXECUTION_COMPLETE"}]}, "order_package": {"id": "cfca0e5e-66bb-42ac-a586-60e4d5d04156", "client": "[flumine.clients.betfairclient.BetfairClient object at 0x7f08e2a6a540](flumine.clients.betfairclient.BetfairClient object at 0x7f08e2a6a540)", "market_id": "1.230213698", "orders": ["139389135940498137"], "order_count": 1, "package_type": "Place", "customer_strategy_ref": "7b7f8aff23d6", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}}



{"asctime": "2024-06-29 00:34:28,149", "levelname": "INFO", "message": "Order status update: Cancelling", "taskName": null, "market_id": "1.230213698", "selection_id": 63123802, "handicap": 0, "id": "139389135940498137", "customer_order_ref": "10486de05a131-139389135940498137", "bet_id": "352983020564", "date_time_created": "2024-06-29 00:26:34.049894", "publish_time": "2024-06-29 00:26:34.048000", "market_version": null, "async": false, "trade": {"id": "b7996cca-1057-482b-8cde-05d1cd654818", "strategy": "laying", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["139389135940498137"], "offset_orders": [], "notes": "", "market_notes": "4.6,6,None", "status": "Live", "status_log": "Pending, Live"}, "order_type": {"order_type": "Limit", "price": 6, "size": 0.9, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null, "price_ladder_definition": "CLASSIC"}, "info": {"side": "LAY", "size_matched": 0.9, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 6}, "responses": {"date_time_placed": "2024-06-29 00:26:34.374398", "elapsed_seconds_executable": null}, "runner_status": null, "line_range_result": null, "status": "Cancelling", "status_log": "Pending, Executable, Cancelling", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "6.5993862158248975,1105.950219", "market_notes": "4.6,6,None", "client": ""}```

my orders file shows the bet was matched and resulted in a loss with 474 seconds elapsed executable. even though it was matched instantly

---

## 2024-06-29 03:03:08 - random channel

**James**

Yeah I have tried on AWS in that region, and I could do a VPN. Just a lot of stuff around for something that had been working fine :face_with_rolling_eyes:

Thanks.

---

## 2024-06-29 02:06:12 - issues channel

**Unknown**

here's the latency on my new server. you can see it's not high. I should add that i'm only taking bets not offering them, so the matching should be near instant, but definitely not worse than from the other side of the planet

---

## 2024-06-28 10:43:41 - random channel

**foxwood**

Couple of their IP's show they have a presence on AWS [http://ap-southeast-2.compute.amazonaws.com|ap-southeast-2.compute.amazonaws.com](http://ap-southeast-2.compute.amazonaws.com|ap-southeast-2.compute.amazonaws.com) - try hosting in that region ?

---

## 2024-06-28 08:09:50 - general channel

**Paul**

Note: I’m sat in a Californian garden after an evening of drinking my Dad’s whisky collection for a few hours. Sorry if I’m a bit harsh. I am, however, slightly amazed that people don’t know how to get a unix server to start a script on boot, but hope to make gazillions on 0.1% edges that may or may not exist and competing with literal billionaires…

---

## 2024-06-28 07:46:13 - general channel

**Paul**

:face_palm: If you’re doing `docker run` from a terminal via ssh and haven’t learned how to spin up an image/server to start something on boot (hint: Dockerfile is a fundamental concept of Docker), what the hell is the rest of your code like? Come on. Basics. You don’t need to be a pro sys admin to nail simple stuff… 

---

## 2024-06-28 06:51:30 - random channel

**liam**

Which endpoint are you calling? I use aws and haven’t had any issues 

---

## 2024-06-28 02:13:51 - random channel

**James**

Hey, I’ve been calling the an undocumented, but public api to provide some race info to a manual in play trader. They want to know at a quick glance, Driver/Jockey, track conditions etc and bookie sites and other racing sites are bloated and load slowly.



It was running fine for about a year, and a little while back they blocked access to their API from a massive number of cloud providers (I assume by IP ranges) from AWS/GCP right down to digital ocean and Vultr.



As a temporary solution I have a local server running on the network for this trader, and his IP can access the API without any issues. I want to host it again so it can be accessed away from his home network (without a VPN) and it has CORS settings in place meaning it needs to happen on a server rather than client side. Any ideas from this crowd? I know its more softwarey/infrastructurey than the data questions that usually end up here but thought I’d ask all the same.

---

## 2024-06-27 09:26:03 - general channel

**Adrian**

i'm guessing the rest of you use AWS session manager or something

---

## 2024-06-27 09:14:08 - general channel

**Adrian**

I'll get around to using a portainer edge agent eventually, if that's any better. For now i just want my flumine to stay alive

---

## 2024-06-26 15:21:34 - general channel

**Derek C**

I would expect to be able to retrieve the password from a compiled binary, unless you've made an effort to obfuscate it. EC2 servers can be protected from external access using security groups.

---

## 2024-06-26 14:28:39 - general channel

**D C**

Jeez this is making me feel very blase about my server security settings. My stuff runs as compiled binaries so everything required is embedded within the binary and that has assuaged my paranoia about account credentials (until now). But my server configuration doesn't differ that much from the default (AWS EC2 Ubuntu). Certainly not doing anything with port access control. How big a concern "should" this be for me?

---

## 2024-06-26 14:05:23 - general channel

**PeterLe**

Thanks Derek Im using Lightsail and I dont think you can use secrets with that only EC2

I feel more at ease now that Lee has explained it

---

## 2024-06-26 13:59:15 - general channel

**Derek C**

you could put the credentials in AWS secrets manager if you want to avoid storing on the server in case the server is compromised. (assuming you are using AWS EC2)

---

## 2024-06-25 02:48:10 - general channel

**Ben Coleman**

Hi Guys,

I am wanting to access the market catalogue within the process_closed_market function within a flumine strategy class to gather a few bits of information about the markets I have bet on including the market base rate and the market name. I tried calling market.market_catalogue however it just returned None. I have managed to call this successfully before in other strategies however it doesn't seem to be working for me now. Is there anything you need to specify to be able to gather the market catalogue information?

Also, unrelated question, is a bet directly into the SP considered a 'live bet' in the context of setting the argument max_live_trade_count when calling your class, I assume not?

Cheers

---

## 2024-06-23 12:39:19 - general channel

**Jared King**

Hi All, is there a flumine way to trigger a bet after a bet is executed? eg trigger a lay after the back is executed. Thinking checking order.trade.status_log in process_orders, then saving the bet_id, or using max_live_trade_count

---

## 2024-06-22 19:39:42 - strategies channel

**foxwood**

To set your own values add `max_live_trade_count=nnn` as argument to strategy constructor - other options in the docs at [https://betcode-org.github.io/flumine/strategies/#parameters](https://betcode-org.github.io/flumine/strategies/#parameters)

---

## 2024-06-22 11:44:52 - strategies channel

**Adrian**

`'Error: strategy.validate_order failed: live_trade_count (1) >= max_live_trade_count (1)'`

Is this limit explicitly defined somewhere?

I haven't changed it. Is it just a default safety measure in flumine or something?

---

## 2024-06-21 09:32:19 - random channel

**liam**

Being completely honest, not sure I have found a single paper that a) I find interesting and b) actually understand, they all seem just full of pointless maths and equations.



The most interesting part of sports right now is live data / tracking / ai tracking etc. something which fits more into CS as well



[https://www.totalperformancedata.com/](https://www.totalperformancedata.com/)

[https://jabbr.ai/](https://jabbr.ai/)

[https://pysport.org/](https://pysport.org/)

[https://www.statsperform.com/artificial-intelligence-in-sport/](https://www.statsperform.com/artificial-intelligence-in-sport/)

---

## 2024-06-18 23:51:12 - issues channel

**Cian Ryan**

Hi all, I am following the betfairlightweight tutorial using Colab notebook. When I call the API however i get a HTTP 403 error. Could this be due to the server potentially being in the US? Same code works fine on local drive, although i have to use the less secure login_interactive() in both cases as I couldn't get it to authenticate my certs.



is there any way around this?

---

## 2024-06-16 23:58:38 - random channel

**Rob**

As someone else said, I would personally prioritise using the exact same code in production vs development, otherwise, with just me in my spare time,  the chance of making a mistake there is too large (different story if you're a massive business with the resource for lots of testing)

---

## 2024-06-16 23:56:22 - random channel

**Rob**

I tend to work in RAM, would consider:



• don't store every row if you don't need to - does every row change every tick? if not, store each unique once with a from/to datetime

•  use the smallest possible data types

• do feature engineering or get feature importance on a smaller sample, and then build your model on a much dataset of columns that amtter  - obviously YMMV, but I don't build models with the full set of traded volumes at every price

• sample - lots of the time, you're not going to get a massively different answer with a subset that does fit in RAM

• buy more RAM (it's relatively cheap compared to the time you're investing - 128Gb of DDR5 isn't crazy)

• get it working locally on a sample, then use a cloud server (e.g. EC2) to fun your full build

---

## 2024-06-16 19:23:48 - random channel

**Ralegh**

Is this live? For research I would just get a big ec2 instance

---

## 2024-06-16 18:58:27 - random channel

**Dave**

Outside of flumine I use the historical generator stream from bflw, replay the files and handle it tick by tick. You don't need all ticks going back to when the market was open, so you can cache the bits you need yourself as you process each next tick. The advantage here as well is that this is how you'll process market data in production, so you can run the same logic for things like feature generation for both historical and realtime data.

---

## 2024-06-13 18:03:03 - issues channel

**Rodolfo**

hey, I am not a pro coder but I managed to create a script that places bets. Works fine. But I am helping a friend that lives in Sweden, he gets a AUTHORIZED_ONLY_FOR_DOMAIN_SE error. How can I ask betfairlightweight to try the SWEDEN exchange?



trading = betfairlightweight.APIClient(EMAIL, PASSWORD, app_key=API_KEY_DELAYED)

trading.login_interactive()



Thank you.

---

## 2024-06-11 14:39:36 - general channel

**George**

so in live trading, after a runner is removed, the `blotter._live_orders` and `blotter._strategy___orders` etc will all be lists of orders with the new (lower) prices?

---

## 2024-06-09 19:53:56 - random channel

**D C**

Yeah I have sent the connection ID and details to Neil via email. It's all on AWS Dublin.

---

## 2024-06-09 12:04:50 - general channel

**Adrian**

I butchered the jupyterloggingcontrol so it would run after framework.run. I don’t know how to make it work on a live strategy

---

## 2024-06-08 19:41:46 - issues channel

**Elliot A**

Hi guys I have a strategy that uses Process New Market to wait for new markets to appear on Betfair and then do some stuff. The code runs 24hrs a day but seems to just stop processing markets after a few hours. Have not changed the default workers so it should be kept alive, any ideas whats happening? 

---

## 2024-05-31 21:21:32 - random channel

**PeterLe**

The way I use it some times DC is to have my prod system running on AWS (Ie Python) and then use betangel on my local PC (Doing nothing but to act as a visual aid) That way there is no overhead.

It may just be the way my brain works but I find it useful to watch what's happening rather that read the logs etc

---

## 2024-05-31 21:20:49 - general channel

**George**

Not sure about that... actually, in simulation, the race condition with order status happens when using the normal logging controls, it's nothing to do with what I've added here.



When I comment out the code I've added, I still see "new" orders being written to the log with size_cancelled > 0 and size_remaining == 0.



That wouldn't happen in live trading because of the real-life latency involved.

---

## 2024-05-29 19:36:31 - random channel

**Dave**

I aim for around min 5% in backtest. Once deployed, as long as it is positive and has no negative impact on my other strategies (latency, wasting streaming bandwidth etc) then that's enough to justify running it

---

## 2024-05-24 00:21:07 - general channel

**Andrew**

If I’ve got that right, if writing simulation code to later use in production then don’t rely on contents of runner book for placed orders, but instead maintain record of orders placed and then later matched or cancelled?

---

## 2024-05-19 19:17:42 - strategies channel

**foxwood**

I'm trying to simulate 2 parallel strategies running on the same market and talking to each using market.context. Seems like they are both created and started ok but then strategy1 goes through all files with no sign of strategy2 getting a look in. From the logs I see each strategy is added but each one gets its own version of the HistoricalStreams created from the market_filter["markets"] file list when the strategy is constructed. Is there a way to make this work in simulation - I assume it works ok live judging by comments in other threads.

---

## 2024-05-16 11:56:19 - random channel

**Ralegh**

I think ridge+feature engineering given enough time and ideas may be sota but for stuff like HFT I have heard deep learning, (and also deep RL for execution/optimiser). There’s just so much data and you can fit to it quickly eg blackrock deploying a new execution algo could be factored in if you recalibrated every day but those transient signals might not fit into features that you’ve come up with historically unless you were literally coming up with new ideas every day. 



I think XTX is pretty big on deep learning and their whole thing is taking large positions while making markets so I assume their forecasts are good. That being said regression is like concrete or steel and probably is the final arbiter of what goes in a forecast.



Also ridge + features are a lot handier for transfer learning. If you moved to a new market with little historical data you’d feel pretty confident just copying stuff across. Would be easier to manipulate and remove features (weight = 0) without the required data in the new market than trying to break apart a web of neural network shenanigans. If I’ve learned anything from success stories the easiest way to make money is take a good strategy and apply it to an inefficient/new market.

---

## 2024-05-16 08:24:00 - issues channel

**Jorge**

Hey guys, I recently started Simulating 2 strategies in the same flumine instance, with the goal of running them live. Until now, I was running both strategies in different scripts. This is my code:



```client = clients.SimulatedClient()

framework = FlumineSimulation(client=client)



control = BacktestLoggingControl(logs_dir=logs_dir)

framework.add_logging_control(control)



with patch("builtins.open", smart_open.open):

    framework.add_strategy(strategy_1)

    framework.add_strategy(strategy_2)

    framework.add_market_middleware(MarketCatalogueMiddleware(strategy_2.market_filter["markets"]))

    framework.run()```

My issue is with the [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|backtestloggingcontrol.py) , I noticed that in my orders.txt, the orders from strategy_1 are duplicated. The same happens with the markets.txt (cleared_markets), I get double the amount of cleared_markets and the last half is the correct one which includes both strategies. How could I fix this so the [https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|backtestloggingcontrol.py) works when running multiple strategies in the same instance?

---

## 2024-05-11 21:07:30 - general channel

**birchy**

+1 for Debian (or Ubuntu). I run mine on AWS Lightsail. Much less complicated for an old fart like me to understand.

---

## 2024-05-11 18:28:33 - general channel

**Unknown**

Yes [@U016TGY3676](@U016TGY3676) your use case scenario is quite strange but you can use heartbeat to cancel all unmatched bets.



In my app I can execute such script, so user can be sure if connection to betfair api is lost, all his unmatched bets are cancelled. He can use it in similar scenario you describe, so in panic when placing wrong bet he can click on Terminate script button, but minimal heartbeat you can use is 10 seconds. If betfair set it to 1 second it could be used in your scenario, so your placed bets would be cancelled in around 1 second after it is materialized on betfair server.

---

## 2024-05-11 08:57:55 - general channel

**Ammar**

Our entire AWS spend at work 8-10k / month for a pretty sophisticated setup… there’s probably some fat in what they’re doing there

---

## 2024-05-11 05:06:47 - general channel

**Ralegh**

Generally compute is gonna be the biggest cost, s3 is pretty cheap especially for betfair data, I know someone storing tradfi futures data (magnitudes larger) and I think their storage (with intelligent tiering) was about $500-1000/month, their total aws bill was $8000/month ish (mostly spot ec2)

---

## 2024-05-10 19:59:48 - general channel

**liam**

AWS Linux 2 

---

## 2024-05-10 19:30:57 - general channel

**Ammar**

ec2 users … out of interest which AMI are you using?

---

## 2024-05-10 19:25:26 - issues channel

**ShaunW**

Check your sim latency vs live? Sounds like you're getting in too far in front of your live orders. Its hard retesting something you're present in.

---

## 2024-05-09 21:26:08 - issues channel

**Ammar**

&gt;  replacing the ‘or’ with an ‘and’ fixes it?

it’s what’s highlighted in the code block inside the message linked above.  if keep alive returns a `None` then force a login; that’s it

---

## 2024-05-09 21:21:51 - issues channel

**NAS**

my understanding of the logic:

(1) if theres an issue with the current session, try to refresh it/keep it alive

(2) if this for some reason fails, run a new login instead



as you mentioned it looks like there are unexpected responses coming through that throws exceptions and never lets (2) come into play

---

## 2024-05-09 15:33:14 - issues channel

**NAS**

my bots went down earlier today due to the maintenance and when this happens flumine fails to reconnect, seems there's an issue with the keep_alive worker? checking code seems to suggest it should always attempt a new login if theres an issue with the sessions?



`2024-05-09 13:23:24,033 - flumine.clients.betfairclient - ERROR - BetfairClient `keep_alive` error`

`Traceback (most recent call last):`

  `File ".../flumine/clients/betfairclient.py", line 44, in keep_alive`

    `return self.betting_client.keep_alive()`

           `^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^`

  `File ".../betfairlightweight/endpoints/keepalive.py", line 30, in __call__`

    `(response, response_json, elapsed_time) = self.request(session=session)`

                                              `^^^^^^^^^^^^^^^^^^^^^^^^^^^^^`

  `File ".../betfairlightweight/endpoints/keepalive.py", line 60, in request`

    `self._error_handler(response_json)`

  `File ".../betfairlightweight/endpoints/keepalive.py", line 67, in _error_handler`

    `raise self._error(response)`

`betfairlightweight.exceptions.KeepAliveError: API keepAlive FAIL: NO_SESSION`

`2024-05-09 13:23:24,038 - flumine.worker - ERROR - Error in BackgroundWorker keep_alive: 'NoneType' object has no attribute 'status'`

`Traceback (most recent call last):`

  `File ".../flumine/worker.py", line 66, in run`

    `self.function(`

  `File "..../flumine/worker.py", line 106, in keep_alive`

    `if resp is True or resp.status == "SUCCESS":`

                       `^^^^^^^^^^^`

`AttributeError: 'NoneType' object has no attribute 'status'`

---

## 2024-05-09 11:41:51 - general channel

**birchy**

I may be going senile...can a pending order be cancelled? I've tested in Flumine simulation and can't cancel a pending order because it doesn't get a bet id until it turns executable. Async = False. Not tested live, but I swear that I've cancelled pending orders in the past? The basic scenario is:

a) I place an inplay bet

b) order is pending during the delay

c) I realise that I placed a 'bad' bet

d) I want to cancel the bet before it becomes executable

---

## 2024-05-08 22:07:35 - general channel

**birchy**

[@U04M35GKUMP](@U04M35GKUMP) So having briefly read this thread, it seems to me that you have 2 things going on: 1. You want to automate your routine of choosing bets to place, and 2. You want to automate placement of said bets? Flumine will handle the latter, but you'll probably need to supply it with your bet selections. Basically, if your bets are chosen by using market data or external 'live' data feeds, then you can do all of that in Flumine. If your strategy is not time sensitive, i.e. you preprocess some third party sources, then (to me) that's a separate project where you create a list of bets and execute them using Flumine.

---

## 2024-05-07 07:51:39 - strategies channel

**Unknown**

My cpu utilisation is pretty much always under 2%. I am running it on AWS and this is that metric over the last week

---

## 2024-05-06 13:36:55 - general channel

**Adrian**

Single market:

&gt;        1044 function calls (1041 primitive calls) in 0.001 seconds

&gt; 

&gt;    Ordered by: standard name

&gt; 

&gt;    ncalls  tottime  percall  cumtime  percall filename:lineno(function)

&gt;         8    0.000    0.000    0.000    0.000 &lt;frozen genericpath&gt;:121(_splitext)

&gt;         8    0.000    0.000    0.000    0.000 &lt;frozen posixpath&gt;:117(splitext)

&gt;         8    0.000    0.000    0.000    0.000 &lt;frozen posixpath&gt;:140(basename)

&gt;         8    0.000    0.000    0.000    0.000 &lt;frozen posixpath&gt;:41(_get_sep)

&gt;        24    0.000    0.000    0.000    0.000 &lt;frozen posixpath&gt;:52(normcase)

&gt;         1    0.000    0.000    0.001    0.001 &lt;string&gt;:1(&lt;module&gt;)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1087(flush)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1098(emit)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:123(getLevelName)

&gt;         2    0.000    0.000    0.000    0.000 __init__.py:1319(disable)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1479(info)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1561(findCaller)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1595(makeRecord)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1610(_log)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1636(handle)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:164(&lt;lambda&gt;)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1690(callHandlers)

&gt;         2    0.000    0.000    0.000    0.000 __init__.py:1720(getEffectiveLevel)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:1734(isEnabledFor)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:183(dumps)

&gt;        24    0.000    0.000    0.000    0.000 __init__.py:194(_is_internal_frame)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:2223(handle)

&gt;         2    0.000    0.000    0.000    0.000 __init__.py:228(_acquireLock)

&gt;         2    0.000    0.000    0.000    0.000 __init__.py:237(_releaseLock)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:292(__init__)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:368(getMessage)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:606(formatTime)

&gt;        16    0.000    0.000    0.000    0.000 __init__.py:815(filter)

&gt;        16    0.000    0.000    0.000    0.000 __init__.py:922(acquire)

&gt;        16    0.000    0.000    0.000    0.000 __init__.py:929(release)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:942(format)

&gt;         8    0.000    0.000    0.000    0.000 __init__.py:965(handle)

&gt;         1    0.000    0.000    0.000    0.000 accountresources.py:17(__init__)

&gt;         5    0.000    0.000    0.000    0.000 baseclient.py:101(transaction_count_total)

&gt;         5    0.000    0.000    0.000    0.000 baseclient.py:120(info)

&gt;         5    0.000    0.000    0.000    0.000 baseclient.py:87(username)

&gt;         5    0.000    0.000    0.000    0.000 baseclient.py:94(current_transaction_count_total)

&gt;         2    0.000    0.000    0.000    0.000 baseexecution.py:156(shutdown)

&gt;         2    0.000    0.000    0.000    0.000 baseflumine.py:124(log_control)

&gt;         1    0.000    0.000    0.000    0.000 baseflumine.py:128(_add_default_workers)

&gt;         1    0.000    0.000    0.000    0.000 baseflumine.py:401(_process_end_flumine)

&gt;         2    0.000    0.000    0.000    0.000 baseflumine.py:404(info)

&gt;         2    0.000    0.000    0.000    0.000 baseflumine.py:412(&lt;listcomp&gt;)

&gt;         1    0.000    0.000    0.000    0.000 baseflumine.py:417(__enter__)

&gt;         1    0.000    0.000    0.000    0.000 baseflumine.py:445(__exit__)

&gt;         1    0.000    0.000    0.000    0.000 baseresource.py:11(__init__)

&gt;         5    0.000    0.000    0.000    0.000 clientcontrols.py:108(current_transaction_count_total)

&gt;         5    0.000    0.000    0.000    0.000 clientcontrols.py:112(transaction_count_total)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:56(login)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:66(logout)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:71(update_account_details)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:76(simulated)

&gt;         2    0.000    0.000    0.000    0.000 clients.py:84(info)

&gt;         2    0.000    0.000    0.000    0.000 clients.py:86(&lt;dictcomp&gt;)

&gt;         6    0.000    0.000    0.000    0.000 clients.py:87(&lt;dictcomp&gt;)

&gt;         1    0.000    0.000    0.000    0.000 clients.py:94(__len__)

&gt;         8    0.000    0.000    0.000    0.000 encoder.py:105(__init__)

&gt;         7    0.000    0.000    0.000    0.000 encoder.py:161(default)

&gt;         8    0.000    0.000    0.000    0.000 encoder.py:183(encode)

&gt;         8    0.000    0.000    0.000    0.000 encoder.py:205(iterencode)

&gt;        11    0.000    0.000    0.000    0.000 enum.py:1255(value)

&gt;        11    0.000    0.000    0.000    0.000 enum.py:193(__get__)

&gt;         2    0.000    0.000    0.000    0.000 events.py:39(__init__)

&gt;         7    0.000    0.000    0.000    0.000 inspect.py:456(istraceback)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:176(add_fields)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:193(_perform_rename_log_fields)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:198(process_log_record)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:205(jsonify_log_record)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:213(serialize_log_record)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:217(format)

&gt;         8    0.000    0.000    0.000    0.000 jsonlogger.py:28(merge_record_extra)

&gt;         7    0.000    0.000    0.000    0.000 jsonlogger.py:59(default)

&gt;         4    0.000    0.000    0.000    0.000 markets.py:54(markets)

&gt;         2    0.000    0.000    0.000    0.000 markets.py:58(open_market_ids)

&gt;         2    0.000    0.000    0.000    0.000 markets.py:60(&lt;listcomp&gt;)

&gt;         2    0.000    0.000    0.000    0.000 markets.py:69(__iter__)

&gt;         2    0.000    0.000    0.000    0.000 markets.py:72(__len__)

&gt;         8    0.000    0.000    0.000    0.000 process.py:189(name)

&gt;         8    0.000    0.000    0.000    0.000 process.py:37(current_process)

&gt;         1    0.000    0.000    0.000    0.000 simulatedclient.py:18(login)

&gt;         1    0.000    0.000    0.000    0.000 simulatedclient.py:24(logout)

&gt;         1    0.000    0.000    0.000    0.000 simulatedclient.py:27(update_account_details)

&gt;         1    0.000    0.000    0.001    0.001 simulation.py:29(run)

&gt;         1    0.000    0.000    0.000    0.000 strategy.py:142(finish)

&gt;         1    0.000    0.000    0.000    0.000 strategy.py:286(start)

&gt;         1    0.000    0.000    0.000    0.000 strategy.py:298(finish)

&gt;         2    0.000    0.000    0.000    0.000 strategy.py:306(__iter__)

&gt;         1    0.000    0.000    0.000    0.000 strategy.py:96(start)

&gt;         1    0.000    0.000    0.000    0.000 streams.py:254(start)

&gt;         1    0.000    0.000    0.000    0.000 streams.py:263(stop)

&gt;         4    0.000    0.000    0.000    0.000 streams.py:271(__iter__)

&gt;         2    0.000    0.000    0.000    0.000 thread.py:216(shutdown)

&gt;         2    0.000    0.000    0.000    0.000 threading.py:1125(_wait_for_tstate_lock)

&gt;         8    0.000    0.000    0.000    0.000 threading.py:1152(name)

&gt;         2    0.000    0.000    0.000    0.000 threading.py:1192(is_alive)

&gt;         8    0.000    0.000    0.000    0.000 threading.py:1453(current_thread)

&gt;         2    0.000    0.000    0.000    0.000 threading.py:1501(enumerate)

&gt;         4    0.000    0.000    0.000    0.000 threading.py:575(is_set)

&gt;         2    0.000    0.000    0.000    0.000 threading.py:931(__repr__)

&gt;         1    0.000    0.000    0.000    0.000 utils.py:31(__enter__)

&gt;         1    0.000    0.000    0.000    0.000 utils.py:37(__exit__)

&gt;        16    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}

&gt;         1    0.000    0.000    0.001    0.001 {built-in method builtins.exec}

&gt;        53    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}

&gt;        61    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method builtins.iter}

&gt;       8/5    0.000    0.000    0.000    0.000 {built-in method builtins.len}

&gt;        40    0.000    0.000    0.000    0.000 {built-in method posix.fspath}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method posix.getpid}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method sys._getframe}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method time.gmtime}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method time.strftime}

&gt;         8    0.000    0.000    0.000    0.000 {built-in method time.time}

&gt;         4    0.000    0.000    0.000    0.000 {built-in method utcnow}

&gt;         2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}

&gt;         2    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}

&gt;        18    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}

&gt;         2    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}

&gt;         1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}

&gt;         8    0.000    0.000    0.000    0.000 {method 'flush' of '_io.TextIOWrapper' objects}

&gt;        94    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}

&gt;        25    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}

&gt;         8    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}

&gt;         1    0.000    0.000    0.000    0.000 {method 'pop' of 'dict' objects}

&gt;         2    0.000    0.000    0.000    0.000 {method 'put' of '_queue.SimpleQueue' objects}

&gt;        18    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}

&gt;        24    0.000    0.000    0.000    0.000 {method 'rfind' of 'str' objects}

&gt;        37    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}

&gt;        16    0.000    0.000    0.000    0.000 {method 'update' of 'collections.OrderedDict' objects}

&gt;         6    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}

&gt;         8    0.000    0.000    0.000    0.000 {method 'write' of '_io.TextIOWrapper' objects}

---

## 2024-05-06 09:15:37 - general channel

**liam**

The market recorder is designed for this, recording the live data and market catalogue for loading during simulation

---

## 2024-05-06 06:28:12 - general channel

**Adrian**

[@UBS7QANF3](@UBS7QANF3) i use MC to get runner ids, names, rugs/boxes and the race number from the market name. it's not a major issue as i can recreate what i need from other betfair data sources, but would be nice just to have it all in one spot so i could treat my historical data the same way I would a live market

---

## 2024-05-05 19:13:53 - strategies channel

**Dave**

Not at all, just in absence of me doing any live profiling. I'm mainly curious about what people see for: a) streaming update from Betfair to arrive, b) how long it takes flumine to decode + call process_market_book. I can easily profile the logic after that, I am just wondering how much time is spent at various stages before process_market_book and wondering if there were some numbers already!

---

## 2024-05-04 14:23:08 - strategies channel

**liam**

I always use python lists / dictionary’s for this when live 

---

## 2024-05-04 10:20:34 - strategies channel

**Justice**

Does anyone on here have any insights into migrating potentially hundreds of features created on a dataframe into a live production scenario, especially within the context of a flumine strategy? E.g. rolling windows, moving averages etc.

---

## 2024-05-01 14:30:58 - random channel

**PeterLe**

Thanks Chaps, I might give the Billy Walters a miss Paul!

I recently finished 'Algorithms to live by'...A bit of a mixed bag

There were some things that I will go back over but some sections/Chapters even that were a waste of time

(Helped me think more about Financial planning in future retirement) OK if you want to 'spend' an audible credit though

---

## 2024-04-30 20:34:51 - general channel

**birchy**

Flumine simulation: what's the definition of a "passive" bet? I know it's a price that is not currently available, e.g. back @ ATB + 1 tick, but what impact does the inplay delay have? i.e. let's say the ATB/ATL is 3.2/4.0 and we place a back bet @ 3.6. Bet lands in the live market 1 second later and the prices have changed to 3.6/4.0 and matches instantly. Is that still "passive"? To me it's not, but I'd like some clarification.

---

## 2024-04-30 16:07:47 - strategies channel

**liam**

When deploying you want to run the python script through cli/bash or ideally in a container

---

## 2024-04-30 15:16:15 - strategies channel

**Ger Gleeson**

Ill piggy back on this thread. Also pretty new in this space. Im looking to move running my scripts from running on my local machine to run within the cloud etc. My scripts are pretty straight forward (at the moment). I just to have them hosted in IRE / UK to avoid geoblocking etc, and also run in "background". Im used to running scipts through Anaconda / jupyter lab / chrome etc but this seems vey slow (start up, general navigation etc) on a 1GB RAM setup within AWS lightsail. Anaconda documentation seems to recommend 4BG+ RAM, which looking at the lightsail pricing per month is coming out at 40 USD per month. Can anyone suggest simplier set up i can go with which requires less RAM etc would allow a cheaper rate per month

---

## 2024-04-26 21:13:22 - random channel

**PeterLe**

A quick question please if someone can advise. I was looking at environmental variables (As I read on here that Storing App keys, passwords etc is best achieved via .env files?)

Is that so and if it is, How do you get around 2FA thats appended on the password? (hope that makes sense and I haven't misunderstood how its used).

I also notice that there is AWS Secrets Manager too?

If someone can give me a pointer on best practice, for storing and retrieving these key bits of data,  I can work it from there

Thanks



..Edit : Having read up a little, it seems this isnt an ideal way to go about it. So ill prob just input the 2FA's for each account manually..no big task really..

---

## 2024-04-25 21:18:05 - issues channel

**Ammar**

It looks to me like there’s an unexpected response type (as `None`) which is coming thru from the call stack — which looks entirely plausible as I read thru the code all the way back to `betfairlightweight`



to be clear, this isn’t impacting the data stream; but it does look like very unhappy noise in the logs, which is never nice :)



steps to replicate:

• leave the market recorder going on one machine — this is _all_ I’m using flumine for, it’s a single process which I kick off each day.  (currently on a laptop, behind expressVPN, which may be a noteworthy point) 

• use betfair to manually execute and check markets via the mobile app, and other computers

• over the course of 8-10 hours the error will start to present



the fix could be very light in the keepalive (similar to how it’s handling a failure already)





```def keep_alive(context: dict, flumine) -&gt; None:

    """Attempt keep alive if required or

    login if keep alive failed

    """

    for client in flumine.clients:

        if client.EXCHANGE == ExchangeType.BETFAIR:

            if client.betting_client.session_token:

                resp = client.keep_alive()



                # start change

                if resp is None:  # this is the unexpected response type I'm seeing

                    client.login()

                # end chage



                if resp is True or resp.status == "SUCCESS":

                    continue

        elif client.EXCHANGE == ExchangeType.BETCONNECT:

            resp = client.keep_alive()

            if resp:

                continue

        # keep-alive failed lets try a login

        client.login()```

---

## 2024-04-25 10:54:44 - issues channel

**Ammar**

```

{"asctime": "2024-04-24 21:20:22,236", "levelname": "ERROR", "message": "BetfairClient `keep_alive` error", "exc_info": "Traceback (most recent call last):

File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/flumine/clients/betfairclient.py\", line 44, in keep_alive

return self.betting_client.keep_alive()

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/keepalive.py\", line 30, in __call__

(response, response_json, elapsed_time) = self.request(session=session)

                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

                           File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/keepalive.py\", line 60, in request

                               self._error_handler(response_json)

                                 File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/keepalive.py\", line 67, in _error_handler

                                     raise self._error(response)

                                     betfairlightweight.exceptions.KeepAliveError: API keepAlive FAIL: NO_SESSION", "client": "APIClient", "trading_function": "keep_alive", "response": "API keepAlive FAIL: NO_SESSION"}





{"asctime": "2024-04-24 21:20:22,240", "levelname": "ERROR", "message": "Error in BackgroundWorker keep_alive: 'NoneType' object has no attribute 'status'", "exc_info": "Traceback (most recent call last):

File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/flumine/worker.py\", line 66, in run

self.function(

File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/flumine/worker.py\", line 106, in keep_alive

if resp is True or resp.status == \"SUCCESS\":

^^^^^^^^^^^

AttributeError: 'NoneType' object has no attribute 'status'", "worker_name": "keep_alive", "function": "[function keep_alive at 0x1041e99e0](function keep_alive at 0x1041e99e0)", "context": {}}```



---

raw json format from the logger

```{"asctime": "2024-04-24 21:02:11,658", "levelname": "ERROR", "message": "BetfairClient `account.get_account_details` error", "exc_info": "Traceback (most recent call last):\n  File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/flumine/clients/betfairclient.py\", line 84, in _get_account_details\n    return self.betting_client.account.get_account_details()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/account.py\", line 54, in get_account_details\n    (response, response_json, elapsed_time) = self.request(method, params, session)\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 54, in request\n    self._error_handler(response_json, method, params)\n  File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 80, in _error_handler\n    raise self._error(response, method, params)\nbetfairlightweight.exceptions.APIError: AccountAPING/v1.0/getAccountDetails \nParams: {} \nException: None \nError: {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}}, 'id': 1}", "client": "APIClient", "trading_function": "account.get_account_details", "response": "AccountAPING/v1.0/getAccountDetails \nParams: {} \nException: None \nError: {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}}, 'id': 1}"}

{"asctime": "2024-04-24 21:02:11,762", "levelname": "ERROR", "message": "BetfairClient `account.get_account_funds` error", "exc_info": "Traceback (most recent call last):\n  File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/flumine/clients/betfairclient.py\", line 98, in _get_account_funds\n    return self.betting_client.account.get_account_funds()\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/account.py\", line 35, in get_account_funds\n    (response, response_json, elapsed_time) = self.request(method, params, session)\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 54, in request\n    self._error_handler(response_json, method, params)\n  File \"/Users/user/.virtualenvs/b2023/lib/python3.11/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 80, in _error_handler\n    raise self._error(response, method, params)\nbetfairlightweight.exceptions.APIError: AccountAPING/v1.0/getAccountFunds \nParams: {} \nException: None \nError: {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}}, 'id': 1}", "client": "APIClient", "trading_function": "account.get_account_funds", "response": "AccountAPING/v1.0/getAccountFunds \nParams: {} \nException: None \nError: {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}}, 'id': 1}"}```

---

## 2024-04-25 10:54:32 - issues channel

**Ammar**

I think there’s a bug in the keep alive worker where it’s not logging in properly



Traceback in thread



I’ve been getting this for a couple of days wheen leaving the stream running for a long while (to build up local data as discussed  elsewhere) but also logging into betfair from other devices to look at things or manually execute.



The fix should be relatively straightforward, happy to open a PR into the flumine repo if that’s the done thing?

---

## 2024-04-23 13:10:56 - strategies channel

**Mo**

Yeah Betfair aren’t in AWS but they are in Dublin. You’ll have to hire me as a consultant for any more information 

---

## 2024-04-23 13:08:49 - strategies channel

**ShaunW**

You might be right James, things change.  I subscribed to half a dozen and ran tests, AWS came out on top. But if AWS is good enough for Netflix and NASA then it's certainly good enough for my a little betting system.

---

## 2024-04-23 12:53:09 - strategies channel

**James**

Maybe I was told incorrectly, I was however definitely encouraged to host in Ireland on AWS for the best latency from a cloud point of view.

---

## 2024-04-23 12:49:42 - strategies channel

**ShaunW**

I don't think Betfair use AWS? I thought they ran their own hardware, in Dublin, near AWS. Either way the cost of the service has to be balanced with performance, no point saving a dollar to earn $1000 less.

---

## 2024-04-23 12:45:02 - strategies channel

**James**

Digital ocean apps containers are $10 per month for a 1gb 1 cpu machine. So cheap.



Betfair servers are in Ireland on AWS, so minimum latency is to host is there, ECS containers or Lightsail tend to be easiest and fast, but not as cheap as DO.

---

## 2024-04-18 22:48:46 - general channel

**JL**

yes live. So the information seem to be available when `process_market_catalogue` is called which solved it!

---

## 2024-04-18 19:31:53 - general channel

**Dario Scardina**

Hello [@U06RE5C6X2P](@U06RE5C6X2P), sorry maybe I didn't explain myself well. I already know how to obtain the market_profit_and_loss, but those values indicate the "final" outcome, not the current cashout that I could execute on the market. If I back €1 at odds of 1.50, on that selection I have a theoretical profit of €0.50 (minus taxes). But during the live event, I can execute a cashout (the one that Betfair suggests on its website) to achieve an all-green outcome (in the luckiest scenario, hehe :D).

---

## 2024-04-18 17:43:04 - general channel

**Dario Scardina**

Hello everyone :)

Is there anyone generous who wants to share a Python snippet to replicate Betfair's cashout function? Not the order placement, but I would like to understand how to obtain that value that Betfair reports on a live market.

Thank you in advance :wink:

---

## 2024-04-17 20:50:02 - random channel

**Michael**

There are many obvious shortcomings but I can totally see why they did it that way. From the perspective of their prospective customers (bookies) the most important thing is to avoid creating opportunities for arbitrage with BF. The value of the bets can be generated just by apply an over round so the model needn't be intrinsically amazing. I can also see that trying to evaluate against actual outcomes would be a nightmare, especially given that the data set is relatively small. Still, I thought it was interesting, prior to that conversation I wondered if TPD had a model that could be simply deployed to out-bet us, but then I realised that they weren't even trying to make one. Their model has to generate accurate prices for every runner from start to finish, our job is much easier because we only have to be able to point and laugh when someone else is getting it wrong. The other interesting thing was that the developer wasn't interested in botting the markets himself. If I had access to his skills and resources I'd be all over that but it just didn't seem to be his bag, he seemed to think it was futile because BF prices are true, which is sort of right and wrong at the same time.

---

## 2024-04-17 20:14:32 - random channel

**liam**

So anyone do this?



&gt; Smart quant crypto traders will spin up many thousands of EC2 instances, measure latency to the exchange, and only keep the best instances, continually. This process allows them to eventually get EC2 instances to trade from probably in the same rack, or maybe on the same bare metal box, as the exchange. 

---

## 2024-04-17 09:22:50 - issues channel

**Ammar**

maybe a silly q (and hopefully I’ve not missed something in docs); but can you add strategies after flumine has started running, or does it have to be done before the `run()` call?



this works as usual:

```# main thread only

for s in strategies:

    framework.add_strategy(s)



framework.run()```



doesn’t seem to work - debug logging isn’t showing files being written to (but it’s just been a quick spike this morning)

```# run() on background thread

framework_thread = threading.Thread(

    name="Flumine Main",

    target=framework.run,

)

framework_thread.start()



# add strategy via main thread

for s in strategies:

    framework.add_strategy(s)```

(the strategies are just MarketRecorder from the examples with custom market filters based on relevant events, countrycodes and market_types to avoid hitting the 200 makret subscription limit which betfair seem to have)



I’ve not tried adding strategies from a background thread yet, but will do that later today.



reason for this is that I want to have one flumine instance running on my ec2, with a trickling in of football events as they are 2hours from start.



so I’ll have the events stored in RDS (which are periodically refreshed) and then have a loop which grabs relevant event_id’s to construct the market_filter going into flumine



is there a better pattern to use for something like this?

---

## 2024-04-16 07:35:10 - issues channel

**Clive**

During a simulation, what would be the cause of messages such as:



`flumine.strategy.runnercontext - WARNING - Trade 'a5fb009a-fb61-11ee-bde9-eb5357042996' not present in RunnerContext live_trades on reset`

---

## 2024-04-15 18:30:42 - strategies channel

**ShaunW**

AWS Lightsail instances are simple to set up and a reasonable price.  I can't see any reason not to use SQL, free, lightening fast, 50yr+ track record.

---

## 2024-04-15 12:52:39 - strategies channel

**A**

Thanks. I’ve created a Digital Ocean Droplet. Looks a little less overwhelming than AWS for me to get started.

---

## 2024-04-15 12:21:53 - strategies channel

**Fab**

If AWS machines are too expensive, have a look at DigitalOcean, which I totally recommend.

---

## 2024-04-15 10:27:19 - strategies channel

**liam**

What ever you prefer tbh, most use AWS

---

## 2024-04-15 10:20:43 - strategies channel

**A**

Can anyone recommend a cheap place to deploy a flumine bot? Been a while since I’ve done any web stuff. Is Heroku a viable option these days?



My stack is basically just Python + MongoDB.

---

## 2024-04-14 22:07:12 - general channel

**AndyL**

on a typical busy Saturday my AWS instance peaks about 4% CPU ....

---

## 2024-04-14 18:54:02 - general channel

**Dave**

The context is that I want to free up some connections to launch additional markets. But I'm worried that if I "merge streams" in this way, ticks may arrive "slower", (e.g. in this case GB ticks and AU ticks will be interleaved on the same stream rather than receiving them independently on two streams), or maybe even subject to forced conflation. Obviously you're still at the mercy of the GIL + the fact that flumine is single process, but still. I am imaging a scenario where stream throughput is always the same on Betfair's side, so subscribing to GB+AU on one stream may result in suboptimal delivery of ticks vs 2 streams with GB + AU independently. (just using GB + AU for example sake btw, obviously more concerned about markets that tick heavily in paralell).

---

## 2024-04-11 22:50:53 - issues channel

**Matthew Lawrence**

Hi, trying to use flumine for the first time. I'm starting off by testing an adjusted version of betfair data scientists: how to automate 2. I can see from the logging that the bets are never being matched, although they are being requested at the correct price and size. Any help would be appreciated.

```trading = betfairlightweight.APIClient('...','...!',app_key='...')

client = clients.BetfairClient(trading, interactive_login=True)



# Login

client = clients.SimulatedClient()

framework = FlumineSimulation(client=client)



# Logging

logger = logging.getLogger()

custom_format = "%(asctime) %(levelname) %(message)"

log_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_handler.setFormatter(formatter)

logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))  # Set to logging.CRITICAL to speed up simulation



class BackFavStrategy(BaseStrategy):



    # Defines what happens when we start our strategy i.e. this method will run once when we first start running our strategy

    def start(self) -&gt; None:

        print("starting strategy 'BackFavStrategy'")



    def check_market_book(self, market: Market, market_book: MarketBook) -&gt; bool:

        # process_market_book only executed if this returns True

        if market_book.status != "CLOSED":

            return True



    def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:



        # Collect data on last price traded and the number of bets we have placed

        snapshot_last_price_traded = []

        snapshot_runner_context = []

        for runner in market_book.runners:

                snapshot_last_price_traded.append([runner.selection_id,runner.last_price_traded])

                # Get runner context for each runner

                runner_context = self.get_runner_context(

                    market.market_id, runner.selection_id, runner.handicap

                )

                snapshot_runner_context.append([runner_context.selection_id, runner_context.executable_orders, runner_context.live_trade_count, runner_context.trade_count])



        # Convert last price traded data to dataframe

        snapshot_last_price_traded = pd.DataFrame(snapshot_last_price_traded, columns=['selection_id','last_traded_price'])

        # Find the selection_id of the favourite

        snapshot_last_price_traded = snapshot_last_price_traded.sort_values(by = ['last_traded_price'])

        fav_selection_id = snapshot_last_price_traded['selection_id'].iloc[0]

        [http://logging.info|logging.info](http://logging.info|logging.info)(snapshot_last_price_traded) # logging



        # Convert data on number of bets we have placed to a dataframe

        snapshot_runner_context = pd.DataFrame(snapshot_runner_context, columns=['selection_id','executable_orders','live_trade_count','trade_count'])

        [http://logging.info|logging.info](http://logging.info|logging.info)(snapshot_runner_context) # logging



        for runner in market_book.runners:

            if runner.status == "ACTIVE" and market.seconds_to_start &lt; 60 and market_book.inplay == False and runner.selection_id == fav_selection_id and snapshot_runner_context.iloc[:,1:].sum().sum() == 0:

                trade = Trade(

                    market_id=market_book.market_id,

                    selection_id=runner.selection_id,

                    handicap=runner.handicap,

                    strategy=self,

                )

                order = trade.create_order(

                    side="BACK", order_type=LimitOrder(price=runner.last_price_traded, size=5)

                )

                market.place_order(order)



# Fields we want to log in our simulations

FIELDNAMES = [

    "bet_id",

    "strategy_name",

    "market_id",

    "selection_id",

    "trade_id",

    "date_time_placed",

    "price",

    "price_matched",

    "size",

    "size_matched",

    "profit",

    "side",

    "elapsed_seconds_executable",

    "order_status",

    "market_note",

    "trade_notes",

    "order_notes",

]



# Log results from simulation into csv file named sim_hta_2.csv

# If the csv file doesn't exist then it is created, otherwise we append results to the csv file

class BacktestLoggingControl(LoggingControl):

    NAME = "BACKTEST_LOGGING_CONTROL"



    def __init__(self, *args, **kwargs):

        super(BacktestLoggingControl, self).__init__(*args, **kwargs)

        self._setup()



    def _setup(self):

        if os.path.exists("sim_hta_2.csv"):

            [http://logging.info|logging.info](http://logging.info|logging.info)("Results file exists")

        else:

            with open("sim_hta_2.csv", "w") as m:

                csv_writer = csv.DictWriter(m, delimiter=",", fieldnames=FIELDNAMES)

                csv_writer.writeheader()



    def _process_cleared_orders_meta(self, event):

        orders = event.event

        with open("sim_hta_2.csv", "a") as m:

            for order in orders:

                if order.order_type.ORDER_TYPE == OrderTypes.LIMIT:

                    size = order.order_type.size

                else:

                    size = order.order_type.liability

                if order.order_type.ORDER_TYPE == OrderTypes.MARKET_ON_CLOSE:

                    price = None

                else:

                    price = order.order_type.price

                try:

                    order_data = {

                        "bet_id": order.bet_id,

                        "strategy_name": order.trade.strategy,

                        "market_id": order.market_id,

                        "selection_id": order.selection_id,

                        "trade_id": order.trade.id,

                        "date_time_placed": order.responses.date_time_placed,

                        "price": price,

                        "price_matched": order.average_price_matched,

                        "size": size,

                        "size_matched": order.size_matched,

                        "profit": order.simulated.profit,

                        "side": order.side,

                        "elapsed_seconds_executable": order.elapsed_seconds_executable,

                        "order_status": order.status.value,

                        "market_note": order.trade.market_notes,

                        "trade_notes": order.trade.notes_str,

                        "order_notes": order.notes_str,

                    }

                    csv_writer = csv.DictWriter(m, delimiter=",", fieldnames=FIELDNAMES)

                    csv_writer.writerow(order_data)

                except Exception as e:

                    logger.error(

                        "_process_cleared_orders_meta: %s" % e,

                        extra={"order": order, "error": e},

                    )



        [http://logger.info|logger.info](http://logger.info|logger.info)("Orders updated", extra={"order_count": len(orders)})



    def _process_cleared_markets(self, event):

        cleared_markets = event.event

        for cleared_market in cleared_markets.orders:

            [http://logger.info|logger.info](http://logger.info|logger.info)(

                "Cleared market",

                extra={

                    "market_id": cleared_market.market_id,

                    "bet_count": cleared_market.bet_count,

                    "profit": cleared_market.profit,

                    "commission": cleared_market.commission,

                },

            )



# Searches for all betfair data files within the folder sample_monthly_data_output

data_folder = r"C:\Users\matth\OneDrive\Documents\output_2022_02"

data_files = os.listdir(data_folder,)

data_files = [f'{data_folder}/{path}' for path in data_files]



# Set Flumine to simulation mode

client = clients.SimulatedClient()

framework = FlumineSimulation(client=client)



# Set parameters for our strategy

strategy = BackFavStrategy(

    # market_filter selects what portion of the historic data we simulate our strategy on

    # markets selects the list of betfair historic data files

    # market_types specifies the type of markets

    # listener_kwargs specifies the time period we simulate for each market

    market_filter={

        "markets": data_files,

        'market_types':['MATCH_ODDS', 'BOTH_TEAMS_TO_SCORE'],

        "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

        },

    max_order_exposure=1000,

    max_selection_exposure=1000,

)

# Run our strategy on the simulated market

framework.add_strategy(strategy)

framework.add_logging_control(

    BacktestLoggingControl()

)

framework.run()```



---

## 2024-04-08 18:58:38 - general channel

**Trex44**

Hmm, that's really odd. I have absolutely no idea how I have reached 10 then. Have to dig about and see if I have left an old EC2 instance running by mistake or something.



I could combine the filters for the market recorders perhaps. They are different filters for each sport at the moment because I wanted to record different countries for some sports and because I use the context on each class instance/sport to direct the files to be saved in different S3 buckets.

---

## 2024-04-08 09:13:47 - random channel

**Paul**

Well, wait a second. I would not run vpn software on the same instance as where my prod strategies are. Also, depending on how that server is set up (EC2? Fargate?), you might be trying to run a container inside a container. That’s potentially not great for a bunch of reasons.

---

## 2024-04-08 09:10:03 - random channel

**Jonjonjon**

Sorry, my VPM question was in response to [@USWDY5P7G](@USWDY5P7G)'s suggestion that I turn my Linux cloud server into a personal VPN server.

---

## 2024-04-08 09:08:00 - issues channel

**Johnnb**

This is the code that sets up the logging :

```logger = logging.getLogger()

custom_format = "%(asctime) %(levelname) %(message)"

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_file_name = 'multi_recorder.log'

log_handler_file = logging.FileHandler(log_file_name)

log_handler_file.setFormatter(formatter)

log_handler_file.setLevel(logging.WARNING)

logger.addHandler(log_handler_file)```

and this is the whole contents of the log file :

```{"asctime": "2024-04-07 12:06:30,768", "levelname": "WARNING", "message": "[FlumineStream: 4001]: Latency high: 0.5309743881225586"}

{"asctime": "2024-04-07 12:22:38,592", "levelname": "WARNING", "message": "[FlumineStream: 3001]: Latency high: 0.5384097099304199"}

{"asctime": "2024-04-07 12:38:51,513", "levelname": "WARNING", "message": "[FlumineStream: 3001]: Latency high: 0.618567705154419"}

{"asctime": "2024-04-07 13:22:52,309", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.7499873638153076"}

{"asctime": "2024-04-07 13:22:52,309", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.7483816146850586"}

{"asctime": "2024-04-07 13:22:52,309", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.6966326236724854"}

{"asctime": "2024-04-07 13:22:52,309", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.5819070339202881"}

{"asctime": "2024-04-07 13:22:52,310", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.5760924816131592"}

{"asctime": "2024-04-07 13:32:00,671", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.6668083667755127"}

{"asctime": "2024-04-07 14:21:34,134", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.5454201698303223"}

{"asctime": "2024-04-07 14:21:34,135", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.5427672863006592"}

{"asctime": "2024-04-07 14:29:44,438", "levelname": "WARNING", "message": "[FlumineStream: 3001]: Latency high: 0.5154736042022705"}

{"asctime": "2024-04-07 15:04:02,227", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.7697350978851318"}

{"asctime": "2024-04-07 15:04:02,229", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.669356107711792"}

{"asctime": "2024-04-07 15:04:02,229", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.5565588474273682"}

{"asctime": "2024-04-07 15:04:02,229", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.5507323741912842"}

{"asctime": "2024-04-07 15:04:02,229", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.452915906906128"}

{"asctime": "2024-04-07 15:04:02,230", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.3592793941497803"}

{"asctime": "2024-04-07 15:04:02,230", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.3554434776306152"}

{"asctime": "2024-04-07 15:04:02,230", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.2436447143554688"}

{"asctime": "2024-04-07 15:04:02,230", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.1467902660369873"}

{"asctime": "2024-04-07 15:04:02,231", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.142089605331421"}

{"asctime": "2024-04-07 15:04:02,231", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 1.0893239974975586"}

{"asctime": "2024-04-07 15:04:02,231", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.9845194816589355"}

{"asctime": "2024-04-07 15:04:02,231", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.8789470195770264"}

{"asctime": "2024-04-07 15:04:02,232", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.8751261234283447"}

{"asctime": "2024-04-07 15:04:02,232", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.7643704414367676"}

{"asctime": "2024-04-07 15:04:02,241", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.7685585021972656"}

{"asctime": "2024-04-07 15:04:02,242", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.6691379547119141"}

{"asctime": "2024-04-07 15:04:02,242", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.574512243270874"}

{"asctime": "2024-04-07 15:06:28,467", "levelname": "ERROR", "message": "DataStream 2001 run error", "exc_info": "Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/flumine/streams/datastream.py\", line 172, in run\n    self._stream.start()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 60, in start\n    self._read_loop()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 228, in _read_loop\n    received_data_raw = self._receive_all()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 258, in _receive_all\n    raise SocketError(\nbetfairlightweight.exceptions.SocketError: [Connect: 2002]: Connection closed by server"}

{"asctime": "2024-04-07 15:15:02,315", "levelname": "WARNING", "message": "[FlumineStream: 3001]: Latency high: 0.6098229885101318"}```

---

## 2024-04-08 08:53:32 - issues channel

**Johnnb**

That's the only log I've got sorry. I'm pretty new to linux and aws. I ran the code from the command line in a detached shell. Could that have caused the issue?

---

## 2024-04-08 08:25:55 - issues channel

**Johnnb**

Hi everyone, I'm new here so might be missing something obvious. I'm running an s3marketrecorder in a lightsail instance. I started it yesterday morning and it still seems to be recording ok, but yesterday afternoon it stopped removing the txt and gz files after the markets closed. I got this error at about the time it stopped :

```{"asctime": "2024-04-07 15:06:28,467", "levelname": "ERROR", "message": "DataStream 2001 run error", "exc_info": "Traceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/flumine/streams/datastream.py\", line 172, in run\n    self._stream.start()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 60, in start\n    self._read_loop()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 228, in _read_loop\n    received_data_raw = self._receive_all()\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py\", line 258, in _receive_all\n    raise SocketError(\nbetfairlightweight.exceptions.SocketError: [Connect: 2002]: Connection closed by server"}```

The gz files are still being sent to the bucket and there's nothing else in the log apart from a few latency warnings

---

## 2024-04-08 08:13:36 - random channel

**Paul**

Or, you know, you could respect local laws. :-) 

---

## 2024-04-07 22:36:58 - random channel

**Fab**

Also, if your automation runs on a Linux cloud server (for example an AWS machine), you can transform that into your personal VPN server.



This way you'd connect to Betfair from your smartphone/browser abroad via your own VPN which means using your usual IP address.

---

## 2024-04-04 15:02:38 - strategies channel

**tone**

Does anyone know a good way to exit a program/strategy when the live market is closes? This happens automatically in simulation but the program just hangs idly when running live!

---

## 2024-04-03 21:29:42 - issues channel

**Derek C**

you could try using an absolute file path for 'certs' to rule out that being the problem. For example, mine is:



``` '/home/ec2-user/certs'```



---

## 2024-04-03 18:19:15 - issues channel

**Ammar**

Hi - me again — I’ve had confirmation my live key is streaming enabled, but I am getting this error back on the API when trying to run a simple strategy to just test the connection



`{"op":"status","id":2002,"statusCode":"FAILURE","errorCode":"UNEXPECTED_ERROR","errorMessage":"Unknown error authenticating","connectionClosed":true,"connectionId":"204-030424170351-166509"}`



I’ve raised a ticket to BDP, but wondering if anyone has any insight?

---

## 2024-03-29 18:09:55 - general channel

**PeterLe**

Thanks for the reply Peter

That makes sense.

I can see that Im being limited by max_live_trade_count etc (as the window scrolls past!) so wanted to look more in depth at the logs...

Im keen to look at the info to help with signals as you say too

Thanks Again

---

## 2024-03-29 18:07:27 - issues channel

**Peter**

Hopefully that clears things up for you. But if you need further help, please confirm that you are using a live key with streaming enabled. The delayed key is useful for nothing other than confirming that you can connect, but beyond that just causes problems.

---

## 2024-03-29 13:22:25 - issues channel

**Ammar**

or is it that streaming doesn’t work with the non-production api key?

---

## 2024-03-29 13:15:39 - issues channel

**Ammar**

does flumine work with the non-production app key?

---

## 2024-03-28 09:11:16 - general channel

**Adrian**

Hopefully this isn't as dumb of a question as my last. But I need to get runner names from Betfair market catalogue in order to match them up with my data. Is the catalogue only available from live markets? If so, is there any functionality in flumine to record these?

---

## 2024-03-28 07:22:46 - general channel

**liam**

You are in the right place :+1: 



Tbh not sure there is anything that would help, what experience do you have when it comes to deployment? 

---

## 2024-03-27 21:31:32 - general channel

**Ammar**

&gt;  I use docker on EC2 with portainer as my UI, let someone else do the hard work

Is there a recipe somewhere you’d be comfortable sharing?  Just want to get a sense of how everything gets set up

---

## 2024-03-27 21:19:10 - general channel

**liam**

I use docker on EC2 with portainer as my UI, let someone else do the hard work 

---

## 2024-03-27 18:22:54 - general channel

**Ammar**

Has anyone done any work to run flumine on AWS (or similar) - I’d be interested to have control and visibility over it on the go; but not sure how much of a lift it would be …

---

## 2024-03-27 15:10:16 - random channel

**D C**

Anyone experiencing problems today? I've had a total of 4 different bots running on distinct servers ALL crap out at around 13:30 today. NO_SESSION and HTTP 400 errors amongst other things?

---

## 2024-03-26 15:12:49 - general channel

**Brøndby IF**

[@U4H19D1D2](@U4H19D1D2) I tried to analyze both possibilities, but I couldn't find it (I apologize if I'm doing something wrong), see the `vars` of each of them from a live tennis event (`eventId`: 33136177) for example as there is no live football event with Live Video:



get_event_timeline:



```{'elapsed_time': 0.341503381729126, '_datetime_created': datetime.datetime(2024, 3, 26, 15, 5, 32, 960247), '_datetime_updated': datetime.datetime(2024, 3, 26, 15, 5, 32, 960247), '_data': {}, 'event_id': None, 'elapsed_regular_time': None, 'event_type_id': None, 'in_play_match_status': None, 'status': None, 'time_elapsed': None, 'score': None, 'update_detail': []}```

get_scores:



```{'elapsed_time': 0.34694671630859375, '_datetime_created': datetime.datetime(2024, 3, 26, 15, 8, 17, 616935), '_datetime_updated': datetime.datetime(2024, 3, 26, 15, 8, 17, 616935), '_data': {'eventTypeId': 2, 'eventId': 33136177, 'score': {'home': {'name': 'Daniel Rincon', 'score': '0', 'halfTimeScore': '', 'fullTimeScore': '', 'penaltiesScore': '', 'penaltiesSequence': [], 'games': '3', 'sets': '0', 'gameSequence': [], 'isServing': False, 'highlight': False, 'serviceBreaks': 0}, 'away': {'name': 'Carlos Taberner', 'score': '0', 'halfTimeScore': '', 'fullTimeScore': '', 'penaltiesScore': '', 'penaltiesSequence': [], 'games': '3', 'sets': '0', 'gameSequence': [], 'isServing': True, 'highlight': True, 'serviceBreaks': 0}}, 'currentSet': 1, 'currentGame': 7, 'fullTimeElapsed': {'hour': 0, 'min': 0, 'sec': 0}}, 'event_id': 33136177, 'elapsed_regular_time': None, 'elapsed_added_time': None, 'event_type_id': 2, 'match_status': None, 'time_elapsed': None, 'time_elapsed_seconds': None, 'status': None, 'current_day': None, 'current_set': 1, 'description': None, 'match_type': None, 'current_game': 7, 'current_point': None, 'full_time_elapsed': &lt;betfairlightweight.resources.inplayserviceresources.FullTimeElapsed object at 0x0000023DDC97B400&gt;, 'score': &lt;betfairlightweight.resources.inplayserviceresources.Score object at 0x0000023DDC97B010&gt;, 'state_of_ball': None}```

---

## 2024-03-26 14:45:26 - general channel

**Brøndby IF**

I don't have slack premium so I can't read old messages (only at most 2 months ago) and I believe this topic has been discussed before so I apologize in advance for a repeated question.



Well, could tell me the best way or if there is at least a way to filter the soccer games that have Betfair's "Live Video"?



In addition to the endpoints for collecting markets, I have also searched in `in_play_service.get_scores` and found nothing regarding this event detail.

---

## 2024-03-20 08:34:49 - general channel

**liam**

For live you need a worker, here is mine



```import logging

from flumine.utils import chunks

from betfairlightweight import BetfairError

from flumine.events.events import CustomEvent



logger = logging.getLogger(__name__)





def poll_race_card(context: dict, flumine) -&gt; None:

    client = flumine.clients.get_betfair_default()

    trading = client.betting_client

    if trading.race_card.app_key is None:

        trading.race_card.login()

    markets = _get_markets(flumine.markets)

    for market_ids in chunks(markets, 10):

        try:

            race_cards = trading.race_card.get_race_card(market_ids=market_ids)

        except BetfairError as e:

            logger.error(

                "poll_race_card error",

                exc_info=True,

                extra={"trading_function": "get_race_card", "response": e},

            )

            continue

        if race_cards:

            flumine.handler_queue.put(CustomEvent(race_cards, callback))





def _get_markets(markets: list, event_type_id: str = "7") -&gt; list:

    """Restrict to open and eventTypeId"""

    _markets = []

    for market in markets:

        if not market.closed and market.event_type_id == event_type_id:

            _markets.append(market.market_id)

    return _markets





def callback(flumine, event):

    # update market context

    race_cards = event.event

    for race_card in race_cards:

        for market in flumine.markets:

            for race_card_market in race_card.race.markets:

                if market.market_id == race_card_market.market_id:

                    market.context["race_card"] = race_card```



---

## 2024-03-11 16:35:27 - issues channel

**foxwood**

I think [@U012XF5CNPN](@U012XF5CNPN) explains it well and encapsulates the reasons I have turned the "feature" off. There are a mass of issues that can cause Python to throw concrete excpetions and they are not just related to Python, flumine or flumine strategy code. They include exceptions from the operating system and bugs in any of the host of python packages used - all of which would be ignored by flumine. Imho the probability of an exception arising is real and I favour the dead bot theory rather than the faulty live one.

---

## 2024-03-11 14:13:48 - issues channel

**Paul**

I think this default behaviour might be surprising behaviour to most. I think order controls in flumine in general can be a little surprising (selection exposure is not related to P&amp;L, only net exposure of trades on that selection, correct?), and in live trading these surprises could get expensive. I think defaults could defer to risk minimisation and that might make more sense: a dead bot that crashed out due to an exception can’t win big, but it also critically can’t lose big either. You can’t say that about a faulty live one…

---

## 2024-03-10 15:38:34 - issues channel

**Newbie99**

Okay (had to go for 3.9 as 3.11 wouldn't work for some reason), now oddly I get this (which I think some people mentioned before, but I am UK based):



```{"asctime": "2024-03-10 15:36:14,413", "levelname": "ERROR", "message": "OrderStream 1000 run error", "exc_info": "Traceback (most recent call last):\n  File \"/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/flumine/streams/orderstream.py\", line 40, in run\n    self.stream_id = self._stream.subscribe_to_orders(\n  File \"/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/betfairlightweight/streaming/betfairstream.py\", line 180, in subscribe_to_orders\n    self._send(message)\n  File \"/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/betfairlightweight/streaming/betfairstream.py\", line 284, in _send\n    self.authenticate()\n  File \"/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/betfairlightweight/streaming/betfairstream.py\", line 92, in authenticate\n    self._data(received_data)\n  File \"/home/ec2-user/.pyenv/versions/3.9.0/lib/python3.9/site-packages/betfairlightweight/streaming/betfairstream.py\", line 273, in _data\n    raise ListenerError(self.listener.connection_id, received_data)\nbetfairlightweight.exceptions.ListenerError: connection_id: 107-100324153614-1292830, data: {\"op\":\"status\",\"id\":1002,\"statusCode\":\"FAILURE\",\"errorCode\":\"INVALID_SESSION_INFORMATION\",\"errorMessage\":\"DSC-0036\",\"connectionClosed\":true,\"connectionId\":\"107-100324153614-1292830\"}\r\n"}```

---

## 2024-03-10 14:11:21 - issues channel

**Newbie99**

I just updated flumine and now I can't get it to start (looks like an issue with BFLW if I'm interpreting this correctly):



```  File "/home/ec2-user/trading/env/projects/startup.py", line 4, in &lt;module&gt;

    import betfairlightweight

  File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/__init__.py", line 3, in &lt;module&gt;

    from .apiclient import APIClient

  File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/apiclient.py", line 6, in &lt;module&gt;

    from . import endpoints

  File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/endpoints/__init__.py", line 10, in &lt;module&gt;

    from .streaming import Streaming

  File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/endpoints/streaming.py", line 2, in &lt;module&gt;

    from ..streaming import (

  File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/streaming/__init__.py", line 1, in &lt;module&gt;

    from .betfairstream import BetfairStream, HistoricalStream, HistoricalGeneratorStream

  File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 10, in &lt;module&gt;

    from .listener import BaseListener

  File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/streaming/listener.py", line 5, in &lt;module&gt;

    from .stream import BaseStream, CricketStream, MarketStream, OrderStream, RaceStream

  File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/streaming/stream.py", line 116

    if (cache := self._caches.get(market_id)) is not None and cache.active```

---

## 2024-03-08 18:44:48 - random channel

**ShaunW**

Usual caviat about flumine exposure [@UQL0QDEKA](@UQL0QDEKA) but my view would be to start each day the same, cleared logs, archived data, clock set, ie everything just like day 1. Then it should theoretically run indefinatly. All Mrs W needs to do is change the logon/api key, pay AWS, and it should keep paying out should the worst happen without any interaction with it. :wink:. Automation init, needs to be automated.

---

## 2024-03-07 14:46:54 - issues channel

**foxwood**

Had an issue last week with a test strategy running live. It was going through the runners placing bets and then hit an outlier not allowed for that resulted in a divide by zero. Flumine neatly ate the exception and fed the next `mc` to the strategy which placed bets, hit the outlier again and crashed again - rinse and repeat.



The impact of this was to start draining the bank rather quickly - fortunately I was monitoring it but it still took a big bite. After investigating, I discovered that flumine ate non-flumine exceptions due to `config.raise_errors = False` Taken 2+ years to fall down that hole. I know all the guff about testing and defensive code etc but the reality is that most of us are in permanent beta and things slip through.



I think this should default to `True` which would stop any runaways like mine and, more importantly, would be the expected behaviour for people new to flumine. In the meantime all my flumine wrappers set it to `True` right at the start.

---

## 2024-03-06 09:02:00 - general channel

**Prime**

mumbai (for example) to london network latency is around 120ms. If this is a manual user thing, I dont think that's particularly relevant. It also doesnt matter where the server is, as long as its "in the path" between the user and London, as the information will still have to travel from their fingers to London at a maximum of lightspeed.

---

## 2024-02-29 09:42:09 - strategies channel

**Lee**

You mentioned you’re using listener kwargs for your backtest, are actually you filtering in a similar way within the strategy when running live?

It should be quite easy to spot what’s going on just comparing bets of the two.

---

## 2024-02-28 16:55:05 - random channel

**PeterLe**

Thanks Peter. I was thinking about this last night after Lee sent his message

I did something similar when I was going down the ML path, ie I was running the backtest against recorded markets and out putting the bits i was interested into a CSV...then into Pandas. -&gt; ML etc

I wasn't quite sure i this was the way to do when running live strategies and if that was the right way of doing it etc

Thanks for the pointers

---

## 2024-02-28 14:10:01 - strategies channel

**ShaunW**

Shouldn't be any difference, it's just sourcing data from either the stream or the store and from then on presumably it's just the same code?   Live is different though as you're present in the data, live 'you' gets matches that the test instance therfore doesn't.  You've then got to fudge the latency so the test gets there before live did.

---

## 2024-02-28 11:47:43 - issues channel

**foxwood**

Anybody else seeing latency issues on market stream. I've been getting 0.5 seconds off almost continuously since I started about an hour ago. Have reset the clock so not that. BF issue, BF smacking my legs for some reason, poor connection (is HS fibre) AWS server seems ok recording - just local ?

---

## 2024-02-28 11:41:54 - strategies channel

**Andrey Luiz Malheiros**

Both scenarios have a market filter for WIN markets for greyhound racing in GB, and both have the same middlewares. The live strategy is running with no listener_kwargs, while the simulated one is running with listener_kwargs={"seconds_to_start": 600, "inplay": False}

---

## 2024-02-28 11:23:47 - strategies channel

**Andrey Luiz Malheiros**

Hey guys, I'm running a strategy in live paper trade mode and recording the markets with a market recorder. When I run a simulation for the same strategy over the same period using recorded files, I'm noticing a significant difference in results. So, I have two questions:

• Is there a big difference in results between the simulation and live paper trade mode?

• If not, can someone tell me the possible reasons why I'm seeing such a significant difference? (I'm observing a 320% difference in net profit)



---

## 2024-02-26 07:59:58 - general channel

**Mo**

I've just tried running the above Python code with the following changes:



1. `market_id` set to `1.225270654`

2. Added `app_key=""` argument to `betfairlightweight.APIClient`

With the following output:



```[&lt;RaceCard&gt;]

RaceCard £9000.00 1 Indian Louis, 2 Travail D'orfevre, 3 Nights In Venice

Fia Fuinidh (IRE) Made all in 2m handicap hurdle here last winter and promise when runner-up twice in 2m handicap chases this winter. Faded into fourth back from wind surgery here (2m) 13 days ago and remains to be seen whether this step up in trip suits.

Indian Louis (IRE) Dual point winner who left his qualifying runs over hurdles behind when making a winning chase debut at Musselburgh (2½m) on New Year's Day. Let down by his jumping when only fifth over C&amp;D since but he's probably worth another chance.

Travail D'orfevre (FR) Scored at Carlisle (2m) on his return and good runner-up efforts on all 3 outings since, including C&amp;D. Enters calculations again.

Jolly Nellerie (FR) Fairly useful winning hurdler in France but yet to fire in 4 starts so far for his current trainer, including switched to fences at Newcastle before Christmas. Sports first-time blinkers back from a short break.

Nights In Venice Fair maiden handicap hurdler who took to chasing when fourth of 12 over 3m at Carlisle 3 weeks ago but he didn't deliver much off the bridle (not for first time). Possible the return to shorter,

Ardera Cross (IRE) Nine-time course winner, the latest when accounting for Fia Fuinidh over 2m as the turn of the year. Does need to shrug off a poor run back here since, though.

[{'raceId': '1.2.1240226.1', 'markets': [{'marketId': '1.225270654', 'marketType': 'WIN', 'numberOfWinners': 1}, {'marketId': '1.225270659', 'marketType': 'EACH_WAY', 'numberOfWinners': 2}, {'marketId': '1.225270656', 'marketType': 'PLACE', 'numberOfWinners': 2}, {'marketId': '1.225270658', 'marketType': 'OTHER_PLACE', 'numberOfWinners': 3}, {'marketId': '924.395521980', 'marketType': 'WIN', 'numberOfWinners': 1}, {'marketId': '927.264606400', 'marketType': 'WIN', 'numberOfWinners': 1}], 'distance': 4510, 'startDate': '2024-02-26T14:05:00.000Z', 'raceClassification': {'code': 'H', 'classification': 'Handicap', 'classificationAbbr': 'Hcap', 'displayName': 'Handicap Chase', 'displayNameAbbr': 'HcapCh'}, 'raceTitle': 'New Bet-In-Race With Coral Handicap Chase (Qualifier) (4)', 'raceType': {'key': 'C', 'abbr': 'Ch', 'full': 'Chase'}, 'raceClass': 4, 'course': {'courseId': '1.2', 'name': 'Ayr', 'country': 'Scotland', 'countryCode': 'GB', 'courseType': 'Both', 'timeformCourseCode': 'Ayr', 'surfaceType': 'Turf', 'timezone': 'Europe/London'}, 'going': {'key': 'V', 'abbr': 'Hy', 'full': 'Heavy'}, 'prizeMoney': '£9000.00', 'eligibilityCriteria': {'ageLimitText': '5yo+'}, 'betfairMeetingId': '33052007', 'raceIdExchange': '33052007.1405', 'resultsStatus': 'NoResults'}]

{'raceId': '1.2.1240226.1', 'markets': [{'marketId': '1.225270654', 'marketType': 'WIN', 'numberOfWinners': 1}, {'marketId': '1.225270659', 'marketType': 'EACH_WAY', 'numberOfWinners': 2}, {'marketId': '1.225270656', 'marketType': 'PLACE', 'numberOfWinners': 2}, {'marketId': '1.225270658', 'marketType': 'OTHER_PLACE', 'numberOfWinners': 3}, {'marketId': '924.395521980', 'marketType': 'WIN', 'numberOfWinners': 1}, {'marketId': '927.264606400', 'marketType': 'WIN', 'numberOfWinners': 1}], 'distance': 4510, 'startDate': '2024-02-26T14:05:00.000Z', 'raceClassification': {'code': 'H', 'classification': 'Handicap', 'classificationAbbr': 'Hcap', 'displayName': 'Handicap Chase', 'displayNameAbbr': 'HcapCh'}, 'raceTitle': 'New Bet-In-Race With Coral Handicap Chase (Qualifier) (4)', 'raceType': {'key': 'C', 'abbr': 'Ch', 'full': 'Chase'}, 'raceClass': 4, 'course': {'courseId': '1.2', 'name': 'Ayr', 'country': 'Scotland', 'countryCode': 'GB', 'courseType': 'Both', 'timeformCourseCode': 'Ayr', 'surfaceType': 'Turf', 'timezone': 'Europe/London'}, 'going': {'key': 'V', 'abbr': 'Hy', 'full': 'Heavy'}, 'prizeMoney': '£9000.00', 'eligibilityCriteria': {'ageLimitText': '5yo+'}, 'betfairMeetingId': '33052007', 'raceIdExchange': '33052007.1405', 'resultsStatus': 'NoResults'}```

---

## 2024-02-25 19:47:52 - issues channel

**foxwood**

Probably doing something stupid but I'm getting an exception `Execution unknown error` thrown live (ok on backtest) using `market.replace_order()` called from within override of `stratBase.process_orders()` From the log the exception is thrown because the dictionary item `new_price` is missing from `update_data` although generated solely by `order.replace()` as part of the normal flow.

The scenario is playing with sub £1 bets by laying at very low odds, changing the size (works ok every time) and then replacing the order with a new price - which fails on about 2% of bets - I have a workaround that ensures the size/price combo doesn't hit `INVALID_PROFIT_RATIO` error.

All seems to happen on different thread - sample stack trace below. Can't reproduce at will. Feels like could possibly be python assign/copy issue around the "lost" dictionary item. Any clues / ideas welcome



`{"asctime": "2024-02-25 18:26:59,554", "levelname": "CRITICAL", "message": "Execution unknown error", "exc_info": "Traceback (most recent call last):`

  `File \"\\flumine\\execution\\betfairexecution.py\", line 251, in _execution_helper  response = trading_function(order_package, http_session)`

  `File \"\\flumine\\execution\\betfairexecution.py\", line 222, in replace  instructions=order_package.replace_instructions,`

  `File \"\\flumine\\order\\orderpackage.py\", line 164, in replace_instructions  return [`

  `File \"\\flumine\\order\\orderpackage.py\", line 165, in &lt;listcomp&gt;  order.create_replace_instruction()`

  `File \"\\flumine\\order\\order.py\", line 425, in create_replace_instruction  return {\"betId\": self.bet_id, \"newPrice\": self.update_data[\"new_price\"]}`

  `KeyError: 'new_price'", "trading_function": "replace", "exception": "'new_price'", "order_package": {"id": "4baba968-543d-49d4-84b4-a75355ab6a94", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x000001FE966E3EB0&gt;", "market_id": "1.225213820", "orders": ["139281784190412005"], "order_count": 1, "package_type": "Replace", "customer_strategy_ref": "stratXX", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}}`

---

## 2024-02-24 15:11:07 - general channel

**Trex44**

```max_selection_exposure=20,

max_order_exposure=20,

max_live_trade_count=2,

context={"stake": 20},```

Been using the strat for months and this has never occurred before. Luckily it won, so I can have a few extra pints later. Note strat_y also fired on the runner and behaved as expected, only placing one bet.

```

bet_id,strategy_name,market_id,selection_id,trade_id,date_time_placed,price,price_matched,size,size_matched,profit,side,elapsed_seconds_executable,order_status,market_note

339328190102,strat_x,1.225197849,42355492,4bb70e45-4e65-4378-adf8-e7eaeaf04249,2024-02-24 14:22:00.240611,3.05,0.0,20,0.0,0.0,BACK,20.14354,Execution complete,"3,3.05,3"

339328318899,strat_y,1.225197849,42355492,e15ba5ef-b32d-425d-a3e4-8c7e9a88a63a,2024-02-24 14:22:20.712415,2.94,0.0,20,0.0,0.0,BACK,20.203602,Execution complete,"2.92,2.94,2.94"

,strat_x,1.225197849,42355492,1565f6b5-d914-418d-bb6b-6529bc9dccc7,2024-02-24 14:22:40.971425,2.8,0.0,20,0.0,0.0,BACK,0.000393,Execution complete,"2.78,2.8,2.78"

339328394435,strat_x,1.225197849,42355492,4c1d4558-64ad-475c-96d1-fff3ea21dff8,2024-02-24 14:22:41.247138,2.8,0.0,20,0.0,36.0,BACK,20.15453,Execution complete,"2.78,2.8,2.8"

339328470581,strat_x,1.225197849,42355492,e75483b8-ea3f-4dc1-af85-c135771fbb7e,2024-02-24 14:23:01.665815,2.8,0.0,20,0.0,36.0,BACK,20.068519,Execution complete,"2.78,2.8,2.78"

339328552148,strat_x,1.225197849,42355492,995c2e62-c080-49c0-a92a-c3ace657498b,2024-02-24 14:23:21.966241,2.88,0.0,20,0.0,37.6,BACK,20.04243,Execution complete,"2.86,2.88,2.86"

339328632974,strat_x,1.225197849,42355492,943480d6-e9e4-48ab-9eae-4916ae78012c,2024-02-24 14:23:42.393435,2.88,0.0,20,0.0,17.86,BACK,20.438296,Execution complete,"2.86,2.88,2.86"

339328660906,strat_y,1.225197849,42355492,79919a27-7708-49c4-a01c-58635c5c0ea6,2024-02-24 14:23:50.201002,2.84,2.86,20,20.0,37.2,BACK,20.193885,Execution complete,"2.86,2.88,2.86"

339328709246,strat_x,1.225197849,42355492,4ab65045-1cd0-42a6-a8e9-d6e9d013263c,2024-02-24 14:24:03.043500,2.84,0.0,20,0.0,36.8,BACK,20.144299,Execution complete,"2.82,2.84,2.84"

339328835977,strat_x,1.225197849,42355492,7daaea34-4351-4fb7-bad3-e2d58ab24584,2024-02-24 14:24:23.507317,2.86,2.86,20,20,37.2,BACK,15.967436,Execution complete,"2.84,2.86,2.84"```



---

## 2024-02-22 10:07:51 - general channel

**jp**

C# + SQL Server here. Seemed to be a good choice at the time I started, and still works fairy well. If I had started today, I would probably go for flumine, as that takes away the need to write all the basic stuff yourself, and concentrate on strategies.

---

## 2024-02-17 14:17:19 - strategies channel

**D C**

TPD has been available through Gruss, BA and BM for quite some time now. It is also not free for users getting it through OTS software. The amount of utter horseshit assumptions about TPD I see on twitter from people who have NOT used it is incredible. TPD is great - but it has flaws. If it were that simple, TPD would likely never have made it available for use by paying customers and just milked the markets themselves. There are plenty of clever people on here who have said they've tried and failed to make TPD pay (or at least its not augmented their already profitable mehods). How many have paid X hundred quid for a month or two access only to find that they don't know what to do with it and then given up??? For sure the more people who access it will result in more people finding a way to use it profitably but they will be in the minority. Meanwhile those who try and fail because they think its a "pay to win" solution have given their money to other players and given up when they discover the truth.

---

## 2024-02-16 10:50:30 - issues channel

**Peter**

Hi [@U06KEECDXBJ](@U06KEECDXBJ) The exchange can be confused at first, but it's vitally important to get to understand it at a fairly deep level, as there are lots of very smart people, including in this Slack, waiting to take your money if you get it wrong.



If you want to back at 280, you're asking somebody to accept a lay that would cost them 280 - 1 = 279 per unit if they get it wrong. If the best back price is 1.1, then the layer willing to take the biggest risk is only prepared to risk 1.1 - 1 = 0.1 unit. The gap between 0.1 and 279 is sooo big, that it's not surprising that the bet is not matched.



Conversely if you try to lay at 280, when the best lay price is 560, that tells us that the least aggressive backer wants a potential profit of 559 before risking his hard earned points. But you're only offering him (or her) 279, so again no surprise that the bet isn't matched.



This is a simulation based on what actually happened, so the market never got to see your bets at 280. It is possible that had they been made in the live market, that would have caused the market to move and even get one or other of these bets matched, but realistically they're so far from the prices at that stage in the market's formation that it's very unlikely.



[@U4H19D1D2](@U4H19D1D2)’s cheat code allows you to test whether your strategy would be profitable if you were matched. It's useful for testing whether, over a large number of markets, your strategy is fundamentally profitable (or not). However, as you'll quickly find it's not that difficult to uncover strategies that would be profitable if somebody else were willing to take the bets, but a lot more difficult to find strategies with counter parties whose view of the market is sufficiently different to yours that you can get those matches.

---

## 2024-02-16 10:45:48 - general channel

**liam**

No flumine does the hard work for you as you have both available, the market object which has the `market.market_book` as it was at time x and the `sports_data` ie the tpd race update, this is how you would develop a TPD strategy as well, no change from simulating to live, its very powerful



```def process_sports_data(self, market, sports_data) -&gt; None:

    # called on each update from sports-data-stream

    print(market, sports_data)```

---

## 2024-02-16 09:08:04 - issues channel

**Al**

data from betfair live streaming that is recorded by other team members

---

## 2024-02-16 08:53:18 - issues channel

**Al**

live data

---

## 2024-02-10 13:10:28 - general channel

**Peter**

No, you can't use the market_filter for streaming. You have to use the streaming_market_filter. As the previous answers suggest, one-off API calls (market_filter) work in a fundamentally different way to streams (streaming_market_filter).



There is constant addition of new markets and removal of old markets from the stream, but this is done by Betfair rather than Flumine. Flumine's role is to initiate the stream, keep it open and deliver the results (market books or orders, depending on the type of stream) to your strategies.

---

## 2024-02-09 15:25:32 - random channel

**D C**

I used to find strategy reference string a really useful thing but it ultimately ended up hampering my performance and in conjunction with other things, cost me a lot of money.

I don't use it at all now and just use distinct suffix per strategy on customer order reference strings to distinguish between strats. You live and learn...

---

## 2024-02-05 21:29:07 - strategies channel

**Unknown**

So naturally after posting about it I managed to catch it in the act this morning.



Here are 3 logs, two right after one another, and a third one minutes later.



The bet moves from 0.0 matched, to 0.59 matched, but the order stays live/pending. Even minutes later. Because I’m not hedging until orders are execution complete I think this is likely the cause.



I’m running a slightly outdated version at 2.5.4 of flumine. I’ll try bump that again but is anyone else seeing this behaviour? Or is there something more reliable I can rely on for execution complete?

---

## 2024-02-05 09:18:05 - issues channel

**PeterLe**

Jeff thats the beauty of Flumine. You can create a strategy, backtest it and then just uncomment a few lines and you can then run it live. Testing to live in 10 seconds

Liam has wriiten the code for everything you need to do. There no point writing your own.

Ill send you a couple of files to get you started

---

## 2024-02-05 08:43:04 - issues channel

**Jeff Waters**

Hi Derek



I've been investigating what Flumine and what Better Lightweight can do, but for the bet placement at least, I think I will use Flumine. I've used it for back testing, and found it really useful. I think it will be fairly straightforward to adapt my back testing code to implement the system I want to go live with.



Thanks

---

## 2024-02-04 20:15:50 - issues channel

**Jeff Waters**

It's now working, with the exact same code as I posted above! Bizarre. I can only assume it was a server side issue that's now been resolved? Either that or there was something stuck in the cache locally that was causing issues?



Anyway, thanks guys.

---

## 2024-02-04 19:53:03 - general channel

**liam**

As a above, one process is enough to handle multiple strategies (10+ easy), multi processing isn’t helpful when running live 

---

## 2024-02-04 19:42:20 - issues channel

**Jeff Waters**

Hi Peter



Thanks for taking a look.



I'm not using logging. This is the stack trace, though:



*Exception in thread Thread-1:*

*Traceback (most recent call last):*

  *File "C:\Users\User\AppData\Local\Programs\Python\Python39\lib\threading.py", line 973, in _bootstrap_inner*

    *self.run()*

  *File "C:\Users\User\AppData\Local\Programs\Python\Python39\lib\threading.py", line 910, in run*

    *self._target(*self._args, **self._kwargs)*

  *File "C:\Users\User\Documents\golive\venv\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 60, in start*

    *self._read_loop()*

  *File "C:\Users\User\Documents\golive\venv\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 232, in _read_loop*

    *self._data(received_data)*

  *File "C:\Users\User\Documents\golive\venv\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 273, in _data*

    *raise ListenerError(self.listener.connection_id, received_data)*



This is my full code:



```import betfairlightweight

import queue

import threading

import json



from betfairlightweight.filters import streaming_market_filter, streaming_market_data_filter

from log_in import log_in_and_get_API_client_object



# [https://github.com/betcode-org/betfair/blob/master/examples/examplestreaming.py](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaming.py)



trading = log_in_and_get_API_client_object()

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener)



# create filters (GB WIN racing)

market_filter = streaming_market_filter(

    event_type_ids=["4339"], country_codes=["GB"], market_types=["WIN"]

)

market_data_filter = streaming_market_data_filter(

    fields=["EX_BEST_OFFERS", "EX_MARKET_DEF"], ladder_levels=3

)



# subscribe

streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=1000,  # send update every 1000ms

)



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()



"""

Data can also be accessed by using the snap function in the listener, e.g:



    market_books = listener.snap(

        market_ids=[1.12345323]

    )



Errors need to be caught at stream.start, resubscribe can then be used to

prevent full image being sent, e.g:



    streaming_unique_id = stream.subscribe_to_markets(

        market_filter=market_filter,

        market_data_filter=market_data_filter,

        conflate_ms=1000,  # send update every 1000ms

        initial_clk=listener.initial_clk,

        clk=listener.clk,

    )



The streaming unique id is returned in the market book which allows multiple

streams to be differentiated if multiple streams feed into the same queue.

"""



# check for updates in output queue

while True:

    market_books = output_queue.get()

    print(market_books)



    for market_book in market_books:

        print(

            json.dumps(market_book.streaming_update),  # json update received

        )```

If I change the event type id back from "4339" to "7", the app works fine.

---

## 2024-02-02 07:37:47 - general channel

**Aiyaj Khalani**

no, i am just thinking about how can we improve latency while placing a bet from my server which is based on india to betfair API

---

## 2024-02-02 05:06:31 - general channel

**Aiyaj Khalani**

Hi All,

I have deployed my server in london which is interacting with betfair API. i am calling API for placing bet hosted on my sever. for live events, it seems betfair is taking around 5.5s to place a bet. but since my user is not from the london and its from india, i see extra 1s overhead on sending request to my server. i thought of moving my server to india but then betfair APIs will not be accessible from my server. i was thinking to add VPN in my server to interact with betfair API but again it will add some extra overhead due to VPN proxy. I saw many exchanges place bets around 5.5s, is there any good way to reduce latency in my case?

---

## 2024-02-01 18:13:31 - issues channel

**D C**

No I'm not overly concerned. When I say "plain text" I mean spread across multiple string literals that get contatenated at when the login stuff is called. Probably makes it a bit harder anyway but you have to draw the line and accept defeat at some point. If your server is compromised then you've got a problems regardless.

---

## 2024-01-30 14:25:07 - general channel

**Trex44**

```QueryFailed: TYPE_MISMATCH: Unable to read parquet data. This is most likely caused by a mismatch between the parquet and metastore schema. You may need to manually clean the data at location '[s3://aws-athena-query-results-630653152022-eu-west-2/tables/3cfa2ebf-9d15-438d-8615-822f68e2af17](s3://aws-athena-query-results-630653152022-eu-west-2/tables/3cfa2ebf-9d15-438d-8615-822f68e2af17)' before retrying. Athena will not delete data in your account.```

Yea they typically look like the above. I don't understand why the metastore schema is different. I generate the schema off of the parquet files using a crawler. If I save the files as csv's I get no problems at all. I crawl the csv's, get the table and then there is no issue.

---

## 2024-01-28 14:31:16 - strategies channel

**liam**

I would say more recently my signals come from fundamentals or live data and execution is driven by price action 

---

## 2024-01-28 10:08:34 - general channel

**D C**

He was a strange one. I remember all that stuff like "my codebase providing the most advanced betting framework out there" was hard to take. Then we had gems like "there is more than one value price for an outcome". Could never get my hear around why someone who had worked for a horse syndicate would start off his solo career with a partner who modelled tennis!! Ended up blocking him on twitter as his ego was writing cheques that his IQ could not deliver. Last I heard he was doing something with GPS and greyhounds.

Honestly surprised he never released an e-book.

Genuinely miss him though. We need more quirky characters!!

---

## 2024-01-27 15:39:34 - general channel

**Dave**

Looking to upgrade to latest flumine version, have been on 1.21.4 for a few years. Noticed a few things changed (renaming Backtest to Simulation), otherwise my backtests are producing the same numbers (which is a massive relief). Any gotchas that come to mind to watch out for, that might not pop up when comparing backtests (e.g. live order entry, maybe logging control behaviour etc)? Thanks in advance. Trying to minimize any disruption in prod...

---

## 2024-01-27 11:20:26 - strategies channel

**birchy**

I feel your pain [@UUE6E1LA1](@UUE6E1LA1), mainly because I've built a heavy reliance on simulation and now know that I've binned quite a few potentially good strategies because they under/over performed in simulation. For passive bets, Flumine simulation is pretty accurate, although I have found it tends to under match vs live, but that's ok as long as you account for it. So in conclusion, simulation is probably best used for development, bug fixing and general analysis but going live is the only true way to find out how our bets affect XM, market impact, match rates, etc.

---

## 2024-01-27 10:19:52 - strategies channel

**D C**

Yeah I don't blame you [@U4H19D1D2](@U4H19D1D2) just thinking about all of the interactions and complexities makes my head hurt. I'm going to just leave mine as it is and use it as a rough indicator of a strategy being worth moving to live or not. It might make me design strategies in a way that minimises these issues from now on though.

---

## 2024-01-27 10:08:34 - strategies channel

**D C**

I was thinking about it all day after your initial message made the penny drop for me. I suppose it might only be a problem if you are placing large numbers of bets within a smal time frame. I thought I had a dirty "solution" but I didn't bother implementing it because I then thought of another issue. Not sure if flumine caters for this but  my own simulator certainly doesn't: if your strat uses LTP or TV as a signal when you place an actual bet, you'd receive an API update with the LTP and your own traded volume. In simulation you'd not receive these values - to properly simulate you'd need to artificially insert a new stream update containing your own simulated match (or add to an existing update if the times matched - does flumine actually do this??).

So in reality, any strategy (and other concurrently run strategies) running live, is potentially acting on market signals that they themselves create - but my simulator doesn't add these extra stream updates.

This made me start thinking about running simulations on recorded market data that your own strategies ran on historically - so you could be simulating matches of your own offered money, or competing with your own historical orders.

So basically the whole quantum mechanical nature of simulation made me slightly depressed and ask if it is even worth trying to get more accurate.

Don't get me wrong though [@U016TGY3676](@U016TGY3676) -  deep down I am glad you posted your original post. It's always good to try and find holes in your own code.

---

## 2024-01-27 00:28:23 - strategies channel

**birchy**

[@UUE6E1LA1](@UUE6E1LA1) you described the issue perfectly. [@UBS7QANF3](@UBS7QANF3) so all of my simulation stuff assumes bpe is off as I've not (yet) found a single strategy that is better with it enabled. I used to keep track of the previous price and only checked my signals if the price had changed in the current update. That has an issue where: if the price was adjusted but the size available remains the same, then I'm still taking more money than is available.

I also tried various delays and also forcing my bets to be passive, where I enter at 1 tick away from where I want to be but then the strategy is no longer executing at the desired price.

The main issue is simulating strategies which are operating on market movement where missing the target price affects profit. I've currently settled for one bet per price/side/selection but that significantly reduces the hit rate. Testing these strategies live, I'm seeing different matches vs simulation of the same markets. My best live strategy (which has been running for several years) would of been binned if I had simulated it before going live and still performs poorly in simulation, so I guess I'm operating within XM/market impact territory and live testing with small stakes is probably the way forth.

---

## 2024-01-26 13:40:29 - general channel

**jnik**

Hi I have found a winning strategy on greyhounds (around 3 months of data), I've successfully tested this strategy past few days,  but problem is I need to spend 12 hours a day betting and its extremely tiring. So I'd like to automate this.



The information is contained in my own db -&gt; mysql and server is a remote server that runs  ubuntu.



Is their any guide to building this automated bot?

---

## 2024-01-24 10:43:18 - general channel

**Aiyaj Khalani**

Hi,



I am using streaming API of betfair with betfairlightweight with live API key. but still the odds which i am getting from streaming API differs from [http://betfair.com|betfair.com](http://betfair.com|betfair.com) and it;s playground. what can be the issue with this?

---

## 2024-01-22 16:25:23 - random channel

**Paul**

It’s from flumine logs, pretty certain it should be RTT of the api operation. When you’re just pinging the servers from the command line you’re getting a different (lower, faster) RTT down in the ICMP. Coincidentally the original inventor of ping (and ntp), died last week.

---

## 2024-01-22 16:10:00 - random channel

**foxwood**

Bored so thought I'd try that ping on my new Irish Lightsail server - it's meaningless for what I do but shows that milliseconds are too coarse for latency measurement these days

`Approximate round trip times in milli-seconds:`

    `Minimum = 0ms, Maximum = 1ms, Average = 0ms`

---

## 2024-01-21 19:23:32 - random channel

**Ralegh**

Jiggling about with servers to get a good connection is a valid strategy 

---

## 2024-01-18 12:37:19 - random channel

**PeterLe**

I just fed some of my code into Chatgpt for comments on how to optimse.

it suggested that this code ;



```def calculate_book_percentage(runners):

    back_book = 0

    lay_book = 0

    ltp_book = 0



    for runner in runners:

        if runner.status == "ACTIVE":

            try:

                back = get_price(runner.ex.available_to_back, 0)

                if back:

                    back_book += 1 / back

            except AttributeError:

                return None



            try:

                lay = get_price(runner.ex.available_to_lay, 0)

                if lay:

                    lay_book += 1 / lay

            except AttributeError:

                return None



            if runner.last_price_traded:

                ltp_book += 1 / runner.last_price_traded



    return round(back_book, 4), round(lay_book, 4), round(ltp_book, 4)```

be replaced by this code

```def calculate_book_percentage(runners):

    back_book, lay_book, ltp_book = 0, 0, 0



    for runner in runners:

        if runner.status != "ACTIVE":

            continue



        back = get_price(runner.ex.available_to_back, 0)

        lay = get_price(runner.ex.available_to_lay, 0)

        ltp = runner.last_price_traded



        if back:

            back_book += 1 / back

        if lay:

            lay_book += 1 / lay

        if ltp:

            ltp_book += 1 / ltp



    return round(back_book, 4), round(lay_book, 4), round(ltp_book, 4)```

The rationale presented as ;

Key Changes:

1. Removed `try-except` Blocks:

    ◦ The original version used `try-except` blocks to catch `AttributeError` if the `available_to_back` or `available_to_lay` prices were not available. This can be inefficient in a loop.

2. Direct Checks for Price Availability:

    ◦ The optimized version directly checks if the `back` and `lay` prices are available using simple `if` statements, which is more efficient than handling exceptions.

3. Simplified Logic:

    ◦ Streamlined the logic to be more concise and straightforward, improving readability and performance.

By making these changes, the function becomes more efficient, especially in a real-time data processing context where

every millisecond counts



I've tested in back tests (all ok) and ran it live yesterday, all OK.

As I only know a very basic amount about Python, would any more experienced coders comment as to whether it is a positive change for the better? Thanks

---

## 2024-01-16 18:15:41 - general channel

**PeterLe**

Thanks [@U02RN7YDRQ9](@U02RN7YDRQ9) the thing is, it sometimes does it when I first start it running, moments after I sync the time. Anyway, I have a Ubuntu system on AWS I might just move my flumine stuff there, simple enough to do 

---

## 2024-01-16 15:22:45 - general channel

**PeterLe**

Well Ive had these on and off intermittently Liam since I started using Flumine (either when running locally or on AWS).

Some days its fine, last 3 or 4 days have been fine for instance..

I usually have the terminal window visible so i can keep my eye on it when doing the day job alongside...ie of any error messages etc..

When the latency message comes through it just streams down the page and I can see other messages

I know its there for a reason but its working and making profits etc

---

## 2024-01-16 12:29:16 - general channel

**Tony**

hello all i am going to start using the market recorder and have an aws account does any one know if i can get away with a micro ec2 instance or does it need more juice?

---

## 2024-01-15 20:40:56 - general channel

**Steve**

Hello :wave:  apologies I'm new to this and I'm sure you've probably seen my questions 100 times before, but I was just hoping for a little direction if possible:



1. I'm looking to parse the historic data files for American football data and preferably to analyse in a csv file.  For this I thought I might need to use betfairlightweight rather than flumine? As I'm not looking to do automatic trading atm I thought BFW was the right program for that?

2. If so, I was looking at the [https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py|example historical stream](https://github.com/betcode-org/betfair/blob/master/examples/examplestreaminghistorical.py|example historical stream). Given the data file structure is in the format of `{month}/{day}/{marketID}/{...bz2}` I guess I need to traverse the folders and load in each file individually?

3. I've seen people talk about recording prices themselves though the [https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|MarketRecorder](https://github.com/betcode-org/flumine/blob/master/examples/strategies/marketrecorder.py|MarketRecorder) rather than buying historic data after the fact. Do you generally put that on an EC2 or something running 24/7?

Apologies if these are all very basic!

---

## 2024-01-15 19:25:13 - general channel

**Jonjonjon**

I'm only part time, but I could live off my Betfair winnings. A reason I don't is that my model could stop working at any point in time (similar to being able to get fired at any time I guess), and that being employed gives you benefits such as NI contributions (but if you have children, you can claim child benefit and that will give you NI contributions too without having to work).



But in your favour, I think some of the most successful guys here are software devs. I'm a professional-level Python dev (which doesn't mean I'm one of the best, it just means that I'm good enough to convince people to pay me to do it.)



One thing to consider is... What was your worst ever losing day, and how did you feel about it?

---

## 2024-01-07 22:53:37 - strategies channel

**foxwood**

Guessing he tweaks the latency factors in config.py ...

```# latencies used for simulation

place_latency = 0.120

cancel_latency = 0.170

update_latency = 0.150

replace_latency = 0.280```

Might be interesting to find out my live latency and mod those variables to match so backtests closer to (my) real world timings - yet another thing for the puzzles board.

---

## 2024-01-04 15:20:55 - issues channel

**NAS**

when i launch my flumine instance, it runs correctly for a while (30-60 mins) but then starts throwing invalid session errors when performing account operations



flumine.clients.betfairclient - ERROR - BetfairClient `account.get_account_funds` error

flumine.clients.betfairclient - ERROR - BetfairClient `account.get_account_details` error



Full Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'AANGX-0002', 'data': {'exceptionname': 'AccountAPINGException', 'AccountAPINGException': {'requestUUID': 'null', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}}}, 'id': 1}



guess there's a problem in the keep-alive updating?

---

## 2024-01-03 12:05:19 - general channel

**Trex44**

Thanks for the info Mo. Very interesting. I have hardly any edge in horse racing. The strategies have back tested well but have been live for such a short amount of time that I can't get confident enough to put large sums into them yet. They don't require live data feeds but I spend a fair bit on AWS services crunching through the data to find the edge. I picked betfair trading to try my hand at over financial markets back in 2017ish as I thought it would be easier that financial markets :woozy_face:. I did Peter Webb's course about 5 or 6 years ago, met him in person. Lovely guy, but realised I would never have the temperament to sit in front of a screen and do what he does, would much rather automate it but didn't know how then. So it went on the back burner whilst other things took over. Got serious about it again at the start of 2023. Part of the appeal for getting back into it was Flumine, I wanted to learn python and having a project to work on rather than craming leet code is a good way to do it. It has been great for learning python and how AWS works.

---

## 2024-01-02 15:50:09 - general channel

**Ricky**

Hi everyone! I'm fairly new to the automated betting world and would like to start by getting my hands on some of the historical data that betfair supplies at [https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home). Problem is that I live in Sweden and this service is not accessible for us swedes... Does anyone know an alternate way to get access to historical data? Thanks in advance! :raised_hands:

---

## 2023-12-29 08:53:57 - random channel

**Adrian**

[@UQL0QDEKA](@UQL0QDEKA) i have used the backtester in limited capacity only since I only have free historical data to work with. That can only give me a rough indication of whether or not I am beating SP, but not how my execution if performing. I have created a simulation using last traded prices of that data and at this point can only make an estimated guess as to how well I would get matched because there is no volume data and the intervals are unusable. This has been one of the major hurdles of production.. actually having quality data to work with. Unfortunately betfair requires you to put money down in order to get data one way or another. So there is that barrier to entry to contend with

---

## 2023-12-13 09:52:13 - strategies channel

**Adam**

long story short - they’re slightly different strategies that I want to test on live data rather than on historical data and if they’re sequential, the latter will be effected by the delay to processing

---

## 2023-12-12 09:17:59 - general channel

**Justice**

[@UUE6E1LA1](@UUE6E1LA1) Yes, you can get access to the Proform DB. If I remember, it installs an old version of SQL Server locally on your machine. You can view the raw database with something like SSMS or DataGrip, export it elsewhere and get it into the format you want

---

## 2023-12-07 17:46:03 - general channel

**Trex44**

Hey all, just a general question on the fastest most cost effective way to back test. I currently run back tests on a spot instance on AWS, usually has 256 vCPU's. I use multiprocessing and I usually test on 20,000 horse racing win markets at a time  and save the data to S3 in parquet files, then load up in jupyter. Whilst fast, it still takes me longer than I would like to back test the markets (a single strat takes 10's of minutes). I was wondering if anyone with can suggest how to scale this or if there is something I am doing that is obviously wrong.

---

## 2023-12-05 16:06:05 - issues channel

**ChrisM**

ah ok, the live key is the one you pay the £299 fee for? which is not a problem i was just hoping to get setup in testing first

---

## 2023-12-05 16:03:35 - issues channel

**liam**

thats your problem, all delayed key use is banned on here, send me a message with your live key and i can get it activated if not already

---

## 2023-12-05 08:46:31 - strategies channel

**PeterLe**

Hi [@U016TGY3676](@U016TGY3676), yes it did:

```# Feature Importance

print("Calculating the feature importance....")

importances = random_forest_model.feature_importances_

sorted_indices = np.argsort(importances)[::-1]

print("Feature Importance:")

for index in sorted_indices:

    print(f"{features[index]}: {importances[index]}"```

But it doesn't tell you much detail, that's why I opted for SHAP as i thought it would provide more detailed info

By the way; its been pointed out to me that whilst my initial thoughts were to see if ML could enhance a current system, the ML strat maybe a completely different strat to what my live one is

So now i know there is a better strat that the one im running live, I just have to work out what it is :grinning: :exploding_head:

---

## 2023-12-04 10:33:21 - random channel

**Unknown**

It might be unrelated actually as that error seems to come up fairly often and it just looks like the server isn't responding.  Just went back a bit further and see a cryptic "Killed" line in the logs, and then the container restarts on the next line... I'll do a smaller market filter and report back.



CPU is 32 cores of a Xeon from 2017, 32gb allocated.

---

## 2023-12-03 12:09:35 - random channel

**river_shah**

Have you guys managed to run container registries on it? Seems like AWS / GCP artifact registries also become obsolete with this. Both cloud buckets with their egress charges and artifact regustries are egregiously priced on AWS / GCP and can't wait to optimize that out.

Did you guys discover any downsides for R2 / cloudflare?

---

## 2023-12-03 11:47:48 - random channel

**Mo**

I've seen it advertised as an AWS S3 killer but haven't tried it myself

---

## 2023-12-01 15:25:36 - betfair-news channel

**Carl**

Hi everyone,

I'm pleased to announce the launch of our 3-part Cricket webinar which can be found below;



[https://betting.betfair.com/betfair-announcements/exchange-news/forecasting-for-cricket-markets-webinar-watch-episode-1-of-our-expert-guide-221123-272.html|https://betting.betfair.com/betfair-announcements/exchange-news/forecasting-for-cricke[…]webinar-watch-episode-1-of-our-expert-guide-221123-272.html](https://betting.betfair.com/betfair-announcements/exchange-news/forecasting-for-cricket-markets-webinar-watch-episode-1-of-our-expert-guide-221123-272.html|https://betting.betfair.com/betfair-announcements/exchange-news/forecasting-for-cricke[…]webinar-watch-episode-1-of-our-expert-guide-221123-272.html)



In a bid to increase Cricket education amongst our more technical clients, we've teamed up with Ian McHale (Professor of Analytics, University of Liverpool) and Muhammad Asif (Head of Statistics at the University of Malakand). Alongside Ed Hawkins, they run through the basics of Cricket before progressing quite quickly on to the more technical elements, including areas of their academic research such as the Duckworth & Lewis method.



Part 2 will focus on factors of their own in-play model which should be released before Xmas. Part 3 will follow in the new year and is _expected_ to introduce player ratings although this is subject to change.

---

## 2023-11-29 12:56:52 - random channel

**George Swindells**

I use AWS RDS because it's the cloud provider with which I'm most familiar, I doubt there is much difference between all the big providers so I would say just choose whichever you know best. Though a colleague recently had a lot to say about how awkward it is to migrate a database out of the AWS ecosystem to deploy elsewhere, and said in Azure it's much simpler but I don't know about this first hand.

---

## 2023-11-29 08:51:03 - strategies channel

**Unknown**

I have a model and starting to use it in anger, out of interest is this the best way to be calling it in a production env, seems mega hacky..

---

## 2023-11-27 16:00:19 - random channel

**Peter**

Make sure you know what you're getting if you go for a cheaper option.



I went down that route a few years back and got a big VPS at a really good price. However, when I stepped up the volume to use that power, they severely throttled my throughput to the vCPUs arguing that my processing load contravened their "fair use" policy. So headline power isn't necessarily what you'll get in practise.



I also had issues with the IP address allocated to the VPS, as despite being repeatedly assured that the appropriate registries had been updated to show the VPS as in the UK, which I believe it was, Betfair (most likely going of the IP address) has never accepted this.



Many people here just use AWS. I have for the past three years and have never had any issues.

---

## 2023-11-27 15:46:46 - random channel

**Javier Martín Pérez**

Hi! Not sure if anybody else has this problem but in terms of football trading Saturday and Sunday afternoons require a lot of computing and need to upgrade my server. At the moment I am using [http://MVPS.net|MVPS.net](http://MVPS.net|MVPS.net) and the prices grow a bit too much when you go from 4 processors to 8. However I have seen others like OVH that offers the same for a quarter of the price. Any experience with them?

---

## 2023-11-26 00:15:25 - general channel

**Adam**

```thoroughbreds_event_filter = {

        "eventTypeIds": ["7"],

        "marketCountries": ["AU"],

        "marketStartTime": {"to": end_of_day_str},

        "marketTypes": ["WIN"],

        "raceTypes": ["Flat"]

    }

market_filter={

            "markets": market_ids,

            'market_types': ['WIN'],

            "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

            "event_processing": True

        },```

```strategy = Strategy(

    market_filter={

        "markets": market_ids,

        'market_types': ['WIN'],

        "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

        "event_processing": True

    },

    market_data_filter=market_data_filter,```

Hi all. I have generated a list of market_ids (e.g `market_ids = [1.170212754, 1.170212755, 1.170212759, …]`) which consists of all thoroughbred markets that start and end in a given day (determined by `thoroughbreds_event_filter`).

The goal is that for each race, the flumine strategy only starts processing events 80 seconds before the start of each race and continues receiving events while the race is in play. My question is, can I use `listener_kwargs` param with multiple `market_ids`? Do I need to set `inplay` to true if I want to monitor events while the race is live, and do I need to set `event_processing` to True? Thank you

---

## 2023-11-25 07:35:19 - general channel

**db888**

[@U067MDW0YPK](@U067MDW0YPK) Same issue I was having, despite only attempting to scrape it every few minutes throughout a live event. I've found that the rate limiting is non-deterministic - for some events I can hammer it and get what I want without issues, but in other cases it just fails with a 403 error. Hence why I feel the `Public Streaming API` may be a better option than the `loadRunnerChart` private API. Worth also noting that the priv API endpoint will purge data shortly after a match has ended - so if you wanted to scrape something after the fact, you won't get data via this method.

---

## 2023-11-21 15:59:31 - random channel

**Unknown**

Can anyone recommend study material so I can learn how to calculate the correct score for live football matches based on handicap and goals data?

---

## 2023-11-21 09:57:38 - random channel

**Jimmy**

Simulations against data — all bets under 2 are being rejected (even the ones above £1, which would be fine live). Is there a setting I’m missing.



(Apologies for the recent surge in daft questions!)

---

## 2023-11-18 11:25:49 - issues channel

**Derek C**

Thanks Peter, I wondered if that would be the solution. My reason for trying to get it to work the way I've coded it is because I run my simulation in the cloud (docker + fargate) and it would be more convenient if it could access AWS dynamically within middleware rather than download locally first. I take your point about the network traffic. Mental laziness on my part :slightly_smiling_face: Looking at the class "SimulatedDateTime" gives me the impression I should be able to switch modes.

---

## 2023-11-17 20:31:11 - issues channel

**Derek C**

I'm having a problem using middleware in Flumine. Based on the  example: _examples/middleware/marketcatalogue.py,_ I want to populate market catalogues in a simulation. The trouble is that I store the market catalogue files on AWS S3 and the calls to AWS get rejected because the API call is using the monkey-patched version  of datetime.utcnow() in the request, causing AWS to reject the request.



The AWS error message is:

>  _"botocore.exceptions.ClientError: An error occurred (RequestTimeTooSkewed) when calling the GetObject operation: The difference between the request time and the current time is too large."_

So my question is - how can I use a middleware class without the monkey-patch?



I've tried using the SimulatedDateTime class but I'm stumped.

---

## 2023-11-17 19:22:59 - issues channel

**AndyL**

[@U4H19D1D2](@U4H19D1D2) thanks Liam, yes, I think i've worked it out, it's not a memory leak per-se. Basically since I added PLACE to the listener, I am now subscribing GB,IE and WIN,PLACE for this one strategy, and it actively trades from 3hours out from startTime. What I see is a gradual increase in CPU towards mid-day when the main afternoon races fall within that 3hour window. I roughly calculated the market processing thread will be going around 50 markets, and my strategy code is probably not as efficient as it could be... My VM CPU then hits 50% in a 2vCPU box, which then starts to limit it, the affect it had was to cause the market update processing to lag the live updates, causing a steady progression of increase in latency, eventually towards a latency of 30mins!!! My trading  logic once the latency hit around 5mins was starting to put orders in for markets that were now closed! Thus getting lots of BF errors, and what made things worse for my hedge bets I was rather stupidly immediately re-placing the failed hedge causing yet more of a race......and eventually bang!

What i've learned:

1. I need to profile my strategy code more to try and make it more efficient. I've already done a good set of fixes...

2. I'm perhaps pushing my luck adding PLACE and doubling the number of active markets in that 3hour trade window, running on a single Flumine instance....

3. Importantly I need to monitor "latency" to prevent this sort of CPU race condition

I've done a set of fixes for 1 & 3 and I'm testing them now.....

---

## 2023-11-17 13:10:08 - general channel

**Unknown**

Hi I'm new to the `betfairlightweight` Python library. I'm aiming to get a data stream of prices over time for live matches that's reflective of the viz that the Betfair UI shows you (below). Is there a method that will do this for live events, or do I need to use the `Streaming API` throughout the event and subsequently store the results manually?

---

## 2023-11-15 09:42:40 - random channel

**D C**

[@UQL0QDEKA](@UQL0QDEKA) I did try this. I was out of work at the peak of pandemic as my contract was not renewed so turned to this full time. Was approached by a betting company and started working there - great chaps but I honestly could not get motivated. I was getting paid a quarter of what I was making in profits on Betfair and the London commute was costing me 3 hours a day (had to be on site 3 days a week). That combined with not enjoying what they had me working on meant my motivation just dropped off a cliff and I ended up resigning. I think that I would have the same problem anywhere else now so if I did end up on the grid again I think I would have to just be self-employed in some way. Didn't help that it was a startup and most of the people were really enthusiastic youngsters at that live-to-work stage of their lives. They were all great people but I am at that age now where going on the piss after work is not what I am looking for so I missed out on a lot of the social stuff I would have lapped up 20 years ago. The penny finally dropped when they had some golf up on one of the internal TVs and some young shaver referred to Sergio Garcia as "one of the old guard"

---

## 2023-11-10 19:09:41 - issues channel

**AndyL**

so in the example above, the one being cancelled was active for 8mins, and was cancelled in the logic show above, here's the response on that one, which appears at the exact same time as the above placed order (completion!)

```{"asctime": "2023-11-10 17:19:30,301", "levelname": "WARNING", "message": "HEDGE_COMPLETE: BACK : 29 : 1.220974195:32494174 size_matched 0.0 avg_price_matched: 0.0 req_size: 7.36 req_price: 1000.0 order : Order 328103107234: Execution complete : {\"market_id\": \"1.220974195\", \"selection_id\": 32494174, \"handicap\": 0, \"id\": \"63929374292148811\", \"customer_order_ref\": \"653232d17db55-63929374292148811\", \"bet_id\": \"328103107234\", \"date_time_created\": \"2023-11-10 17:11:26.564031\", \"publish_time\": \"2023-11-10 17:11:25.635000\", \"market_version\": null, \"async\": false, \"trade\": {\"id\": \"acf023f5-f990-4a2e-ac25-2d9475cb5477\", \"strategy\": \"LiveHorseUK\", \"place_reset_seconds\": 0.0, \"reset_seconds\": 0.0, \"orders\": [\"63929374292148811\"], \"offset_orders\": [], \"notes\": \"\", \"market_notes\": \"21,22,22\", \"status\": \"Complete\", \"status_log\": \"Pending, Live, Pending, Live, Complete\"}, \"order_type\": {\"order_type\": \"Limit\", \"price\": 1000.0, \"size\": 7.36, \"persistence_type\": \"MARKET_ON_CLOSE\", \"time_in_force\": null, \"min_fill_size\": null, \"bet_target_type\": null, \"bet_target_size\": null, \"price_ladder_definition\": \"CLASSIC\"}, \"info\": {\"side\": \"BACK\", \"size_matched\": 0.0, \"size_remaining\": 0.0, \"size_cancelled\": 7.36, \"size_lapsed\": 0.0, \"size_voided\": 0.0, \"average_price_matched\": 0.0}, \"responses\": {\"date_time_placed\": \"2023-11-10 17:11:26.684107\", \"elapsed_seconds_executable\": 483.44282}, \"runner_status\": null, \"status\": \"Execution complete\", \"status_log\": \"Pending, Executable, Cancelling, Execution complete\", \"violation_msg\": null, \"simulated\": {\"profit\": 0.0, \"piq\": 0.0, \"matched\": []}, \"notes\": \"513,21.5,GB,WIN\", \"market_notes\": \"21,22,22\", \"client\": \"aaaa\"}"}```

---

## 2023-11-10 18:38:46 - issues channel

**AndyL**

Hmm [@U4H19D1D2](@U4H19D1D2) not sure I can explain this order, it's a MARKET_ON_CLOSE placed at 30secondToStart, and immediately EXECUTION_COMPLETE, notice time elapsed: 1.3e-05

and notice "size_remaining\": 6.73 . It's a TakeSP bet completed, also orderId is None:

```1.220974195:32494174 size_matched 0.0 avg_price_matched: 0.0 req_size: 6.73 req_price: 1000.0 order : Order None: Execution complete : {\"market_id\": \"1.220974195\", \"selection_id\": 32494174, \"handicap\": 0, \"id\": \"1076299499178434720\", \"customer_order_ref\": \"653232d17db55-1076299499178434720\", \"bet_id\": null, \"date_time_created\": \"2023-11-10 17:19:30.003743\", \"publish_time\": \"2023-11-10 17:19:29.994000\", \"market_version\": null, \"async\": false, \"trade\": {\"id\": \"f2462edf-5e87-4e87-8f59-98259226e948\", \"strategy\": \"LiveHorseUK\", \"place_reset_seconds\": 0.0, \"reset_seconds\": 0.0, \"orders\": [\"1076299499178434720\"], \"offset_orders\": [], \"notes\": \"\", \"market_notes\": \"23,24,24\", \"status\": \"Complete\", \"status_log\": \"Pending, Live, Complete\"}, \"order_type\": {\"order_type\": \"Limit\", \"price\": 1000.0, \"size\": 6.73, \"persistence_type\": \"MARKET_ON_CLOSE\", \"time_in_force\": null, \"min_fill_size\": null, \"bet_target_type\": null, \"bet_target_size\": null, \"price_ladder_definition\": \"CLASSIC\"}, \"info\": {\"side\": \"BACK\", \"size_matched\": 0.0, \"size_remaining\": 6.73, \"size_cancelled\": 0.0, \"size_lapsed\": 0.0, \"size_voided\": 0.0, \"average_price_matched\": 0.0}, \"responses\": {\"date_time_placed\": \"2023-11-10 17:19:30.046825\", \"elapsed_seconds_executable\": 1.3e-05}, \"runner_status\": null, \"status\": \"Execution complete\", \"status_log\": \"Pending, Execution complete\", \"violation_msg\": null, \"simulated\": {\"profit\": 0.0, \"piq\": 0.0, \"matched\": []}, \"notes\": \"29,23.5,GB,WIN\", \"market_notes\": \"23,24,24\", \"client\": \"aaaa\"}"} ```

---

## 2023-11-10 18:06:51 - issues channel

**AndyL**

thanks [@UQL0QDEKA](@UQL0QDEKA) So i've moved to a AWS t4g, and no latency issues anymore,which is progress.

However, bot still having issues which looks like a coding bug of mine, it's interesting it's taking a while to develop given this version of the instance has been running a month, but this week my "hedging" is failing, and i'm getting loads of these exceptions:

```Order does not currently have a betId```

Which is odd, unfortunately it's in my hedging logic, I need to handle it better I think.

But [@U4H19D1D2](@U4H19D1D2) how does an Order not have a betId ?

---

## 2023-11-10 10:19:51 - issues channel

**PeterLe**

Andy Im running 6 flumine instances on an AWS Windows instance 4gb 2vcpus and never had any issues (lightsail) $40/month

(As Im expecting a few sick emojis re Windows, Ill mention I also have a few instances running on the same 4gb 2 vcpus ubuntu but thats only $20/month) :grinning:

---

## 2023-11-09 20:47:16 - issues channel

**Unknown**

yes the crash yes, although i've just been looking at my AWS CPU history and i've recently noticed periodic spikes in CPU. I recently added PLACE markets an am wondering if my VM is under powered....

---

## 2023-11-09 19:24:32 - issues channel

**AndyL**

[@U4H19D1D2](@U4H19D1D2) had a bit of an issue with one of my Flumine processes that eventually crashed the VM, thought i'd pass it by you incase it looks familiar. My naive view it looks a bit like a run-away thread/resource issue..?

• Flumine 2.4.2 betfairlightweight 2.17.3

looking at the "logs", the issue looked to start gradually with periodic:

```High latency between current time and OrderPackage creation time, it is likely that the thread pool is currently exhausted", "trading_function": "cancel", "session": "&lt;requests.sessions.Session object at 0x7f70f376bc40&gt;", "latency": 0.381, "order_package": {"id": "xxxxx", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f7119b1b6a0&gt;", "market_id": "1.220926100", "orders": ["139188221331829894"], "order_count": 1, "package_type": "Cancel", "customer_strategy_ref": "XXXXX", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}, "thread_pool": {"num_threads": 26, "work_queue_size": 3}}```

then a few mins later got:

```"High latency between current time and MarketBook publish time", "market_id": "1.220920848", "latency": 2.08534836769104, "pt": "2023-11-09T11:37:19.521000"}```

this repeated then "continuously" with latency time steadily increasing... message being issued about 50/second

```{"asctime": "2023-11-09 11:40:30,227", "levelname": "WARNING", "message": "High latency between current time and MarketBook publish time", "market_id": "1.220926100", "latency": 14.246732711791992, "pt": "2023-11-09T11:40:15.981000"}```

then about 150 warnings/second

```{"asctime": "2023-11-09 11:54:22,299", "levelname": "WARNING", "message": "High latency between current time and MarketBook publish time", "market_id": "1.220920734", "latency": 176.0338258743286, "pt": "2023-11-09T11:51:26.266000"}```

eventually the increasing CPU usage killed the AWS VM which I had to reboot via the console

---

## 2023-11-06 14:55:09 - random channel

**D C**

Does anyone who obtains TPD via betfair ever come across the stream feed time parameter is sometimes later than the betfair publish time parameter? It's not all the time and I've not noticed if off by more than 200 millisecs. Just be curious to know if its an actual thing or a bug with my gps collector program. I've never thought to check it in actual production code just always assumed chronological ordering would be a given.

---

## 2023-11-01 21:10:47 - general channel

**Trex44**

[@UUCD6P13J](@UUCD6P13J) No the instances were on different EC2 environments. I didn't notice that behaviour again so shut down the test instance and am just continuing with the main one. I am willing to bet the firing of the strat on one instance an not the other is related to the seconds_to_start_parameter in the strategy as its possible for this to be different across two different instances for the same market update due to the EC2's local clock being used in the calculation.  I am very happy with the fix and am pushing ahead, I was just reporting back to you guys to let you know.



Curious, as I haven't used GitHub much before, will the commit that fixed this now become part of the main codebase?

---

## 2023-10-31 13:23:51 - strategies channel

**liam**

1. The [https://github.com/betcode-org/flumine/blob/06451c28252b87a94132dc1f29777cc696937f98/examples/strategies/lowestlayer.py#L46|code](https://github.com/betcode-org/flumine/blob/06451c28252b87a94132dc1f29777cc696937f98/examples/strategies/lowestlayer.py#L46|code) currently just checks for live orders on a market and places if zero, but you could change this to look at `runner_context.trade_count`

2. As above 

3. To simulate you just run the script? To run live you would just copy/paste the [https://github.com/betcode-org/flumine/blob/master/examples/example.py|example](https://github.com/betcode-org/flumine/blob/master/examples/example.py|example) but change the strategy 

---

## 2023-10-31 13:22:04 - random channel

**D C**

This may have been asked before, but what type of workflow to people have from idea to live deployment? Although I am currently changing my ways, I've never done anything more than pretty rough and dirty backtesting on things, preferring to just go from idea -> basic analysis -> implementation -> min stakes live -> kill strat or work on staking.



I do things "properly" when it comes to things involving fundamental analysis because I am more at home in that environment, but for purely market data driven stuff that is how I do things.



Despite not caring much what anyone thinks of it, I readily accept that my approach will be considered by many to be a filthy way of going about things and so I am curious about the work flow of others here (assuming people are willing to share of course). Specifically I am talking about the process for totally new ideas rather than refinement of something already in place. Also interested in if people backtest/simulate to "mine" for ideas or just for verification something might be feasible in reality.

---

## 2023-10-31 13:20:48 - strategies channel

**Riccardo Fresi**

Hi guys, looking at lowestlayer is not clear to me:

1. once placed an order for a marketid the strategy add new orders till market is open? or it make just 1 order for 1 market once the logic match?

2. if n°1 is true (more orders for 1 market) is there any way to place just one?

3. i don't get how to simulate it, i mean, i use simulation script as template, but i don't get how to use it on live event?

---

## 2023-10-26 11:32:57 - random channel

**Mo**

I believe so. It’s teaching you to think about markets and where there might be inefficiencies that you are in a position to exploit and how to do that in a process driven way. Although the markets talked about in the course are tradfi/crypto, the principles apply to any kind of market. Robot James started taking an interest in betting markets over the past year or so so for all I know he will at least touch on these as well



I thought some of the best content in the course was Robot James doing a live demo of some exploratory data analysis. Fundamental stuff which can apply to any kind of data but the value is in the way he articulates it. That part gave me an idea for a new (small scale) betting strategy that more than covered the cost of the course

---

## 2023-10-26 09:37:15 - random channel

**Mo**

Material releases on a weekly basis but you do it at your own pace. There are live weekly webinars which are worth attending if you can as you can pose questions but not essential to do so as they are recorded and you can always submit questions ahead of time

---

## 2023-10-22 21:27:49 - general channel

**James**

[https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L28](https://github.com/betcode-org/flumine/blob/master/flumine/order/order.py#L28)



Here is the OrderStatus definition.

Note that there are live and complete statuses.



An order is execution_complete when there is nothing left to be matched. That could be because it has all been matched, or some combination of matched, lapsed, cancelled and voided.

---

## 2023-10-22 13:07:53 - strategies channel

**Adam**

Apology in advance if this is the incorrect channel to ask this question. If so, please let me know where to go, thank you :slightly_smiling_face:



Hey all, I’m new to flumine and I’m following through this amazing guide: [https://betfair-datascientists.github.io/tutorials/How_to_Automate_1/](https://betfair-datascientists.github.io/tutorials/How_to_Automate_1/). When it comes to streaming horse markets, I was wondering if there is a way to continually add markets to an strategy (that is already streaming events) as the markets become live?

The reason being is I want to set up the strategy at the beginning of the day, and as races become live, I want to add that race to the strategy for processing, and remove it as the race becomes closed or suspended. I can see there’s a `remove_market` method in the `BaseStrategy` but I can’t see an equivalent `add_market`. This suggests that the way it’s supposed to work is that we create the strategy filtering on all the australian horse markets at the beginning of the day and remove them when they’re suspended? There’s also a method called `process_new_market` but is this called when a market becomes live or does this suggest it can be used for what I’m asking?

---

## 2023-10-20 07:55:53 - general channel

**Eddy**

Hi everyone, another new-ish user to the betfair API.



I am trying to achieve a couple of things as I play around with the data, such as identifying a list of all market_ids in the upcoming day and beginning to record market data.



I am currently still using the delayed key, but having some troubles obtaining the data I am looking for (the market catalogue for example takes forever to populate). I just wanted to check whether this could be driven by the use of the delayed key and if obtaining the live key may solve the problem.



Thanks in advance!

---

## 2023-10-19 10:50:21 - general channel

**liam**

Correct but you don't use a MarketRecorder for this, the recorder is for recording live data into a format that can then be processed by the simulation. You are currently trying to record data you have already recorded which isn't going to work



Have you had a look at any of the docs or [https://github.com/betcode-org/flumine/blob/master/examples/simulate.py|examples](https://github.com/betcode-org/flumine/blob/master/examples/simulate.py|examples)?

---

## 2023-10-18 13:36:26 - issues channel

**Neil T (Betfair)**

Hi [@U4H19D1D2](@U4H19D1D2),  keep alive should work as expected for Italian users when requested within the session expiry time.  Regardless of this, clients should still handle INVALID_SESSION_INFORMATION errors by creating a new API login request

---

## 2023-10-18 13:28:34 - issues channel

**Riccardo Fresi**

there is no dependency from update frequency? if for a event there is no update for let's say 1 hour, flumine call keep_alive ?

---

## 2023-10-18 10:13:24 - issues channel

**liam**

I am not sure, flumine should call keep_alive every 10 minutes for IT users

---

## 2023-10-16 14:27:19 - strategies channel

**PeterLe**

The biggest learning for me (so far) is clearly understand the problem to which you are trying to gain an insight/solution.

As a complete newbie to ML I incorrectly thought that you could just supply a large datset and let ML do its thing.

From the thread I started on ML, one thing im keen to delve more into is particle swarm optimisation. Time permitting.

As an aside; Ive been running something live since the 19th Sept based on something I found via ML and the graph looks like the flight path of the first wright bros plane :grinning:

Still optimistic though..I think it will fly

---

## 2023-10-15 15:43:37 - issues channel

**PeterLe**

Im running four instances Liam on the same AWS instance, three were giving that error (if Ive checked it correctly and didnt miss it on the one I thought was OK..)

The one that looked ok -  I restarted that one yesterday after I changed some parameters? Not sure if that had something t do with it?

Anyway, all good now, been a good day so far :grinning:

---

## 2023-10-12 17:45:08 - issues channel

**AndyL**

all my bots had "connection closed by server" errors at about 06:20 this morning, followed by numerous MAX_CONNECTIONS_EXCEEDED you have 11 max 10 ! Which meant several failed to instantiate the order stream I think, caused all sorts of mess !! Closed them all off now, will restart them later.

Anyone else have issues today?

---

## 2023-10-12 13:50:10 - issues channel

**Riccardo Fresi**

my script was a loop with wait, basically a keep_alive every cycle; inddeed i used to make a try call for every api and a reconnect call in case of error

---

## 2023-10-12 11:56:12 - issues channel

**Unknown**

i can set as below, i remember that without flumine i used to keep alive and session go well all night long

---

## 2023-10-12 11:51:11 - issues channel

**liam**

any error logs

```"BetfairClient `keep_alive` error"```



---

## 2023-10-12 11:37:02 - issues channel

**Riccardo Fresi**

don't know, can i use keep alive in the market recorder?

---

## 2023-10-11 00:00:52 - general channel

**foxwood**

Good stuff and a nice solution to managing the data warehouse - which I keep meaning to do ! Had a quick look and a couple of gotchas from my setup that may also apply to others you might like to consider at some point in the future

1. Rightly or wrongly all my market catalogues are gzipped - I thought that was the standard for the default flumine market recorder used here. The implementation seems to require these to be in unzipped form.

2. It's effectively tied to sqlite. Since my need is sql server via sqlalchemy it would be useful if the sql specific bits (possibly statements as well since there are differing sql dialects) were subsumed into a class on their own. That would allow users to implement / contribute their own flavour of sql.

Is this an open project for others to provide contributions or just one you control ? Don't know enough about how github works to answer that - i still use a 30 year old legacy GUI VCS lol.

---

## 2023-10-09 18:03:26 - betconnect channel

**Elliot**

Hi, Is anyone using websockets for price updates?

I have some code but its not quite performing as I imagine.

I subscribe to an event and the websocket is open however i receive no updates in real time. The ws stays open then dumps all the updates at once when the event starts and immediately closes the ws.

I imagined that i should get these onmessage returns as the price updates are happening live as opposed all at once to when the ws is closed.

I can send over code in a dm if anyone can help

Thanks

---

## 2023-10-09 16:44:56 - issues channel

**John Goodacre**

Hi - Sorry to dive in, I realise this was a month ago and so might be missed, but I was also interested in this question. I am not fully familiar with the flumine library, but am using it. I was looking to understand more fully how orders were matched in simulation and the parameters we have to play with, for example, sure traded value at that price, but also if prices at the touch moved through your asking price in simulation (thus assuming you would have been filled), and also the fact we do have information on net adds in the market book rather than trades themselves so potentially have a rough idea of where we might be in the queue?



Sorry to dive in, but I was also interested in how far you might have gone in these areas and if not, then what is doable/ planned. Finally if we set simulation_available_prices = True, then what is implemented thus far? For example, one simple test I was interested in was to trade and store a live game, then run the same game (or games) in backtest and see how closely they matched.

---

## 2023-10-09 14:31:37 - general channel

**Muhammad Adeel Zahid**

No. I am trying to first analyze the orders placed in the market. I was going through [https://github.com/betcode-org/flumine/tree/master/examples|examples](https://github.com/betcode-org/flumine/tree/master/examples|examples) and it appears that `process_orders` will only be called for my own orders. However, I tried to get the overall orders or trades as suggested by [@UEA14GBRR](@UEA14GBRR) in `process_market_book` but did not succeed.

```def process_market_book(self, market: Market, market_book: MarketBook) -&gt; None:



    orders = [o for o in market.blotter.live_orders]

    if orders:

        print('Process market book')

        print(orders)

    for runner in market_book.runners:

        r_context = self.get_runner_context(market.market_id, runner.selection_id, runner.handicap)

        trades = [t for t in r_context.trades]

        live_trads = r_context.live_trades

        if trades or live_trads:

            print("Trades = ", trades)

            print("Live Trades = ", live_trads)```

Nothing shows up here either

---

## 2023-10-09 14:10:11 - general channel

**Muhammad Adeel Zahid**

Ok. Below are my strategy parameters

```strategy = LayStrategy(

    market_filter=streaming_market_filter(

        event_type_ids=["7"], # Horse racing

        country_codes=["GB", 'IE'],

        market_types=["WIN"], # Win Markets

    )

)



framework.add_strategy(strategy)



framework.run()```

and this is my `process_orders` function in `LayStrategy`

```def process_orders(self, market: Market, orders: list) -&gt; None:

    print('process orders')

    if orders:

        print(orders)

    aorders = [o for o in market.blotter.live_orders]

    if aorders:

        print('process order')

        print(aorders)

    for order in orders:

        print(order)```

But not even the first statement is being called (`print('process orders')`. I don't know what is wrong with this code.  Nothing shows up related to orders

---

## 2023-10-09 12:24:40 - general channel

**liam**

`process_orders` is triggered on any order update through the stream or every 0.25s if there are live orders.



The [https://betcode-org.github.io/flumine/markets/|blotter](https://betcode-org.github.io/flumine/markets/|blotter) is in the `Market` object and you can use whenever you want

---

## 2023-10-05 20:17:05 - issues channel

**liam**

The market recorder is for recording *live* data, what are you trying to do?

---

## 2023-10-05 09:40:12 - general channel

**liam**

[https://docs.docker.com/config/containers/logging/awslogs/](https://docs.docker.com/config/containers/logging/awslogs/)

---

## 2023-10-05 09:25:04 - general channel

**liam**

Pipe to AWS cloud watch (I use the docker plugin), very easy to then view / search / graph :ok_hand: 

---

## 2023-10-04 12:07:25 - issues channel

**Rishab**

Its live. So lets say a new tick arrives &amp; streaming_update suggests that there's a trade in this tick. market in the last tick was 4.2@4.4 &amp; i was having a back order @4.4. Now in this tick, I check the order.size_remaining for my order &amp; it says its still the same(i.e I didn't get match) &amp; the market in this tick is 4.2@4.5. And my 4.4 order actually got matched

---

## 2023-10-04 10:51:39 - general channel

**Andy B**

Quick question.  When I set up a strategy and set max_trade_count and max_live_trade_count to 1 in a simulation, is that only for the specific strategy?  If I have 5 strategies, could I in theory place 5 bets on the same runner, or is it governed by the max_trade_count and max_live_trade_count setting in another strategy?  The code I am currently using is:



strategy_A = SimsAll_A(

        market_filter={

            "markets": markets,

            'market_types':['PLACE'],

            "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

            "event_processing": True

            },

    max_order_exposure= 50, # Max bet sizes of $50

    max_trade_count=1, # Max of trade/bet attempt per selection

    max_live_trade_count=1, # Max of 1 unmatched Bet per selection

    )

---

## 2023-10-01 09:18:08 - general channel

**Andy B**

I have it set up in flumine like this:

# Searches for all betfair data files within the folder sample_monthly_data_output

data_folder = 'Greyhounds\\Data'

data_files = os.listdir(data_folder,)

data_files = [f'{data_folder}/{path}' for path in data_files]



def run_process(markets):

    """Replays a Betfair historic data. Places bets according to the user defined strategy and tries to accurately simulate matching by replaying the historic data.



    Args:

        markets (list: [file paths]): a list of file paths to where the historic data is stored locally. e.g. user/zhoui/downloads/test.csv

    """

    # Set Flumine to simulation mode

    client = clients.SimulatedClient()

    framework = FlumineSimulation(client=client)



    # Set parameters for our strategy

    # strategy = FlatIggyModel(

    strategy = BoxPercSim(

        market_filter={

            "markets": markets,

            'market_types':['WIN','PLACE'],

            "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

            "event_processing": True

            },

    max_order_exposure= 50, # Max bet sizes of $50

    max_trade_count=1, # Max of trade/bet attempt per selection

    max_live_trade_count=1, # Max of 1 unmatched Bet per selection

    )

    framework.add_strategy(strategy)



    framework.add_logging_control(

        BacktestLoggingControl()

    )

    framework.run()



# Multi processing

if __name__ == "__main__":

    all_markets = data_files  # All the markets we want to simulate

    processes = os.cpu_count()  # Returns the number of CPUs in the system.

    markets_per_process = 8   # 8 is optimal as it prevents data leakage.



    _process_jobs = []

    with futures.ProcessPoolExecutor(max_workers=processes) as p:

        # Number of chunks to split the process into depends on the number of markets we want to process and number of CPUs we have.

        chunk = min(

            markets_per_process, math.ceil(len(all_markets) / processes)

        )

        # Split all the markets we want to process into chunks to run on separate CPUs and then run them on the separate CPUs

        for m in (utils.chunks(all_markets, chunk)):

            _process_jobs.append(

                p.submit(

                    run_process,

                    markets=m,

                )

            )

        for job in [http://futures.as|futures.as](http://futures.as|futures.as)_completed(_process_jobs):

            job.result()  # wait for result



So it should be populating the list of markets correctly I think.

---

## 2023-10-01 09:06:07 - general channel

**Andy B**

Thanks (again) Liam.  I'm still not getting the response I was expecting though.  My strategy looks like:



strategy = BoxPercSim(

        market_filter={

            "markets": markets,

            'market_types':['WIN','PLACE'],

            "listener_kwargs": {"inplay": False, "seconds_to_start": 80},

            "event_processing": True

            },

    max_order_exposure= 50, # Max bet sizes of $50

    max_trade_count=1, # Max of trade/bet attempt per selection

    max_live_trade_count=1, # Max of 1 unmatched Bet per selection

    )



And then the rest of my code is:



place_market = market.event["PLACE"]

                            print(place_market)

                            for a in place_market:

                                print('event:', a.event, 'market_type:', a.market_type, 'place_market_type:', type(place_market))

                            place_market_book = place_market[0].market_book



I put that bit of debug in there to try and see what is being returned and I can see that place_market is indeed a flumine object as per my code in the live stream - &lt;flumine.markets.market.Market object at 0x000001B593B74AF0&gt;.



In the second part of the debug, I can see the output as:

event: defaultdict(&lt;class 'list'&gt;, {'WIN': [&lt;flumine.markets.market.Market object at 0x000001B592FECDF0&gt;], 'PLACE': [&lt;flumine.markets.market.Market object at 0x000001B593B74AF0&gt;]}) market_type: PLACE place_market_type: &lt;class 'list'&gt;



Using the vscode debugger, I can see that market.event = 'WIN" and the value of place_market is an empty list i.e. [].  Presumably I need to be getting the value for place_market from somewhere else, but looking through the debuger output in vscode isn't giving me the answers I expect.  I did wonder if the market I was accessing was only a WIN market, but it does this for every market when I let the back test run, so that can't be the answer.  It has to be the way I am trying to access it.

---

## 2023-09-30 08:14:00 - general channel

**Andy B**

I have this code working well in the live stream using flumine, but I can't get it going in a back test.  In my live code I use:

place_market = market.event["PLACE"]

place_market_book = place_market[0].market_book



So I initially tried using the same in a back test, but no dice.  I then tried it as:

place_market = market.market_book.market_definition.market_type["PLACE"]

place_market_book = place_market.market_book



But again it didn't work and it throws the error TypeError: string indices must be integers.



I tried looking at it in the debugger and both market.event and market.market_book.market_definition.market_type show the value as 'WIN', which is correct, and leads me to think that maybe the context of the place market either doesn't exist in the recorded data, or there is yet another way to access it.  Any thoughts?

---

## 2023-09-29 03:50:37 - general channel

**Andy B**

Hi All, I am trying to create a backtest based on the Aus PRO greyhound files from betfair.  I can see that the box\trap number is included in the bane value in the raw files, but I can't find a way to get it out in the back test.  In the live stream I would do something like:



Box = [x.runner_name for x in market.market_catalogue.runners if x.selection_id == runner.selection_id]

Box = Box[0].split('. ')[0]



As far as I can work out, there is no market_catalogue when using the PRO files to backtest, although maybe I have missed something logical.  I could provide the data I need from an alternative source, but I don't really want to do that because a:) it is very memory intensive to store large amounts of data in a multi-threaded backtest and b:) the data is in the raw files, so I should be able to access it.  I've searched through previous threads in here and I've looked at the betcode FAQ on github, and I am feeling like I may not be able to do this simply using flumine, but I am sureother people have found a way around this, so I am curious how it is being done.

---

## 2023-09-26 17:38:13 - general channel

**Trex44**

Thanks [@U9JHLMZB4](@U9JHLMZB4). I am running the servers on an EC2 instance and I think this may have been related to CPU credit balance running out.

---

## 2023-09-26 17:16:50 - random channel

**PeterLe**

I use lightsail too [@U02RN7YDRQ9](@U02RN7YDRQ9) It charged by the month unlike EC2 (ping times the same etc)

---

## 2023-09-26 10:32:37 - random channel

**foxwood**

Looking to change my server provider so checking out the confusing Amazon offers and it ended up suggesting a server at 4 times the cost I currently pay ! I've probably ticked the wrong boxes so can anyone who runs a Windows server with Amazon (ec2 ?) give an indication of the size/model reference they find works for live running about 10 strategies 24x7 with about 40GB storage and all backtest / development done locally ? Tried a different configuration and a "t3" seemed a vague possibility - sadly it's a jargon nightmare.

---

## 2023-09-25 08:44:35 - issues channel

**Software Engineer**

Hello Everyone, I am getting an issue with the Betfair api

I am retrieving horse racing data for every data with Live api key. I save the live races data ( almost 30-60 races a day ) and retrieve from frontend.

But the data saving is delayed as there are more races, ( 1 second per 1 race ) and I need to see them changed every 3-5 seconds.

I am using MongoDB and Python for the Backend to get data from Betfair api.  Thanks

---

## 2023-09-24 11:48:06 - general channel

**TT**

Yeah, I seem to recall discussions about historic data not matching what you receive live. So without access to TPD through betfair, recording the stream yourself and integrating into flumine with middleware seems the only option.



Do you know what the requirements are for getting a TPD feed through betfair are? Is it for PC customers only?

---

## 2023-09-24 11:36:55 - general channel

**TT**

I've not delved into the code too much but I saw this:

[https://github.com/TotalPerformanceData/gmaxfeed/blob/87e58cf32fcb2e2c3ea147dcdeaa15fbd4f96091/gmaxfeed/feeds/record_live.py](https://github.com/TotalPerformanceData/gmaxfeed/blob/87e58cf32fcb2e2c3ea147dcdeaa15fbd4f96091/gmaxfeed/feeds/record_live.py)

---

## 2023-09-23 15:07:58 - general channel

**Peter**

This is [https://betcode-org.github.io/faq/deploying-strategies/#problems|covered in the FAQ](https://betcode-org.github.io/faq/deploying-strategies/#problems|covered in the FAQ).

---

## 2023-09-20 18:32:23 - general channel

**Mona**

It is weird that I get high latency between current time and MarketBook publish time with my newly subscribed LIVE_APP_KEY.

---

## 2023-09-18 21:14:21 - random channel

**Ralegh**

Can’t speak from experience but I’d expect ms minimum, depending on server location, but you’re competing with people with microsecond or even nanosecond latency for higher tech exchanges 

---

## 2023-09-18 13:41:39 - general channel

**Peter**

Yes.



Betfairlightweight supports the Betfair API's listCurrentOrders, which delivers all orders (with a 1000 per page limit) irrespective of how you placed them. So your website bets can be filtered based on the absence of customerStrategyRef.



Flumine's `market.blotter.orders` will return all orders (including those placed on the website) for the specific market for which you're receiving an update. Note though that the orders delivered to a strategy's `process_orders` method, will be limited to orders for that specific strategy and so would not include your website orders.

---

## 2023-09-18 13:34:26 - general channel

**Peter**

The market recorder provides the changes to the market only in a raw format. To see the actual state of the market you would run this through Flumine which creates and maintains an internal record of the state of the market as it would be after each update and makes it available to your trading strategies.



No the sample for the live event doesn't save the data to a local file. But once you have the data and have decided in which data and in what format, writing it out to a file is the easy bit. One tip for doing that would be to ensure that you record the time that the data is received, so that you can match that with the market data later.



It's easier to run separate scripts to collect event and market data. especially as the market recorder is already written for you. But there will come a point at which you want to be able to collect both together in order to make trading decisions and place bets. So it's worth working out how to do that early. If you're looking at external sources of data, you'll want to look at how Flumine supports works and middleware. These can be very useful for polling external data sources and merging event data with market data (via the market.context attribute) and feeding it to your trading strategies. There's not an easy way to answer the "how?" part of this as it depends a lot on how your accessing event data and how you plan to use it in your trading strategies.

---

## 2023-09-17 18:13:50 - general channel

**Riccardo Fresi**

thanks!

i runned the market recorded, even if i need to understand better the output json (not so clear to me)

tha sample for event live event does not save anything to local file right? i have to implement that in the strategy?

is it better to run different script with different strategies or the tool foreseen to run different strategies' if yes, how?

---

## 2023-09-17 17:07:58 - general channel

**Peter**

It's done very commonly, but there's not a neat example of it as it depends on how the live stats are being consumed. However, you have the [https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py|market recorder example](https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py|market recorder example) for prices and for live event data the [https://github.com/betcode-org/flumine/blob/master/examples/tennisexample.py|tennis example](https://github.com/betcode-org/flumine/blob/master/examples/tennisexample.py|tennis example) shows how you can poll an external API, while the [https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py|sportsdata example](https://github.com/betcode-org/flumine/blob/master/examples/example-sportsdata.py|sportsdata example) shows how to handle streamed event data if you have access to that.

---

## 2023-09-17 16:12:57 - general channel

**Riccardo Fresi**

is there in flumine some sample of recording both price and live stats?

---

## 2023-09-17 10:11:45 - general channel

**PeterLe**

Morning,

I noticed i had some STRATEGY_EXPOSURE errors  yesterday (This is a new strategy; I was expecting a lot more bets to be placed than it did)

I have a number of strategies all running on a single system...

Each one of which is run from the command line...

The code at the bottom of each strat is like so:



`strategy = Sub6(`

    `name="WIN",`

    `market_filter=betfairlightweight.filters.streaming_market_filter(`

        `event_type_ids=["7"],`

        `country_codes=["GB", "IE"],`

        `market_types=["WIN"],`

    `),`

    `max_order_exposure=50,`

    `max_selection_exposure=100,`

    `max_trade_count=5000,`

    `max_live_trade_count=8`

`)`

`framework.add_strategy(strategy)`



`framework.run()`



I can also see that we also have these settings in strategy.py too :



 `def __init__(`

        `self,`

        `market_filter: Union[dict, list],`

        `market_data_filter: dict = None,`

        `sports_data_filter: List[`

            `str`

        `] = None,  # 'raceSubscription', 'cricketSubscription'`

        `streaming_timeout: float = None,`

        `conflate_ms: int = None,`

        `stream_class: Type[BaseStream] = MarketStream,`

        `name: str = None,`

        `context: dict = None,`

        `max_selection_exposure: float = 15, # 15`

        `max_order_exposure: float = 2,`

        `max_trade_count: int = 1000,`

        `max_live_trade_count: int = 15,`

        `multi_order_trades: bool = False,`

    `):`



So my question is: Am I correct in setting the exposures in the individual strategy files and if so, do these setting take precedence/over ride over any other areas of code?

Thanks in advance

---

## 2023-09-17 06:04:26 - general channel

**AI Trader**

Guys, has anyone had issues creating stream connections using betfairlightweight in EC2 instances? I guess it's more an AWS question (sorry), but when I run my docker in AWS ECS Fargate, it works fine. Whenever I try to run the same container on ECS using an EC2 instance, I get a Timeout error (max retries exceeded). Error in the comments. I have tried to ssh into the instance and ping betfair server to make sure it wasn't some firewall or connectivity issue, but the ping works.

---

## 2023-09-16 15:18:08 - strategies channel

**birchy**

6% is workable live but, in my experience, is generally a rollercoaster that makes a small profit long term.

The 'real' edges will give you 0% as [@UQL0QDEKA](@UQL0QDEKA) suggested.

Also worth noting that I've found that strategies that barely break even in flumine simulation can be much better when run live due to real-world match rates. For me, the key is to find strategies where the simulation ROI is decent (i.e. &gt;5%) when you assume a 100% match rate. I have my own code to calculate the P&amp;L at 100% match rate but I believe [@U4H19D1D2](@U4H19D1D2) has added it to flumine simulation recently.

---

## 2023-09-16 11:17:46 - issues channel

**Riccardo Fresi**

Today is not a good day! Internal error 500 both local machine and aws……you too?

---

## 2023-09-15 15:22:32 - issues channel

**Pramod S**

So you want to say we have to use only live key with flumine

---

## 2023-09-15 14:38:34 - random channel

**foxwood**

Never looked at the low level feeds in flumine but presume the socket handling / buffering lives in it's own thread world and will manage things with multiple buffers etc as needed ?

---

## 2023-09-14 19:00:28 - general channel

**birchy**

I last updated production flumine about 6 months ago, so probably version ~2.3.5

---

## 2023-09-14 18:43:10 - issues channel

**Riccardo Fresi**

Yes, I’d like to move on cloud soon, but I want an stable “product” before; indeed I have also to consider the cost of cloud, I’ve tried with free tier aws

---

## 2023-09-14 18:14:49 - issues channel

**Riccardo Fresi**

for sure is something related to my connection or my firewall, i upload everything on ec3 aws (user, psw, app key, cert)

seems everything working properly



i don't know ho modify my local setup

---

## 2023-09-14 10:29:26 - issues channel

**Riccardo Fresi**

different

```DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): [http://identitysso-cert.betfair.it:443|identitysso-cert.betfair.it:443](http://identitysso-cert.betfair.it:443|identitysso-cert.betfair.it:443)

DEBUG:urllib3.connectionpool:[https://identitysso-cert.betfair.it:443](https://identitysso-cert.betfair.it:443) "POST /api/certlogin HTTP/1.1" 200 125

INFO:betfairlightweight.streaming.listener:[Register: 1]: marketSubscription

INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: "MarketStream" created

DEBUG:betfairlightweight.streaming.betfairstream:[Subscription: 2] Sending: b'{"op":"authentication","id":2,"appKey":"XXXXX","session":"XXXXXX="}\r\n'

DEBUG:betfairlightweight.streaming.betfairstream:[Subscription: 2] Sending: b'{"op":"marketSubscription","id":1,"marketFilter":{"eventTypeIds":["7"],"marketTypes":["WIN"],"countryCodes":["GB"]},"marketDataFilter":{"fields":["EX_BEST_OFFERS","EX_MARKET_DEF"],"ladderLevels":3},"initialClk":null,"clk":null,"conflateMs":1000,"heartbeatMs":null,"segmentationEnabled":true}\r\n'

Exception in thread Thread-14 (start):

Traceback (most recent call last):

  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1038, in _bootstrap_inner```

```'\nData can also be accessed by using the snap function in the listener, e.g:\n\n    market_books = listener.snap(\n        market_ids=[1.12345323]\n    )\n\nErrors need to be caught at stream.start, resubscribe can then be used to\nprevent full image being sent, e.g:\n\n    streaming_unique_id = stream.subscribe_to_markets(\n        market_filter=market_filter,\n        market_data_filter=market_data_filter,\n        conflate_ms=1000,  # send update every 1000ms\n        initial_clk=listener.initial_clk,\n        clk=listener.clk,\n    )\n\nThe streaming unique id is returned in the market book which allows multiple\nstreams to be differentiated if multiple streams feed into the same queue.\n'```

```    self.run()

  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 975, in run

    self._target(*self._args, **self._kwargs)

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 67, in start

    return

^^^^^^^^^^^

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 226, in _read_loop

    if self._running:

                      

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 256, in _receive_all

    "[Connect: %s]: Connection closed by server"

^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

betfairlightweight.exceptions.SocketError: [Connect: 2]: Connection closed by server```



---

## 2023-09-13 23:56:39 - general channel

**Unknown**

I'm using flumine loggingControl to log live orders to CSV. Attached is the main lump of my code minus [http://logging.info|logging.info](http://logging.info|logging.info)() calls. The main idea is to add BSP's to my CSV data for preplay strategies. The CSV file is missing most (but not all) of the markets I participated in and looking at my logs, it seems that there is some sort of race condition between `_process_cleared_orders_meta()` and `_process_closed_market()`. The weird thing is that my variable `self.csv_data` gets populated as expected via `_process_cleared_orders_meta(),` _however_ the `_process_closed_market()` call often fails to find the market_id in `self.csv_data` and I can see that there are currently 30+ markets sat in there waiting to be written to CSV long after the markets have closed. I suspect that closed_orders is getting called before cleared_orders_meta.



Can someone please explain:

a) the difference between `_process_cleared_orders()` and  `_process_cleared_orders_meta()`

b) if `_process_closed_market()` is the only event in loggingControl that exposes the marketBook/BSP

c) if/why my logic is flawed

---

## 2023-09-13 16:58:28 - strategies channel

**Justice**

[@U027P3N2WMQ](@U027P3N2WMQ) It was a huge automated data pipeline project I did for a client. I believe it is still in use. Scrapes data from racing post, timeform, at the races and also the betfair sp data. The data and all the pipelines are hosted on Azure and I don't have access to them anymore. I'll see if I can find a pre-production schema

---

## 2023-09-13 06:25:39 - issues channel

**Paras Stefanopoulos**

Anyone know what could be happening here? been getting this for months randomly from betfair stream:



```2023-09-11 08:09:43,688 ERROR [Connect: 37]: Socket [Errno 104] Connection reset by peer

Traceback (most recent call last):

  File "/home/ec2-user/.local/lib/python3.9/site-packages/betfairlightweight/streaming/betfairstream.py", line 244, in _receive_all

    part = self._socket.recv(self.buffer_size)

  File "/usr/lib64/python3.9/ssl.py", line 1227, in recv

    return self.read(buflen)

  File "/usr/lib64/python3.9/ssl.py", line 1102, in read

    return self._sslobj.read(len)

ConnectionResetError: [Errno 104] Connection reset by peer



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "…", line 32, in wrapper

    result = await async_func(*args, **kwargs)

  File "…", line 108, in main

    raise betfair_streamer.error

  File "/usr/lib64/python3.9/threading.py", line 980, in _bootstrap_inner

    self.run()

  File "/usr/lib64/python3.9/threading.py", line 917, in run

    self._target(*self._args, **self._kwargs)

  File "…", line 50, in _start_stream

    raise e

  File "...", line 47, in _start_stream

    self.stream.start()

  File "/home/ec2-user/.local/lib/python3.9/site-packages/betfairlightweight/streaming/betfairstream.py", line 67, in start

    self._read_loop()

  File "/home/ec2-user/.local/lib/python3.9/site-packages/betfairlightweight/streaming/betfairstream.py", line 226, in _read_loop

    received_data_raw = self._receive_all()

  File "/home/ec2-user/.local/lib/python3.9/site-packages/betfairlightweight/streaming/betfairstream.py", line 248, in _receive_all

    raise SocketError("[Connect: %s]: Socket %s" % (self._unique_id, e))

betfairlightweight.exceptions.SocketError: [Connect: 107]: Socket [Errno 104] Connection reset by peer```

---

## 2023-09-12 14:38:11 - strategies channel

**Johnny**

[@U4H19D1D2](@U4H19D1D2) on your ML medium article you wrote "Implementing random sleeps in between scrapes and using a mix of ec2 instances and serverless I was able to get a years worth of data after a few days (still scraping the rest)".  Assuming these were not live markets, how did you achieve this?

---

## 2023-09-10 12:53:31 - general channel

**Mo**

Like you say, for one thing there is a question of discipline



Not everyone will have the skills, knowledge and discipline to avoid blowing up on their way to profitability. Perhaps some have the discipline but it takes them a long time and sizeable losses that we get to enjoy before they get there



Perhaps the backtest looks good but they don't understand market capacity so they give a lot back when they try to scale up



Perhaps they go live too quickly, lose £50 and never come back. All we need in that case is enough churn



Without putting too fine a point on it, you want to give them enough rope to hang themselves with

---

## 2023-09-09 13:26:58 - general channel

**Steve**

Now just need to figure out how to hook AWS up to pycharm.

---

## 2023-09-09 13:25:20 - general channel

**Steve**

I'm using D3_v2 through Azure batch. It has 32gb of memeory which I think is what is killing me. Sounds like I need to be checking out AWS.

---

## 2023-09-08 09:30:11 - issues channel

**Riccardo Fresi**

copy from example, but the while at the end

```# setup logging

logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))  # change to DEBUG to see log all updates



# create trading instance (app key must be activated for streaming)

trading = betfairlightweight.APIClient(BETFAIR_USER, 

                                       BETFAIR_PASSWORD, 

                                       app_key=BETFAIR_APPKEY, 

                                       certs=CERT_PATH, 

                                       locale='italy')



# login

trading.login()



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener)



# create filters (GB WIN racing)

market_filter = streaming_market_filter(

    event_type_ids=["7"], country_codes=["GB"], market_types=["WIN"]

)

market_data_filter = streaming_market_data_filter(

    fields=["EX_BEST_OFFERS", "EX_MARKET_DEF"], ladder_levels=3

)



# subscribe

streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=1000,  # send update every 1000ms

)



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()```

same error



```INFO:betfairlightweight.streaming.listener:[Register: 1]: marketSubscription

INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: "MarketStream" created

Exception in thread Thread-5 (start):

Traceback (most recent call last):

  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1038, in _bootstrap_inner

    self.run()

  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 975, in run

    self._target(*self._args, **self._kwargs)

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 67, in start

    self._read_loop()

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 226, in _read_loop

    received_data_raw = self._receive_all()

                        ^^^^^^^^^^^^^^^^^^^

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 256, in _receive_all

    raise SocketError(

betfairlightweight.exceptions.SocketError: [Connect: 2]: Connection closed by server```



---

## 2023-09-08 09:08:55 - issues channel

**Riccardo Fresi**

this is the full error

Exception in thread Thread-12 (start):

Traceback (most recent call last):

  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 1038, in _bootstrap_inner

    self.run()

  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.11_3.11.1520.0_x64__qbz5n2kfra8p0\Lib\threading.py", line 975, in run

    self._target(*self._args, **self._kwargs)

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 67, in start

    self._read_loop()

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 226, in _read_loop

    received_data_raw = self._receive_all()

                        ^^^^^^^^^^^^^^^^^^^

  File "C:\Users\riccardo.fresi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\betfairlightweight\streaming\betfairstream.py", line 256, in _receive_all

    raise SocketError(

betfairlightweight.exceptions.SocketError: [Connect: 2]: Connection closed by server

---

## 2023-09-08 09:04:11 - issues channel

**Riccardo Fresi**

from here everything ok

```logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))  # change to DEBUG to see log all updates



# login

trading.login()

# create queue

output_queue = queue.Queue()

output_queue

# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)

listener

# create stream

stream = trading.streaming.create_stream(listener=listener)

stream

# create filters (GB WIN racing)

market_filter = streaming_market_filter(

    event_type_ids=["1"]#, country_codes=["GB"], market_types=["WIN"]

)

market_data_filter = streaming_market_data_filter(

    fields=["MATCH_ODDS"], ladder_levels=3

)

# subscribe

streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=1000,  # send update every 1000ms

)

# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)```

INFO:betfairlightweight.streaming.listener:[Register: 1]: marketSubscription

INFO:betfairlightweight.streaming.stream:[MarketStream: 1]: "MarketStream" created



from here i got the warning message

```t.start()```

after that the line "market_books = output_queue.get()" is endless

```streaming_unique_id = stream.subscribe_to_markets(

        market_filter=market_filter,

        market_data_filter=market_data_filter,

        conflate_ms=1000,  # send update every 1000ms

        initial_clk=listener.initial_clk,

        clk=listener.clk,

    )

# check for updates in output queue

while True:

    market_books = output_queue.get()

    print(market_books)



    for market_book in market_books:

        print(

            market_book,

            market_book.streaming_unique_id,  # unique id of stream (returned from subscribe request)

            market_book.streaming_update,  # json update received

            market_book.market_definition,  # streaming definition, similar to catalogue request

            market_book.publish_time,  # betfair publish time of update

        )```



---

## 2023-09-07 17:01:43 - strategies channel

**thambie1**

I have all three strategies currently deployed for various models/situations. #3 is ideal but takes more effort, #1 is second best and is super easy, #2 for when the loss is too little to be worth adapting your strategy for.

---

## 2023-09-06 12:39:23 - issues channel

**Troy Edwards**

Hi guys does the betfair Non-Interactive login required a self signed certificate as per [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Non-Interactive+%28bot%29+login](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Non-Interactive+%28bot%29+login)????



I coded my login code back in 2014 which worked well and returned my Session ID (SSOID), however as I have continued to call the function Keep Betfair Alive [https://identitysso.betfair.com/api/keepAlive](https://identitysso.betfair.com/api/keepAlive) I believe this function has been keeping my SSOID current and maybe I have missed Betfair have increased their login security ??   I changed my password the other day and then when the betbot try to login I get Errors.









Also slightly unrelated but does anyone here use the Betfair Developer Forum [https://forum.developer.betfair.com/](https://forum.developer.betfair.com/) and can you login ?   I assume the login is your current betfair login but it just will not allow me to login to the forum yet I used to have access :disappointed:

---

## 2023-09-06 10:36:30 - general channel

**PeterLe**

Check out AWS lightsail too, not quite as many functions as EC2 but good enough and much cheaper 

---

## 2023-09-06 08:38:06 - general channel

**Trex44**

I use AWS EC2 as well. 3 of the micro instances, one for market recording, one for live strats and another for experimenting. Median daily cost is $2.23 a day and the instances account for just over half that. My main spend each month is on SageMaker and Athena which can easily add another $20 daily depending on what I am doing.

---

## 2023-09-06 08:22:25 - general channel

**Peter**

Same approach as [@U4H19D1D2](@U4H19D1D2). I have a small collection of AWS EC2 instances costing me about £5 each mainly based around different sports. My main spend is on RDS (hosted database) not the VMs running the strategies.

---

## 2023-09-06 08:16:36 - general channel

**liam**

I ran on AWS EC2 for about £5 a month for a few years, what instance type are you using / why?

---

## 2023-09-04 17:26:00 - issues channel

**foxwood**

In 2 dog races today a matched live bet was logged as being `OrderStatus.EXECUTABLE` (ie not matched) at `process_closed_market()`

From the live logs it appears that the update message for the bet `"Order status update: Execution complete"` came in about half a second AFTER the `"Market closed"` message.

I always thought everything was done and dusted at `process_closed_market()` ?

In simulation it works fine of course but how to handle this on live to avoid being misled as to what bets have really been matched ?

---

## 2023-08-31 02:48:09 - general channel

**Andre Mello**

Hi guys

help me

I'm having trouble with this certificate, I've never done it

``````

`openssl req -new -config openssl.cnf -key client-2048.key -out client-2048.csr`





`Country Name (2 letter code) [AU]:BR`

`State or Province Name (full name) [Some-State]:PR`

`Locality Name (eg, city) []:PARANAGUÁ`

`Organization Name (eg, company) [Internet Widgits Pty Ltd]:[http://yourcompany.com|yourcompany.com](http://yourcompany.com|yourcompany.com)  ????? *[https://www.betfair.com/sport/](https://www.betfair.com/sport/) ?? I don't know what to put*`

`Organizational Unit Name (eg, section) []:Security Team - *I don't know what to put*`

`Common Name (e.g. server FQDN or YOUR name) []:Test API-NG Certificate  *I don't know what to put*`

`Email Address []:[mailto:my.name@mydomain.com|my.name@mydomain.com](mailto:my.name@mydomain.com|my.name@mydomain.com) - *MY email betfair??*`

 

`Please enter the following 'extra' attributes`

`to be sent with your certificate request`

`A challenge password []:  -` _*`my passord betfair??`*_

`An optional company name []: - my name??`

---

## 2023-08-30 16:14:51 - issues channel

**foxwood**

thought I'd try chatgpt on this - it didn't seem to know at first so I had to tell it that it was a known term ! However it's answer may help ...



I apologize for the confusion. It seems that "SSLEOFError" is indeed a known term related to SSL errors, though it might not be as commonly referenced as other SSL error types.



SSLEOFError stands for "SSL EOF Error," where "EOF" stands for "End of File." This error occurs when an SSL/TLS connection is unexpectedly terminated before completing the necessary handshake or data exchange. In simpler terms, it indicates that the connection was closed prematurely by one of the parties involved.



Common causes of SSLEOFError include:



1. **Network Issues:** Sudden network interruptions, packet loss, or connectivity problems between the client and the server can lead to an SSL EOF Error.



2. **Server Termination:** The server might have terminated the connection due to its own issues or misconfigurations.



3. **Client Termination:** The client might have intentionally or unintentionally closed the connection before the SSL handshake could complete.



4. **Firewall or Proxy Interference:** Intermediary devices like firewalls or proxies might interfere with the connection and cause premature termination.



5. **Resource Limitations:** The server or client might have encountered resource limitations, leading to a forced connection closure.



6. **Misconfigured SSL/TLS Settings:** Incorrect configurations of SSL/TLS settings on either the client or server side can also lead to this type of error.



7. **Protocol Version Issues:** Mismatched or incompatible SSL/TLS protocol versions can cause connection termination.



To troubleshoot and address an SSLEOFError, you should review server logs, client logs, and network logs to identify the underlying cause. It's also important to ensure that both the client and server are correctly configured to support the same SSL/TLS version and cipher suite. If the issue persists, examining network conditions and potential interference can help diagnose the problem.

---

## 2023-08-28 11:54:40 - random channel

**Andrew**

Before moving to Flumine I was using C#. Click-Once deployment works, or even use XCOPY or RSync over SSH to your host. .NET doesn’t need installing. Copying package files works fine.

---

## 2023-08-28 11:46:02 - random channel

**PeterLe**

Thanks DC

This is the answer I git back on that suggestoin ;



Your colleague's suggestion of static compilation is more commonly associated with languages like C and C++, where the idea is to include all libraries directly in the executable. This results in a larger file size but removes dependencies on shared libraries.

In the C# and .NET ecosystem, the concept doesn't translate directly because .NET applications typically rely on a separate runtime to execute. However, the self-contained deployment option is somewhat similar. In a self-contained deployment, the .NET runtime and all dependencies are bundled with the application. This makes the application larger but ensures it can run on any system without requiring a separate runtime or dependencies.

Pros and Cons of Self-Contained Deployment

Pros:

• No need to worry about runtime dependencies or versions on the target system.

• Simplified deployment as everything needed to run the application is bundled together.

Cons:

• Larger file size, as your colleague pointed out.

• Updating dependencies or the runtime means redistributing the entire application.

While C# doesn't offer static compilation in the same way that C/C++ does, self-contained deployments achieve a similar objective of packaging an application and its dependencies into a single bundle. It's a good option when you want to isolate your application from system-wide changes or when you're not sure whether the target system has the necessary runtime and dependencies.

In summary, if you're looking for an approach in the C#/.NET world that's akin to static compilation in C/C++, self-contained deployments are your closest option.



maybe Docker?

---

## 2023-08-28 11:31:28 - random channel

**PeterLe**

Morning All,

I asked ChatGPT this question :

_I have a C# program that I run on a local PC._

_Id like to run it on a virtual server (AWS)_

_What's the easiest way to package the program and all its dependencies into a single bundle that I can easily deploy on a virtual server_

_Can you make suggestions_



it replied as follows :



Certainly! Packaging a C# program along with its dependencies for deployment on a virtual server can be accomplished in a variety of ways. Here are some common methods:

1. Publish as Self-Contained Deployment

A self-contained deployment (SCD) includes everything needed to run your application: the .NET runtime, .NET libraries, and your application and its dependencies. This method is advantageous because it isolates your application from any .NET changes on the system.

Steps:

1. Open your project in Visual Studio or use the command line.

2. Publish your project as a self-contained deployment.

Using Visual Studio:

• Right-click the project -&gt; Publish -&gt; Choose "Folder" -&gt; Set Deployment Mode to "Self-contained" -&gt; Publish

Using Command Line:

```bash



dotnet publish -c Release -r &lt;RUNTIME_IDENTIFIER&gt; --self-contained true ```

Replace `&lt;RUNTIME_IDENTIFIER&gt;` with the runtime identifier for your target system, like `win-x64` for a 64-bit Windows OS.

2. Publish as Single File

.NET 5 and .NET Core 3.1 introduced the option to package your application as a single file.

Steps:

1. Open your project in Visual Studio or use the command line.

2. Publish your project as a single file.

Using Visual Studio:

• Right-click the project -&gt; Publish -&gt; Choose "Folder" -&gt; Enable "Produce Single File" -&gt; Publish

Using Command Line:

```bash



dotnet publish -c Release -r &lt;RUNTIME_IDENTIFIER&gt; /p:PublishSingleFile=true ```

3. Use Docker

If your virtual server supports Docker, this could be another straightforward option. You can package your application as a Docker container, which would include everything needed to run it.

Dockerfile example:

```Dockerfile



# Use the official image as a parent image FROM [http://mcr.microsoft.com/dotnet/aspnet:5.0|mcr.microsoft.com/dotnet/aspnet:5.0](http://mcr.microsoft.com/dotnet/aspnet:5.0|mcr.microsoft.com/dotnet/aspnet:5.0) AS base WORKDIR /app EXPOSE 80 # Use SDK image to build the app FROM [http://mcr.microsoft.com/dotnet/sdk:5.0|mcr.microsoft.com/dotnet/sdk:5.0](http://mcr.microsoft.com/dotnet/sdk:5.0|mcr.microsoft.com/dotnet/sdk:5.0) AS build WORKDIR /src COPY ["YourApp/YourApp.csproj", "YourApp/"] RUN dotnet restore "YourApp/YourApp.csproj" COPY . . WORKDIR "/src/YourApp" RUN dotnet build "YourApp.csproj" -c Release -o /app/build FROM build AS publish RUN dotnet publish "YourApp.csproj" -c Release -o /app/publish FROM base AS final WORKDIR /app COPY --from=publish /app/publish . ENTRYPOINT ["dotnet", "YourApp.dll"] ```

Then you can build and run the Docker image.

```bash



docker build -t your-app . docker run -p 80:80 your-app```

To those of you who live and breath this stuff; which method would you recommend? I'll work it out myself if someone can give me a steer please. Thanks

---

## 2023-08-25 10:53:41 - general channel

**Unknown**

hi guys, need some help



i implemented a kelly method function

```def kelly_method(odds):

    b = odds - 1

    p = 0.94 #0.9851668726823238  # precision value for prediction model or historical win rate

    f = ((odds * p) - 1) / b

    return odds, b, p, f```

i have some doubt: p should be the % winning probability of the bet, i'm not sure to use the precision of model or the ex-post % based on actual historical results



in the first case i have a very bad performance

in the second case, not so bad;



by the way also the function give me negative results if the odds has more implicit probability respect the p variable (it makes sense to you?) so i use a 0.5 fixed for negative value (it also make sense to you?)



Other thing: the one i've loss, i suspect that i collect very bad the live odds for this single event, may i ask you if you can send me the historical data for that since seems in italy we cannot retrieve historical data? of course if is not a problem

---

## 2023-08-23 22:43:20 - strategies channel

**Mo**

For live - [https://github.com/betcode-org/betfair/blob/master/examples/exampleinplayservice.py|Betfair inplayservice](https://github.com/betcode-org/betfair/blob/master/examples/exampleinplayservice.py|Betfair inplayservice)

---

## 2023-08-23 22:41:48 - strategies channel

**Y B**

Anybody trades soccer here and doesn't mind sharing what datasources you are using for historical scores / match stats AND what data sources you are using for live data?

---

## 2023-08-23 10:11:57 - random channel

**PeterLe**

Morning Joe. This was really my first attempt at looking at ML. Probably best described as a random walk.:grinning:

My initial thoughts were, how to use ML to enhance, refine existing starts and secondly; use ML to look at the recorded data I had and run it against that to see what it found. (no preconceived ideas etc)

Having read up a little, I thought that feature importance may be a good place to start.

I did learn something from the feature importance (Which is not shown above). It is something that I haven't used in my in play stuff but now that ML has made it aware to me, it makes perfect sense and Im testing that in a live environment (1 day in).

As input Joe, maybe you could tell me the basic building blocks how you might go about this (just bullet points will do please)

ie :

• Record Live update data from the API (Doing that)

• use the price recorded to capture various variables (eg back, lay, ltp, books etc ) to a large CSV (Doing that)

What generic steps would you take from this point?

Thanks Joe

(PS I need to go offline for a few hours Joe, work calls - Thanks)

---

## 2023-08-22 22:30:18 - random channel

**AndyL**

Doh, penny dropped finally!

So a Random Forest Classifier, is for "classifying" groups as best as possible,  so you need to have a strategy to start with say "X", which says makes £1000, and you can then train a RFC to optimize the "X" entry points for "profit". So once you have trained your RFC, when using "live", rather than doing "X", you simply ask "RFC" will the entry here be profitable? Yes or No ? .... and with a bit of luck you may end up with £2000 !

Right, so I just need to find a decent "X"......!

---

## 2023-08-22 21:08:42 - random channel

**PeterLe**

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) I started out by learning how to create the CSV  (using the price recorder) into Pandas DF's etc. (I know you are much more accomplished than me in terms of programming by the way)

The added in a couple more columns to include winner, P/L Sum of the books etc

I then started with something simple like regression

And then a random forest (ive only got the estimators set low for the time being)

I have two strats one a simple one and one an ML. They both use the same parameters eg, if the simple strat has a condition tha says back &lt; 10 then so does the ML.

to answer your question - Kind of :grinning: (remember we all do things differently and I know next to nothing about this) You need to have a 'Target'.  In my case I chose the target as 'winner' (from that you can determine whether you bet won or lost) Set up a value in the P/L column in your CSV based on say £1. if you dont have the CSV data (Which is every update) you need to get that up and running first (Lee helped me get this running, but is simple enough). use the price recorder against your recorded data (One month is plenty)

ChatGPT is great for this sort of thing it bootstraps your learning,

For your simple model just create something simple like lay the fav (in running) when X occurs. The ML will work it out for itself. The problem I had is understanding what the ML did so that I could employ that in my live stuff. Ive got some new stuff to look at now thanks to the earlier posts.

---

## 2023-08-22 19:14:03 - random channel

**PeterLe**

Thanks [@U02RN7YDRQ9](@U02RN7YDRQ9) yes I intend to split out certain things and test. Although I do know that the ltp book and lay book do have a bearing on results as Im running this  as a live strat

I agree, the differences between the two strat does look suspicious. The results were taken from one of the first tests I ran, as I progressed it looked more like this :



```# Total Profit/Loss using ML strategy: £9322.959999999966

# Total Profit/Loss using simple strategy: £2207.2800000000384```

It was interesting (and though provoking for me) to think that ML could turn a large losing strat into a positive? :thinking_face:

The data was split into two sets (80% training, 20% testing). there is no separate validation set in the code. Thanks for this, Ill edit to include this.

Train : 60%

Test: 20%

Validation ; 20%

(Would that look about right?)

Re PM: There is a slice of secret sauce in here :grinning:, come to he next meet up if you can, worth th trip :grinning:

[@U05KECSV68N](@U05KECSV68N) Thanks for the reply. Ive just read up on that...AmI correct in thinking that R-squared is more suited to regression models (Whereas the bit that gave me the most insight in the code was a RandomForset Classifier and that it may not be a great fit for that?)

By the way, Im not just going to use the regression and Random Forest, Im going to look at other methods too, its just as a learning exercise if nothing else.

My main questoin was, once you had an ML Model how do you then interpret whats happening in he background so you can incorporate those things into a live environment?

(This is all in play by the way)

---

## 2023-08-22 14:51:07 - random channel

**liam**

Yep, flumine is for live/simulation but when it comes to modelling/backtesting you want to get things into pandas

---

## 2023-08-19 14:57:18 - strategies channel

**D C**

Must be my server then (I'm also not using flumine). Having to kill connections due to heartbeat timeouts about 20 times so far this afternoon. Usually its about 2 a week.

---

## 2023-08-18 10:43:09 - issues channel

**Harry Corrigan**

i can't seem to get from blotter, i have an unmatched lay order at price 1.01 size 1

i'm reading from the blotter like this from within process_market_book

```            print("Reading from blotter")

            for order in market.blotter.client_orders(self.manager.flumine.clients.get_default()):

                print(order)

            for order in market.blotter.live_orders:

                print(order)```

both loops print nothing

---

## 2023-08-18 09:50:20 - random channel

**Mo**

Sorry, documentation says 3 minutes: [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Best+Practice#BestPractice-HTTPPersistentConnection](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Best+Practice#BestPractice-HTTPPersistentConnection)



&gt; *Please note*: Idle keep-alive connection to the API endpoints are closed every 3 minutes.

Not sure if that is still accurate since the move to Cloudflare. [@UNW8Q88EL](@UNW8Q88EL)?

---

## 2023-08-17 08:23:49 - general channel

**liam**

I think we have our wires crossed, if you are using AWS then you want to be using cloudwatch, there a few plug and play python modules that literally stream the logs straight to CW. For example I use docker, few lines of code and it does it all.



Not sure why you would be overwriting, just updating with the latest surely? Often results get changed. 

---

## 2023-08-16 19:09:30 - general channel

**Frenkie**

Hi all, a newbie here. I am trying to go through the documentation of Flumine in order to deploy my own strategy, I wanted to check if there's any way to fetch my current exposure at each runner.

One way is for me to calculate this locally by tracking all my previous matchings but I wanted to know if there's any method/attribute that provides this, or how do you guys fetch this info?

---

## 2023-08-14 16:45:21 - general channel

**mzaja**

The reason why I even dived into this matter is because I noticed that Flumine was rejecting order placement with

```"strategy.validate_order failed: live_trade_count (1) &gt;= max_live_trade_count (1)"```

when I tried to reuse the Trade to place a closing order. This never happened in the simulation so I investigated further and found out that a percentage of opening bets does not change status to "Execution complete" when the bet is fully matched (and even if it isn't, it should be cancelled). Therefore, it cannot be down to a delay in the logging controls because Flumine's behaviour on a live market confirms it.

---

## 2023-08-14 14:47:43 - general channel

**Mo**

AWS Dublin

---

## 2023-08-14 12:23:29 - general channel

**mzaja**

The way I see it, it is either:

1. Betfair erroneously does not send an update, hence I am asking whether someobody else experienced the same thing.

2. Betfair sends an update but it somehow gets dropped before it reaches me.

3. Betfair sends an update and it is received on my end, but Flumine does not process it.

The core issue is that Flumine leaves the trade in live state in the order does not complete, which then intereferes with the trading controls.

---

## 2023-08-14 12:11:07 - general channel

**mzaja**

Hi, has anybody had similar experience to this? I have been placing FILL_OR_KILL orders through Flumine and noticed that the order stream update for setting the order status to "Execution complete" is sometimes severely delayed. A FILL_OR_KILL order is either matched instantaneously or rejected, so it should go straight from "Executable" to "Execution complete". Normally, it looks like this, and one can see that the delta between the last two updates is 52 ms:

```Time                       Selection ID  Bet ID        Order status        Side      Matched    Remaining  Trade status

-----------------------  --------------  ------------  ------------------  ------  ---------  -----------  --------------

2023-08-13 10:35:25,142        55258054  None          Pending             BACK         0            2     Live

2023-08-13 10:35:25,340        55258054  316818562580  Executable          BACK         2            0     Pending

2023-08-13 10:35:25,392        55258054  316818562580  Execution complete  BACK         2            0     Live```

However, for a sizeable proportion of the time, the "Execution complete" status update is received several minutes later, when the market gets resulted. Note that the bet is already fully matched when it reaches the status of "Executable", but it takes almost 9 minutes for the "Exection complete" update to arrive.

```Time                       Selection ID  Bet ID        Order status        Side      Matched    Remaining  Trade status

-----------------------  --------------  ------------  ------------------  ------  ---------  -----------  --------------

2023-08-13 10:11:58,988        58973557  None          Pending             BACK            0            2  Live

2023-08-13 10:11:59,296        58973557  316815184967  Executable          BACK            2            0  Pending

2023-08-13 10:20:39,839        58973557  316815184967  Execution complete  BACK            2            0  Live```

Any thougts on this? I am trying to figure out whether this is

1. A problem on Betfair's side.

2. A problem with Flumine.

3. A problem with my setup.

---

## 2023-08-13 16:39:00 - random channel

**D C**

It appears that my component is limited to 6 max parallel connections to the server. Last year this cost me a lot of money as I was not using async bet placement. So lets say I had 15 strategies, each wanting to have bets placed (inplay). 6 would get sent straight away, but the remaning 9 would be queued until one of the 6 connections was free. Inplay horses that takes 1 second. My strategies could request more bets to be placed on any price update - so you see how quickly that could escalate. I've changed my stuff significantly since then but I still get the odd excessive lag if things line up in the right (wrong) way. It is a problem that would simply disappear completely with a thread pool I feel

---

## 2023-08-13 16:12:43 - random channel

**D C**

I've been chatting here with some people and I'm considering starting to use flumine in order to get around some issues I am having with the HTTPS client component that I currently use in my own setup. I've a question about the use of thread pools really. At times I will need to place bets across many distinct strategies in a way that they are all fired within as little as 1 millisecond of eachother. This causes the component I use a few problems the result being that my requests are queued in an opaque manner and I suffer placement latency.



When using a thread pool I'm trying to find out what the lay of the land is. Alternatively, if I write my own thread pool trying to find a best practice implementation based around the betfair API:



1. Does each thread in the pool maintain a HTTPS connection to the server (in the keep-alive sense) at all times so that when called up it is ready to immediately transmit the request, or does it sit idle and go through the whole SSL handshake process every time?

2. Are threads created at the start of the program and persist until it terminates? I am guessing this is the case due to overheads in firing up new threads in an on-demand manner?

3. Within each thread, is a "global" session token shared across the entire application, or does each thread make a separate login and manage its own connection? If the latter, is that frowned upon by betfair (do they have a limit on the number of active sessions each user account can make)?

I am a bit of a dunce with how HTTPS works under the surface and I am trying to get some support for the components I am using but the CS reps are all but accusing me of trying to create something that causes a DOS on a target so I'm getting nowhere (other than getting pissed off with them). When mentioning that I might want to fire off several requests to a server within a single figure millisecond time interval I am being told that my use case is "very unusual" and they are proving to be very unhelpful.

---

## 2023-08-13 12:25:28 - strategies channel

**R**

[@UBS7QANF3](@UBS7QANF3) solving kelly for multiple simultaneous outcomes only has analytical solutions for when they are mutually exclusive (i.e. only one horse can win a race and the rest lose - excluding draws).



For when things get more complex (e.g. placing WIN and FORECAST bets for example), don't you need to know the correlation structure between bets in order to determine kelly?

---

## 2023-08-12 22:21:06 - issues channel

**George**

Similar to the above - I've received an error with the message "MarketStream 2001 run error" - and further down it says "SocketError Connect 2002: Connection closed by server".



Does that mean I have to kill the flumine process and start again? Or is it all good?

---

## 2023-08-08 03:16:01 - issues channel

**Grant Allison-Young**

Hi there, last night I created my certificates and created a not-interactive function to login to Betfair.  All good :  {"sessionToken":"ZZZZZZZ=","loginStatus":"SUCCESS"}.  Today when I woke up and ran it again,without any changes I got  “The remote server returned an error: (500) Internal Server Error.”



I have no Idea what to check.  I ran the same process through Postman and it worked fine.  What end point should I be targeting?  When I did it last night i used  ("[https://identitysso-api.betfair.com/api/certlogin](https://identitysso-api.betfair.com/api/certlogin)") but the Postman version used  [https://identitysso-cert.betfair.com/api/certlogin](https://identitysso-cert.betfair.com/api/certlogin) - I get the same response with both....

---

## 2023-08-07 19:02:50 - strategies channel

**foxwood**

Training per race would be slower but that is only while training - applying the model to live data would be no different. To add to the headache have a look at a regression model instead of classification - that should give you a score instead of a binary win/lose - effectively a probability.

---

## 2023-08-07 08:46:04 - strategies channel

**Unknown**

*Day 1*



Setting the scene, the plan is to improve a live (profitable) strategy by optimising its current parameters (features) using ML.  This is going to be a learning exercise so if you have any questions / advice / criticism please get involved :wave:



The strategy is for TPD inplay racing, low stakes and taking Lay prices, market impact can be ignored for now. To keep things simple I am going to limit to Flat racing for the past 6 months, Jan-June inclusive, which gives around 1500 markets.



One of the features is what I would call the model and gives a % chance of the horse winning/losing and is what I have used for the past 4 yrs as the most important feature in the `ifs`. However I know that with the inclusion of other features it can be improved but I have never been able to optimise manually and get results.



Stats: £6.5 profit per market, 11.6% roi, £15.8 matched per selection

---

## 2023-08-06 12:33:33 - general channel

**AI Trader**

[@UBS7QANF3](@UBS7QANF3), I am using it for my live deployment, but for research purposes I am reconstructing the historical orderbook data. I will try to find where this happens on betfairlightweight to see the logic being used

---

## 2023-08-05 23:20:18 - general channel

**Trex44**

All brokers provide live data. interactive brokers have menagerie to pick from depending on what markets you want to trade [https://www.interactivebrokers.com/en/pricing/research-news-marketdata.php](https://www.interactivebrokers.com/en/pricing/research-news-marketdata.php). You can also pay for for market depth etc. I think they lower or wave fees once you trade over a certain volume. You can get historical data from them and other sources too e.g. Kinetic. Its the historical stuff that I was surprised how cheap it is relative to BF historic data.

---

## 2023-08-05 22:12:27 - general channel

**George**

where are you going to get the live data stream from?

---

## 2023-08-04 16:41:25 - random channel

**Burrito**

What CPU utilization should a user expect on an AWS micro instance, running a strategy that does nothing, but which is subscribed to every horse racing market?

---

## 2023-08-04 15:30:59 - strategies channel

**foxwood**

Did a lot of low-level experimenting with Encog about 10 years ago and later with Tensorflow, Keras etc and most recently [http://ML.NET|ML.NET](http://ML.NET|ML.NET). Have used recorded data and bought in data. To avoid complexities of integrating different packages for live I tend to use a csv file to handshake with the ML lol. Processing time was not generally important so easy solution. Sadly none of the ventures has proved better than "trad" methods of "if's" regardless of fields selected for training or type of model used. An itch I still scratch - so yes - very interested in this area. If you make the work as public as flumine though I think others will copy and edges will vanish.

---

## 2023-08-04 13:11:21 - random channel

**D C**

Can anyone recommend a path-of-least-resitance method of installing python on macbook? I recently bought one purely for music production but I would quite like to do a little scripting with it too just to enable me to get a bit more use out of it. I know there will be a multitude of search engine results but I've more faith in a recommendation from here tbh.

---

## 2023-08-04 01:50:57 - issues channel

**Guy Incognito**

Hi guys, looking at my log file I see this:



```2023-08-04 09:18:40:ERROR:53:OrderStream 1001 run error

Traceback (most recent call last):

  File "/usr/local/lib/python3.10/site-packages/flumine/streams/orderstream.py", line 51, in run

    self._stream.start()

  File "/usr/local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py", line 67, in start

    self._read_loop()

  File "/usr/local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py", line 226, in _read_loop

    received_data_raw = self._receive_all()

  File "/usr/local/lib/python3.10/site-packages/betfairlightweight/streaming/betfairstream.py", line 256, in _receive_all

    raise SocketError(

betfairlightweight.exceptions.SocketError: [Connect: 1002]: Connection closed by server

2023-08-04 09:18:42:INFO:21:Starting OrderStream 1001

2023-08-04 09:18:42:INFO:23:[Register: 1002]: orderSubscription

2023-08-04 09:18:42:WARNING:25:[Listener: 1002]: stream already registered, replacing data

2023-08-04 09:18:42:INFO:115:[OrderStream: 1002]: "OrderStream" created 

```

Does anyone know what this means? My guess is that my connection gets closed but the flumine opens a new one afterwards?

---

## 2023-08-03 16:51:53 - general channel

**George**

really i was just doing a little bit of testing guys - this was not a "deployed" strategy. i was just messing around with a few £ here and there. i totally agree about bankroll to stake ratio.

but point taken

---

## 2023-08-03 07:12:10 - general channel

**liam**

By cell I am going to assume you are trying to run this in a notebook, the idea with flumine / recorder is to run the program on its own process and just leave it (not designed for stop / start) 



Have you seen the raw streaming data before? / read the docs? The files are not valid json, the lines are, at a guess you want to be using bflw or flumine to read the files.



It’s recording the streaming data so the level will be based on how active the market is assuming you are using your live key. 

---

## 2023-08-02 14:32:23 - general channel

**rob smith**

Hello, is it possible to get the VWAP using bflw? Not for historical data but when trading live. Cheers

---

## 2023-08-02 13:13:48 - issues channel

**foxwood**

Running it live today and will let you know if it works . Another issue that I've not looked at with this is that bets are voided - wondering what happens to orders in blotter for both simulation and live ? Market info for dogs : _If a non-runner or reserve runner is declared all bets with the exception of Betfair SP bets on unaffected traps, placed on the race prior to the update of the market on Betfair will be void._ I've been ignoring all this in the past and just accepting the swings and roundabouts this causes but having to look at it now matching to external data.

---

## 2023-08-01 17:48:13 - issues channel

**foxwood**

Just did a quick mod of `BFLWexampleone.py` to dump current live GH catalogues - lots of runner in latest catalogues today with `"(res)"` suffix where reserve taking place of original dog. Therefore issue when recording doesn't appear to be a BF issue.

---

## 2023-07-31 17:52:42 - issues channel

**foxwood**

I'm trying to match dogs' names to external data but falling over with reserves. The reserve selection is in the market and marked as active and the retired dog as removed. When I map the runner selectionID to the catalogue however there is no entry for the new runner ie the catalogue has not been updated (yet?). Any ideas of a way round this - happens on backtest - probably dependent on BF publishing new catalogue ? Maybe there is a way of forcing a catalogue update in flumine and suspending processing until completed without jamming the whole process waiting for BF ?

Edit: Looking at json catalogue no entry of the new runner at all - recording problem ? Not sure if this happens on live or not now.

---

## 2023-07-27 08:20:36 - general channel

**liam**

EC2.nano, setup and forget, much easier than managing it yourself

---

## 2023-07-27 07:58:56 - issues channel

**Nacho Uve**

My Betfair account is in Spain. I was running Flumine on a server that I would like to cancel. I was completing the migration to an EC2 instance (region Paris), but when trying to connect from the server, it throws an error:

&gt; betfairlightweight.exceptions.LoginError: API login: BETTING_RESTRICTED_LOCATION

In Europe, EC2 offers regions: London, Dublin, Stockholm, Paris, or Frankfurt, none of which are in Spain.



Do you think any of these will work, or is there a way to operate from AWS with a Spanish account?

---

## 2023-07-23 11:39:07 - general channel

**Guy Adini**

Do you know which datacenter offers the lowest latency when connecting to Betfair?

I'm already on AWS so London or Dublin make the most sense for me, but it would also be good to know in general - if I want the very lowest latency in the future, if there's some other recommended provider.

---

## 2023-07-23 09:31:51 - general channel

**Jonjonjon**

Using the market recorder example in the Flumine codebase. Then run it on AWS or a similar service

---

## 2023-07-19 12:44:08 - issues channel

**Nacho Uve**

Hummm... paper_trade is using simulation engine, so I will find the same behaviour even connecting to live data stream to test this kind of strategies... Am I right?

---

## 2023-07-14 12:39:57 - strategies channel

**R**

very nice, I didn't know about flumine workers.  Cheers Peter.



I'm in a similar position RE offline evaluation.  What I'm wanting for is basically using flumine as part of the evaluation so the code I use to test and evaluate is as close as possible to the code that will be deployed.   My guess is that I need to just reduce the frequency of market updates in that case.  Or adapt flumine a bit to get it faster for the backtest case.

---

## 2023-07-14 00:46:48 - issues channel

**Andy**

[@U4H19D1D2](@U4H19D1D2)  the problem came back overnight…



I was previously getting this error 100% of the time within a minute when I tried to get all horses and dogs win and place for AU racing.



• After my live key was pumped up to 1k markets, the problem seemingly went away instantly and stayed away for ~14 hours. 

• Last night it came back. There were a lot of horse racing markets on site at the time (more than I’d probably ever received before). All of Fridays thoroughbred and trots meetings (15 meetings) and 5 of Saturdays thoroughbred already. But no greyhound racing at all as they hadn’t been loaded yet for the next day. This is the part that perplexes me. 

• I think it’s clear that whatever was done to pimp my API key the previous day had some desirable effect but in saying that, I wasn’t near 1k markets when the crash happened….



Now betfair have come back with the following:





Hi Andy,



I checked with the UK API team and got the response below overnight. Sounds like the issue is related to the socket handling outlined in this GitHub issue thread - [https://github.com/betcode-org/betfair/issues/11|https://github.com/betcode-org/betfair/issues/11](https://github.com/betcode-org/betfair/issues/11|https://github.com/betcode-org/betfair/issues/11)



If you could check those settings and see if it resolves the issue.



UK Response



A TIMEOUT error on a subscription request has a clear root cause – this is indicated by the error description and in our docs (":"Connection isnot subscribed and is idle: 15000 ms)  (see [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Exchange+Stream+API#ExchangeStreamAPI-TCP/SSLConnection|https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Exchange+Stream+API#ExchangeStreamAPI-TCP/SSLConnection](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Exchange+Stream+API#ExchangeStreamAPI-TCP/SSLConnection|https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Exchange+Stream+API#ExchangeStreamAPI-TCP/SSLConnection))



Avoiding TIMEOUT on connection

Once you have established a connection you should send a message within 15 seconds to avoid receiving a TIMEOUT error



Strange that he would receive this error in any other circumstances though, so yes, it could be related to socket handling as mentioned in that github issue thread.

---

## 2023-07-12 02:34:30 - issues channel

**Andy**

Betfair response:



Hi Andy,



Thanks for sending through the details of the error messages you’re getting. It looks like you’re hitting the market limit on your live key.



We’ll forward this to our API team in the UK and get them to increase the market limit. That should happen overnight tonight and we’ll confirm tomorrow.

---

## 2023-07-11 03:23:44 - issues channel

**Andy**

• Wanting to get WIN and PLC markets for horse and greyhounds but currently it's impossible.

Ideally would have one filters.streaming_market_filter with event_type_ids=["7","4339"] and market_types=["WIN","PLACE"]



• Scenario 1) Using 1 streaming_market_filter with both of the above settings shows the following error multiple times and returns nothing:

Encountered an error (connection_id: 208-110723015800-1550840, data: {"op":"status","statusCode":"FAILURE","errorCode":"TIMEOUT","errorMessage":"Connection is

not subscribed and is idle: 15000 ms","connectionClosed":true,"connectionId":"208-110723015800-1550840"}) when starting or running the stream.



• Scenario 2) Using 1 streaming_market_filter with both of the above settings works for market_types=[WIN] only.

• *Scenario 3) Using 1 filter - event_type_ids=["7"] and market_types=["WIN", "PLACE"] works?? (only scenario with multiple parameters set where any place markets are received)*

• Scenario 4) Using 1 filter - event_type_ids=["4339"] and market_types=["WIN", "PLACE"] *does not work*. Same error as trying event_type_ids=["7","4339"] and market_types=["WIN","PLACE"]

Encountered an error (connection_id: 210-110723020837-1593171, data: {"op":"status","statusCode":"FAILURE","errorCode":"TIMEOUT","errorMessage":"Connection is not subscribed and is idle: 15000 ms","connectionClosed":true,"connectionId":"210-110723020837-1593171"}) when starting or running the stream.



• Scenario 5) trying two streaming_market_filters (one with event_type_ids=["7"] and another one with event_type_ids=["4339"]) and market_types=["WIN","PLACE"] for both does not work, throws error:

Encountered an error (connection_id: 203-110723021056-1561190, data: {"op":"status","statusCode":"FAILURE","errorCode":"TIMEOUT","errorMessage":"Connection is not subscribed and is idle: 15000 ms","connectionClosed":true,"connectionId":"203-110723021056-1561190"}) when starting or running the stream.



• *Scenario 6) trying two streaming_market_filters (one with event_type_ids=["7"] and another one with event_type_ids=["4339"]) and market_types=["WIN"] also works..*

• Scenario 7) Subscribing with 4 different streaming filters shows this error twice:

Encountered an error (connection_id: 210-110723013905-1590774, data: {"op":"status","statusCode":"FAILURE","errorCode":"TIMEOUT","errorMessage":"Connection is

not subscribed and is idle: 15000 ms","connectionClosed":true,"connectionId":"210-110723013905-1590774"}) when starting or running the stream.

Very ocassionally get this issue:

Encountered an error ([Connect: 5]: Socket [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:2546)) when starting or running the stream



Using Live API key which I'm told has 10 max connections by default

---

## 2023-07-10 18:07:30 - random channel

**Charaka A**

I was trying to visualise the live ladder using Flumine, anyone has suggestions on how to go about this. Initially thought of writing ladders to a queue and have a thread read from the queue and display the runner ladder however this doesn't work. Would appreciate thoughts on this if anyone has done this before

---

## 2023-07-10 17:53:38 - general channel

**Matthew**

I add some things that are not totally clear to me. From the listener of the betfair lightweight I can enable the cumulative market volume and runner volume but this is discouraged, why? How these two parameters behaves when I read from bz2 files vs live feed?



There is also the streaming updates list in the market book where I can see some volumes which does not seem to be cumulative, maybe I can pick those one, even if it is not a friendly list 

---

## 2023-07-10 08:39:49 - issues channel

**Andy**

def start_stream(bfl_trading, bfl_client, event_types=["horse", "greyhound"]):

    """Retrieves a json dictionary with the stream data from the specified event types."""

    for event in event_types:

        event_types[event_types.index(event)] = EVENT_TYPE_IDS[event]



    # create queue

    output_queue = queue.Queue()



    # create stream listener

    listener = betfairlightweight.StreamListener(output_queue=output_queue)



    # create stream

    streamH = bfl_trading.streaming.create_stream(listener=listener)

    streamG = bfl_trading.streaming.create_stream(listener=listener)



    # Create filters to get the events

    horse_market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["AU"],

        market_types=["PLACE", "WIN"]

    )

    greyhound_market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["4339"],

        country_codes=["AU"],

        market_types=["WIN"]

    )



    market_data_filter = filters.streaming_market_data_filter(

        fields=["EX_BEST_OFFERS", "EX_MARKET_DEF"], ladder_levels=3

    )





    # subscribe

    horse_streaming_unique_id = streamH.subscribe_to_markets(

        market_filter=horse_market_filter,

        market_data_filter=market_data_filter,

        conflate_ms=5000,  # send update every 1000ms

    )



    greyhound_streaming_unique_id = streamG.subscribe_to_markets(

        market_filter=greyhound_market_filter,

        market_data_filter=market_data_filter,

        conflate_ms=5000,  # send update every 1000ms

    )



    # start stream in a new thread (in production would need err handling)

    tH = threading.Thread(target=streamH.start, daemon=True)

    tH.start()

    tG = threading.Thread(target=streamG.start, daemon=True)

    tG.start()



    return streamH, output_queue

---

## 2023-07-09 15:36:26 - general channel

**Gary Cook**

I gather some data to inform my trades. I'm actually very good at it but I didn't mind losing money while testing some things.  I don't place trades with the API. I don't think that should matter though? The money is still going through Betfair just their normal web UI. The reason I mention that is because just gathering data I don't really need streaming. Im just doing snapshots, not in-play live trading with the API.

---

## 2023-07-07 06:08:02 - general channel

**Mo**

I believe the error is related to how frequently you are connecting to the database so if you have reduced that frequency then that is a workaround



Another workaround I was going to suggest would be to use the IP address of the database server, assuming it is fixed, to avoid having to resolve the name



In my case, although I recall I had to mess around with the default name resolver setup in Ubuntu, that doesn't seem to be reflected in my current configuration so I assume it was fixed at some point. I am running 22.04. Upgrading should be a permanent solution

---

## 2023-07-04 09:40:07 - issues channel

**Johnny**

Potentially a very basic question here.  I'm trying out the example marketrecorder.py and have changed only my login details and local_dir - it starts up and finds all the markets, and creates an empty folder with a UUID, but then all the workers stop.  The strategy continues to run though with no data being supplied.  Does anyone have what else I need to do to get the standard recorder working?



```{"asctime": "2023-07-04 08:31:24,036", "levelname": "INFO", "message": "[MarketStream: 2001] 1.215758531 added, 53 markets in cache"}

{"asctime": "2023-07-04 08:31:24,036", "levelname": "INFO", "message": "[MarketStream: 2001] 1.215758545 added, 54 markets in cache"}

{"asctime": "2023-07-04 08:31:24,037", "levelname": "INFO", "message": "[MarketStream: 2001] 1.215758486 added, 55 markets in cache"}

{"asctime": "2023-07-04 08:31:24,037", "levelname": "INFO", "message": "[MarketStream: 2001] 1.215758498 added, 56 markets in cache"}

{"asctime": "2023-07-04 08:31:24,038", "levelname": "INFO", "message": "[MarketStream: 2001] 1.215727888 added, 57 markets in cache"}

{"asctime": "2023-07-04 08:31:24,039", "levelname": "INFO", "message": "[FlumineStream: 2001]: 57 mc added"}

{"asctime": "2023-07-04 08:31:24,039", "levelname": "INFO", "message": "Adding: 1.215727854 to markets", "client": {"id": "61ef3ddb", "exchange": "Betfair", "betting_client": "APIClient", "chargeable_transaction_count": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxOrderCount object at 0x7f014c174310&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 0, "open_market_count": 0, "live_orders": false, "markets": []}, "streams": ["&lt;OrderStream(OrderStream, started daemon 139643035035200)&gt;", "&lt;DataStream(DataStream, started daemon 139643026642496)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 139643610474304)&gt;", "&lt;Thread(Thread-2, started daemon 139643479651904)&gt;", "&lt;Heartbeat(Thread-3, started daemon 139643471259200)&gt;", "&lt;HistorySavingThread(IPythonHistorySavingThread, started 139643446081088)&gt;", "&lt;ParentPollerUnix(Thread-1, started daemon 139643437688384)&gt;", "&lt;BackgroundWorker(keep_alive, started daemon 139643076998720)&gt;", "&lt;BackgroundWorker(poll_account_balance, started daemon 139643068606016)&gt;", "&lt;BackgroundWorker(poll_market_catalogue, started daemon 139643060213312)&gt;", "&lt;BackgroundWorker(poll_cleared_orders, started daemon 139643051820608)&gt;", "&lt;Thread(TEST_load_processor, started daemon 139643043427904)&gt;", "&lt;OrderStream(OrderStream, started daemon 139643035035200)&gt;", "&lt;DataStream(DataStream, started daemon 139643026642496)&gt;", "&lt;Thread(OrderStream_output_thread, started daemon 139642540127808)&gt;"]}

{"asctime": "2023-07-04 08:31:24,042", "levelname": "INFO", "message": "Stopped DataStream 2001"}

{"asctime": "2023-07-04 08:31:24,042", "levelname": "INFO", "message": "Shutting down Execution (SimulatedExecution)"}

{"asctime": "2023-07-04 08:31:24,042", "levelname": "INFO", "message": "Stopped OrderStream 1001"}

{"asctime": "2023-07-04 08:31:24,044", "levelname": "INFO", "message": "Shutting down Execution (BetfairExecution)"}

{"asctime": "2023-07-04 08:31:24,045", "levelname": "INFO", "message": "BackgroundWorker keep_alive shutting down", "worker_name": "keep_alive", "function": "&lt;function keep_alive at 0x7f0144ce3670&gt;"}

{"asctime": "2023-07-04 08:31:24,235", "levelname": "INFO", "message": "Stopped output_thread (OrderStream 1001)"}

{"asctime": "2023-07-04 08:31:28,046", "levelname": "INFO", "message": "BackgroundWorker poll_account_balance shutting down", "worker_name": "poll_account_balance", "function": "&lt;function poll_account_balance at 0x7f0144ce3940&gt;"}

{"asctime": "2023-07-04 08:31:32,047", "levelname": "INFO", "message": "BackgroundWorker poll_market_catalogue shutting down", "worker_name": "poll_market_catalogue", "function": "&lt;function poll_market_catalogue at 0x7f0144ce38b0&gt;"}

{"asctime": "2023-07-04 08:31:36,049", "levelname": "INFO", "message": "BackgroundWorker poll_cleared_orders shutting down", "worker_name": "poll_cleared_orders", "function": "&lt;function poll_cleared_orders at 0x7f0144ce39d0&gt;"}

{"asctime": "2023-07-04 08:31:40,172", "levelname": "INFO", "message": "Exiting flumine", "client": {"id": "61ef3ddb", "exchange": "Betfair", "betting_client": "APIClient", "chargeable_transaction_count": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxOrderCount object at 0x7f014c174310&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 1, "open_market_count": 1, "live_orders": false, "markets": ["1.215727854"]}, "streams": ["&lt;OrderStream(OrderStream, stopped daemon 139643035035200)&gt;", "&lt;DataStream(DataStream, stopped daemon 139643026642496)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 139643610474304)&gt;", "&lt;Thread(Thread-2, started daemon 139643479651904)&gt;", "&lt;Heartbeat(Thread-3, started daemon 139643471259200)&gt;", "&lt;HistorySavingThread(IPythonHistorySavingThread, started 139643446081088)&gt;", "&lt;ParentPollerUnix(Thread-1, started daemon 139643437688384)&gt;", "&lt;BackgroundWorker(keep_alive, started daemon 139643076998720)&gt;", "&lt;BackgroundWorker(poll_account_balance, started daemon 139643068606016)&gt;", "&lt;BackgroundWorker(poll_market_catalogue, started daemon 139643060213312)&gt;", "&lt;BackgroundWorker(poll_cleared_orders, started daemon 139643051820608)&gt;", "&lt;Thread(TEST_load_processor, started daemon 139643043427904)&gt;"]}

---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-2-ce727f3ddda0&gt; in &lt;module&gt;

     20 framework.add_strategy(strategy)

     21 

---&gt; 22 framework.run()



~/anaconda3/lib/python3.8/site-packages/flumine/flumine.py in run(self)

     27 

     28                 elif event.EVENT_TYPE == EventType.RAW_DATA:

---&gt; 29                     self._process_raw_data(event)

     30 

     31                 elif event.EVENT_TYPE == EventType.CURRENT_ORDERS:



~/anaconda3/lib/python3.8/site-packages/flumine/baseflumine.py in _process_raw_data(self, event)

    211             for strategy in self.strategies:

    212                 if stream_id in strategy.stream_ids:

--&gt; 213                     strategy.process_raw_data(publish_time, datum)

    214 

    215     def _process_market_catalogues(self, event: events.MarketCatalogueEvent) -&gt; None:



TypeError: process_raw_data() missing 1 required positional argument: 'data'```



---

## 2023-07-01 02:32:25 - issues channel

**Tom**

Is order.trade the only way to access the trade objects? I'm trying to save them to runner_context so I can access them (can't figure out how else to call them), and if the program restarts, runner_context.live_trades and trades just has the trade id's. I'm trying to manage multiple orders through a single Trade object, but the flow of logic seems to be strategy-market-blotter-orders-trade; where I want it to be strategy-strategies-runner_context-trade-order where strategies are notes in trade and order used to identify them.



Any suggestions gentlemen?

---

## 2023-06-30 10:24:46 - general channel

**Stef**

Hi all,

maybe this question was already answered, in that case apologies in advance&gt;

I have a live key, which I activated a while ago before Betfair started charging £299. It works fine with my old scripts, but it doesn't seem to be working with streaming. AM I doing something wrong?



Thanks :)

---

## 2023-06-28 17:34:15 - general channel

**George**

Now I think I understand workers and my code is working - thanks very much. But I still have an issue.

Whilst the simulation is of course much faster than "real-time", there is no equivalent way to speed up the worker code which is inherently slow (e.g. calls to an external database or API).

This means that the worker output - which writes to `market.context` - is totally misaligned (in terms of 'clock') with the market-book. Isn't this a huge issue for some users?

Now I am lucky, because my worker output does not change frequently over time. However, I do need the worker to execute its "first pass" before the first `process_market_book`  is called. I can, of course, instruct my worker to start 10 "minutes" before `check_market_book` returns True, but 10 simulation-minutes is much quicker than the time it takes for the worker output to be generated, so it solves the issue in live trading but not in simulation.

I think that I can workaround this issue by checking the `market.context` during the `check_market_book`. However, it is not ideal, because `check_market_book` will return False in the simulation, whereas, in real life it would have returned True.

Depending on how slow the worker is, most of the simulated trading window could have been missed, just waiting for the worker's first pass!

I imagine this is not a new question - what is the standard answer to dealing with this?

---

## 2023-06-26 11:12:24 - issues channel

**James**

I found this

[https://github.com/betcode-org/flumine/issues/665](https://github.com/betcode-org/flumine/issues/665)



And am happy to have a crack at it. Just curious if anyone has done any related interrupt handling in their own deployments?

---

## 2023-06-26 10:14:09 - general channel

**Jorge**

I guess my model is too slow and when there are many live games it takes too long to run process_market_book. For sure increasing the number of CPUs and optimizing the model would help me. I'd just like to add another safety check in case this ever happens again

---

## 2023-06-23 10:44:10 - strategies channel

**Peter**

[https://www.researchgate.net/publication/4748916_Probability_and_Statistical_Models_for_Racing|This paper might help you](https://www.researchgate.net/publication/4748916_Probability_and_Statistical_Models_for_Racing|This paper might help you) but as [@UBS7QANF3](@UBS7QANF3) implies, if these models gave you an edge, people would be very quick to exploit it and the edge would disappear. In the case of Harville, there's a clear bias in the results that it delivers towards shorter priced runners, making it a poor estimator for place odds.

---

## 2023-06-21 16:29:06 - general channel

**Alejandro Pablos Sánchez**

Coming back to this, I'm still not able to fix it with the keep_alive method

---

## 2023-06-21 07:53:07 - general channel

**Peter**

Is it possible that your session token expired? With betfairlightweight you need to call the keep_alive method yourself to keep it active.

---

## 2023-06-16 09:24:48 - issues channel

**Aaron Smith**

[@U4H19D1D2](@U4H19D1D2) i totally agree and besides that, there were plenty of more occasions where the rational mind should kick in make sure things dont go wild when hitting that live button, i m totally aware of those solutions. However i m still curious about why my terminate function didnt do its job properly and how to act in case i send another one of these boys out there, even if its due to my own stupidity :smile:

---

## 2023-06-16 06:42:07 - issues channel

**Aaron Smith**

hey guys! Yesterday i had my first bot going rogue. Looking at my terribly lazy workflow and messy code at times, its actually a miracle that this has been the first time in all these years. I know very well how to prevent this in the future, as i know what caused it (lazy workflow a la "why paper trade - cool kids go live" :smile: ). Knowing myself however, i could still be dumb enough to let it happen again, even after this incident.

Anyways, as i watched my rogue bot throw out 100% of my bankroll in a fraction of a second, i was wondering: how to best deal with this? I didnt want to just kill it, as then i d be left with thousands of bets (idk how many it was, it hit transaction limit, so whatever that limit is) all over betfair that are completely unattended and i d have to manually cancel them all via the betfair_webUI. As it was supposed to only be a test run, i had it set to only run for 1 hour, so i decided to just let it do its thing, as this way, while it may have broken any exposure limits imaginable, it still babysat my bets in a somewhat "smart" way.



However, after 1 hour, the bot did in fact stop, but not as gracefully as hoped. It just left all the bets up so i still had to manually go through the markets and cancel them.



This leaves me with 2 questions:

1. Is there a way to gracefully kill a rogue bot?

2. What went wrong when my bot was supposed to cancel all bets before stopping as planned?

regarding 2, here is the relevant code that terminated the bot and was supposed to cancel all bets left:

```def terminate(context: dict, framework) -&gt; None:

    for market in framework.markets:

        for order in market.blotter:

            if order.status == OrderStatus.EXECUTABLE:

                market.cancel_order(order)

    framework.handler_queue.put(TerminationEvent)





terminator = flumine.worker.BackgroundWorker(flumine=framework, function=helpers.terminate, interval=100, start_delay=60 * self.run_time_in_mins)  # termiante after run_time_in_mins



framework.add_worker(terminator)```



Also, i know you guys would ve loved for me to throw some cash at you, but luckily i didnt get punished as i should ve been. The bot actually made 10 pounds (minus what the other bots could ve made in that hour maybe, if they had any bankroll left to play with), enough to get me a well deserved pint after a productive day at "work".  But no worries, i m sure i ll unleash one of these bad boys again some time.

---

## 2023-06-13 18:12:34 - strategies channel

**Jonjonjon**

Good question. when creating a Strategy you can limit your selection exposure, order exposure, number of live orders. I think it's in the BaseStrategy class

---

## 2023-06-11 13:21:54 - general channel

**PeterLe**

Hi, sorry newbie question...can someone please explain this setting in simple terms :

```multi_order_trades: bool = False,```

If you were trading the same market, how would it work in conjunction with :

```max_live_trade_count: int = 15,```

Thanks in advance

---

## 2023-06-11 11:10:57 - random channel

**D C**

I think there is a lot of tech like this. I was working  for someone a couple of years ago that asked me to look at NFL tracker data - [http://nfl.com|nfl.com](http://nfl.com|nfl.com) have a live tracker page for certain games (at least they did). I also worked for a single day at Hawkeye many moons ago but that stuff is all camera based. People have been striving to do this stuff for years. I suppose the problem for many companies is the entry step  into the regulatory bodies around sports. The cynic in my tells me it has to be a great opportunity for sports governing bodies to generate themselves some healthy backhanders.

---

## 2023-06-10 10:51:02 - strategies channel

**liam**

Yeah that looks fine assuming live is the same, if you can get some strategy code that replicates the issue I can run on my recorded market.



Btw any reason you need 20s of pre play rather than filter just in play on the kwargs?

---

## 2023-06-10 10:47:31 - strategies channel

**AndyL**

```def run_process(markets):

    uid = str(os.getpid())



    client = clients.SimulatedClient()

    framework = FlumineSimulation(client=client)

    bsp_file = "hrbsp/hrbsp_"+uid

    strategy = UkHrBot(framework, bsp_file,

        market_filter={"markets": markets,

                   "listener_kwargs": {"seconds_to_start": 20}

                  },

        max_order_exposure=500,

        max_selection_exposure=500,

        max_live_trade_count=4

    )



    with mock_patch("builtins.open", smart_open.open):

        control = BacktestLoggingControl("results/hrorders_"+uid+".txt")

        framework.add_logging_control(control)

        framework.add_strategy(strategy)

        framework.run()```

---

## 2023-06-10 10:24:21 - strategies channel

**AndyL**

Say for example trying to "take" £5 low odds back bets in the last 20seconds..., I found Simulated I was getting 80% match rate, live only 25%, and thus adverse selection was a lot higher... My theory there is Flumine can't judge the actual "demand" maybe? so match rate low..

---

## 2023-06-09 21:26:13 - strategies channel

**AndyL**

I've being working on some racing inplay strategies recently, and notice the Flumine simulation has a hard time accurately simulating matching "inplay". Backtest vs Live seems a wider gap compared to pre-race. Trying to understand why, I am guessing order flow and volatility is high inplay, so hard to simulate accurately? The best way to approach "inplay" seems to be to put a toe in the water and see how it goes!

---

## 2023-06-07 13:14:05 - random channel

**PeterLe**

Quick Question please: ..Can I run Flumine in paper_trade on an account that is also using a separate application to place live bets without it messing up the paper_trade results? does it merely just subscribe to the stream ignoring all live bets? Thanks

---

## 2023-06-05 23:16:51 - issues channel

**foxwood**

In `process_closed_market()` I walk the blotter and log all the bets marked as `EXECUTION_COMPLETE` For the first time today I found there were lots of unmatched lapsed bets that were not logged. From the live strategy log it seems that the market was closed and removed before `"Order status update: Execution complete"` was received for these bets ! Is this normal everyday stuff that I've not noticed before or was it just BF having a bad day ?



Edit: All the bets were `MARKET_ON_CLOSE` and less than £10 liability so guessing that's maybe what happens if not enough to make the SP draw. Wonder what the `OrderStatus` would be in that case ?

---

## 2023-06-04 16:40:50 - general channel

**Trex44**

Ahh ok. I was using the Notebook tab in AWS's Sagemaker service to create and use Jupyter notebooks. I wasn't actually using SageMaker studio its self. I think I steered away from it because I couldn't figure out how much it was going to cost me where as when you launch a stand alone notebook you can instantly see the resources you are using e.g. ml.t3.2xLarge so you know how much it will cost.



How much does using a Sagemaker studio instance cost? I see from the tutorial videos you can launch standalone notebooks in the Studio instance that utilise different resources so I am guessing you get charged for that. But is there a charge simply for keeping a Sagemaker Studio instance running even when there are no notebooks active?

---

## 2023-06-02 18:29:55 - random channel

**Newbie99**

The only reason I use a local DB is to save on costs as I was a bit worried they could spiral with AWS?

---

## 2023-06-02 18:06:06 - random channel

**liam**

Is it that slow? I use RDS on AWS all day

---

## 2023-06-02 14:18:09 - random channel

**Peter**

You're right that there are different approaches to database replication. The [https://dev.mysql.com/doc/refman/8.0/en/replication-solutions.html|MySQL documentation explains many of them](https://dev.mysql.com/doc/refman/8.0/en/replication-solutions.html|MySQL documentation explains many of them). But it's a pretty technical area and I wouldn't try to set it up and manage it yourself unless you're an experienced DBA (i which case why ask the question?).



The right solution absolutely depends on why you need a replicated database, so how others do it is pretty irrelevant. but since you asked, it's a standard feature of most cloud database services including AWS's RDS, which is how I do it, allowing me to focus on my trading apps and not get bogged down with irrelevant database issues.

---

## 2023-06-02 14:12:21 - general channel

**Derek C**

Everything runs inside Jupyter, using sagemaker studio. I guess you might be using the older version of Sagemaker? Inside SageMaker Studio



`import awswrangler as wr` 



`df = wr.athena.read_sql_query(sql=my_sql_text, database="bf", boto3_session=session, keep_files=False)`

---

## 2023-06-02 09:40:38 - issues channel

**Andy**

def start_stream(bfl_trading, bfl_client, event_types=["horse", "greyhound"]):

    """Retrieves a json dictionary with the stream data from the specified event types."""

    for event in event_types:

        event_types[event_types.index(event)] = EVENT_TYPE_IDS[event]



    # create queue

    output_queue = queue.Queue()



    # create stream listener

    listener = betfairlightweight.StreamListener(output_queue=output_queue)



    # create stream

    streamH = bfl_trading.streaming.create_stream(listener=listener)

    streamG = bfl_trading.streaming.create_stream(listener=listener)



    # Create filters to get the events

    horse_market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["AU"],

        market_types=["WIN"]

    )

    greyhound_market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["4339"],

        country_codes=["AU"],

        market_types=["WIN"]

    )



    market_data_filter = filters.streaming_market_data_filter(

        fields=["EX_BEST_OFFERS", "EX_MARKET_DEF"], ladder_levels=3

    )





    # subscribe

    horse_streaming_unique_id = streamH.subscribe_to_markets(

        market_filter=horse_market_filter,

        market_data_filter=market_data_filter,

        conflate_ms=5000,  # send update every 1000ms

    )



    greyhound_streaming_unique_id = streamG.subscribe_to_markets(

        market_filter=greyhound_market_filter,

        market_data_filter=market_data_filter,

        conflate_ms=5000,  # send update every 1000ms

    )



    # start stream in a new thread (in production would need err handling)

    tH = threading.Thread(target=streamH.start, daemon=True)

    tH.start()

    tG = threading.Thread(target=streamG.start, daemon=True)

    tG.start()



    return streamH, output_queue

---

## 2023-06-01 16:13:41 - general channel

**Derek C**

[@U03N4QBJ0TV](@U03N4QBJ0TV) I do a lot of the same thing, except that I use Athena to populate pandas DataFrames in SageMaker from my files in S3. The benefit is that I can filter the rows and the columns that will be brought into memory in the SQL logic, and also sometimes do grouping to further reduce the size of the dataset that lands in Pandas. The benefit of Athena is that you don't have to actually migrate anything into a SQL database - you just tell it where the csv files are in S3 and they 'become' the database that you can then query. The awswrangler library makes it easy to integrate Pandas and Athena/S3 if you're not already using that.

---

## 2023-06-01 10:47:17 - general channel

**Trex44**

[@U04NWADNCFR](@U04NWADNCFR) Thanks very much for the answer. I currently use csv's stored in S3 then load them into a Jupyter note book in AWS Sagemaker and use Pandas to explore them. I have considered moving everything over to an SQL database but I like the graphical output I can get from using Pandas. Loading into the notebook takes an age and the kernel dies if I try to manipulate large data sets as the instance doesn't have the memory needed.

---

## 2023-05-31 23:22:03 - issues channel

**AI Trader**

[@U053U2PUK2M](@U053U2PUK2M) did you find out the reason? Im experiencing the same issue, ie, it works locally, but when deploying in the cloud (with the same requirements.txt and python version, i SOMETIMES get this error

---

## 2023-05-26 19:12:44 - general channel

**AI Trader**

Hi guys,

What is the physical location of betfair servers? Asking because I want to run my strategies in the same location for lower latency. Is it London?

---

## 2023-05-25 11:34:17 - random channel

**Herugrim**

Yeah and because you pay commission per market per account you would generally end up paying more commission, especially if your accounts ran competing strategies. It’s painful to setup and even more painful to manage. 



IMO the only reason to need one is if you were deploying bots that have to potential to go rogue and drain your balance, but that comes back to improving your coding and best practices. It’s easier just to use strategy refs

---

## 2023-05-23 09:46:43 - issues channel

**Jorge**

Apparently my session_token has expired but I do not understand why, because my [https://github.com/betcode-org/flumine/blob/master/flumine/flumine.py#L71|ka_interval](https://github.com/betcode-org/flumine/blob/master/flumine/flumine.py#L71|ka_interval) is correct.

{"levelname": "INFO", "message": "BackgroundWorker keep_alive starting", "worker_name": "keep_alive", "function": "&lt;function keep_alive at 0x7f75951af3a0&gt;", "context": {}, "start_delay": 0, "interval": 600.0, "func_args": [], "func_kwargs": {}}

I see this log but then no mentions to keep_alive until it starts failing

---

## 2023-05-23 09:45:09 - issues channel

**Jorge**

I think it's just a typo, because keep_alive() returns None for both the case where no keep_alive is required and when it is required but an Exception is raised (my case)

---

## 2023-05-23 09:41:43 - issues channel

**Jorge**

It never reaches this line [https://github.com/betcode-org/flumine/blob/master/flumine/worker.py#LL108C1-L108C23|client.login()](https://github.com/betcode-org/flumine/blob/master/flumine/worker.py#LL108C1-L108C23|client.login()) so whenever it fails to keep alive, it does not try to login again to continue the execution

---

## 2023-05-23 09:39:57 - issues channel

**Jorge**

They told me the session token has expired. I think this could be improved in flumine: [https://github.com/betcode-org/flumine/blob/master/flumine/clients/betfairclient.py#L46|keep_alive()](https://github.com/betcode-org/flumine/blob/master/flumine/clients/betfairclient.py#L46|keep_alive()) is logging an Exception but returns None, so in [https://github.com/betcode-org/flumine/blob/master/flumine/worker.py#L100|worker.py](https://github.com/betcode-org/flumine/blob/master/flumine/worker.py#L100|worker.py) it does not [https://github.com/betcode-org/flumine/blob/master/flumine/worker.py#LL108C1-L108C23|client.login()](https://github.com/betcode-org/flumine/blob/master/flumine/worker.py#LL108C1-L108C23|client.login())

---

## 2023-05-23 07:56:04 - general channel

**Peter**

If you're using Flumine it will handle creating a session and keeping it alive. If you're using betfairlightweight or betconnect you'll need to keep the session alive yourself.

---

## 2023-05-14 06:25:49 - general channel

**Derek C**

[@U03N4QBJ0TV](@U03N4QBJ0TV) that's a tough question because there are so many options and it depends on how your data is structured and what you're preferences are in terms of analysis tools.



If your issue is just disk space, then using S3 instead of local storage is a good way to go.



If the issue is analysing large datasets then something I do quite a lot of, is to keep my data files in Amazon S3 and use Athena SQL to query them. It's a little tricky to set up but once you have defined your 'database' as this set of files (e.g. CSV files) you can then run SQL queries against it like any other DB. Of course this relies on you being comfortable with AWS/S3/SQL in the first place.



Another SQL option would be to get a trial Snowflake account and load the data into Snowflake - more user-friendly than Athena but more expensive once the trial expires.

---

## 2023-05-13 10:44:19 - general channel

**Trex44**

Morning all. Does anyone have a good resource where you can learn how to work with big data files (4-40GB) in Python. Struggling to work with this Data using AWS Jupyter instances at the moment because of its size (and my lack of knowledge).

---

## 2023-05-08 11:50:10 - general channel

**Jorge**

Hey guys. Using flumine my Strategy behaves strangely when Latency is high. Could it happen that my [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L118|market.blotter.live_orders](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L118|market.blotter.live_orders) gets updated but my [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L74|market.blotter.strategy_selection_orders](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L74|market.blotter.strategy_selection_orders) does not get updated? For example, in the event of an order filled, I'm thinking [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L118|market.blotter.live_orders](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L118|market.blotter.live_orders) could change but [https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L74|market.blotter.strategy_selection_orders](https://github.com/betcode-org/flumine/blob/master/flumine/markets/blotter.py#L74|market.blotter.strategy_selection_orders) doesn't and therefore my exposure calculation is wrong.

---

## 2023-05-08 09:10:47 - general channel

**Peter**

Sort of need to answer question 2 first. The most important factor is whether you are making or taking a price. If you're taking then the results will probably be quite accurate, as long as you don't keep taking the same liquidity. However if you're making prices (offering) then there are two conflicting forces that can send your actual results in either direction (or cancel out).



I've had strategies where the actual fill rate is less than that implied by backtesting, and those that are matched have a higher propensity to lose. This is known as adverse selection. Doesn't necessarily invalid the strategies, but makes them less profitable.



On the other hand some people here report seeing better results when running live than in backtesting, as their offers attract additional funds into the market giving them a better match rate.



So there's not a clear answer and by offering prices you are affecting the market in ways that backtesting can't anticipate. What that effect is will depend on how tight the spreads are how liquid the market is.

---

## 2023-05-08 08:20:42 - general channel

**Nick**

Hi All - A couple of questions for those using Flumine for running backtests on their strategies and their automation needs:

1. How close do your backtests compare with reality once you go live on a strategy?

2. What are some of the factors that influence differences between backtests and live results?

3. Any experiences / comparisons people want to share?

---

## 2023-05-07 23:15:39 - general channel

**Jeff Waters**

I've put some code in process_market_book to log the WOM when my orders are at least partially matched:



```for order in market.blotter.live_orders:

    runner = Repository().get_runner_from_order(order)

    if order.size_matched &gt; 0 and not hasattr(order, "wom"):

        order.wom = Wom().get_wom_for_runner(runner)```

When I print my results to a spreadsheet, I use:



```wom = order.wom if hasattr(order, "wom") else 0```

When I print the results, for orders that were matched, some of my entries are zero and some are the wom. I don't understand why it's sometimes the case that the order object is given a 'wom' attribute when money has been matched, and sometimes it isn't?



I wondered whether it might be due to race conditions, as I'm using multi-processing. I'm aware that, with multi-processing, each process has its own memory space, but Chat GPT assures me that race conditions can nevertheless happen (and yes, I'm aware that GPT isn't an authoritative source! :smile:). Anyway, I applied a lock to everything in process_market_book, but that didn't fix the issue.



Any suggestions would be appreciated.



Thanks



Jeff

---

## 2023-05-04 07:44:48 - issues channel

**Nick**

Hi All, I'm hoping you can help me. I'm new to flumine and I'm having trouble achieving what I'm trying to achieve for a backtest simulation.



I'm trying to run a simulation on soccer MATCH_ODDS market with the following conditions:



1. Record the odds of the favourite at 60secs before kick off

2. Place a BACK bet on the pre match favourite whilst match is in play and the odds of the favourite have increased by 20%

I'm not sure where it is going wrong or whether I am using the market.context correctly.



Code as per this:

def check_market_book(self, market, market_book):

        if (market_book.status == "OPEN") and market.seconds_to_start &lt; 60:

            # store favourite's starting price

            runners = sorted(

                market_book.runners, key=lambda r: r.ex.available_to_back[0]['price']

            )

            if runners:

                fav = runners[0]

                market.context["fav_starting_price"] = fav.ex.available_to_back[0]['price']

                market.context["market_start_time"] = market_book.market_definition.market_time

                return True



    def process_market_book(self, market, market_book):



        if "fav_starting_price" in market.context:

            # get favourite

            runners = sorted(

                market_book.runners, key=lambda r: r.ex.available_to_back[0]['price']

            )

            if runners:



                fav = runners[0]

                # calculate favourite's odds



                if market.seconds_to_start &lt; 60 and market.seconds_to_start &gt; 50:

                    fav_starting_price = fav.ex.available_to_back[0]['price']

                    fav_starting_price = market.context["fav_starting_price"]







                elif market_book.inplay == True:

                    if fav.ex.available_to_back[0]['price'] &gt; market.context["fav_starting_price"] * 1.2:

                        # place BACK bet on the favourite

                        runner_context = self.get_runner_context(

                            market.market_id, fav.selection_id, fav.handicap

                        )

                        if runner_context.live_trade_count == 0:

                            trade = Trade(

                                market.market_id, fav.selection_id, fav.handicap, self

                            )

                            order = trade.create_order(

                                side="BACK",

                                order_type=LimitOrder(

                                    get_price(fav.ex.available_to_back, 0),

                                    size=5,

                                ),

                            )

                            market.place_order(order)

---

## 2023-05-03 16:07:48 - general channel

**Brøndby IF**

I had even started a method using the `open_date` in games that pass through the `in_play_only=True` filter (It's useful because the last `open_date` saved record is exactly the moment the ball started rolling in the game), and when the `_datetime_created - open_date` was greater than `45:00`, we would be in additional time, and when the `Half Time` market closed, that would mean the first half was end.



However, this way I would not be able to know when the second half started, limiting it to only the first half. ((And this method has several flaws in it, it was just an attempt to find an option))

---

## 2023-04-28 03:52:55 - general channel

**Unknown**

Hi, I am comparing the live total matched by runner_id from the polling API vs the traded volume (tv) from streaming (using flumine market_recorder). The total matched from 'Polling / Matched' is the same as what was displayed on Betfair website for the race. However, the totals from the streaming are different.

Any ideas what assumption I am getting wrong that would explain the discrepancies or what I should check?  Thanks

---

## 2023-04-23 13:21:41 - general channel

**Unknown**

Hi, here is the log:



2023-04-22 08:26:33 |  Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failed: live_trade_count (1) &gt;= max_live_trade_count (1)



I manually/visually checked the market (on betfair website). There were no orders from this strategy (as they all had been cancelled by the code).

---

## 2023-04-23 10:04:31 - general channel

**John**

Hi all and [@U4H19D1D2](@U4H19D1D2), I was trying to use `max_live_trade_count` (=1). I notice that even the existing orders have been cancelled by the code in `process_orders()` after 2 seconds for all runners, my code still thinks there are live orders executable for all runners (so below line is true):



[https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/strategy/strategy.py#L199](https://github.com/betcode-org/flumine/blob/47af44b2173cf9d6b32fd9daef22dae510adea3f/flumine/strategy/strategy.py#L199)



hence my code doesn't place any new bets, though I'd expect the code to start placing new bets or those runners when their bets had been cancelled. Please could anyone give me a pointer if I am missing something very obvious?



Many thanks!

---

## 2023-04-23 03:47:08 - issues channel

**Andy B**

Hi All, I am clearly missing something here as I am not gettting the strategy name passed through in my flumine strategies.  I thought that it used the name value e.g.

loc_strategy = FlatLOCModel(

    market_filter=streaming_market_filter(

        event_type_ids=["4339"], # Greyhound Racing

        country_codes=["AU"], # Australian Markets

        market_types=["WIN"], # Win Markets

    ),

    name = "LOC_1.07_3_3.80",

    max_order_exposure= 50, # Max bet sizes of $50

    max_trade_count=1, # Max of trade/bet attempt per selection

    max_live_trade_count=1, # Max of 1 unmatched Bet per selection

)



However, this just passes my machine name as the customer_strategy_ref.  Where should I be adding the customer_strategy_ref so that I can use it to filter my results?

---

## 2023-04-21 07:32:29 - general channel

**EJono**

the rest of the arguments passed to the strategy class look like the following:



```strategy_one = StrategyOne(

    max_order_exposure=30,

    max_live_trade_count=20,

    max_selection_exposure=30,

    market_filter=streaming_market_filter(

        event_type_ids=["1"],

        country_codes=country_codes,

        market_types=["MATCH_ODDS"],

    ),

    name='instance_1',

    context={

        "event_type_ids": ["1"],

        "market_types": "MATCH_ODDS",

        'number_of_markets_for_placement': eval(number_of_markets_to_place_on),

        "cancel_all_bets": eval(cancel_all_bets),

        'lower_limit': 0.0,

        'upper_limit': 0.33333333,

        'max_liability': 11,

        'strategy_type': 'first_to_score',

        'market_ids_for_placement': []

    }

)```

Strategy 2 and 3 are exactly the same apart from variations in "name", "lower_limit", and "upper_limit"

---

## 2023-04-18 02:12:47 - general channel

**Andrey Luiz Malheiros**

Hey guys, my name is Andrey and I'm starting to use Flumine.

I want to run my strategies on a Django server and be able to activate and deactivate them using Flumine's API.

Currently, I'm using Django and Celery to start an asynchronous task when I call an API. Once this task initializes, Flumine starts running for a strategy that is passed in the body of the API request. However, the issue with this approach is that every time I call the API to start a strategy, I need to run another Flumine instance. I'm not sure if this is the best approach for my situation. Does anyone have a similar infrastructure or any ideas on how I can handle running my strategies on a server?

---

## 2023-04-17 18:49:46 - issues channel

**eugene brown**

This is with python 3.9.16 and betfairlightweight==2.17.1



The login is successful within a docker container locally however I am deploying this container within a custom training job on google cloud platform and that is where I'm experiencing this error.

---

## 2023-04-17 10:10:02 - random channel

**Paul**

I’m stacked at the moment but will get some sorted next month perhaps. There is also the AWS event this week I posted a while back, could be if interest to some of you

---

## 2023-04-11 12:11:18 - general channel

**Jonjonjon**

Are there rumours of something changing to UK gambling laws?

---

## 2023-04-09 09:07:57 - general channel

**Derek C**

fyi, as I'm a lazy person I use this one: [https://aws.amazon.com/s3/storage-classes/intelligent-tiering/](https://aws.amazon.com/s3/storage-classes/intelligent-tiering/)

---

## 2023-04-07 08:46:20 - general channel

**Tom**

I had to make a list and put market_ids=market_ids into the filter.



But clunky but at least it works



I already process the data that gives me the list of Market ID's I want in the BaseStrategy class init, so might just have to recreate it until I get the live access

---

## 2023-04-04 00:51:03 - issues channel

**Andy B**

Thanks Lee, I have done a bit of logging to trap the error, and it does appear at times that the marketID is non-existent even though I the market is live, but that really doesn't make sense.  I see the issue in both live scripts and back-testing of markets through flumine, so I guess I'll just have to find a market that doesn't work and step through it again.  I can check to see if the market consistently errors or if it is random, but it feels like a strange issue.

---

## 2023-03-28 00:25:19 - general channel

**RDr**

Hi, I am only getting started and found some codes to get stream racing data using befairlightweight.

It sounds like Flumine could facilitate the process and run different market_ids streams in parallel/concurrently and sequentially.

Could someone assist and let me know what the Flumine codes would be to run and get/save different streaming data in parallel (different venues) and also in sequence (same venue for different races once the previous race market closes) ?

Thanks!



#===================================================

# Sample section of codes for betfairlightweight version (only 1 stream worked for me, not sure how to adapt the codes or use existing Flumine functions to record multiple markets in  parallel or sequentially)



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener)



# create filters (AU WIN racing)

# market_filter = streaming_market_filter(

#    event_type_ids=["7"], country_codes=["AU"], market_types=["WIN"],

# )

my_market_ids = [..., ...., ...]

market_filter = filters.market_filter(market_ids=my_market_ids)



market_data_filter = streaming_market_data_filter(

    fields=["EX_MARKET_DEF", "EX_LTP", "EX_BEST_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "SP_TRADED", "SP_PROJECTED"], ladder_levels=3

)



# subscribe

streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=1000,  # send update every 1000ms

)



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()



# Open a file for the market stream data

filename = f"{market_id}.txt"

filepath = os.path.join("data", filename)

with open(filepath, "w") as f:



    # check for updates in output queue

    while True:

        market_books = output_queue.get()

        print(market_books)



        for market_book in market_books:

            # print(

            #     market_book,

            #     market_book.streaming_unique_id,  # unique id of stream (returned from subscribe request)

            #     market_book.streaming_update,  # json update received

            #     market_book.market_definition,  # streaming definition, similar to catalogue request

            #     market_book.publish_time,  # betfair publish time of update

            # )



            # write data to file

            f.write(str(market_book.streaming_unique_id) + '\n')

            f.write(str(market_book.streaming_update) + '\n')

            f.write(str(market_book.market_definition) + '\n')

            f.write(str(market_book.publish_time) + '\n')

---

## 2023-03-26 21:06:29 - general channel

**Peter**

You can certainly do that with Flumine. It will deliver the near-real-time state of the market as it updates from which you can calculate your analytics. But how you would compute them would depend on the specific analytics you want and possibly what you want to use them from.

---

## 2023-03-25 11:01:41 - issues channel

**Lee**

the default connection limit for a live key is 10 and the delayed key has a limit of 2 which is the error you are getting so might be worth double checking

---

## 2023-03-24 07:50:25 - general channel

**Herugrim**

Information from the Best Practice page:



• Logins sessions last up to 12 hours by default and you can use [https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909|Keep Alive](https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909|Keep Alive) to extend the session beyond the stated session expiry time. 

• A single session can be used across multiple API calls/threads simultaneously.  *Please note:*  *you don't need to make a new login request for every API call*.

• You should ensure that you handle the *INVALID_SESSION_TOKEN* error within your code by creating a new session token via the [https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909|API login](https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909|API login) method.

• If[https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909#Login&amp;SessionManagement-LoginRequestLimits| login limits ](https://docs.developer.betfair.com/pages/viewpage.action?pageId=3834909#Login&amp;SessionManagement-LoginRequestLimits| login limits )are exceeded, you'll be automatically prevented from making further login requests for a period of 20 minutes. During this time all existing sessions will remain valid.



---

## 2023-03-23 22:11:46 - general channel

**Jesus Perdomo**

Hey guys, so glad I found this library, very cool!



Just trying to figure out what would be best practice for my use (I read the official betfair stuff but doesn't really answer the question).

My understanding is that there is a limit to how many login requests the API can receive before soft-banning. The 'best practices' page states that a single login session can carry out multiple API calls as long as it's alive. I'm trying to make an app that gets some information from the API, hopefully in the future extend it to allow betting through the app as well. The app will make some requests to find out if a particular markets exists (by searching for a team name), and what the odds are at the time of request. The app will be using non-interactive logon as it is hands-off



Would best practice be as follows? Logon --&gt; Get Data --&gt; Logout?

Or should I just keep the connection open for the next API call? - I'd say the time in-between calls can be variable, sometimes in quick succession (seconds to minutes), but most of the time it would be in the minutes to hours range.



If the best thing to do is to keep the connection open, does the API send an exception code or some other response if a session is already open?

I'm just trying to figure out how to code the logic: If already logged-in, send call, otherwise, attempt login and get a session id

---

## 2023-03-23 12:36:43 - general channel

**Peter**

It's a process that runs independently on an EC2 instance. But that's mainly because of how I constructed it originally. It could just as well have been contained by a logging control as the market was closed, with pros and cons each way. E.g. logging control would avoid the need to re-visit the files, but re-visiting the files confirms that they made it to S3 and helps handle those with closing wrinkles (e.g, multiple closures or none recorded).

---

## 2023-03-20 18:21:29 - strategies channel

**George**

do you find that there is enough in flumine for a market making strategy that relies a lot on tracking existing orders and liabilities, both live and executed? or did you have to add to flumine to get what you needed in that respect?

---

## 2023-03-17 11:06:50 - issues channel

**Andrew**

Connected to pydev debugger (build 191.7479.30)

{"asctime": "2023-03-17 10:56:16,147", "levelname": "INFO", "message": "Adding trading control ORDER_VALIDATION"}

{"asctime": "2023-03-17 10:56:16,147", "levelname": "INFO", "message": "Adding trading control MARKET_VALIDATION"}

{"asctime": "2023-03-17 10:56:16,148", "levelname": "INFO", "message": "Adding trading control STRATEGY_EXPOSURE"}

{"asctime": "2023-03-17 10:56:16,148", "levelname": "INFO", "message": "Adding strategy back_strat_42"}

{"asctime": "2023-03-17 10:56:16,148", "levelname": "INFO", "message": "Creating new &lt;class 'flumine.streams.marketstream.MarketStream'&gt; (2000) for strategy back_strat_42"}

{"asctime": "2023-03-17 10:56:16,148", "levelname": "INFO", "message": "Starting flumine", "clients": {"username": "", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": null, "transaction_count_total": null, "trading_controls": [], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 0, "open_market_count": 0}, "streams": ["&lt;OrderStream(OrderStream, initial daemon)&gt;", "&lt;MarketStream(MarketStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 18028)&gt;", "&lt;WriterThread(pydevd.Writer, started daemon 11888)&gt;", "&lt;ReaderThread(pydevd.Reader, started daemon 14476)&gt;", "&lt;PyDBCommandThread(pydevd.CommandThread, started daemon 8612)&gt;"]}

{"asctime": "2023-03-17 10:56:16,293", "levelname": "ERROR", "message": "BetfairClient `login` error", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\clients\\betfairclient.py\", line 29, in login\n    return self.betting_client.login()\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\betfairlightweight\\endpoints\\login.py\", line 31, in __call__\n    self.url, session=session\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\betfairlightweight\\endpoints\\login.py\", line 57, in request\n    check_status_code(response)\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\betfairlightweight\\utils.py\", line 34, in check_status_code\n    raise StatusCodeError(response.status_code)\nbetfairlightweight.exceptions.StatusCodeError: Status code error: 500", "client": "APIClient", "trading_function": "login", "response": "Status code error: 500"}

{"asctime": "2023-03-17 10:56:16,509", "levelname": "INFO", "message": "Adding worker keep_alive"}

{"asctime": "2023-03-17 10:56:16,509", "levelname": "INFO", "message": "Adding worker poll_market_catalogue"}

{"asctime": "2023-03-17 10:56:16,510", "levelname": "INFO", "message": "Adding worker poll_account_balance"}

{"asctime": "2023-03-17 10:56:16,510", "levelname": "INFO", "message": "Adding worker poll_market_closure"}

{"asctime": "2023-03-17 10:56:16,512", "levelname": "INFO", "message": "BackgroundWorker keep_alive starting", "worker_name": "keep_alive", "function": "&lt;function keep_alive at 0x000001823546B268&gt;", "context": {}, "start_delay": 0, "interval": 1200, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-03-17 10:56:16,513", "levelname": "ERROR", "message": "Error in BackgroundWorker keep_alive: 'Flumine' object has no attribute 'clients'", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\worker.py\", line 65, in run\n    self.context, self.flumine, *self.func_args, **self.func_kwargs\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\worker.py\", line 97, in keep_alive\n    for client in flumine.clients:\nAttributeError: 'Flumine' object has no attribute 'clients'", "worker_name": "keep_alive", "function": "&lt;function keep_alive at 0x000001823546B268&gt;", "context": {}}

{"asctime": "2023-03-17 10:56:16,514", "levelname": "INFO", "message": "BackgroundWorker poll_market_catalogue starting", "worker_name": "poll_market_catalogue", "function": "&lt;function poll_market_catalogue at 0x000001823546B378&gt;", "context": {}, "start_delay": 10, "interval": 60, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-03-17 10:56:16,516", "levelname": "INFO", "message": "BackgroundWorker poll_account_balance starting", "worker_name": "poll_account_balance", "function": "&lt;function poll_account_balance at 0x000001823546B400&gt;", "context": {}, "start_delay": 10, "interval": 120, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-03-17 10:56:16,518", "levelname": "INFO", "message": "BackgroundWorker poll_market_closure starting", "worker_name": "poll_market_closure", "function": "&lt;function poll_market_closure at 0x000001823546B488&gt;", "context": {}, "start_delay": 10, "interval": 60, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-03-17 10:56:16,520", "levelname": "INFO", "message": "Starting streams.."}

{"asctime": "2023-03-17 10:56:16,521", "levelname": "INFO", "message": "Starting OrderStream 1000", "stream_id": 1000, "customer_strategy_refs": "LAPTOP-SDCL040E", "conflate_ms": null, "streaming_timeout": 0.25, "client_username": ""}

{"asctime": "2023-03-17 10:56:16,522", "levelname": "INFO", "message": "Starting output_thread (OrderStream 1000)"}

{"asctime": "2023-03-17 10:56:16,523", "levelname": "INFO", "message": "[Register: 1001]: orderSubscription"}

{"asctime": "2023-03-17 10:56:16,524", "levelname": "INFO", "message": "[OrderStream: 1001]: \"OrderStream\" created"}

{"asctime": "2023-03-17 10:56:16,590", "levelname": "INFO", "message": "[OrderStream: 1001]: connection_id: 209-170323105616-5848216"}

{"asctime": "2023-03-17 10:56:16,622", "levelname": "INFO", "message": "[OrderStream: 1002]: SUCCESS (1 connections available)"}

{"asctime": "2023-03-17 10:56:16,626", "levelname": "INFO", "message": "[OrderStream: 1001]: SUCCESS (1 connections available)"}

{"asctime": "2023-03-17 10:56:16,658", "levelname": "INFO", "message": "[OrderStream: 1001]: 0 oc added"}

{"asctime": "2023-03-17 10:56:16,780", "levelname": "INFO", "message": "Starting MarketStream 2000", "stream_id": 2000, "market_filter": {"eventTypeIds": ["7"], "marketTypes": ["WIN"], "countryCodes": ["GB", "IE"]}, "market_data_filter": {"fields": ["EX_BEST_OFFERS", "EX_LTP", "EX_MARKET_DEF"], "ladderLevels": 1}, "conflate_ms": 1000, "streaming_timeout": null}

{"asctime": "2023-03-17 10:56:16,781", "levelname": "INFO", "message": "Starting output_thread (MarketStream 2000)"}

{"asctime": "2023-03-17 10:56:18,785", "levelname": "INFO", "message": "Starting MarketStream 2000", "stream_id": 2000, "market_filter": {"eventTypeIds": ["7"], "marketTypes": ["WIN"], "countryCodes": ["GB", "IE"]}, "market_data_filter": {"fields": ["EX_BEST_OFFERS", "EX_LTP", "EX_MARKET_DEF"], "ladderLevels": 1}, "conflate_ms": 1000, "streaming_timeout": null}

{"asctime": "2023-03-17 10:56:20,794", "levelname": "INFO", "message": "Starting MarketStream 2000", "stream_id": 2000, "market_filter": {"eventTypeIds": ["7"], "marketTypes": ["WIN"], "countryCodes": ["GB", "IE"]}, "market_data_filter": {"fields": ["EX_BEST_OFFERS", "EX_LTP", "EX_MARKET_DEF"], "ladderLevels": 1}, "conflate_ms": 1000, "streaming_timeout": null}

{"asctime": "2023-03-17 10:56:24,811", "levelname": "INFO", "message": "Starting MarketStream 2000", "stream_id": 2000, "market_filter": {"eventTypeIds": ["7"], "marketTypes": ["WIN"], "countryCodes": ["GB", "IE"]}, "market_data_filter": {"fields": ["EX_BEST_OFFERS", "EX_LTP", "EX_MARKET_DEF"], "ladderLevels": 1}, "conflate_ms": 1000, "streaming_timeout": null}

{"asctime": "2023-03-17 10:56:26,530", "levelname": "ERROR", "message": "Error in BackgroundWorker poll_market_catalogue: 'Flumine' object has no attribute 'clients'", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\worker.py\", line 65, in run\n    self.context, self.flumine, *self.func_args, **self.func_kwargs\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\worker.py\", line 113, in poll_market_catalogue\n    client = flumine.clients.get_betfair_default()\nAttributeError: 'Flumine' object has no attribute 'clients'", "worker_name": "poll_market_catalogue", "function": "&lt;function poll_market_catalogue at 0x000001823546B378&gt;", "context": {}}

{"asctime": "2023-03-17 10:56:26,531", "levelname": "ERROR", "message": "Error in BackgroundWorker poll_account_balance: 'Flumine' object has no attribute 'clients'", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\worker.py\", line 65, in run\n    self.context, self.flumine, *self.func_args, **self.func_kwargs\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\worker.py\", line 155, in poll_account_balance\n    for client in flumine.clients:\nAttributeError: 'Flumine' object has no attribute 'clients'", "worker_name": "poll_account_balance", "function": "&lt;function poll_account_balance at 0x000001823546B400&gt;", "context": {}}

{"asctime": "2023-03-17 10:56:26,532", "levelname": "ERROR", "message": "Error in BackgroundWorker poll_market_closure: 'Flumine' object has no attribute 'clients'", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\worker.py\", line 65, in run\n    self.context, self.flumine, *self.func_args, **self.func_kwargs\n  File \"C:\\Users\\andre\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\flumine\\worker.py\", line 166, in poll_market_closure\n    for client in flumine.clients:\nAttributeError: 'Flumine' object has no attribute 'clients'", "worker_name": "poll_market_closure", "function": "&lt;function poll_market_closure at 0x000001823546B488&gt;", "context": {}}

{"asctime": "2023-03-17 10:56:32,822", "levelname": "INFO", "message": "Starting MarketStream 2000", "stream_id": 2000, "market_filter": {"eventTypeIds": ["7"], "marketTypes": ["WIN"], "countryCodes": ["GB", "IE"]}, "market_data_filter": {"fields": ["EX_BEST_OFFERS", "EX_LTP", "EX_MARKET_DEF"], "ladderLevels": 1}, "conflate_ms": 1000, "streaming_timeout": null}

{"asctime": "2023-03-17 10:56:48,827", "levelname": "INFO", "message": "Starting MarketStream 2000", "stream_id": 2000, "market_filter": {"eventTypeIds": ["7"], "marketTypes": ["WIN"], "countryCodes": ["GB", "IE"]}, "market_data_filter": {"fields": ["EX_BEST_OFFERS", "EX_LTP", "EX_MARKET_DEF"], "ladderLevels": 1}, "conflate_ms": 1000, "streaming_timeout": null}

---

## 2023-03-13 20:11:59 - general channel

**birchy**

[https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html|https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html|https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)

---

## 2023-03-13 20:10:03 - general channel

**birchy**

[@U027P3N2WMQ](@U027P3N2WMQ) I'm using the S3MarketRecorder example which uses boto3 to push to S3. The only extra setup from your end is to create a `.aws` folder in your home directory for your credentials. 

[https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html|https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html|https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)

---

## 2023-03-13 19:29:26 - general channel

**Michael**

Birchy are you writing to a server on the lightsail instance or doing it another way? I’m currently writing data to mysql on lightsail but its for scraping so not sure how it will react when I’m writing data every second for a market recorder

---

## 2023-03-12 13:41:44 - issues channel

**Mo**

Live data? Self recorded? Betfair historic?

---

## 2023-03-12 13:41:24 - general channel

**birchy**

Regarding Lightsail, I use these instances as they're far easier to setup than EC2. You can add extra storage (see screenshot), however I just run the marketrecorder on a low cost instance and push to S3.

---

## 2023-03-12 12:01:34 - issues channel

**foxwood**

Tracing back some oddball issues from the other day I discovered missing data for some markets. The logs showed connection issues with number of markets when BF published the next days GH races. I had 188 markets in the cache (rest of day + the next day). It looks like BF closed the connection for some reason (?) so Flumine automatically opened another stream but the number of markets ticked over the 200 limit. This kept repeating until 5 more markets had been closed and the count got down to 200. Any workaround for this ?



```{"asctime": "2023-03-09 19:30:47,754", "levelname": "INFO", "message": "[MarketStream: 1002] 1.211067730 removed, 188 markets in cache"}



{"asctime": "2023-03-09 19:32:40,581", "levelname": "INFO", "message": "Created marketCatalogue for 1.211115814", "market_id": ...}

{"asctime": "2023-03-09 19:32:40,581", "levelname": "INFO", "message": "Created marketCatalogue for 1.211115887", "market_id": ...}



{"asctime": "2023-03-09 19:37:27,776", "levelname": "ERROR", "message": "DataStream 1002 run error", "exc_info": "Traceback (most recent call last):\n  raise SocketError(\nbetfairlightweight.exceptions.SocketError: [Connect: 1003]: Connection closed by server"}

{"asctime": "2023-03-09 19:37:29,784", "levelname": "INFO", "message": "Starting DataStream 1002", "stream_id": 1002, "market_filter": {"eventTypeIds": ["4339"], "marketTypes": ["WIN"], "countryCodes": ["GB"]}, "market_data_filter": {"fields": ["EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP", "EX_MARKET_DEF", "SP_TRADED", "SP_PROJECTED"]}, "conflate_ms": null}

{"asctime": "2023-03-09 19:37:29,784", "levelname": "INFO", "message": "[Register: 1003]: Unique id updated on listener and stream"}

{"asctime": "2023-03-09 19:37:29,831", "levelname": "INFO", "message": "[FlumineStream: 1003]: connection_id: 208-090323193730-4882826"}

{"asctime": "2023-03-09 19:37:29,862", "levelname": "INFO", "message": "[FlumineStream: 1004]: SUCCESS (5 connections available)"}

{"asctime": "2023-03-09 19:37:29,894", "levelname": "ERROR", "message": "[FlumineStream: 1003]: SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 205 markets whereas max allowed number was: 200"}

{"asctime": "2023-03-09 19:37:29,894", "levelname": "INFO", "message": "[FlumineStream: 1003]: FAILURE (5 connections available)"}

{"asctime": "2023-03-09 19:37:45,417", "levelname": "ERROR", "message": "[FlumineStream: None]: TIMEOUT: Connection is not subscribed and is idle: 15000 ms"}

{"asctime": "2023-03-09 19:37:45,417", "levelname": "ERROR", "message": "DataStream 1003 run error", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\Administrator\\PycharmProjects\\betfair1\\venv\\lib\\site-packages\\flumine\\streams\\datastream.py\", line 166, in run\n        raise ListenerError(self.listener.connection_id, received_data)\nbetfairlightweight.exceptions.ListenerError: connection_id: 208-090323193730-4882826, data: {\"op\":\"status\",\"statusCode\":\"FAILURE\",\"errorCode\":\"TIMEOUT\",\"errorMessage\":\"Connection is not subscribed and is idle: 15000 ms\",\"connectionClosed\":true,\"connectionId\":\"208-090323193730-4882826\"}"}```

---

## 2023-03-11 18:10:25 - issues channel

**Paul**

I have a newb-style question. I've used bflw a _lot_ in recent years and now moving to flumine (how about them API charges, huh?), and one of my strategies needs to know for each selection both the matched/`OrderStatus.EXECUTED` and live/`OrderStatus.EXECUTABLE` orders broken down by side of market (back/lay).  There's nothing I spotted in docs or examples about best way to access the blotter directly, just the middleware to enable filling out `OrderStatus.EXECUTED` on restart. It seems I can either build this info myself into new objects for each runner on `process_orders` and then access that when processing the market book, (which seems expensive and weird), or by iterating over `RunnerContext.trades` when inside `process_market_book` - this also seems a little weird. Can somebody ELI5 what I'm missing or advise on cheapest route?

---

## 2023-03-06 14:38:07 - random channel

**PeterLe**

TPD were scheduled to do a workshop  today at Wolvs :

-------  TPD Wrote : --------------

Good Morning, in case you are not aware we are running a Workshop at Wolverhampton racecourse on Monday 6th March. The broad agenda being; arrive late afternoon, meet the TPD team, a few presentations re various elements of the service.



The first race is at 5:30pm and the final race at 8:30pm. We plan to do some live betting in the races.



This is a totally free event, there are a limited number of places remaining, if you would like a place reserving please drop an email to [mailto:support@tpd.zone|support@tpd.zone](mailto:support@tpd.zone|support@tpd.zone).

--------------------

There is an email address there DC if it helps?

---

## 2023-03-05 09:19:19 - general channel

**Jonjonjon**

Are you doing it on live data with Pandas? If so, Pandas is too slow if you are updating it with every price update. More primitive types should be fastwr

---

## 2023-03-01 18:03:33 - random channel

**Paul**

This free event is happening in my office in April and might touch on subjects of interest here. I know one of the organisers works with some bookmaker trading desks and I’m hoping to get him or his customers to leak some alpha in a nearby bar afterwards. [https://awsdeepdivesdays.splashthat.com|https://awsdeepdivesdays.splashthat.com](https://awsdeepdivesdays.splashthat.com|https://awsdeepdivesdays.splashthat.com)

---

## 2023-02-27 13:18:30 - issues channel

**foxwood**

There's a level 2 in Dublin that might help [http://ntp.maths.tcd.ie|ntp.maths.tcd.ie](http://ntp.maths.tcd.ie|ntp.maths.tcd.ie) details at [https://support.ntp.org/Servers/PublicTimeServer000073](https://support.ntp.org/Servers/PublicTimeServer000073)

---

## 2023-02-27 11:40:50 - issues channel

**thambie1**

Using AWS?

---

## 2023-02-22 09:45:55 - general channel

**jp**

Anyone got experience with live/inplay soccer data from [https://www.thesports.com/](https://www.thesports.com/)? In particular I am interested in latency, quality of data and pricing.

---

## 2023-02-21 11:40:58 - general channel

**Ke**

Does anyone use 3rd party api to incorporate live score in the strategy? Any recommendations?

---

## 2023-02-20 21:19:00 - general channel

**AI Trader**

*Conflate logic*

Another question guys,

For the Flumine simulation, I am trying to check wether the conflate_ms is working or not, and it seems to me that it is not.

I have added a print of the timestamps (market_book.publish_timestemp_epoch) in the *process_market_book* , and no matter what conflate_ms I use (here I am using 10_000 ms), it always receives the same updates (which seems to be the most granular possible).

Is there anything extra required to make conflate_ms work in simulation mode?



Below is how I am instantiating the FlumineStrategy.

```flumine_strategy = FlumineStrategy(

    strategy = strategy,

    client=client,

    name="bt",

    market_filter={"markets": [market_file]},

    conflate_ms=10_000,

    max_live_trade_count=1000,

    max_selection_exposure=1000,

    max_order_exposure=1000

)```



---

## 2023-02-18 16:02:11 - general channel

**SrFabio**

Hey guys what live data feed would you guys recommend for football games? I'm developing an hybrid trading approach with some automated components and it's very sensitive to delay, I'm not sure if you guys have tried anything I just need to know the events (goals, fouls, corners etc..) as they happen with the least delay possible

---

## 2023-02-12 16:59:16 - general channel

**AI Trader**

Hi [@U4H19D1D2](@U4H19D1D2), thanks a lot!



I am backtesting the strategy using the Flumine Backtester (along with data collected from Flumine data recorder).

My my market_catalogue is never available (market.market_catalogue is always None).



I know it stores data in two different folders:

• marketCatalogue/

• streaming/

I am only giving passing to the backtester the path to the streaming file (in the market_filter argument, see code below).  My understanding is that the streaming data also contains market_catalogue messages from the exchange, so that shouldn't be a problem. Am I missing something? Any help will be appreciated.



```strategy = MakerStrategy(

    client=client,

    name="bt",

    market_filter={"markets": [path_to_streaming_file]},

    conflate_ms=100,

    max_live_trade_count=1000,

    max_selection_exposure=1000,

    max_order_exposure=1000

)```

---

## 2023-02-12 06:43:30 - general channel

**liam**

It’s an order which is ‘live’ / ‘executable’ in the market for all strategies. Any reason you don’t use the helper functions for exposure / runner orders?

---

## 2023-02-12 04:25:18 - issues channel

**Unknown**

Hmm now have the live api key, still problems using the api, unless I’m mis-interpreting the documentation. Essentially want to know the amount traded at each order level, not sure why orders and traded are coming back empty, regardless of the market. In this example I’ve picked a market with &gt;$50k traded, so should definitely have data for me :thinking_face:

---

## 2023-02-12 00:47:07 - issues channel

**Andy B**

Hi All,



I am trying to get to the bottom of Betfair connection issues which have cropped up in the last week.  I have a feeling it is because of the way I have implemented by strategies, because I am running 6 separate scripts which all do repeated logins to Betfair.  The error that has been randomly appearing this week is:



Traceback (most recent call last):

  File "C:\Python\lib\site-packages\urllib3\connectionpool.py", line 386, in _make_request

    self._validate_conn(conn)

  File "C:\Python\lib\site-packages\urllib3\connectionpool.py", line 1042, in _validate_conn

    conn.connect()

  File "C:\Python\lib\site-packages\urllib3\connection.py", line 414, in connect

    self.sock = ssl_wrap_socket(

  File "C:\Python\lib\site-packages\urllib3\util\ssl_.py", line 449, in ssl_wrap_socket

    ssl_sock = _ssl_wrap_socket_impl(

  File "C:\Python\lib\site-packages\urllib3\util\ssl_.py", line 493, in _ssl_wrap_socket_impl

    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)

  File "C:\Python\lib\ssl.py", line 500, in wrap_socket

    return self.sslsocket_class._create(

  File "C:\Python\lib\ssl.py", line 1040, in _create

    self.do_handshake()

  File "C:\Python\lib\ssl.py", line 1309, in do_handshake

    self._sslobj.do_handshake()

socket.timeout: _ssl.c:1105: The handshake operation timed out



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "C:\Python\lib\site-packages\requests\adapters.py", line 489, in send

    resp = conn.urlopen(

  File "C:\Python\lib\site-packages\urllib3\connectionpool.py", line 787, in urlopen

    retries = retries.increment(

  File "C:\Python\lib\site-packages\urllib3\util\retry.py", line 550, in increment

    raise six.reraise(type(error), error, _stacktrace)

  File "C:\Python\lib\site-packages\urllib3\packages\six.py", line 770, in reraise

    raise value

  File "C:\Python\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen

    httplib_response = self._make_request(

  File "C:\Python\lib\site-packages\urllib3\connectionpool.py", line 389, in _make_request

    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)

  File "C:\Python\lib\site-packages\urllib3\connectionpool.py", line 340, in _raise_timeout

    raise ReadTimeoutError(

urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=3.05)



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "C:\Python\lib\site-packages\betfairlightweight\endpoints\baseendpoint.py", line 36, in request

    response = [http://session.post|session.post](http://session.post|session.post)(

  File "C:\Python\lib\site-packages\requests\api.py", line 115, in post

    return request("post", url, data=data, json=json, **kwargs)

  File "C:\Python\lib\site-packages\requests\api.py", line 59, in request

    return session.request(method=method, url=url, **kwargs)

  File "C:\Python\lib\site-packages\requests\sessions.py", line 587, in request

    resp = self.send(prep, **send_kwargs)

  File "C:\Python\lib\site-packages\requests\sessions.py", line 701, in send

    r = adapter.send(request, **kwargs)

  File "C:\Python\lib\site-packages\requests\adapters.py", line 578, in send

    raise ReadTimeout(e, request=request)

requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=3.05)



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "C:\Users\elmob\OneDrive\Punting\Scripts\Betfair\Model6.py", line 649, in &lt;module&gt;

    GetBFRunners()

  File "C:\Users\elmob\OneDrive\Punting\Scripts\Betfair\Model6.py", line 164, in GetBFRunners

    greyhounds_events = trading.betting.list_events(

  File "C:\Python\lib\site-packages\betfairlightweight\endpoints\betting.py", line 123, in list_events

    (response, response_json, elapsed_time) = self.request(method, params, session)

  File "C:\Python\lib\site-packages\betfairlightweight\endpoints\baseendpoint.py", line 45, in request

    raise APIError(None, method, params, e)

betfairlightweight.exceptions.APIError: SportsAPING/v1.0/listEvents

Params: {'filter': {'eventTypeIds': [4339], 'marketCountries': ['AU', 'NZ'], 'marketStartTime': {'to': '2023-02-14T00:24:32Z'}}}

Exception: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=3.05)



To clarify how my scripts run, they all begin by creating the betfair login using:

trading = betfairlightweight.APIClient(username=my_username,

                                    password=my_password,

                                    app_key=my_app_key,

                                    certs=certs_path)



trading.login()



I then get my list of races and runners from Betfair using a custom function, and I suspect this is where it is falling over every time.  I've got a hunch that when I call the BFLW functions within betting.py, the fact that I am calling the same methods multiple times across the 6 scripts is causing it to timeout because it is too many attempts or too much data requested from Betfair.



Does this sound logical or is there something else in the error code that points to the issue?  If it is the problem, what is the best method of implementing my code to manage multiple different strategies with their own independent methods in my code?



Cheers

---

## 2023-02-08 23:32:23 - general channel

**Unknown**

```import math

from collections import OrderedDict

from flumine import BaseStrategy

from flumine.order.trade import Trade

from flumine.order.order import OrderStatus

from flumine.order.ordertype import LimitOrder

from flumine.utils import get_price

import numpy as np





class LowestLayer(BaseStrategy):

    def __init__(self, *args, **kwargs):

        super().__init__(*args, **kwargs)

        self.max_order_exposure = 100



    def check_market_book(self, market, market_book):

        if market_book.status == "OPEN" and not market_book.inplay:

            return True



    def process_market_book(self, market, market_book):



        # count market matched trades

        market_matched_trades_count = sum(

            [self.get_runner_context(market.market_id, r.selection_id, r.handicap).trade_count -

             self.get_runner_context(market.market_id, r.selection_id, r.handicap).live_trade_count for r

             in market_book.runners])



        # get lowest/highest priced runner (depends on sort settings)

        prices = [

            (r.selection_id, get_price(r.ex.available_to_back, 0))

            for r in market_book.runners if r.status == "ACTIVE"



        ]



        if not prices:

            return



        if market.seconds_to_start &gt; 250:

            return



        prices.sort(key=lambda tup: tup[1], reverse=False)

        selection_id = prices[0][0]



        if prices[0][1] &gt; 12:

            return



        # calculate market under/over round for later analysis

        underround = _calculate_underround(market_book.runners)



        for runner in market_book.runners:



            runner_context = self.get_runner_context(

                market.market_id, runner.selection_id, runner.handicap

            )



            if runner.selection_id == selection_id:



                '''Place initial bet'''

                if market_matched_trades_count == 0 and runner_context.live_trade_count == 0 and 250 &gt; market.seconds_to_start &gt; 0:

                    self.create_trade_create_order(market, market_book, runner, underround, self.context['stake'],

                                                   back_lay='LAY', market_depth=0)

                    print('Entry Bet')



                '''Code only here for investigations'''

                if market.seconds_to_start in [104.043, 103.929]:

                    print('&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;')

                    print(f'SECONDS TO START = {market.seconds_to_start}')

                    print(f'strategy_selection_orders{market.blotter.strategy_selection_orders(self, selection_id, runner.handicap)}')

                    print(f'market.blotter.orders = {[o for o in market.blotter]}')

                    print(f'market status = {market.status}')

                    print(f'runner status = {runner.status}')

                    print( f'selection exposure = {market.blotter.selection_exposure(strategy=self, lookup=(runner.handicap, runner.selection_id))}')

                    print(f'stats = {market.blotter.get_exposures(strategy=self, lookup=(runner.handicap, runner.selection_id))}')

                    print(f'number of active runners = {len([r for r in market_book.runners if runner.status == "ACTIVE"])}')



                '''Try to green up on runner - limit order'''

                if runner_context.trade_count == 1 and runner_context.live_trade_count == 0 \

                        and 30 &lt; market.seconds_to_start &lt;= 60:

                    stake, price, backorlay = self.green_all(market, runner, market_depth=0)

                    self.create_trade_create_order(market, market_book, runner, underround,

                                                   stake=stake, back_lay=backorlay, set_price=price)



                '''If green up was unsuccessful then cancel bets'''

                if runner_context.trade_count &gt;= 1 and runner_context.live_trade_count &gt; 0 and market.seconds_to_start &lt; 30:

                    executable_orders = [o for o in

                                         market.blotter.strategy_selection_orders(

                                             self, selection_id, runner.handicap) if o.status.value == 'Executable']



                    if len(executable_orders) &gt; 0:

                        for order in executable_orders:

                            market.cancel_order(order)

                            print(executable_orders)



                '''Place green up at market price

                Will do this only if matched profit if lose &gt; £1 difference that matched profit if win'''

                if market.seconds_to_start &lt; 25:

                    exposures = market.blotter.get_exposures(strategy=self,

                                                             lookup=(runner.handicap, runner.selection_id))



                    matched_profit_if_win = math.sqrt(exposures['matched_profit_if_win'] ** 2)

                    matched_profit_if_lose = math.sqrt(exposures['matched_profit_if_lose'] ** 2)

                    if math.sqrt((matched_profit_if_win - matched_profit_if_lose) ** 2) &gt; 1:

                        stake, price, backorlay = self.green_all(market, runner, dump_position=True)

                        self.create_trade_create_order(market, market_book, runner, underround,

                                                       stake=stake, set_price=price, back_lay=backorlay)



     



    def process_orders(self, market, orders):

        # kill order if unmatched in market for greater than X seconds

        for order in orders:

            if order.status == OrderStatus.EXECUTABLE:

                if order.elapsed_seconds and order.elapsed_seconds &gt; 100:

                    market.cancel_order(order)



    def create_trade_create_order(self, market, market_book, runner, underround, stake,

                                  back_lay: str, market_depth: int = 1, set_price: float = 0.0):



        stake = float((np.round(stake, 2)))

        '''If market_order = True then a higher market_depth results 

        in a worst price. If it is False the reverse is true'''



        if set_price != 0.0:

            bet_template = [back_lay, set_price]

        elif back_lay == 'BACK':

            bet_template = ['BACK', get_price(runner.ex.available_to_lay, market_depth)]

        else:

            bet_template = ['LAY', get_price(runner.ex.available_to_back, market_depth)]



        # create trade

        trade = Trade(

            market_book.market_id,

            runner.selection_id,

            runner.handicap,

            self,

        )

        # create order

        order = trade.create_order(

            side=bet_template[0],

            order_type=LimitOrder(bet_template[1], np.round(stake, 2)),

            notes=OrderedDict(underround=round(underround, 4)),

        )

        # place order for execution

        market.place_order(order)



    def green_all(self, market, runner, market_depth: int = 0, dump_position: bool = False):

        facts = market.blotter.get_exposures(strategy=self, lookup=(runner.handicap, runner.selection_id))



        selection_exposure = market.blotter.selection_exposure(strategy=self,

                                                               lookup=(runner.handicap, runner.selection_id))

        matched_profit_if_win = facts['matched_profit_if_win']

        matched_profit_if_lose = facts['matched_profit_if_lose']

        print('green all called')

        print(matched_profit_if_win)

        print(matched_profit_if_lose)



        if matched_profit_if_win &gt; 0:

            avg_odds = (matched_profit_if_win + selection_exposure) / selection_exposure

            if dump_position is False:

                return self.calc_green(avg_odds,

                                       current_price_of_runner=get_price(runner.ex.available_to_back, market_depth),

                                       total_staked=selection_exposure) + ('LAY',)

            else:

                return self.calc_green(avg_odds,

                                       current_price_of_runner=get_price(runner.ex.available_to_lay, 0),

                                       total_staked=selection_exposure) + ('LAY',)



        if matched_profit_if_win &lt; 0:

            avg_odds = (matched_profit_if_lose + selection_exposure) / matched_profit_if_lose

            if dump_position is False:

                return self.calc_green(avg_odds,

                                       current_price_of_runner=get_price(runner.ex.available_to_lay, market_depth),

                                       total_staked=matched_profit_if_lose) + ('BACK',)

            else:

                return self.calc_green(avg_odds,

                                       current_price_of_runner=get_price(runner.ex.available_to_back, 0),

                                       total_staked=matched_profit_if_lose) + ('BACK',)



    def calc_green(self, avg_odds, current_price_of_runner, total_staked):

        green_all_stake = avg_odds / current_price_of_runner * total_staked

        return green_all_stake, current_price_of_runner





def _calculate_underround(runners: list) -&gt; float:

    return sum(

        [

            1 / get_price(r.ex.available_to_lay, 0)

            for r in runners

            if r.ex.available_to_lay

        ]

    )```

---

## 2023-02-08 17:56:57 - strategies channel

**Peter**

Oops [@UQL0QDEKA](@UQL0QDEKA). Just spotted that you have the parameters in the wrong place. They're parameters for the strategy rather than the market_filter, so your code should be:



```strategy = Sub6(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"], # was 7 for horses, 4339 for greyhounds, 15 = todays card greyhounds

        country_codes=["GB","IE"],

        market_types=["WIN"],

    ),

    max_order_exposure=5,

    max_selection_exposure=20,

    max_trade_count=5,

    max_live_trade_count=1

)```

Apologies for not picking this up first time around.

---

## 2023-02-08 09:32:06 - strategies channel

**PeterLe**

Morning. If you have a number of strategies running on the same system and the same betfair account...How do you set the exposures for each strategy independently.

I currently have this code in a single strategy:

```strategy = Sub6(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"], # was 7 for horses, 4339 for greyhounds, 15 = todays card greyhounds

        country_codes=["GB","IE"],

        market_types=["WIN"],

    ),```

Would I change it to something like this:

```strategy = Sub6(

    name="WIN",

    market_filter=betfairlightweight.filters.streaming_market_filter(

        event_type_ids=["7"], # was 7 for horses, 4339 for greyhounds, 15 = todays card greyhounds

        country_codes=["GB","IE"],

        market_types=["WIN"],

        max_order_exposure=5,

        max_selection_exposure=20,

        max_trade_count=5,

        max_live_trade_count=1

    ),```

So can the exposures be set in the strategy itself rather than in Strategy.py Sorry only basic python skills so i might be talking $%^&amp; Thanks :grinning:

---

## 2023-02-07 21:39:43 - general channel

**Trex44**

```{"asctime": "2023-02-07 21:17:57,216", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.138465931", "selection_id": 10200239, "handicap": 0, "id": "138950974772156791", "customer_order_ref": "4d8a4a5a825e8-138950974772156791", "bet_id": null, "date_time_created": "2018-01-01 13:05:50.071000", "publish_time": "2018-01-01 13:05:50.071000", "market_version": null, "async": false, "trade": {"id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 3.7, "size": 5.0, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null, "price_ladder_definition": "CLASSIC"}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 5.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Pending", "status_log": "Pending", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "0.9912", "market_notes": null, "client": "72f36d32"}

{"asctime": "2023-02-07 21:17:57,216", "levelname": "INFO", "message": "1 order packages executed in transaction", "market_id": "1.138465931", "order_packages": [{"id": "e47d1978-a72c-11ed-93b3-3fe541d536d4", "client": "&lt;flumine.clients.simulatedclient.SimulatedClient object at 0x7f61a3560250&gt;", "market_id": "1.138465931", "orders": ["138950974772156791"], "order_count": 1, "package_type": "Place", "customer_strategy_ref": "ip-172-31-38-7", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}], "transaction_id": 1, "client_username": "72f36d32"}

Entry Bet

{"asctime": "2023-02-07 21:17:57,217", "levelname": "INFO", "message": "Trade status update: Pending", "id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Pending", "status_log": "Pending"}

{"asctime": "2023-02-07 21:17:57,218", "levelname": "INFO", "message": "Order Place: SUCCESS", "bet_id": null, "order_id": "138950974772156791", "status": "SUCCESS", "error_code": null}

{"asctime": "2023-02-07 21:17:57,218", "levelname": "INFO", "message": "Order status update: Executable", "market_id": "1.138465931", "selection_id": 10200239, "handicap": 0, "id": "138950974772156791", "customer_order_ref": "4d8a4a5a825e8-138950974772156791", "bet_id": "100000000001", "date_time_created": "2018-01-01 13:05:50.071000", "publish_time": "2018-01-01 13:05:50.071000", "market_version": null, "async": false, "trade": {"id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Pending", "status_log": "Pending"}, "order_type": {"order_type": "Limit", "price": 3.7, "size": 5.0, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null, "price_ladder_definition": "CLASSIC"}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 5.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": "2018-01-01 13:05:50.294000", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Executable", "status_log": "Pending, Executable", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 25.65, "matched": []}, "notes": "0.9912", "market_notes": "3.7,3.8,3.7", "client": "72f36d32"}

{"asctime": "2023-02-07 21:17:57,218", "levelname": "INFO", "message": "Trade status update: Live", "id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Live", "status_log": "Pending, Live"}

{"asctime": "2023-02-07 21:17:57,220", "levelname": "INFO", "message": "Order status update: Execution complete", "market_id": "1.138465931", "selection_id": 10200239, "handicap": 0, "id": "138950974772156791", "customer_order_ref": "4d8a4a5a825e8-138950974772156791", "bet_id": "100000000001", "date_time_created": "2018-01-01 13:05:50.071000", "publish_time": "2018-01-01 13:05:50.071000", "market_version": null, "async": false, "trade": {"id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Live", "status_log": "Pending, Live"}, "order_type": {"order_type": "Limit", "price": 3.7, "size": 5.0, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null, "price_ladder_definition": "CLASSIC"}, "info": {"side": "LAY", "size_matched": 5.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 3.7}, "responses": {"date_time_placed": "2018-01-01 13:05:50.294000", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Execution complete", "status_log": "Pending, Executable, Execution complete", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0, "matched": [[1514811950696, 3.7, 5.0]]}, "notes": "0.9912", "market_notes": "3.7,3.8,3.7", "client": "72f36d32"}

{"asctime": "2023-02-07 21:17:57,220", "levelname": "INFO", "message": "Trade status update: Complete", "id": "e47d1976-a72c-11ed-93b3-3fe541d536d4", "strategy": "LowestLayer", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138950974772156791"], "offset_orders": [], "notes": "", "market_notes": "3.7,3.8,3.7", "status": "Complete", "status_log": "Pending, Live, Complete"}

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;

SECONDS TO START = 104.043

strategy_selection_orders[Order 100000000001: Execution complete]

market.blotter.orders = [Order 100000000001: Execution complete]

market status = OPEN

runner status = ACTIVE

selection exposure = 13.5

stats = {'matched_profit_if_win': -13.5, 'matched_profit_if_lose': 5.0, 'worst_potential_unmatched_profit_if_win': 0.0, 'worst_potential_unmatched_profit_if_lose': 0.0, 'worst_possible_profit_on_win': -13.5, 'worst_possible_profit_on_lose': 5.0}

number of active runners = 8

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;

SECONDS TO START = 103.929

strategy_selection_orders[]

market.blotter.orders = [Order 100000000001: Execution complete]

market status = OPEN

runner status = ACTIVE

selection exposure = -0.0

stats = {'matched_profit_if_win': 0.0, 'matched_profit_if_lose': 0.0, 'worst_potential_unmatched_profit_if_win': 0.0, 'worst_potential_unmatched_profit_if_lose': 0.0, 'worst_possible_profit_on_win': 0.0, 'worst_possible_profit_on_lose': 0.0}

number of active runners = 8

{"asctime": "2023-02-07 21:17:58,369", "levelname": "INFO", "message": "Market 1.138465931 closed", "market_id": "1.138465931", "event_id": "28531405", "event_type_id": "7", "event_name": "Sthl 1st Jan", "market_type": "WIN", "market_start_datetime": "2018-01-01 13:10:00", "country_code": "GB", "venue": "Southwell", "race_type": null, "orders_cleared": [], "market_cleared": [], "closed": true}

{"asctime": "2023-02-07 21:17:58,370", "levelname": "INFO", "message": "Market cleared", "market_id": "1.138465931", "order_count": 1, "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

{"asctime": "2023-02-07 21:17:58,370", "levelname": "INFO", "message": "Market level cleared", "market_id": "1.138465931", "profit": 5.0, "bet_count": 1}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Market closed", "market_id": "1.138465931", "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Removing market 1.138465931", "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Completed historical market '/home/ubuntu/Python_projects/tmp/races/1.138465931'"}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Simulation complete"}

{"asctime": "2023-02-07 21:17:58,371", "levelname": "INFO", "message": "Shutting down Execution (SimulatedExecution)"}

{"asctime": "2023-02-07 21:17:58,372", "levelname": "INFO", "message": "Shutting down Execution (BetfairExecution)"}

{"asctime": "2023-02-07 21:17:58,372", "levelname": "INFO", "message": "Client logout", "username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-02-07 21:17:58,372", "levelname": "INFO", "message": "Exiting flumine", "clients": {"Betfair": {}, "Simulated": {"72f36d32": {"username": "72f36d32", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 1, "transaction_count_total": 1, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7f61a0256950&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "BetConnect": {}}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140057390804992)&gt;"]}

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;

Profit: 5.00

10200239 2018-01-01 13:05:50.294000 OrderStatus.EXECUTION_COMPLETE 3.7 3.7 5.0 5.0```

---

## 2023-02-07 10:41:53 - issues channel

**Paras Stefanopoulos**

Will be purchasing a live key tomorrow so hopefully solves my problems

---

## 2023-02-07 09:08:58 - general channel

**Peter**

[@U04980ZT6UX](@U04980ZT6UX) Apologies. Missed your follow up questions when you originally posted them. In case they're still relevant ... market.blotter.live_orders() will give you an iterable of the live orders for that market only, or if ypu're working in your strategy's process_orders() method, the list of orders passed into it will be filtered by the strategy and market combination, but will be all such orders, though easily filtered by OrderStatus.EXECUTABLE.



[@U01D23DDMTQ](@U01D23DDMTQ) is right that to change price and size requires a cancel and new place as Betfair won't allow changing price and size in a replace. Personally I'd fire off the cancel as soon as I knew that I no longer wanted the bet and then watch the response (if working synchronously, or the order stream if working asynchronously) before placing the new order - which may be what he meant by implementing a wait queue.

---

## 2023-02-05 12:32:55 - general channel

**Trex44**

Same as me really.  Would be much harder to learn Python if it weren't for Betfair, always easier to learn when you have a project you enjoy building.Also pretty proficient with AWS as a result of BF and Flumine.

---

## 2023-02-05 12:12:51 - general channel

**Trex44**

Hey Jamie, welcome.  I have never heard of DataSpell. I use PyCharm for building Flumine strategies and for remote deployment. I use Jupyter notebook for analysing data that I extract from market data using some custom scripts I built. Although I think you could probably just build a flumine strategy and use it in back test mode to extract the data, no need for a custom script.

---

## 2023-02-03 08:41:49 - betfair-news channel

**Herugrim**

In the long run, bookies will ban you if you're winning too much (unless you're protected by minimum bet laws). Your initial question was a bit vague is all. You can beat BSP and still lose if you're making bad selections.

---

## 2023-02-01 08:36:35 - general channel

**QuickLearner8888**

This was extremely helpful, thank you.

Especially the point that I need a live key was essential. I was a bit confused that absolutely nothing happened when I was running the script, no matter what I was editing or adding.



I will most likely only be interested in one line (OVER_UNDER_XX) for each match, and max 20 matches pr day, so I need to get the filters right.



Thanks a lot!

---

## 2023-01-31 18:05:19 - general channel

**Newbie99**

Just to add to [@U01HQ97J5HA](@U01HQ97J5HA), there are far better people on here than I to advise, but you could modify this script:



[https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py](https://github.com/betcode-org/flumine/blob/master/examples/marketrecorder.py)



Changing this section:



```event_type_ids=["7"],

        country_codes=["GB", "IE"],

        market_types=["WIN"],```

to:



```event_type_ids=["1"],

market_types=["OVER_UNDER_25"],```

Some things to consider:



1. You need to have the live key enabled for streaming (the delayed key is useless for this unfortunately)

2. You need to be placing real money bets at the same time (even if only in small size), you can't just record, otherwise you will get your account blocked

3. There are A LOT of football markets, so you may find it necessary to use multiple connections (i.e. a different connection for each OVER_UNDER_XX if you are looking at more than one)...you may find you are initially limited to 200 markets per connections and 10 connections, this can be tricky for football as if there are over 200 markets (quite likely) you will get an error! If that happens you can either try to look at market_types that have fewer live markets or speak to Betfair to get your limit increased (but referring back to point 1, you may need to be placing live orders for this to happen).

4. You can loop through the market book to get the price data and need to match the market_id &amp; selection_id (and handicap, but this will always be 0 for football) to the market_catalogue, there is an example here: [https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py](https://github.com/betcode-org/flumine/blob/master/examples/strategies/lowestlayer.py)

That hopefully helps you get started, but of course, please feel free to ask away if you get stuck!

---

## 2023-01-31 13:52:39 - general channel

**QuickLearner8888**

Hi All.



I am new to this channel, and to Flumine in general.



Are there any tutorials out there, to explain how to use the marketrecorder?



I am interested in getting a market-stream, of a specific market_id. As an example, a live soccer-match, I would like to have only best_back_offered and best_lay_offered, and offered volumes at any given time, for every runners in the over/under market.



Any help or pointers in right direction would be a huge help.



 Thanks

---

## 2023-01-22 19:26:47 - issues channel

**foxwood**

Is the simulation supposed to delete completed bets when a non-runner / reserve is declared on uk dogs prior to the off ? Investigated a divide by zero bug of mine in a strategy which arose with a REMOVED runner that had an EXECUTION_COMPLETE order in market.blotter that predated the runner removal time - there were other runners bets also in the blotter from before the removal time. Confused about what should happen but current T&amp;Cs say "If a non-runner or reserve runner is declared, then all bets prior to the update of the market on Betfair will be void". Never checked what happens live !

---

## 2023-01-22 15:03:09 - general channel

**Peter**

A bit puzzled as to why this is a question. What's the architecture of your setup that makes this a potential concern?



I also doubt that anybody here will have an answer to this since if the keep_alive method is being used, implicitly or explicitly, to maintain active sessions it's not something we're likely to come up against. So probably one to ask Betfair.

---

## 2023-01-21 14:12:50 - general channel

**James**

How easy/time consuming would it be for a novice in python to set up a data recording/storage system such as Liam suggested in another thread (flumine &gt; aws s3)?

---

## 2023-01-20 08:07:35 - random channel

**Mick**

For the record I'm certainly not promoting [http://freelancer.com|freelancer.com](http://freelancer.com|freelancer.com) (which has many flaws) - my suggestion for a jobs channel was in relation to modelling jobs from _any_ source.

---

## 2023-01-19 08:58:12 - general channel

**liam**

+ if there are live orders every `streaming_timeout` which is 0.2s I think

---

## 2023-01-18 09:01:24 - strategies channel

**R**

Any general thread on odds slippage and how to mitigate?  I'm not seeing much of it in my live system right now but have some ideas that may cause it to become a problem.  Or is this a cheeky "don't leak your alpha" question :wink:

---

## 2023-01-16 20:56:59 - general channel

**Trex44**

```{"asctime": "2023-01-16 20:43:47,370", "levelname": "INFO", "message": "Client added", "username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": null, "transaction_count_total": null, "trading_controls": [], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-01-16 20:43:47,371", "levelname": "INFO", "message": "Adding client control MAX_TRANSACTION_COUNT"}

{"asctime": "2023-01-16 20:43:47,371", "levelname": "INFO", "message": "Adding trading control ORDER_VALIDATION"}

{"asctime": "2023-01-16 20:43:47,371", "levelname": "INFO", "message": "Adding trading control MARKET_VALIDATION"}

{"asctime": "2023-01-16 20:43:47,372", "levelname": "INFO", "message": "Adding trading control STRATEGY_EXPOSURE"}

{"asctime": "2023-01-16 20:43:47,372", "levelname": "INFO", "message": "Adding strategy Lay_Fav_Strat"}

{"asctime": "2023-01-16 20:43:47,372", "levelname": "INFO", "message": "Creating new &lt;class 'flumine.streams.datastream.DataStream'&gt; (2000) for strategy Lay_Fav_Strat"}

{"asctime": "2023-01-16 20:43:47,372", "levelname": "INFO", "message": "Starting flumine", "clients": {"Betfair": {"x": {"username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fa89d18bac0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}}, "Simulated": {}, "BetConnect": {}}, "markets": {"market_count": 0, "open_market_count": 0}, "streams": ["&lt;OrderStream(OrderStream, initial daemon)&gt;", "&lt;DataStream(DataStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140362210664448)&gt;"]}

{"asctime": "2023-01-16 20:43:47,598", "levelname": "INFO", "message": "Client login", "username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fa89d18bac0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-01-16 20:43:47,881", "levelname": "INFO", "message": "Client update account details", "username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fa89d18bac0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}

{"asctime": "2023-01-16 20:43:47,882", "levelname": "INFO", "message": "Adding worker keep_alive"}

{"asctime": "2023-01-16 20:43:47,882", "levelname": "INFO", "message": "Adding worker poll_market_catalogue"}

{"asctime": "2023-01-16 20:43:47,882", "levelname": "INFO", "message": "Adding worker poll_account_balance"}

{"asctime": "2023-01-16 20:43:47,882", "levelname": "INFO", "message": "Adding worker poll_market_closure"}

{"asctime": "2023-01-16 20:43:47,883", "levelname": "INFO", "message": "BackgroundWorker keep_alive starting", "worker_name": "keep_alive", "function": "&lt;function keep_alive at 0x7fa89d147c70&gt;", "context": {}, "start_delay": 0, "interval": 1200, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-01-16 20:43:47,883", "levelname": "INFO", "message": "BackgroundWorker poll_market_catalogue starting", "worker_name": "poll_market_catalogue", "function": "&lt;function poll_market_catalogue at 0x7fa89d1540d0&gt;", "context": {}, "start_delay": 10, "interval": 60, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-01-16 20:43:47,883", "levelname": "INFO", "message": "BackgroundWorker poll_account_balance starting", "worker_name": "poll_account_balance", "function": "&lt;function poll_account_balance at 0x7fa89d154160&gt;", "context": {}, "start_delay": 10, "interval": 120, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-01-16 20:43:47,884", "levelname": "INFO", "message": "BackgroundWorker poll_market_closure starting", "worker_name": "poll_market_closure", "function": "&lt;function poll_market_closure at 0x7fa89d1541f0&gt;", "context": {}, "start_delay": 10, "interval": 60, "func_args": [], "func_kwargs": {}}

{"asctime": "2023-01-16 20:43:47,884", "levelname": "INFO", "message": "Starting streams.."}

{"asctime": "2023-01-16 20:43:47,884", "levelname": "INFO", "message": "Starting OrderStream 1000", "stream_id": 1000, "customer_strategy_refs": "ip-172-31-38-7", "conflate_ms": null, "streaming_timeout": 0.25, "client_username": "x"}

{"asctime": "2023-01-16 20:43:47,884", "levelname": "INFO", "message": "Starting output_thread (OrderStream 1000)"}

{"asctime": "2023-01-16 20:43:47,885", "levelname": "INFO", "message": "[Register: 1001]: orderSubscription"}

{"asctime": "2023-01-16 20:43:47,885", "levelname": "INFO", "message": "[OrderStream: 1001]: \"OrderStream\" created"}

{"asctime": "2023-01-16 20:43:47,955", "levelname": "INFO", "message": "[OrderStream: 1001]: connection_id: 108-160123204347-4921521"}

{"asctime": "2023-01-16 20:43:47,984", "levelname": "INFO", "message": "[OrderStream: 1002]: SUCCESS (9 connections available)"}

{"asctime": "2023-01-16 20:43:47,985", "levelname": "INFO", "message": "[OrderStream: 1001]: SUCCESS (9 connections available)"}

{"asctime": "2023-01-16 20:43:48,001", "levelname": "INFO", "message": "[OrderStream: 1001]: 0 oc added"}

{"asctime": "2023-01-16 20:43:48,135", "levelname": "INFO", "message": "Starting DataStream 2000", "stream_id": 2000, "market_filter": {"marketIds": ["1.208749303"], "eventTypeIds": ["7"], "marketTypes": ["WIN"], "venues": ["Wolverhampton"], "countryCodes": ["GB"]}, "market_data_filter": {"fields": ["EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP", "EX_MARKET_DEF", "SP_TRADED", "SP_PROJECTED"]}, "conflate_ms": null}

{"asctime": "2023-01-16 20:43:48,135", "levelname": "INFO", "message": "[Register: 2001]: marketSubscription"}

{"asctime": "2023-01-16 20:43:48,136", "levelname": "INFO", "message": "[FlumineStream: 2001]: \"FlumineStream\" created"}

{"asctime": "2023-01-16 20:43:48,212", "levelname": "INFO", "message": "[FlumineStream: 2001]: connection_id: 109-160123204348-5018646"}

{"asctime": "2023-01-16 20:43:48,241", "levelname": "INFO", "message": "[FlumineStream: 2002]: SUCCESS (8 connections available)"}

{"asctime": "2023-01-16 20:43:48,268", "levelname": "INFO", "message": "[FlumineStream: 2001]: SUCCESS (8 connections available)"}

{"asctime": "2023-01-16 20:43:48,285", "levelname": "INFO", "message": "[MarketStream: 2001] 1.208749303 added, 1 markets in cache"}

{"asctime": "2023-01-16 20:43:48,285", "levelname": "INFO", "message": "[FlumineStream: 2001]: 1 mc added"}

{"asctime": "2023-01-16 20:43:48,386", "levelname": "INFO", "message": "Adding: 1.208749303 to markets"}

{"asctime": "2023-01-16 20:43:58,116", "levelname": "INFO", "message": "Created marketCatalogue for 1.208749303", "market_id": "1.208749303", "event_id": "32033309", "event_type_id": "7", "event_name": "Wolverhampton 18th Jan", "market_type": "WIN", "market_start_datetime": "2023-01-18 15:15:00", "country_code": "GB", "venue": "Wolverhampton", "race_type": "Flat", "orders_cleared": [], "market_cleared": [], "closed": false}

{"asctime": "2023-01-16 20:43:58,286", "levelname": "INFO", "message": "Client update account details", "username": "x", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fa89d18bac0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}```

---

## 2023-01-16 11:10:46 - issues channel

**liam**

This is very tricky to isolate however the only regression I can find is that the simulation engine will now continue to match when an order is in a 'CANCELLING'/'REPLACING'/'UPDATING' state which is theoretically more accurate to live, can you try running `1.21` with the following patch to confirm this is what is causing the differences



```from flumine.markets.middleware import SimulatedMiddleware

from flumine.order.order import OrderStatus





def __process_simulated_orders(self, market, market_analytics: dict) -&gt; None:

    for order in market.blotter.live_orders:

        if order.status == OrderStatus.EXECUTABLE and order.simulated:

            runner_analytics = market_analytics[

                (order.selection_id, order.handicap)

            ]

            order.simulated(market.market_book, runner_analytics.traded)





SimulatedMiddleware._process_simulated_orders = __process_simulated_orders



# your code```

---

## 2023-01-15 18:14:42 - general channel

**Trex44**

Hey all, for the Flumine [https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/examples/strategies/marketrecorder.py|marketrecorder](https://github.com/betcode-org/flumine/blob/04d607e1d749a2dbaa8297bbf74c062990867a6b/examples/strategies/marketrecorder.py|marketrecorder). Other than passing the name of the bucket as a string in the context kwarg dict is there anything else that needs to be done to ensure the recorded markets are moved to the bucket? At the moment the files are recorded and zipped but aren't being uploaded to the bucket despite me passing the bucket name. the files is then automatically deleted. I have the correct IAM role assigned to the EC2 instance to allow it to access S3 so I am uncertain what the issue is.

---

## 2023-01-12 16:57:16 - strategies channel

**D C**

Me too. It's really interesting to see difference in the scale of operating - in terms of bets and deployment. Still getting my head around 128 threads to be honest.

---

## 2023-01-11 22:29:05 - issues channel

**JFP**

```    # Get betfair data files

    data_folder = 'C:/Data/Data/File'

    data_files = os.listdir(data_folder,)

    data_files = [f'{data_folder}/{path}' for path in data_files]



    strategy = BookLayOrders(

        # market_filter selects what portion of the historic data we simulate our strategy on

        # markets selects the list of betfair historic data files

        # market_types specifies the type of markets

        # listener_kwargs specifies the time period we simulate for each market

        market_filter={

            "markets": data_files,  

            'market_types':['WIN'],

            'country_codes':["GB"],

            "listener_kwargs": {"seconds_to_start": 190},  

            },

        max_order_exposure=10000,

        max_selection_exposure=10000,

        max_live_trade_count=10000,

        max_trade_count=10000,

        multi_order_trades = True,

    )

    # Run our strategy on the simulated market

    framework.add_strategy(strategy)

    framework.add_logging_control(

        BacktestLoggingControl()

    )



    framework.run()```

---

## 2023-01-11 14:15:15 - issues channel

**JFP**

Prob should clarify. I have a strategy that places bets live. Have been running this with profit. When I simulate this strat with historical data, I get negative results

---

## 2023-01-11 12:36:09 - issues channel

**JFP**

Just noticed a significant discrepancy when backtesting with different versions of Flumine.

Original backtesting completed back in Dec 2021 with Flumine 1.19.14 showed consistent profit for a strategy that places a lot of passive bets.

Have been running this strat live with success.

Just reran the same data with latest version of Flumine 2.3.1 which gave approx. break even. Ran backtest against periods when it was running live and it gave negative results when my live results were positive.

Reinstalled 1.19.14 and rerun Backtest, got original positive results and also got positive results that roughly match my actual results for live trading periods.

Installed Flumine 1.21.0 (when smart matching was implemented) and get the same negative results as per 2.3.1.

The docs for smart matching says, for backtesting, it removes double counting of size for passive orders. Just after some more clarification on this, is this to remove double counting when running multiple strategies (ie: simulated middleware with live strat)?

---

## 2023-01-11 12:00:31 - issues channel

**Peter**

[@U04J6LUGCFM](@U04J6LUGCFM) It is a poorer option. It's useful for testing things like log in and requesting fairly static data like competition ids and market catalogues, but will give you very strange results if you use it to request the really useful stuff like market odds. The delays appear to be pretty random and my suspicion is that they just snapshot the live data from time-to-time and then serve that up to delayed key users, so it often bears little relation to the true state of the market.

---

## 2023-01-09 17:34:55 - general channel

**Ralegh**

Is there a good way to download historical data straight to AWS? Rather than scp, would be a lot faster, I tried using the link from the website but didn’t work

---

## 2023-01-09 11:02:19 - general channel

**Andrew**

How do I confirm an order has been cancelled after market.cancel_order(order)? I think I have a race condition trying the place an order before cancellation. On market that’s not a problem but my strategy requires single live order per runner at any moment, plus need to ensure order has cancelled and not executed (before cancel instruction received).

---

## 2023-01-05 13:30:55 - general channel

**birchy**

Thanks [@U4H19D1D2](@U4H19D1D2). Having read a handful of articles, I was swaying towards Ansible but now I'm not so sure, particularly given that AWS Lightsail (my preferred VPS as it's a simplified EC2 setup) has capability to add containers by simply supplying a docker file. I know you use EC2, but is your preference to run lots of small instances, or to have a larger one with multiple containers? With Docker, are there any issues with running different versions of python on a Linux (Ubuntu) VPS? For example, if my Ubuntu version comes with python 3.9, how easy is it to install 3.11 inside a container? I've had issues in the past with installing newer versions as it can get a bit messy, particularly if we have to resort to using PPA's.

---

## 2023-01-05 09:57:08 - general channel

**birchy**

Thanks all, I need to get my R&amp;D cap on and see which of your suggestions are best suited to my operation. My main requirements are to be able to easily deploy multiple Flumine instances in a way that allows for future expansion. Not sure if I need a full container setup, but will investigate and ask questions later... :grinning:

---

## 2023-01-04 23:23:20 - general channel

**Oliver**

Docker/containers have merits, and can be another layer to go with virtualenvs if you want, or you can cut out the virtualenv entirely because you are installing packages straight inside the container instead of a containerised virtualenv. I don't know your deployment experience/environment, but you may find docker is OTT.



I'd say docker/containers (as there are other alternative container runtimes) is an additional isolation/encapsulation layer which can be pretty cool but does have management overhead. You could go in other directions for similar effect, like using packer to produce virtual machine images to then deploy to machines (e.g. EC2/GCP/... instances), and in that or outside of that you could use things like ansible/chef/puppet (ansible probably is best for this kind of smaller scenario thing so can see why Mo mentioned it), and also a layer like terrafrorm (for extra info: terragrunt is also a good layer for it but I doubt so for the scale required in this case) above that to manage the provisioning of your infrastructure (e.g. from the point of spinning up cloud resources if you've gone that way).



I'd still put emphasis on the likes of Pipenv as your first stop as it isn't conceptually (or in operations) much more than the virtualenvs you are using, but has things that are significant for long term management and reproducibility (i.e. exact package version pinning using the lockfile, a bit like using `pip freeze &gt; requirements.txt` but without being a such a mess to untangle when you want to upgrade something). Another good thing is pipenv/virtualenvs will continue to work nicely with editors and you can fire them up easily and without extra permissions on machines (although the permissions is likely not an issue for you).



Just for more info: a thing from before the likes of Pipenv was [https://pypi.org/project/pip-tools/|pip-tools](https://pypi.org/project/pip-tools/|pip-tools) which isn't far removed from the niceness of Pipenv. That will do the dependency locking side of things, but you'd still want to make the environments you install into (fresh VMs, constainers, virtualenvs, or bare metal).



If I blogged or was in consultancy, I'd be inclined to draw out graphs (or a graph) showing the potential dependencies of provisioning with spiel about the different parts...

---

## 2023-01-04 23:16:37 - general channel

**Fab**

My suggested approach is also Docker. Here's a simple blueprint to get you started...



Assuming you have two Python programs (`run-oldstrategies.py`, `run-newstrategies.py`), we are on your dev machine (Linux Debian) in the directory that contains those two programs.



*Step 1 - Create separate environment requirement files*

Activate the environment containing libraries for the old strategies then do `pip freeze &gt; requirements-oldstrategies.txt`. Do the same for the new strategies environment to generate `requirements-newstrategies.txt`.



*Step 2 - Create separate Dockerfile(s)*

Create two empty text files: `Dockerfile-oldstrategies`, `Dockerfile-newstrategies`. As an example, here's what you could put inside Dockerfile-oldstrategies (you can easily figure out the other):

```FROM python:3.9-slim-bullseye

RUN apt update -y

WORKDIR /opt/myproject

COPY ./requirements-oldstrategies.txt .

RUN cat requirements-oldstrategies.txt | grep -v '==0.0.0' &gt; requirements-oldstrategies.sanitised.txt

RUN pip install --no-cache-dir --upgrade -r requirements-oldstrategies.sanitised.txt

COPY . .

ENTRYPOINT ["python", "run-oldstrategies.py"]```

*Step 3 - Install Docker*

[https://docs.docker.com/engine/install/debian/](https://docs.docker.com/engine/install/debian/)



*Step 4 - Build separate Docker images for the two programs*

```docker build -t run-oldstrategies -f ./Dockerfile-oldstrategies .

docker build -t run-newstrategies -f ./Dockerfile-newstrategies .```

*Step 5 - Install Docker on your production machin*e

Same as Step 3



*Step 6 - Transfer Docker images from your current machine to the production machine*

```docker save run-oldstrategies | bzip2 | ssh &lt;USER&gt;@&lt;PRODUCTION_IP&gt; docker load

docker save run-newstrategies | bzip2 | ssh &lt;USER&gt;@&lt;PRODUCTION_IP&gt; docker load```

*Step 7 - Log into production machine, make sure images are there*

```docker images```

*Step 8 - Start both Docker containers*

```docker run -d run-oldstrategies

docker run -d run-newstrategies```

*Step 9 - How to check logs for a Docker container*

Run `docker ps -a` to get the hash of each container then you run the following command:

```docker logs &lt;CONTAINER_HASH&gt;```



---

## 2023-01-04 22:55:32 - general channel

**dont**

ansible - bleah. You can just `pip freeze` your venv into a `requirements.txt` which then you get to install on the production machine. Alternatively you can just work with a docker container as Liam says on both dev/prod and deploy the container.

---

## 2023-01-04 09:12:33 - issues channel

**Peter**

[@U016TGY3676](@U016TGY3676) If I were polling yes, but when streaming live the process_sports_data method receives both the current market book and the sports update making it possible (and probably preferable) to trade immediately if triggered.



However, when backtesting, the middleware is delivering the sports update with the next market update. In effect it delays delivering the sports update, until a market update triggers it, and indeed may delay several sports updates if it takes a while for the next market update to come along. In effect we have what data scientists would term a data leak as we're acting on the sports update with market data that wasn't available at the time it was supposedly received.



I realise now that I did know this, but whilst approaching the question from a different direction had a senior moment! :blush:

---

## 2023-01-04 09:04:11 - general channel

**D**

[@U016TGY3676](@U016TGY3676) understood - I don't create new environments that often in production so I'm setting up manually. And in dev I'm letting Pycharm create its own envs for me.

---

## 2023-01-03 23:54:23 - strategies channel

**birchy**

Personally I like to keep the stakes low (£1 - £10 MAX) for simulations as that's more real world and then reassess stakes once I've got some live data.

---

## 2023-01-03 23:03:49 - general channel

**birchy**

[@U02RN7YDRQ9](@U02RN7YDRQ9) I'm fine with Pycharm on my development machine. My production stuff is on a headless Linux VPS. :+1: 

[@URMM9463X](@URMM9463X) I'm thinking more along the lines of automating the setup &amp; launch via a script. It seems like a common workflow for which management tools would already exist but Google has muddied the waters and/or I'm not using the correct terminology. My basic manual workflow would be something like:

1. login to remote server via SSH

2. create venv

3. activate venv &amp; install libraries (Flumine, etc)

4. git clone strategy code

5. launch python strategy

6. logout of SSH

---

## 2023-01-03 22:07:00 - general channel

**D**

I use virtualenv in production, along with tmux. It's a pretty straightforward way of creating independent python environments.

---

## 2023-01-03 21:54:58 - general channel

**birchy**

Bit of a noob question, but what's the recommended beginner friendly way to manage/automate python venv's on a Linux production machine? To be more specific, I've only used venv's created by Pycharm on my development machine but now I'm getting to a point where I have old bots chugging away in production but need to run others with newer libraries . Venv is the obvious way to achieve this without fucking up the older ones running on the main system, but I'm wondering how best to automate venv installation and setup for spinning up new bot instances on a single machine?

---

## 2023-01-01 20:09:03 - random channel

**river_shah**

I started late 2020 on development and deployed early 2021. I work on sports betting on a few sprints a year, but it is fully automated and at a scale where it makes sense to make it grow and polish to see how much more I can squeeze out. Got a few things working well, others not so much. I have other tradfi trading models as well which take the bulk of my time

---

## 2022-12-30 15:59:56 - strategies channel

**birchy**

[@U04380WMKL2](@U04380WMKL2) just FYI, Flumine simulation doesn't (afaik) handle double counting of liquidity but DOES give excellent results vs live for passive bets (i.e. not taking the best available prices).

---

## 2022-12-30 08:53:40 - random channel

**Paul**

I’m in two minds. In the “For AI” camp: ChatGPT is already better than 50% of developers at solving routine problems. The AI code completion tools from GitHub and AWS might run into licensing issues (they’re open source code without attribution), but once that’s solved we are probably 5 years away from programmers just having to write unit and integration tests, and 10 from just needing to write success criteria in a scope doc. 

---

## 2022-12-29 20:24:19 - general channel

**Ralegh**

Suggestions on which EC2 instances to use with flumine? Think I’m going to go for C6/7s for simulating but not sure what I need for live (the actual strategies aren’t super maths heavy)

---

## 2022-12-27 20:40:55 - general channel

**liam**

Yes but it’s local paper trading, basically using the flumine simulation engine on live market data 

---

## 2022-12-27 20:38:50 - general channel

**Liam Querido**

Oh really? Can you paper trade using the live app key and flumine?

---

## 2022-12-20 13:39:27 - general channel

**shashank bansal**

I want to create an interpreter system where people subscribe to a server and server sends the feed from BF to subscribers. There can be 1000s of connection and can burst out the system with so many odds every few ms. S3 will slow down the latency for sure. So, was thinking towards Dynamodb.

---

## 2022-12-20 13:09:24 - general channel

**liam**

The common pattern for this is flumine / marketRecorder -&gt; AWS S3 -&gt; smart_open -&gt; :money_with_wings: :moneybag: 

---

## 2022-12-20 11:15:33 - general channel

**shashank bansal**

I am planning to build below mentioned architecture.

[https://aws.amazon.com/blogs/database/store-and-stream-sports-data-feeds-using-amazon-dynamodb-and-amazon-kinesis-data-streams/](https://aws.amazon.com/blogs/database/store-and-stream-sports-data-feeds-using-amazon-dynamodb-and-amazon-kinesis-data-streams/)



But I need to send the sports feed to the API Gateway and thinking of using some serverless approach.

---

## 2022-12-20 11:11:20 - general channel

**Mo**

I use AWS (EC2)

---

## 2022-12-20 11:07:30 - general channel

**shashank bansal**

Hi, Has anyone tried streaming using aws or any other serverless? And any recommendations on the same?

---

## 2022-12-07 10:25:25 - random channel

**river_shah**

this looks good, thanks. so the python code itself knows nothing about `awslogs` or `gcplogs`?docker handles / abstracts the streaming entirely?

---

## 2022-12-07 10:20:48 - random channel

**liam**

i just use docker `awslogs` to pipe straight to AWS cloudwatch, [https://docs.docker.com/config/containers/logging/gcplogs/|gcp](https://docs.docker.com/config/containers/logging/gcplogs/|gcp) equivalent

---

## 2022-12-04 17:16:31 - general channel

**Newbie99**

Markets are automatically added via the Framework once they are created on the server side. If the market meets your criteria (as specified on startup) then it will get added via Flumine into your cache.



Once the market is closed Flumine receives the message from Betfair and the market is removed.



Flumine is designed to be up and running for days/weeks/months on end, whilst you could stop and re-start frequently, most people wouldn't use it in that way (I suspect).

---

## 2022-11-28 16:39:04 - general channel

**Jorge**

Hi, how can I check if my strategy has any executable or pending order in the market?



I thought `market.blotter.has_live_orders`  was good for this, it works in Simulation. But in live trading it is returning True also when there are orders with "Execution complete"... Could it be that it does not work for "FILL_OR_KILL" orders?

---

## 2022-11-26 19:09:40 - random channel

**Newbie99**

I suspect the answer is no, but did anyone notice any strange data from Betfair (football or horses) today (sorry thats a bit vague as I'm not sure what I'm looking for precisely)?



In short, for no apparent reason (that I can see), all strategies I'm running across football and horse racing didn't cancel any orders after 11:00 this morning (so I've only been getting filled @ -EV prices all day), I have made no code changes and there are no errors in the logs (the logs show no cancel request appears to have been sent).



Therefore all I can assume is that some data is being mishandled at my end (probably something stupid like a None value instead of whatever it should be), but I'm struggling to find what it could be (I've restarted the EC2 instance and its working fine now).



Appreciate its almost certainly just something weird that I have done, but just thought I'd check on the off chance anyone spotted any weird streaming data (I have it recorded so if there is anything should be able to find it and hopefully replicate, just wanted to save time in case anyone else had come across anything).

---

## 2022-11-25 11:24:24 - random channel

**liam**

Anyone successfully build docker images on GitHub actions for ARM64 (EC2 graviton), I am ready to launch my laptop out the window

---

## 2022-11-24 22:00:17 - betconnect channel

**Yll Kelani**

Hey sorry if I've missed something obvious but couldn't find any docs on getting an api key or a "personalised production url" - are these available upon request?

[https://github.com/betcode-org/betconnect#client-requirements](https://github.com/betcode-org/betconnect#client-requirements)

---

## 2022-11-19 15:31:55 - issues channel

**Mo**

Sorry I’m not a windows user so can’t really help but I take the message to mean that your version of OpenSSL installed is incapable of meeting the minimum TLS version that the Betfair login server requires 

---

## 2022-11-18 21:40:49 - random channel

**Newbie99**

A while ago I seem to remember someone on here talking about [http://api-football.com|api-football.com](http://api-football.com|api-football.com), out of curiosity, I believe they refresh the live score once every 15 seconds, but does anyone know roughly how much latency there is in that (i.e. at the point of update, would it be 'live' or is there a noticeable delay)?



For my purposes ms don't matter, its only really if its a large number of seconds away from the other live score services that I guess it could be a problem.

---

## 2022-11-17 21:44:54 - random channel

**Aaron Smith**

Hello ppl! I am thinking about emigrating from the uk to a different country. The destination is not determined at all and i want to consider all possible options. As betfair is my sole income, the most important requirement is that i am able to legally continue with this.



Now, sadly gambling laws are a mess, google lists about this matter are partly out of date, dont seem to consider gambling laws sometimes and also dont differentiate between betfair exchange and sportsbook (as obviously i need the exchange to operate in that country, not only the sportsbook). Also customer support seems useless on this matter, as their statements contradict themselves.



So, does anyone know how to get a trustworthy list on what countries i can consider?

---

## 2022-11-17 09:23:23 - general channel

**Aaron Smith**

depends heavily on the strategy. From my experience, strategies where i am offering bets underperform in backtest compared to live and strategies that take bets are very comparable. i think i m actually still doing slightly better in live than in backtesting, unless i backtest a lot to get the perfect parameters, in which case you can assume that the parameter set that was performing best was probably also a little lucky in that particular run and live will be performing slightly worse.

---

## 2022-11-17 09:19:44 - general channel

**Jorge**

Exactly, that's my plan. But does the simulation match live in flumine? Or are there known limitations?

---

## 2022-11-17 09:13:30 - general channel

**Jorge**

Does the backtest (simulation) match live trading for you guys? I'm testing and want to feel confident they match before running the Strategy live with higher stake and in more markets. But maybe this is too much to ask from the backtest?

---

## 2022-11-16 11:07:17 - issues channel

**Peter C**

Yep, that's it - I updated flumine everywhere but the server where this problem occurs - sorry, I should have realised, thanks!

---

## 2022-11-16 10:59:35 - issues channel

**Peter C**

I have an issue that sometimes appears after a `connection closed by server` error. When the flumine strategy reconnects, it can sometimes get stuck in a cancelling loop. My cancel logic checks whether an order has Executable status, and then cancels based on time executable. Sometimes after a reconnect the strategy attempts to cancel an order, and this transaction fails as `bet_taken_or_lapsed`. The strategy will continually try to cancel these orders, so I'm wondering what the best way to avoid this is.

---

## 2022-11-15 13:48:40 - random channel

**PeterLe**

Just wondered if anyone had upgraded their Python version last night and found it worthwhile?

Im going to read up tonight the best way to do this on ubuntu (on Lightsail) on AWS

is it relativley easy to upgrade? Would you remove the old version completely?

It was easy enough to setup the instance on Ubuntu to start with so maybe create a new instance from scratch ?

Any thoughts please? thanks

---

## 2022-11-15 08:52:48 - general channel

**Jorge**

Other question:sweat_smile:, does the "FILL_OR_KILL" attribute work in Simulation? In live trading I find "Order status update: Execution complete" but I don't receive this in Simulation...

---

## 2022-11-14 15:22:17 - general channel

**Jorge**

Hi, I am debugging an inplay market where I run flumine. The live log indicates an order was placed but I cannot see this order anywhere in the betfair website or quering `trading.betting.list_cleared_orders`



```2022-11-13 14:53:25,981 | INFO | a | a | Order status update: Pending

2022-11-13 14:53:25,982 | INFO | a | a | Deleting requests.Session

2022-11-13 14:53:25,982 | INFO | a | a | New requests.Session created

2022-11-13 14:53:25,987 | INFO | a | a | Thread pool submit

2022-11-13 14:53:25,987 | INFO | a | a | 1 order packages executed in transaction

2022-11-13 14:53:31,266 | INFO | a | a | execute_place

2022-11-13 14:53:31,267 | INFO | a | a | Trade status update: Pending

2022-11-13 14:53:31,267 | INFO | a | a | Order Place: SUCCESS

2022-11-13 14:53:31,267 | INFO | a | a | Order status update: Executable

2022-11-13 14:53:31,267 | INFO | a | a | Trade status update: Live

2022-11-13 14:53:31,505 | INFO | a | a | Order status update: Execution complete

2022-11-13 14:53:31,505 | INFO | a | a | Trade status update: Complete```

Any idea what could have happened?

---

## 2022-11-14 09:29:54 - random channel

**Mo**

Yeah, switched to it in production as soon as it came out. For the performance benefit

---

## 2022-11-10 12:53:40 - strategies channel

**D C**

Depends on what you mean by expensive. I believe that proform is about £180 for an 8 week sub and I believe that you can export the entire database. £180 is less than I pay in AWS fees a month so I would say it is good value. I am quite interested in doing this myself as my IP returns seem to be getting smaller each month so if you do look into this and find that anything I've mentioned above is not true, please do let me know.

---

## 2022-11-09 13:42:56 - random channel

**Joel Beasley**

[@U01B8031PM1](@U01B8031PM1) TL;DR - no :slightly_smiling_face:



The main cloud providers used to not allow gambling companies but they have done for some time now:

[https://aws.amazon.com/solutions/case-studies/sportsbet-case-study/](https://aws.amazon.com/solutions/case-studies/sportsbet-case-study/)

[https://sbcnews.co.uk/technology/2019/06/28/leovegas-completes-systems-migration-to-google-cloud/](https://sbcnews.co.uk/technology/2019/06/28/leovegas-completes-systems-migration-to-google-cloud/)

---

## 2022-11-09 11:18:13 - random channel

**river_shah**

Does AWS, GCP etc care that we are gambling using their infrastructure? For example if my account manager at a cloud provider asks what line of work I am engaged in, could I just say professional gambling and it not be a problem?

---

## 2022-11-02 23:48:59 - random channel

**D C**

On the stream API, when making requests you supply an "id" field to match responses to a subscription request. Docs say specifically that



`id - A unique counter you should supply on a RequestMessage and which will be supplied back on a ResponseMessage.`



I've always assumed that the "uniqueness" here is with respect to each connection, rather than accross ALL connections you may use. Essentially I am asking if it is possible that I am causing problems running say 2 streams connections but using the same ID on the market subscription requests? Clutching at straws a bit here but just wondering if people use globally unique "id" fields across their entire production setup or if uniqueness inside each connection is sufficient.

---

## 2022-11-02 23:26:09 - random channel

**D C**

Just one last sanity check here [@UBS7QANF3](@UBS7QANF3) (or anyone who can suggest anything useful) prior to me emailing Neil. Ran 2 pairs of bots tonight on 2 different AWS instances. Each AWS instance runs a passive node.js data GPS stream logger and an active C++ production bot (one with bet placement disabled for obvious reasons).

Gaps in RRC transmission match up between node and C++ bots running on the same instance.

---

## 2022-11-02 18:06:11 - random channel

**D C**

Any idea who I could take this up with? I don't have an AM but I have verified these gaps on bots written in 2 different languages running on separate AWS instances. Overlap seems to be just the account and the idiot who wrote the bots - one of which simply dumps received JSON to a file (assuming it can be parsed as JSON). Not really sure what else to try as these gaps are causing me serious problems now - large spots of dead time within races is a bit of a problem.

---

## 2022-11-01 12:54:30 - issues channel

**Liam Querido**

Perfect. One other thing, is it possible to adjust the frequency of the data that Flumine records. I believe that, for the delayed app key, the default is every 180 seconds. Is this able to be changed (or just for the live key)?

---

## 2022-11-01 06:28:49 - issues channel

**Muhammad Adeel Zahid**

Ok, when I try to run the following code

```trading = betfairlightweight.APIClient(username = my_username, password = my_password, app_key = my_app_key, certs=certs_path)

trading.login()

market_id = "1.199777126"

selection_id = 31484513

resources = trading.race_card.get_race_card(market_ids=[market_id])

print(resources)```

I get the exception

`Traceback (most recent call last):`

  `File "E:\Git\betfair\dailyracevenuedatagrabber.py", line 37, in &lt;module&gt;`

    `resources = trading.race_card.get_race_card(market_ids=[market_id])`

  `File "E:\Git\betfair\venv\lib\site-packages\betfairlightweight\endpoints\racecard.py", line 59, in get_race_card`

    `raise RaceCardError(`

`betfairlightweight.exceptions.RaceCardError: You need to login before requesting a race_card`

`APIClient.race_card.login()`

when I change `trading.login()` to `trading.race_card.login()`

I get the following exception

`Traceback (most recent call last):`

  `File "E:\Git\betfair\venv\lib\site-packages\urllib3\connectionpool.py", line 703, in urlopen`

    `httplib_response = self._make_request(`

  `File "E:\Git\betfair\venv\lib\site-packages\urllib3\connectionpool.py", line 386, in _make_request`

    `self._validate_conn(conn)`

  `File "E:\Git\betfair\venv\lib\site-packages\urllib3\connectionpool.py", line 1040, in _validate_conn`

    `conn.connect()`

  `File "E:\Git\betfair\venv\lib\site-packages\urllib3\connection.py", line 414, in connect`

    `self.sock = ssl_wrap_socket(`

  `File "E:\Git\betfair\venv\lib\site-packages\urllib3\util\ssl_.py", line 449, in ssl_wrap_socket`

    `ssl_sock = _ssl_wrap_socket_impl(`

  `File "E:\Git\betfair\venv\lib\site-packages\urllib3\util\ssl_.py", line 493, in _ssl_wrap_socket_impl`

    `return ssl_context.wrap_socket(sock, server_hostname=server_hostname)`

  `File "C:\Users\adeel\AppData\Local\Programs\Python\Python310\lib\ssl.py", line 512, in wrap_socket`

    `return self.sslsocket_class._create(`

  `File "C:\Users\adeel\AppData\Local\Programs\Python\Python310\lib\ssl.py", line 1070, in _create`

    `self.do_handshake()`

  `File "C:\Users\adeel\AppData\Local\Programs\Python\Python310\lib\ssl.py", line 1341, in do_handshake`

    `self._sslobj.do_handshake()`

`ssl.SSLError: [SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:997)`



`During handling of the above exception, another exception occurred:`



`Traceback (most recent call last):`

  `File "E:\Git\betfair\venv\lib\site-packages\requests\adapters.py", line 440, in send`

    `resp = conn.urlopen(`

  `File "E:\Git\betfair\venv\lib\site-packages\urllib3\connectionpool.py", line 785, in urlopen`

    `retries = retries.increment(`

  `File "E:\Git\betfair\venv\lib\site-packages\urllib3\util\retry.py", line 592, in increment`

    `raise MaxRetryError(_pool, url, error or ResponseError(cause))`

`urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='[http://www.betfair.com|www.betfair.com](http://www.betfair.com|www.betfair.com)', port=443): Max retries exceeded with url: /exchange/plus/ (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:997)')))`



`During handling of the above exception, another exception occurred:`



`Traceback (most recent call last):`

  `File "E:\Git\betfair\venv\lib\site-packages\betfairlightweight\endpoints\racecard.py", line 28, in login`

    `response = session.get(self.login_url)`

  `File "E:\Git\betfair\venv\lib\site-packages\requests\api.py", line 75, in get`

    `return request('get', url, params=params, **kwargs)`

  `File "E:\Git\betfair\venv\lib\site-packages\requests\api.py", line 61, in request`

    `return session.request(method=method, url=url, **kwargs)`

  `File "E:\Git\betfair\venv\lib\site-packages\requests\sessions.py", line 529, in request`

    `resp = self.send(prep, **send_kwargs)`

  `File "E:\Git\betfair\venv\lib\site-packages\requests\sessions.py", line 645, in send`

    `r = adapter.send(request, **kwargs)`

  `File "E:\Git\betfair\venv\lib\site-packages\requests\adapters.py", line 517, in send`

    `raise SSLError(e, request=request)`

`requests.exceptions.SSLError: HTTPSConnectionPool(host='[http://www.betfair.com|www.betfair.com](http://www.betfair.com|www.betfair.com)', port=443): Max retries exceeded with url: /exchange/plus/ (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:997)')))`



`During handling of the above exception, another exception occurred:`



`Traceback (most recent call last):`

  `File "E:\Git\betfair\dailyracevenuedatagrabber.py", line 34, in &lt;module&gt;`

    `trading.race_card.login()`

  `File "E:\Git\betfair\venv\lib\site-packages\betfairlightweight\endpoints\racecard.py", line 30, in login`

    `raise APIError(None, self.login_url, None, e)`

`betfairlightweight.exceptions.APIError: [https://www.betfair.com/exchange/plus/](https://www.betfair.com/exchange/plus/)` 

`Params: None` 

`Exception: HTTPSConnectionPool(host='[http://www.betfair.com|www.betfair.com](http://www.betfair.com|www.betfair.com)', port=443): Max retries exceeded with url: /exchange/plus/ (Caused by SSLError(SSLError(1, '[SSL: WRONG_VERSION_NUMBER] wrong version number (_ssl.c:997)')))`

Please note that my certificate setup and credentials are correct as I am able to use other API methods like `trading.betting.list_market_catalogue` . What could be the reason of this exception for this particular endpoint?

---

## 2022-10-24 19:59:59 - issues channel

**Beeblebrox**

I'm paper trading a strat on a TOTAL_POINTS_LINE market, but I'm getting an ORDER_VALIDATION error:



```{"asctime": "2022-10-24 18:48:19,823", "name": "flumine.controls", "levelname": "WARNING", "message": "Order has violated: ORDER_VALIDATION Error: Order price is not valid", "control": "ORDER_VALIDATION", "error": "Order price is not valid", "order": {"market_id": "1.205496414", "selection_id": 105545, "handicap": 0, "id": "138859300988413659", "customer_order_ref": "354b9995c9ce8-138859300988413659", "bet_id": null, "date_time_created": "2022-10-24 18:48:18.842366", "publish_time": null, "market_version": null, "async": null, "trade": {"id": "6be453dd-53cc-11ed-a6ba-001b213a8a25", "strategy": "NBABackOvers", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138859300988413659"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 233.0, "size": 10, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 10.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": "Order has violated: ORDER_VALIDATION Error: Order price is not valid", "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}}```

In these markets the back prices are the unders value and the lay prices are the overs value. The actual price is always 2.0.  So for the example above, I tried to place a bet on the overs at 233.0, but this trips up the order validation stuff here: [https://github.com/betcode-org/flumine/blob/b59a077e5e3d09e5042de5438677782e4be330f8/flumine/controls/tradingcontrols.py#L53](https://github.com/betcode-org/flumine/blob/b59a077e5e3d09e5042de5438677782e4be330f8/flumine/controls/tradingcontrols.py#L53)

---

## 2022-10-23 16:27:45 - betconnect channel

**Artiom Giz**

Hi people!

I have a question about Streaming API messages rates, can't find an answer in the forum/docs.

Currently I use a "*delayed*" key and MCMs are received once a minute.

If I move to "*live*" key, will I be able to receive MCMs "*tick by tick*"?

I saw in the forum that there is limitation of 150/sec - so this will be the frequency?

*In general, what is the best option to be able to receive updates (MCM) in highest possible way?*

Thanks!

---

## 2022-10-22 09:44:51 - random channel

**Mo**

Also package everything and have never run in to this problem. But then I deploy by git cloning and pip install -e .

---

## 2022-10-07 23:13:52 - general channel

**birchy**

[@U042PP6NAHM](@U042PP6NAHM) I'm going to assume you're using Flumine...what I would do is use the `market.context` dict to save the price of each runner on each update while inplay=False, then retrieve those prices in `process_closed_market()`



`class ClosingPriceStrategy(BaseStrategy):



    def check_market_book(self, market, market_book):

        if market_book.inplay: return

        return True



    def process_market_book(self, market, market_book):

        market.context['closing_prices'] = {

            r.selection_id: r.last_price_traded

            for r in market_book.runners

            if r.last_price_traded

        }



    def process_closed_market(self, market, market_book):

        # save to csv or whatever...

        for k, v in market.context['closing_prices'].items():

            print(k, v)`



This can be used for either backtesting or live

---

## 2022-10-07 20:09:37 - general channel

**Faye**

Could anyone tell me how to calculate / save the final price, please? I want it for greyhounds to help me see if I'm getting value. Its not live, it's back testing.

---

## 2022-10-02 09:56:16 - general channel

**Alessio**

If you want to speed it up i think aws batch/lambda and equivalents are a reasonable route for "pay-as-you-go"

---

## 2022-10-01 22:33:12 - general channel

**thambie1**

That being said, running on many machines at once ain't such a bad idea :wink:. Would take some effort and money (assuming paying for AWS/GCP)

---

## 2022-10-01 21:19:27 - issues channel

**Aaron Smith**

today i m getting flooded with these bois today:

```"WARNING", "message": "[MarketStream: 2001]: Latency high: 0.5774619579315186"```

I get it correctly that these have nothing to do with code execution and all to do with connection to the betfair server?

Is anyone else experiencing this or is this only on my end?

---

## 2022-09-28 22:56:09 - random channel

**foxwood**

I think [@UBS7QANF3](@UBS7QANF3) is the resident wizard on this stuff - he posted a link recently to a free pdf of a textbook well worth reading - worth searching for. Think whether all the features are useful - if they are not really relevant to the prediction then leave them out since it can mislead the training. Categories can help things along if they represent very different things eg NH racing versus flat. Beware of including any data that would be unknown to your program at the time of running it live eg don't include BSP if it is a pre-race model. Maybe try a binary classification model if you just want yes/no. Worst thing I ever did was leave the result in the data set - training was superb at over 95% :rolling_on_the_floor_laughing:. Correlation matrix might throw some light on meaningful relationships in the data. No easy fixes that I know of unfortunately - just have to  keep at it refining and thinking - rinse and repeat.

---

## 2022-09-28 17:35:08 - general channel

**Mick**

I just want to confirm something betfair's "cash out" button in relation to horses. I had assumed that the cashout process should equalise what happens if your your horse wins or loses. But now AFAICT what it does is it equalises what happens while totally ignoring the effects of rule4 deductions. So if a good horse withdraws at some point between the original back bet and the subsequent cashout, you can be left with a position which is very lopsided with regard winning vs losing. Can anyone confirm this is so?

---

## 2022-09-23 10:17:31 - general channel

**PeterLe**

I was just checking my P&amp;L's this morning and noticed that one particular flumine strat on AWS Ubuntu had done well over the last 30 days or so...so i thought Id log into to check all looked ok disk space etc...

Everything looked good except I noticed that i had run that particular program via Pycharm rather than standalone...:grinning::man-facepalming:

So now im thinking, whether to just leave it running or stop it and run standalone... (I appreciate that the former is not good practice and maybe slower, but what would you do?) Thanks

---

## 2022-09-19 20:57:38 - general channel

**Mo**

1. We've had a lively debate about this before but in my very humble opinion it is not correct to store the market ID as a string. The "1." prefix is a throwback to when there was a separate Australian exchange where the markets had a "2." prefix. As this is no longer the case, it is significantly better to encode market IDs as the integer after dropping the prefix

2. Yes will update it when I have a chance. Poke me if I forget

---

## 2022-09-18 20:10:56 - random channel

**Newbie99**

Only talking about LIVE here, just picking random markets as an example, perhaps more realistically if we look at football, lets say we have a single Flumine instance and the market types are:



MATCH_ODDS

OVER_UNDER_25

OVER_UNDER_15

OVER_UNDER_05



That could be circa 4000 markets some days, so I'm just wondering if there is a preferred way to structure that, i.e. 1 connection with all 4, or perhaps 2 connections with 2 market types each. Or does it simply not matter as you're dealing with 4000 markets either way (appreciate its not always that many, just picking a big number as an example in case it makes a difference)

---

## 2022-09-18 18:13:14 - random channel

**Peter**

Are we talking backtesting, in which case, connections aren't relevant, or live processing, in which case you wouldn't get up to 2000 each for WIN or PLACE markets?



If you're talking live processing then running separate instances of Flumine to achieve separate connections means that you won't be able to link different markets from the same event as they won't be on the same instance.

---

## 2022-09-18 10:16:19 - random channel

**Newbie99**

Sorry just 1 more question on the subject, as it has got me (and I believe a couple of others too) in a bit of a muddle.



Ignoring back-testing for a second and only considering LIVE, when (and why) is it appropriate to use middleware instead of a worker?



I'm thinking of use cases along the following lines:



a) Performing calculations across multiple market types (e.g. WIN / PLACE or OVER_UNDER_X / OVER_UNDER_Y)

b) Receiving live score data from a 3rd party provider and performing some sort of operations on this

---

## 2022-09-17 19:37:46 - random channel

**Newbie99**

I can probably re-write what I need in a single thread in all honesty for this one, I've got into the habit of using workers as in live otherwise I end up with a lot of latency warnings (probably not for this one as it happens, its not very CPU intensive).

---

## 2022-09-17 16:41:30 - issues channel

**foxwood**

I save stats for a market in `process_closed_market`which works fine for backtest and live. In paper trade however many of the bets are doubled up. I think this is due to the blotter never being cleared and BF reopening and closing the market - sometimes even a few hours later - causing the closed market function to be called again. Is there a safe / proper way to wipe the blotter for a market  after I've saved the stats without crashing flumine which I expect would happen somewhere if I just zapped it with

```if self.client.paper_trade is True : market.blotter = None```



---

## 2022-09-16 21:05:55 - random channel

**Unknown**

What sort of CPU utilisations do people have on their servers? This is mine. I'm wondering if it's likely to be a problem or not.



It's a machine with 2 CPU cores and 4GB RAM.

---

## 2022-09-16 15:29:27 - general channel

**EJono**

Is there a problem with calling "trading.keep_alive()" too frequently? Would doing so too many times in a given minute cause betfair to suspend or restrict an account?

---

## 2022-09-14 09:41:14 - random channel

**liam**

pandas is crap for trading live

---

## 2022-09-14 09:26:07 - random channel

**Jorge**

Hi, I'm facing this problem: I need to load 600 files in live-trading, each of them contains a row of a DF, and concat them into a Pandas DataFrame for analysis. Anyone knows what would be a fast file format to do this? I've tried with .parquet which takes 0.03 secs/file but I need something faster...

---

## 2022-09-12 12:51:44 - betconnect channel

**Mauricio Garcia**

Hi guys,



Thanks for your votes, interest and the questions you have sent over to us far. We’ll be postponing the live stream session to next week. We’ll keep you posted with the confirmed dates ASAP!

---

## 2022-09-09 10:23:15 - issues channel

**Jimmy**

Hi all — seeing the odd “Connection closed by server” error in my logs. Can’t find much around why it might occur online — anyone any tips/suggestions? :slightly_smiling_face:

---

## 2022-09-07 13:55:30 - issues channel

**Aaron Smith**

I use flumine, which i think compares local clock versus publish time. Its a linux ec2-instance that its running on (as if i recall correctly windows has a less precise local time?). This could be possible, how would i go about confirming/rejecting this thesis?

---

## 2022-09-06 12:28:04 - general channel

**Newbie99**

Shouldn't matter for live, but just for ease of use I keep everything consistent (the beauty of flumine for backtesting is that it allows you to do that)

---

## 2022-09-06 12:18:00 - general channel

**Jimmy**

will try - question - given this is in the strategy, does it matter for live?

---

## 2022-09-04 21:46:23 - random channel

**foxwood**

That motivated me to close the AWS account that I never use !

---

## 2022-09-04 14:10:48 - random channel

**Aaron Smith**

Had a friend who forgot to turn somthing off and woke up to a 5k bill. He contacted AWS and they ended up not charging anything. He had to give some written explanation on how he is going to prevent this in the future. Looks like you can screw up once and be forgiven, but maybe shouldnt do it much more than that :smile:

---

## 2022-09-04 12:58:08 - random channel

**Graham**

[@UFTBRB3F1](@UFTBRB3F1) I was reading a few of those recently too. But not hacked, just mismanaged - mainly from unknowing startups. 



The billing thresholds don’t actually do anything aside from alerting. But, AWS and GCP have a good track record of forgiving such accounts.



It’s one of my biggest bug bears of them, the pricing is quite difficult to figure out.

---

## 2022-09-04 12:18:59 - random channel

**Newbie99**

This Reddit thread (about AWS costs supposedly from a compromised account) are quite scary!!!



[https://www.reddit.com/r/aws/comments/x03vay/hacked_aws_account_is_facing_200000_in_charges/](https://www.reddit.com/r/aws/comments/x03vay/hacked_aws_account_is_facing_200000_in_charges/)

---

## 2022-09-03 11:25:56 - random channel

**Newbie99**

Ah, so it was quite simple, that worked replacing 0.0.0.0 with the private IP from the AWS console dashboard!

---

## 2022-09-03 09:22:38 - random channel

**Newbie99**

The default output as you say is 127.0.0.1:4000 (or localhost:4000). However I have an elastic IP setup on AWS, which normally I send data to via Flask as follows:



```socketio.run(app, debug=False, host='0.0.0.0', port=PORT_NO)```

I can then view the output as expected from my app.



However (obviously not running my flask app at this point) when I put in the output as 0.0.0.0 and any port from cprofilev the page doesn't exist.



So I'm assuming I'm missing something simple.

---

## 2022-09-03 09:15:47 - random channel

**foxwood**

To me that says you are sending the output to IP address 0.0.0.0. If you want to send it to the local machine (ie the machine it is running on) try the loopback address 127.0.0.1 (which is the default if you omit the IP according to docs) and for remote use your current router external IP - if you have a local network then redirect incoming from EC2 IP at router to specific PC.

---

## 2022-09-02 22:02:04 - random channel

**Newbie99**

...and on the efficiency note, is anyone well versed in cprofilev as I was struggling to get the output via a browser window so I can view in real time?



On a local machine it works fine, but when I try to run it on my EC2 instance, I can't seem to route the output to an my elastic ip address, I assumed it would be:



```python3 -m cprofilev -a 0.0.0.0 startup.py```

and that works correctly in the sense that it runs, but I can't see the output anywhere.

---

## 2022-09-02 19:10:39 - random channel

**Newbie99**

With regards to balancing EC2 instances / Betfair connections &amp; markets, is there a consensus on the best way to split things up?



Currently I just stick everything on a single T3.micro and in the last few weeks I've been getting a lot of latency warnings, to the point where at the weekends it just stops functioning.



This co-insides with me looking at far more football markets than before and whilst of course my code will be inefficient and there will absolutely be improvements, in terms of a sensible approach to grouping, what is the optimal way?



Currently it feels as though a single instance is fine with possibly somewhere between 2-3k markets (across horse racing and football), but above this it struggles.



So obviously I need to look for bottlenecks in my code, but aside from that, should I create a separate EC2 instance or should I split the football markets up across multiple connections etc?

---

## 2022-08-30 18:42:49 - general channel

**Trex44**

I can't get Tkinter to work on EC2 [@U9JHLMZB4](@U9JHLMZB4). I read that Tkinter requires a display so wont work on EC2 as there is no display. That's what prompted me to consider building the display on the client side (where I have a screen) and having the host handle everything else.

---

## 2022-08-30 18:39:17 - general channel

**Trex44**

Almost certainly am overcomplicating it. The files are currently CSV files that are the results from a load of computation done on different races. I am concatenating them into a single pandas data frame then I am basically just using the GUI to filter the results and display various Seaborn graphs.  I am not looking to keep the results permanently though so haven't considered Amazon RDS or anything like that. The CSV's are deleted eventually.



Just wanted the filtering to be done server side then a dictionary to be sent back to the client. Trying to teach my self how apps are built/function whilst doing this. That's half the fun.

---

## 2022-08-30 11:20:30 - general channel

**Peter**

Also, since when was an EC2 instance unable to display a GUI? Do you mean maybe that you can't run desktop apps on it?

---

## 2022-08-30 10:15:01 - general channel

**Mo**

Dublin. AWS Dublin

---

## 2022-08-30 09:57:43 - general channel

**Guy Adini**

Hi guys, and apologies if this has been asked a million times before: I'm new and don't know where to find the FAQ :slightly_smiling_face:

Could someone please tell me where are the Betfair servers physically located, and where to put a server in order to get the lowest possible latency to them?

---

## 2022-08-29 22:36:29 - general channel

**Unknown**

[@U03N4QBJ0TV](@U03N4QBJ0TV) I think there are lots of demos of this kind of application (calculation on the server, view results on the local PC). The 2 example patterns that come to mind are:

1. Publish the output as a web page or web app on the EC2 server and view the results/graphics with a browser;

2. Publish the output to a database on AWS (or to S3) and have the local PC GUI query that.



---

## 2022-08-29 21:13:21 - general channel

**Trex44**

I don't want to transfer files as such. Just data. So the idea is the GUI sends a JSON/DICT to EC2. EC2 executes a command based on the JSON/DICT then returns the ouput to the local host. I can make it work via file transfers if needed though.

---

## 2022-08-29 21:10:33 - general channel

**Newbie99**

That's all you need to transfer files (tbh I haven't tried too much complex stuff, I just use this to grab files from AWS and copy them to my local machine)

---

## 2022-08-29 20:14:45 - general channel

**Trex44**

Thanks Newbie99. It might. I am currently exploring Paramiko having spent the day exploring boto3 and sockets but with no luck.



 I am essentially just building an app and deploying it to cloud then having a client log in. Thought there would be loads of demos of how to do this. If there is anyone here who has actually deployed an app to the cloud for commercial purposes before then would be great to hear how this is done.

---

## 2022-08-29 19:42:41 - general channel

**Trex44**

Still haven't figured out how to get a local python script to connect to my EC2 environment though even though I can easily get my IDE and WinSCP to do so.

---

## 2022-08-29 19:40:52 - general channel

**Trex44**

I am building a GUI to sit on top of a script I have built for analysing data. Getting fed up of changing the filters in the code manually each time. However there is no way an EC2 environment can display a GUI. I want to adapt the script (which sits on my EC2 instance) to accept an input sent from the GUI on my local PC (the client), do the compute on the EC2 environment then return the output data back to my local PC where the GUI can make use of it.   I used WinSCP for one off downloads but that's too slow. I want to be able to move faster.

---

## 2022-08-29 19:21:18 - general channel

**Unknown**

It depends a lot on what you are doing and how you are doing it...



There are a lot of ways to connect to EC2.



One solution is to have the EC2 script write the data to an S3 bucket; you can then access the S3 bucket from your local PC.



For one-off downloads I use FileZilla to grab data from EC2.

---

## 2022-08-29 15:06:37 - general channel

**Trex44**

Guys can any AWS users direct me to a video/doc that will show me how to connect to EC2 remotely and receive data from scripts executed on the EC2 environment back to an application running on  my local PC? keep running into issues trying to do this via boto3 and I figure its not the most secure way to go about it. Wondered if the sockets package was the correct route to go down?

---

## 2022-08-26 13:04:58 - general channel

**Peter**

Don't use PyCharm, but it could be that when running in the console you're using the version of numpy that's native to the M1 architecture, but when running through PyCharm, it's expecting a version that's Intel-compatible.



I ran into a not dissimilar problem and documented it on [https://stackoverflow.com/questions/65942641/docker-image-built-on-mac-osx-wont-run-on-aws-ec2-instance/65952339#65952339|Stackoverflow](https://stackoverflow.com/questions/65942641/docker-image-built-on-mac-osx-wont-run-on-aws-ec2-instance/65952339#65952339|Stackoverflow).

---

## 2022-08-26 09:12:40 - issues channel

**Johnny Boston**

I have containers running on ECS. When I deploy a new strategy, I pull down the container and start a new one. That's what I call a deployment.

---

## 2022-08-26 09:07:51 - issues channel

**liam**

pipe to aws cloudwatch and forgot about it till you need them

---

## 2022-08-26 09:01:42 - issues channel

**liam**

What do you mean code deployments? They never last that long once you stop reading off them

---

## 2022-08-26 08:29:05 - issues channel

**Johnny Boston**

Thank you [@U9JHLMZB4](@U9JHLMZB4). Seems as though I'm in between a rock and a hard place then! I'll reach out to betfair. Would be great if they had an endpoint to query your active connection ids and close down the ones you no longer need. I should have 4 live ones but due to code deployments it looks like I have over 20!

---

## 2022-08-25 21:18:38 - general channel

**Peter**

Not really. The constraint is the size of the internal network pipe at AWS, which no external software can influence. Best package is almost certainly boto3, which I'm sure AWS will have optimised for speed in this area as it's in their interest too to keep these pipes clear.



So the only practical way to optimise the speed is to ensure that the S3 bucket and EC2 instance are in the same region.



However, I've never found the streaming speed from S3 to be a constraint. Profiling and streamlining my code is usually a much more productive use of my time.

---

## 2022-08-25 20:57:14 - general channel

**Trex44**

Is there a way to speed up/optomise the downloading of files from S3 to EC2 does anyone know? A python library perhaps or some hardware tweak.

---

## 2022-08-24 21:24:31 - strategies channel

**Newbie99**

Its not the same thing though is it...no one on here for example (I assume) will stop when they hit a random p&amp;l figure, but bankroll management is a legitimate way to stay in business!



To take an extreme example, you could build the perfect model that always pinpoints maximum value and executes perfectly, but there's still no guarantee the runner will win, hence you need to live to fight another day.



The difference is that for the bigger players on here maximum market liquidity is reached well before any risk tolerance becomes a consideration.

---

## 2022-08-24 19:52:36 - random channel

**VT**

Last month I lost a considerable amount for being 'stuck' in suspended markets in football, this is so recurrent that I had to create a specific category to record these losses in my spreadsheet.



Since yesterday the problem has gotten even worse, they are suspending many times for no reason, something like 20, 30 suspensions in a game, most of them short but sometimes it takes more than 1 minute to return.



An obvious option would be to keep money on Pinnacle to close positions there, but it's not that simple because some bets are against time, lay at high odds, etc... and also because Pinnacle removes events from its grid live when they want, limit markets, anyway, this has happened frequently and it's not a matter of 'scout information' why bookmakers (bet365, WilliamHill) keep the bets open.

---

## 2022-08-23 08:16:45 - random channel

**liam**

My general approach for new strategy development:



• Record raw data (s3)

• Process data to create signals etc. into csv's as and when, I use Strategy classes rather than loggingcontrols (quicker / more data available) through slimmed down flumine simulation (multiprocessing / [speed])

• Load into jupyter using pandas

• Join / process / create more signals if required

• Analyse / model / backtest theoretical pnl / lots of graphs

• Create real strategy / simulate / optimise stake / triggers for market capacity

• Go live

Start to finish is normally a few hours before I get bored :joy:

---

## 2022-08-22 18:01:06 - random channel

**PeterLe**

So a question to the pro’s please

So once you have your data (in sample and out of sample etc)..its useful to tweak existing strategies etc I’ve used this lately with some good success (even found a brand new strategy too)



Question is: What is the next step in using the recorded data, do you collect additional data via logging controls and analyse with pandas/db etc

I just wondered what steps you would take from recording the data through to going live with a new strategy (just bullet points and a few pointers would be great for someone new to this approach)

(By the way; I read an entry by [@UGV299K6H](@UGV299K6H) a week or so ago about testing a well thought out hypothesis live and seeing how it plays out. it really struck a chord with me. This tends to be my approach too. Ill still continue with this but looking at alternative approaches too)

Many thanks

---

## 2022-08-21 18:13:37 - strategies channel

**birchy**

I have a similar graph for a live dogs strategy that goes up steeply, then goes flat for a few weeks and then goes up again. So I'd say it _might_ be a goer but you'll need a bigger sample size (i.e. 6+ months).

Just noticed your graph covers 4 months, so I'd say that 2200 bets in that time period is nowhere near enough.

---

## 2022-08-21 09:39:48 - strategies channel

**Mo**

To be clear I am live trading the strategy just backing



Whether to just move on is an interesting question of how to prioritise the infinite possibilities of what to work on. And perhaps more importantly, whether trying to solve the mystery of the poor laying performance is mainly just an intellectual exercise rather than a profitable one

---

## 2022-08-20 21:14:26 - random channel

**Johnny Boston**

`aws s3 sync` does what you want too

---

## 2022-08-19 16:10:16 - issues channel

**EJono**

im not sure why but for some reason im no longer seeing any markets in flumine.markets when i execute framework.run() anymore. This is the strategy set up i have been using that has been bringing through football succesfuly for the past few months now. Any idea why all of a sudden no markets are coming through on the stream? Last time i succesfully saw things working as they should be was wednesday last week.

```strategy = Strategy(

    max_order_exposure=5,

    max_live_trade_count=5,

    max_selection_exposure=5,

    market_filter=streaming_market_filter(

        event_type_ids=["1"],

        country_codes=country_codes,

        market_types=["MATCH_ODDS"],

    ),

    name=STRAT_NAME,

    context={

        'max_liability': LIABILITY

    }

)```

Im using

```flumine==2.2.3

betfairlightweight==2.16.6```



---

## 2022-08-18 09:18:36 - random channel

**Mo**

Easier to update individual components yeah but I still use the market recorder to scrape the price stream so I face this problem too



I agree with [@U016TGY3676](@U016TGY3676) if it was essential you had no gaps then I think the only solution is you run 2 instances and stagger the deployment of updates



I sort of follow the policy of buying the historic data for the sports I trade so the scraped data acts as something available for immediate analysis while the historic data is - I hope - a cleaner and more comprehensive data set when that is needed. However, as I'm sure you can guess or know the historic data is far from perfect



Given that there's huge overlap in what we're all scraping I think the ideal would be some kind of data sharing but that's a very gray area

---

## 2022-08-17 15:17:57 - general channel

**Peter**

Can't access it at all. The webpage is a Cloudflare 522 error page saying that it although it can connect to the [http://historicdata.betfair.com|historicdata.betfair.com](http://historicdata.betfair.com|historicdata.betfair.com) server, requests aren't being completed i.e. they're timing out.

---

## 2022-08-17 15:12:32 - random channel

**Peter**

[@U012XEYK2FJ](@U012XEYK2FJ) I had a similar message too, though it turned out to be a networking issue interrupting external access EC2 instance that they resolved without them or me having to touch the affected server, and there was no interruption to my data collection.

---

## 2022-08-17 14:04:24 - random channel

**user34**

I recently received an email from AWS about one of my EC2s failing, so I had to manually stop and restart it. Apparently if I'd had Auto Recovery or Auto Scaling this would have been handled automatically, but I guess the current data might still have been lost.

---

## 2022-08-17 12:15:48 - random channel

**liam**

I don't have problems with AWS EC2

---

## 2022-08-17 11:57:16 - random channel

**river_shah**

For long running tasks such as market recorders, do you guys ever experience outages or does AWS /cloud platform of choice handle live migration gracefully and flumine keeps running?

---

## 2022-08-15 17:08:59 - betfair-news channel

**Unknown**

A few days ago I posted on "issues" about a question I have in relation to the values ​​provided by `tradedVolume`, because I want to follow the matches, seeing the movement of the market with each request I make, easily indicating how much in *Back* was transacted between the requests in how much in *Lay* it took to match.



[@U9JHLMZB4](@U9JHLMZB4) helped me to better understand the calculations but I decided to put a complete example so that if there is still an error in what I'm doing, it's easier to see the problem.



So an example without real values ​​(from odds 1.20) from this week's Liverpool game:



```[{"jsonrpc": "2.0", "method": "SportsAPING/v1.0/listRunnerBook", "params": {"marketId":"1.200236669","selectionId":"56323","priceProjection":{ "priceData":["EX_TRADED"]}}, "id": 1}]```

*Request 1:*

`"tradedVolume":[{"price":1.2,"size":100}]`

Back liability: 100

Lay liability: 20

Total Matched: 100

Total Money Transacted on the Betfair System: 120



*Request 2:*

`"tradedVolume":[{"price":1.2,"size":300}]`

Back liability: 300

Lay liability: 60

Total Matched: 300

Total Money Transacted on the Betfair System: 360



*Request 3:*

`"tradedVolume":[{"price":1.2,"size":800}]`

Back liability: 800

Lay liability: 160

Total Matched: 800

Total Money Transacted on the Betfair System: 960

`------------------------------------------------------------`

So, let's say I want to register on the chart how much liability we had between:



_*Request 1 and 2:*_

Back liability: 200

Lay liability: 40

Total Money Transacted on the Betfair System: 240



_*Request 2 and 3:*_

Back liability: 500

Lay liability: 100

Total Money Transacted on the Betfair System: 600



Now is the view I want to have of the separate values ​​correct?



I understand that it is a simplistic view because there are more details that must be taken into account (such as, for example, there is no way to know if it was a single transaction that corresponded to the entire amount or if there were multiple ones) but I want to create a follow-up just to help me to visualize in more detail the financial movement of the market in each odds.



Otherwise, I want to create a real-time chart to see how much money traders will lose if their investment result is 100% negative.

---

## 2022-08-15 10:06:50 - general channel

**PeterLe**

Thanks Foxwood. Ive been dragging the files down from the virtual server for backtesting locally but I may as well just add some extra disk space on AWS. its not that expensive in the grand scheme of things.

Thanks for the reply :+1:

---

## 2022-08-13 13:02:19 - betfair-news channel

**D C**

I've got some raw nodejs scripts in  git repo [@U03TJKFLE8K](@U03TJKFLE8K) . It is not in package form but if you are good with JS then you might be able to use it as a base and improve it. My JS works but its quite rough. Not what you would call "production" standard. DM me if you want a link

---

## 2022-08-11 20:46:32 - strategies channel

**liam**

Manually based off simulations or what the live data was saying 

---

## 2022-08-11 19:17:56 - strategies channel

**Dario Scardina**

```trading = betfairlightweight.APIClient(username=my_username,

                                           password=my_password,

                                           app_key=app_key,

                                           cert_files=(cert, key),

                                           locale="italy",

                                           lightweight=False

                                           )



trading.login()



competitions = trading.betting.list_competitions(

    filter=betfairlightweight.filters.market_filter(

    event_type_ids=[1],  # Soccer's event type id is 1

    in_play_only=True # In play only

    )

)



live_events = trading.betting.list_events(betfairlightweight.filters.market_filter(

    event_type_ids=[1],  # Soccer's event type id is 1

    in_play_only=True,  # in play

    competition_ids=[competition.competition.id for competition in competitions], # get events only for competitions selected

    ))



strategy = ExampleStrategy(

    name="BackUnderSH",

    market_filter=streaming_market_filter(

        event_type_ids=["1"],

        event_ids=[live_event.event.id for live_event in live_events],

        market_types=["OVER_UNDER_05", "OVER_UNDER_15", "OVER_UNDER_25", "OVER_UNDER_35", "OVER_UNDER_45",

                           "OVER_UNDER_55", "OVER_UNDER_65"],

    )

)```

[@U02QDLFCYHG](@U02QDLFCYHG) maybe that could help... Works for me to consider only live events for soccer competitions.

---

## 2022-08-11 19:03:48 - strategies channel

**birchy**

For those of you with profitable strategies, how often do you all have to carry out maintenance or "tweak" settings/signals/etc? Personally I have one particular strategy that was my first live one with Flumine that's been untouched for ~2 years now. It's nothing adventurous but has been slowly ticking along and in recent months has performed exceptionally well. No doubt there will be an equal downturn at some point....

---

## 2022-08-10 14:06:34 - betfair-news channel

**Brøndby IF**

Good morning everyone (good afternoon to the people of Europe)!



A few days ago I asked about keeping my code running on a server in Ireland and I live in Brazil, if that had a problem ([@U80AMMRKP](@U80AMMRKP) even commented to me that there were no problems). This morning I woke up with several access errors on the server and when I went to access the account manually, it asked me to change my password.



I contacted Betfair chat and they told me that there was an unauthorized IP access in Ireland. I argued that this IP and this access I knew, that it was from the server that I kept my code executing the access to the Betfair API.



At that moment the attendant started asking if I worked with Betfair, asked how I didn't know about the other emails or access to Betfair, told me he couldn't help, gave me a contact email and ended the service without even finishing answering my questions.



Has anyone ever experienced this? The attendant talked to me like I was a hacker or something, doubting everything I said.



Is it normal to have the password changed from time to time if we have a server in a different country to which we reside?

---

## 2022-08-08 09:58:12 - issues channel

**liam**

live no problem, the issue is when simulating

---

## 2022-08-08 09:49:33 - betconnect channel

**Peter**

Mine may be different as I was an early API user, but when I access the production API endpoint, my URI is in the form [https://xxxxx.betconnect.com](https://xxxxx.betconnect.com) and not [https://xxxxxapi.betconnect.com](https://xxxxxapi.betconnect.com)

---

## 2022-08-08 08:34:50 - betconnect channel

**rob smith**

Thanks Liam. That throws a different error:

`Traceback (most recent call last):`

  `File "C:/Users/James/PycharmProjects/betConnect/betConnectv1.py", line 20, in &lt;module&gt;`

    `personalised_production_url="[https://xxxxxxxxapi.betconnect.com](https://xxxxxxxxapi.betconnect.com)",`

  `File "C:\Users\James\AppData\Local\Programs\Python\Python37\lib\site-packages\betconnect\apiclient.py", line 35, in __init__`

    `personalised_production_url=personalised_production_url,`

  `File "C:\Users\James\AppData\Local\Programs\Python\Python37\lib\site-packages\betconnect\baseclient.py", line 37, in __init__`

    `self._set_endpoint_uris(environment)`

  `File "C:\Users\James\AppData\Local\Programs\Python\Python37\lib\site-packages\betconnect\baseclient.py", line 142, in _set_endpoint_uris`

    `assert self._personalised_production_url[-16:] == ".[http://betconnect.com/|betconnect.com/](http://betconnect.com/|betconnect.com/)"`

`AssertionError`

---

## 2022-08-08 08:31:56 - betconnect channel

**liam**

I believe it should be



`personalised_production_url="[https://xxxxxxxapi.betconnect.com](https://xxxxxxxapi.betconnect.com)",`

---

## 2022-08-08 08:30:06 - betconnect channel

**rob smith**

Here's the code:

`client = APIClient(`

    `username="xxxxxxxxxxx",`

    `password="xxxxxxxxxxx",`

    `api_key="xxxxxxxxxxxxxxx",`

    `environment=Environment.PRODUCTION,`

    `personalised_production_url="[https://xxxxxxxapi.betconnect.com/api/v2](https://xxxxxxxapi.betconnect.com/api/v2)",`

`)`



`assert client.environment == Environment.PRODUCTION`





`# Login`

`login = client.account.login()`

---

## 2022-08-08 08:29:23 - betconnect channel

**rob smith**

`I'm having a first crack with Bet Connect but have run into this error:`

`Traceback (most recent call last):`

  `File "C:/Users/James/PycharmProjects/betConnect/betConnectv1.py", line 20, in &lt;module&gt;`

    `personalised_production_url="[https://xxxxxxxapi.betconnect.com/api/v2](https://xxxxxxxapi.betconnect.com/api/v2)",`

  `File "C:\Users\James\AppData\Local\Programs\Python\Python37\lib\site-packages\betconnect\apiclient.py", line 35, in __init__`

    `personalised_production_url=personalised_production_url,`

  `File "C:\Users\James\AppData\Local\Programs\Python\Python37\lib\site-packages\betconnect\baseclient.py", line 37, in __init__`

    `self._set_endpoint_uris(environment)`

  `File "C:\Users\James\AppData\Local\Programs\Python\Python37\lib\site-packages\betconnect\baseclient.py", line 142, in _set_endpoint_uris`

    `assert self._personalised_production_url[-16:] == ".[http://betconnect.com/|betconnect.com/](http://betconnect.com/|betconnect.com/)"`

`AssertionError`

---

## 2022-08-06 22:25:24 - general channel

**Trex44**

I don't know unfortunately. How would I check?



 Its my own python code not Flumine. It does what I want it to so that's great but its taking me 8 hours on a 8cpu machine to work through 3000 files. Code basically performs a bunch of calculations and extracts a few time stamps and relevant data for each runner in each race. I actually have no frame of refence to know if 8 hours is  good or bad. Only been coding on and off for two years and AWS for about a year on and off.

---

## 2022-08-06 21:35:16 - general channel

**Trex44**

AWS/computing question if any one has a sec - If I am data mining  using 3000 files on a Volume attached to my EC2 does anyone know why switching from an 8 cpu instance to a 32 cpu instance doesn't result in an decrease in the time taken to process the files? Its the same time per file. I am a bit perplexed, thought increased CPUs would result in faster processing time.

---

## 2022-08-04 11:20:12 - general channel

**Mo**

Minor point but I should add that I output all of this stuff to a directory in my Dropbox so it is technically available from anywhere although perhaps not quite as frictionless as AWS

---

## 2022-08-04 10:34:21 - general channel

**D**

[@U01C12ZEADQ](@U01C12ZEADQ) - agreed, and sqlite is great. Access from everywhere can be useful. For my similar but different use case, I needed to read and update params from one or more ec2.

---

## 2022-08-04 10:16:24 - general channel

**D**

[@UBS7QANF3](@UBS7QANF3) I have a bias towards cloud/serverless over local for most things and find dynamo just really convenient. It might be the lazy option. SQL databases didn't work well for me, was difficult to keep adjusting the structure to match the direction of experiments and variety of parameters as things evolved. [@U01C12ZEADQ](@U01C12ZEADQ)’s point of using psql to store the json makes sense but then I'd have to run a postgres database. Interesting topic!

---

## 2022-08-03 09:51:58 - strategies channel

**liam**

Yes but that strategy is setup for live with the filters 

---

## 2022-08-03 09:49:37 - strategies channel

**liam**

That's a recorder strategy for recording live data 

---

## 2022-08-03 05:01:24 - strategies channel

**Tiago Augusto Ferreira**

Hello. I am try filter my strategy using the competition_ids. But i receive a error SUBSCRIPTION_LIMIT_EXCEEDED. What is the correct form to filter by this field?

`strategy_over = MyStrategy(`

    market_filter=market_filter(competition_ids=[141]),

    name='OVER05HT_MODELO',

    max_trade_count=3,

    max_live_trade_count=3,

    max_order_exposure=12,

    conflate_ms=1000,



)



The error:



2022-08-03 00:57:07 *ERROR*    [MarketStream: 2001]: SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 14995 markets whereas max allowed number was: 200

2022-08-03 00:57:07 INFO     [MarketStream: 2001]: *FAILURE* (9 connections available)

---

## 2022-07-29 16:09:10 - random channel

**PeterLe**

Could someone comment on how I may do this please.

I know BFLW uses the non virtualised prices as deafualt ( eg ex.available_to_back etc)

i tend to use  these prices in live  strategies, but Id like to be able to run some back tests for a particular use case using the virtualised prices.



I can see that there is a parameter set to true for virtualise in the filters, but I think that when this is true it merely receives the virtualise pices too:



```def price_projection(

    price_data: list = None,

    ex_best_offers_overrides: dict = None,

    virtualise: bool = True,

    rollover_stakes: bool = False,```

is there an easy way for me to choose whether to use virtualised or non virtualised prices in my backtesting/simulation?

Thanks in advance

---

## 2022-07-29 15:36:46 - issues channel

**George**

Thanks. I am in AWS Dublin. Could it really be crappy network?

---

## 2022-07-29 12:50:59 - general channel

**WaftyCrancker**

Hi can anyone tell me how I go about getting a live key? for streaming. Thank you.

---

## 2022-07-28 23:49:02 - issues channel

**Brøndby IF**

Quick question guys, my server is in Ireland but I live in Brazil, so API access is in one country and my manual access is in another.

Is there any problem doing this?

---

## 2022-07-27 13:24:33 - issues channel

**birchy**

[@UVB1RFEP5](@UVB1RFEP5) as [@UBS7QANF3](@UBS7QANF3) has said, understanding how Linux works is the biggest barrier but nowadays that's not a particularly steep learning curve. It would probably be worthwhile you playing around with Ubuntu by either installing on your machine as a dual boot or on an old pc/laptop/rasp pi. You can also boot most Linux's as a "live" image that runs off cd/dvd/usb stick without installing anything. They run as a full OS in ram but can be a bit slow as a result, however, it allows you to assess a Linux session without making any permanent changes. Alternatively, you could dive straight into an AWS Lightsail instance and learn how to administer it over SSH as that's what your end goal is. [@UQL0QDEKA](@UQL0QDEKA) has done this recently and said he found it much easier than expected and now has a handful of strategies running remotely.

---

## 2022-07-27 12:07:01 - issues channel

**JFP**

[@UBS7QANF3](@UBS7QANF3) &amp; [@U016TGY3676](@U016TGY3676), is there a lot of changes required to get my windows Flumine scripts to run on Linux?

Originally planned to move to Linux and server, but once I got something up and running, have not put in the time to make the change.

---

## 2022-07-27 10:00:03 - issues channel

**John Foley**

I found that no matter where I moved my server with GCP, I was still getting an IP address based in California. It seems to be a known thing:

[https://stackoverflow.com/questions/46205814/why-do-regional-google-cloud-ip-addresses-all-appear-to-be-in-the-us|https://stackoverflow.com/questions/46205814/why-do-regional-google-cloud-ip-addresses-all-appear-to-be-in-the-us](https://stackoverflow.com/questions/46205814/why-do-regional-google-cloud-ip-addresses-all-appear-to-be-in-the-us|https://stackoverflow.com/questions/46205814/why-do-regional-google-cloud-ip-addresses-all-appear-to-be-in-the-us)



I ended up buying a proxy in Dublin/UK and now route my requests through there instead

---

## 2022-07-25 15:19:18 - general channel

**Peter**

EC2

---

## 2022-07-24 18:15:38 - general channel

**birchy**

I've read somewhere that Lightsail is based on EC2 T2 but that was on S.O. or somewhere like that. I chose it because it's a much simpler preconfigured VPS and that suited me when I first started using AWS.

---

## 2022-07-24 12:48:47 - random channel

**PeterLe**

[@U010GM77S4W](@U010GM77S4W) recently explained how to add more than one strategy to a single instance (thanks madelbot) using this :

import logging

import betfairlightweight

from pythonjsonlogger import jsonlogger



from flumine import Flumine, clients

from flumine.streams.datastream import DataStream

from Strategy1 import Strategy1

from Strategy2 import Strategy2





logger = logging.getLogger()



custom_format = "%(asctime) %(levelname) %(message)"

log_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_handler.setFormatter(formatter)

logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



trading = betfairlightweight.APIClient('username', 'password', app_key='app_key',

					certs='/path/to/certs')

trading.login()



client = clients.BetfairClient(trading)



framework = Flumine(client=client)



markets1 = betfairlightweight.filters.streaming_market_filter(

           event_type_ids=["7"],

           country_codes=["GB", "IE"],

           market_types=["WIN"]

)



markets2 = betfairlightweight.filters.streaming_market_filter(

           event_type_ids=["4339"],

           country_codes=["GB"],

           market_types=["WIN"]

)





strategy1 = Strategy1(

    name="Strategy1",

    market_filter=markets1,

    max_order_exposure=50000000,

    max_selection_exposure=6000000,

    max_trade_count=200,

    max_live_trade_count=1,

    context={"stake": 2},

)



strategy2 = Strategy2(

    name="Strategy2",

    market_filter=markets2,

    max_order_exposure=50000000,

    max_selection_exposure=6000000,

    max_trade_count=2000,

    max_live_trade_count=2,

    context={"stake": 2},

)



framework.add_strategy(strategy)

framework.add_strategy(strategy2)



framework.run()



I assume you can use the same principle using the market recorder to record more than one sport/category?

If so, Ill probably just add one more sport category for now, but is it good practice to add multiple sports. Does it start to slow the system down too much if you add multiple? Would it be reasonable to assume it could handle 3 or 4 categories easily? (Im using ubuntu on AWS 4gb ram, 2 CPU's)

(Can someone remind me of the subscription limit too please?) Thanks in advance

---

## 2022-07-24 10:54:00 - general channel

**PeterLe**

I use Lightsail, windows and Ubuntu. I didn't realise till Birchy mentioned it, that Lightsail is actually based on a EC2 (T Class?) but branded as Lightsail.  Lightsail is a fixed monthly cost and is much cheaper than If you choose an ‘EC2’

---

## 2022-07-24 10:35:58 - general channel

**Mo**

I use EC2 but I've seen Lightsail mentioned a few times on here

---

## 2022-07-23 22:15:47 - general channel

**D**

EC2 

---

## 2022-07-23 20:38:35 - general channel

**Michael**

What cloud services do you guys use for running your bots and storing data? Was just thnking of a EC2 instance

---

## 2022-07-23 10:25:09 - general channel

**PeterLe**

One thing I've found extremely useful is gaining a deeper understanding of the relationship of the key components and execution;  ie matching rates, stake, placement (amongst others) and the very delicate nature of those interactions.

the biggest eye opener for me is that one of the key ingrdients in a very long time running strategy (15 years) 'maybe' isnt that important as i thought it was. In fact Ive stripped it out and started to run a live account from yesterday for comparison.

It will be really useful to see how this perform live.

One of the things that stood out for me when reading Daniel Khanerman's work many years ago is Conjunction Fallacy. (Linda the bank teller). It really struck a chord with me in the realms of what we do. (Check it out of you're not aware). Seems obvious when you read it.

I've always believed that keeping things as simple as possible (occams razor) is a good basis for most things

...anyway on with the testing too much chatting  :grinning:

---

## 2022-07-20 10:19:33 - issues channel

**liam**

This is probably my fault;



*paper trading* is just forward testing, ie. testing on live markets but no real orders being placed



*backtesting* I see as applying a strategy on historical data



*simulation* is accurate play back of historical data as is if it was live using exactly the same code/integration



I changed the name in flumine as I found backtesting to be a bit vague as the simulation in flumine is far more powerful however I accept many will argue backtesting/simulation are the same. I will regularly 'backtest' outside of flumine using csv/pandas/jupyter before coding up properly and simulating in flumine. This allows quicker iterations without the heavy processing 'simulating' requires

---

## 2022-07-19 15:07:15 - betconnect channel

**Mick**

Ok, next problem... in the following code:

`client = APIClient(    username=config("STAGING_BETCONNECT_USERNAME"),`

    `password=config("STAGING_BETCONNECT_PASSWORD"),`    

   `api_key=config("STAGING_BETCONNECT_API_KEY"),` 

   `environment=Environment.STAGING,` 

   `personalised_production_url=config("PRODUCTION_URI"),)`

I think I know what to put for username, password and key, but no idea what to put for "PRODUCTION_URI"

---

## 2022-07-17 20:10:21 - general channel

**thambie1**

That specific change (though in a different language) is on my todo list to speed up my backtest. Doesn't matter much in production though

---

## 2022-07-17 12:30:11 - random channel

**birchy**

Same as EC2. Lightsail IS an EC2 instance.

---

## 2022-07-17 12:29:24 - random channel

**mandelbot**

Improved vs ec2?

---

## 2022-07-17 00:40:01 - issues channel

**Rudeger Jamison**

It's not an ec2 instance [@U016TGY3676](@U016TGY3676), it's a containerised service on ECS.

---

## 2022-07-16 21:51:29 - issues channel

**birchy**

[@UG03WDCHH](@UG03WDCHH) or you could crontab aws-cli cp/mv/sync or whatever

---

## 2022-07-16 20:32:03 - random channel

**birchy**

I chose it because EC2 is/was confusing with (IMO) far too many options. Lightsail works for a simpleton like me. :grinning:

Lightsail is basically a preconfigured, ready to use version of EC2.

---

## 2022-07-16 17:31:29 - random channel

**PeterLe**

By the way; Im on LightSail too. Cheaper than EC2. Do you find any downside using it rather EC2? Seems good enough to me

---

## 2022-07-16 09:15:52 - issues channel

**Rudeger Jamison**

Although each file atm seems to be one markets worth of orders.... I'm not exactly sure. Clutching at straws a bit. I have 4gb of memory and 1 vcpu.

---

## 2022-07-15 17:31:57 - strategies channel

**Aaron Smith**

i could imagine this number dropping in actual production if the market reacts to the new money. Anyways, looks good to me. I do have months like that. Personally i d give it a go and calculate EV each day from production. But thats just me, a more careful type would probs wait some more and collect more data

---

## 2022-07-15 13:55:28 - random channel

**Newbie99**

Can the latest version of Flumine distinguish between types of simulation mode (by which I mean backtesting via historic data vs paper trading)?



The problem I'm trying to solve, is when in simulation (backtesting) use historic score data, but of course when in simulation (paper) use live score data.



I can check whether its in simulation mode, but is it possible to distinguish between the two, or do I need to create something myself?

---

## 2022-07-14 12:32:46 - random channel

**Mo**

1. At a glance the instructions seem reasonable

2. The file is not secure in the sense that if someone could log into your account they could read the file in plain text on disk. This is going to apply to pretty much every solution

3. I've only ever used 2FA on my master so can't comment

Some additional comments:



1. Do you not already have a configuration file for your strategy? If you do then you might as well just add the credentials to that rather than using environment variables

2. I use [https://github.com/fugue/credstash|credstash](https://github.com/fugue/credstash|credstash) for managing secrets in AWS

---

## 2022-07-14 09:02:05 - general channel

**Rudeger Jamison**

ahh ok, I am doing things differently.



My current implementation is



```class DatabaseStream(BaseStream):

    """

    This stream reads from a database and updates the context with the data required

    """



    def __init__(self, sql_path: str, context_key: str, table_name: str, strategy_name_filter: str, *args, **kwargs):

        BaseStream.__init__(self, *args, **kwargs)

        self.sql_path = sql_path

        self.context_key = context_key

        self.table_name = table_name

        self.strategy_name_filter = strategy_name_filter



    def run(self) -&gt; None:

        logger.warning(

            "Starting Stream: {0}".format(self.__class__.__name__),

            extra={"update_interval": self.streaming_timeout, "context_key": self.context_key, "sql_path": self.sql_path},

        )



        while self.is_alive():

            response = self.get_sql(sql_path=self.sql_path, table_name=self.table_name)



            def callback(framework, event):

                for strategy in framework.strategies:

                    if self.strategy_name_filter in strategy._name:

                        if strategy.context:

                            strategy.context.update({self.context_key: event.event})

                        else:

                            strategy.context = {self.context_key: event.event}



            event = CustomEvent(response, callback)

            self.flumine.handler_queue.put(event)

            time.sleep(self.streaming_timeout)  # this is the time to check again```

---

## 2022-07-09 11:15:31 - general channel

**D C**

I'm pretty new to AWS and I just SCP my files to a local store at the end of each day. I looked as S3 but I just saw it as extra costs. What do the people who use S3 consider to be the main upside?

---

## 2022-07-08 23:30:50 - random channel

**LM**

Hey [@UBS7QANF3](@UBS7QANF3). Thanks for the great list of podcasts. I've just listened to the TPD episode you mentioned. Do you know whether betfair provides live TPD data?

Alternatively do you know if historical TPD data can be sourced to model before buying a subscription?

---

## 2022-07-08 17:23:26 - general channel

**Trex44**

As we are doing AWS questions - I currently copy market files from S3 on to my EC2 instance where I back test them. I wondered is there a way to stream direct from S3 without copying the file over? I don't have any problems with the way I currently do it but just wondered if there was a better practice/way it should be done.

---

## 2022-07-08 15:38:04 - random channel

**Mo**

The absolute pinnacle is [https://twitter.com/halfkelly|Risk of Ruin](https://twitter.com/halfkelly|Risk of Ruin). It's described as "Audio documentaries about gambling" and that sums it up perfectly. If I'm nitpicking, the focus can be a bit too heavily on casino advantage play but I've enjoyed every episode nonetheless. Top pick of episode would be "A Seemingly Obvious Idea" featuring Andew Black, one of the co-founders of Betfair



Also fascinating but sadly no longer in production is Matthew Trenhaile's [https://podcast.app/inside-betting-p536238/|Inside Betting](https://podcast.app/inside-betting-p536238/|Inside Betting). I could happily listen to Matthew speak about gambling all day long. Everyone who has ever complained about Betfair Premium Charge should start by listening to Episode 14 "Betting Exchange Premium Charges"



One of the original podcasts is the [https://podcasts.apple.com/gb/podcast/business-of-betting-with-jason-trost/id1223927895|Business of Betting](https://podcasts.apple.com/gb/podcast/business-of-betting-with-jason-trost/id1223927895|Business of Betting). A big word of warning is that it has recently been taken over by Jason Trost, CEO of Smarkets who - in my very humble opinion - is a complete nob. However, don't let that put you off exploring the excellent back catalogue. There are probably too many must listen episodes to mention but to name a few: both episodes (26 and 127) with Marco Blume, director of trading at Pinnacle, episode 107 with Will Duff Gordon, the CEO of TPD - very relevant to anyone doing in-running horse racing - episode 119 with Andrew Mack, author of Statistical Sports Models in Excel volumes 1 and 2, episode 116 with Lloyd Danzig if you're interested in the US market, episode 96 with Matthew Davidow and Ed Miller, authors of The Logic of Sports Betting, episode 29 with William Ziemba who has published some standout papers on statistical modelling of horse racing, episode 28 with Dan Weston if tennis is your sport



Betting adjacent, I like [https://chatwithtraders.com/|Chat With Traders](https://chatwithtraders.com/|Chat With Traders). The host, Aaron Fifield seems quite naive but there's no doubting the quality of talent he gets on the show. Top pick would be Episode 109 with Ed Thorp who is a legend in both gambling and investing. Of some controversy on here is episode 215 featuring William Beauchamp of Seamless Capital. If you search the episodes you can pick out plenty of bettors that have made the jump to financial markets but the pure finance guys can certainly be worth listening to as well



There are quite a few US-centric podcasts like [https://podcasts.apple.com/gb/podcast/bet-the-process/id1291010585|Bet The Process](https://podcasts.apple.com/gb/podcast/bet-the-process/id1291010585|Bet The Process), [https://podcasts.apple.com/gb/podcast/be-better-bettors/id1493902736|Be Better Bettors](https://podcasts.apple.com/gb/podcast/be-better-bettors/id1493902736|Be Better Bettors) and [https://podcasts.apple.com/us/podcast/gambling-with-an-edge/id459476611|Gambling With An Edge](https://podcasts.apple.com/us/podcast/gambling-with-an-edge/id459476611|Gambling With An Edge) but personally I don't much care for them as it can seem like they're speaking a different language. It's not just the obvious focus on US sports, you're also got the increased focus on line betting, the impossible to understand odds system, the obsession with "off shore" bookmakers and Vegas, etc. etc.

---

## 2022-07-07 22:55:22 - issues channel

**mon mon**

I haven't spotted anything nefarious in the logs just before Flumine shuts itself down, but on the console the last message from python is

```Traceback (most recent call last):

  File "live_trade.py", line 717, in &lt;module&gt;

    framework.run()

  File "/usr/local/lib/python3.8/site-packages/flumine/flumine.py", line 57, in run

    self._process_close_market(event)

  File "/usr/local/lib/python3.8/site-packages/flumine/baseflumine.py", line 347, in _process_close_market

    self._remove_market(market)

  File "/usr/local/lib/python3.8/site-packages/flumine/baseflumine.py", line 211, in _remove_market

    self.markets.remove_market(market.market_id)

  File "/usr/local/lib/python3.8/site-packages/flumine/markets/markets.py", line 34, in remove_market

    self.events[event_id].remove(market)

ValueError: list.remove(x): x not in list```

---

## 2022-07-06 22:06:56 - issues channel

**liam**

I can have a look tomorrow, going forward I assume using the MarketBook for the start time in live (rather than cat) would fix things?

---

## 2022-07-06 21:11:51 - issues channel

**Jonjonjon**

I have a strategy that places bets exactly at scheduled off.



For some markets, the scheduled off can change. In my backtests, the bets get fired off at the correct time. But in live trading, sometimes my bets get placed at the prior market start time.



I believe this is because I use "seconds_to_start", which uses "market_start_datetime", which will come from the market_book's market_definition.



In live trading, the market_start_time will come from the market_catalogue. So I am guessing that in live trading, the process that updates the market_catalogue might be misbehaving.



What would be the cleanest way to solve this?

---

## 2022-07-06 15:55:50 - random channel

**EJono**

Thank you for the insight and use case. Its good to know what youve been able to get out of the service. The documentation is slightly unclear what endpoint should be used for live football scores, for instance the "Events API" has an inplay endpoint but its not clear if this data is made available after the match is terminated ie is historical data at that point. Which ebdpoint do you utilise for inplay live scores on football?

---

## 2022-07-06 13:14:07 - betconnect channel

**Sam - DH**

Afternoon BC, hit a bit of a wall using the api and python wrapper (has been blinding up to this point - nice work). Searched your git repo and this workspace, haven't seen this 'issue' addressed yet so, hoping it’s not my incompetence and is just that we're the Lay-side early birds.



In short, I'm struggling to find an acceptable 'True' value for the accept_each_way param in /bet_request_get. I think there is also an issue in the way it's defined in filters.GetBetRequestFilter(), but, the parameter as a whole, server-side, I don't think is working as expected.



The above alone is probably enough for someone more switched on than me to work this out but, I’ll detail some more troubleshooting I did in this thread, in case it’s useful.

---

## 2022-07-06 10:51:05 - random channel

**EJono**

Looking into different sources for football scores outside of bfs inplay service. Does anyone have any experience using "betsapi" ? it looks as if they offer a varied selection of endpoints with aggreagated data from several providers. Im interested in live football scores and the pricing seems quite a bit cheaper than previously suggested feeds from sites such as "thesports" and "sportsmonk" but havent heard much chat about it hear nor much published online. Any experience, good or bad, would be helpful before i dive in cheers.

---

## 2022-07-05 15:34:07 - general channel

**Jonjonjon**

I'm on Digital Ocean as AWS confuses me.

---

## 2022-07-05 15:28:26 - general channel

**mandelbot**

for example like this



```import logging

import betfairlightweight

from pythonjsonlogger import jsonlogger



from flumine import Flumine, clients

from flumine.streams.datastream import DataStream

from Strategy1 import Strategy1

from Strategy2 import Strategy2





logger = logging.getLogger()



custom_format = "%(asctime) %(levelname) %(message)"

log_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_handler.setFormatter(formatter)

logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



trading = betfairlightweight.APIClient('username', 'password', app_key='app_key', 

					certs='/path/to/certs')

trading.login()



client = clients.BetfairClient(trading)



framework = Flumine(client=client)



markets1 = betfairlightweight.filters.streaming_market_filter(

           event_type_ids=["7"],

           country_codes=["GB", "IE"],

           market_types=["WIN"]

)



markets2 = betfairlightweight.filters.streaming_market_filter(

           event_type_ids=["4339"],

           country_codes=["GB"],

           market_types=["WIN"]

)





strategy1 = Strategy1(

    name="Strategy1",

    market_filter=markets1,

    max_order_exposure=50000000,

    max_selection_exposure=6000000,

    max_trade_count=200,

    max_live_trade_count=1,

    context={"stake": 2},

)



strategy2 = Strategy2(

    name="Strategy2",

    market_filter=markets2,

    max_order_exposure=50000000,

    max_selection_exposure=6000000,

    max_trade_count=2000,

    max_live_trade_count=2,

    context={"stake": 2},

)



framework.add_strategy(strategy)

framework.add_strategy(strategy2)



framework.run()```

---

## 2022-07-05 14:59:51 - general channel

**PeterLe**

[@UPMUFSGCR](@UPMUFSGCR) on AWS? Micro?

---

## 2022-07-03 10:54:45 - issues channel

**mon mon**

Are there any conditions/controls that will cause Flumine to exit (gracefully) without prompting?  My instance is running fine for an hour or so but then shuts itself down. Simplified log below....

```19:01:03 | INFO     | Client update account details

19:02:31 | INFO     | Market 1.200669111 closed

19:02:31 | INFO     | Market closed

19:02:31 | INFO     | Removing market 1.200669107

19:02:31 | INFO     | BackgroundWorker Info Update shutting down

19:02:33 | INFO     | BackgroundWorker Market Filter Update shutting down

19:02:37 | INFO     | BackgroundWorker Exchange Rate Update shutting down

19:02:41 | INFO     | BackgroundWorker Market PandL DB  Update shutting down

19:02:45 | INFO     | BackgroundWorker Account History  Update shutting down

19:02:49 | INFO     | BackgroundWorker Order DB Update shutting down

19:02:53 | INFO     | BackgroundWorker DB Update from data scraping - horses shutting down

19:02:57 | INFO     | BackgroundWorker keep_alive shutting down

19:02:58 | INFO     | 1.200669111: 0 cleared orders found, more available: False

19:03:01 | INFO     | BackgroundWorker poll_market_catalogue shutting down

19:03:04 | INFO     | Client update account details

19:03:05 | INFO     | BackgroundWorker poll_account_balance shutting down

19:03:09 | INFO     | BackgroundWorker poll_market_closure shutting down

19:03:13 | INFO     | Stopped OrderStream 1002

19:03:13 | INFO     | Stopped MarketStream 3003

19:03:13 | INFO     | Stopped MarketStream 2006

19:03:13 | INFO     | Shutting down Execution (SimulatedExecution)

19:03:13 | INFO     | Shutting down Execution (BetfairExecution)

19:03:13 | INFO     | Shutting down logging control LOGGING_CONTROL

19:03:13 | INFO     | Stopped output_thread (OrderStream 1002)

19:03:14 | INFO     | Client logout

19:03:14 | INFO     | Exiting flumine```

---

## 2022-06-28 18:02:07 - strategies channel

**Mr West**

The only way to know if a strategy works is to run it in the live market. There's only so much you can do with back testing. Use small stake and be prepared to lose some money.

---

## 2022-06-28 17:22:25 - strategies channel

**John Foley**

Enjoying reading some of the discussion above in relation to backtesting. Very helpful to anyone like myself who has made the humbling decision to try modelling from fundamentals and betting pre-live close to the off :slightly_smiling_face:

Say you have a model and you’re pretty sure your probabilities are better than BSP (humour me). Next sensible step would be to bet against BSP historically and hope for a profit. What strategy/judgement do people use to be sure that the profit is due to edge and not noise? Is there a quantitative way to combine the sample size, the prices you’re taking, your staking plan and coming up with a “degree of certainty” that your profit is definitely due to an edge? In my head what I’m trying to get to is almost a bayesian result where you say “im x% certain that I have an edge here”

---

## 2022-06-26 22:43:02 - random channel

**birchy**

+1 for JuiceSSH. I also use AndFTP (using SSH key auth) and BucketAnywhere for up/downloading files to/from AWS Lightsail and S3. Great tools for maintenance jobs while on your lunch break.

---

## 2022-06-26 13:57:14 - issues channel

**Mo**

Use AWS EC2 instead?

---

## 2022-06-21 21:56:40 - random channel

**birchy**

However many it takes to get to the CPU running at 99%? :grinning: 

Serious answer...depends on how much computing each strategy uses. I don't think there's a limit in flumine other than CPU limitations, particularly on AWS. I guess latency would be the key metric?

---

## 2022-06-21 07:42:08 - issues channel

**thambie1**

This crash looks pretty bad, never seen "500 Internal Server Error" when going to [http://betfair.com|betfair.com](http://betfair.com|betfair.com)

---

## 2022-06-20 09:50:35 - strategies channel

**Jorge**

I basically look at expected value and std when evaluating preplay strategies. And also when checking if a strategy that I have put live is performing as it "should"

---

## 2022-06-16 17:28:27 - issues channel

**Newbie99**

That would make sense given AWS was unaffected, but really odd that the Interactive login then seemed to work!

---

## 2022-06-16 16:02:37 - random channel

**liam**

Reminds me of the legendary inplay [https://community.betfair.com/horse_racing/go/thread/view/94102/28802591/tony-calvin-it-is-an-obvious-error?liveView=0&amp;pg=5|error](https://community.betfair.com/horse_racing/go/thread/view/94102/28802591/tony-calvin-it-is-an-obvious-error?liveView=0&amp;pg=5|error) 

---

## 2022-06-16 13:48:24 - issues channel

**Newbie99**

Actually I lie...Interactive login does actually appear to work now, odd. So it must be cert related (which makes sense of course from the error), odd it still works on AWS with certs though.

---

## 2022-06-16 13:37:27 - issues channel

**Newbie99**

By chance is anyone else getting errors today (can't see anything showing as down):



```ERROR:flumine.clients.betfairclient:BetfairClient `login` error

Traceback (most recent call last):

  File "D:\Python38\lib\site-packages\urllib3\connection.py", line 169, in _new_conn

    conn = connection.create_connection(

  File "D:\Python38\lib\site-packages\urllib3\util\connection.py", line 96, in create_connection

    raise err

  File "D:\Python38\lib\site-packages\urllib3\util\connection.py", line 86, in create_connection

    sock.connect(sa)

socket.timeout: timed out



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "D:\Python38\lib\site-packages\urllib3\connectionpool.py", line 699, in urlopen

    httplib_response = self._make_request(

  File "D:\Python38\lib\site-packages\urllib3\connectionpool.py", line 382, in _make_request

    self._validate_conn(conn)

  File "D:\Python38\lib\site-packages\urllib3\connectionpool.py", line 1010, in _validate_conn

    conn.connect()

  File "D:\Python38\lib\site-packages\urllib3\connection.py", line 353, in connect

    conn = self._new_conn()

  File "D:\Python38\lib\site-packages\urllib3\connection.py", line 174, in _new_conn

    raise ConnectTimeoutError(

urllib3.exceptions.ConnectTimeoutError: (&lt;urllib3.connection.HTTPSConnection object at 0x0000022DED12BC40&gt;, 'Connection to [http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com) timed out. (connect timeout=3.05)')



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "D:\Python38\lib\site-packages\requests\adapters.py", line 439, in send

    resp = conn.urlopen(

  File "D:\Python38\lib\site-packages\urllib3\connectionpool.py", line 755, in urlopen

    retries = retries.increment(

  File "D:\Python38\lib\site-packages\urllib3\util\retry.py", line 574, in increment

    raise MaxRetryError(_pool, url, error or ResponseError(cause))

urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by ConnectTimeoutError(&lt;urllib3.connection.HTTPSConnection object at 0x0000022DED12BC40&gt;, 'Connection to [http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com) timed out. (connect timeout=3.05)'))```

Interestingly Interactive login also doesn't work for me....however this is only when trying to connect from my local machines.



AWS appears to be chugging away happily (all the certs are obviously the same and Interactive login doesn't work locally, so I'm a bit perplexed).

---

## 2022-06-15 19:41:09 - strategies channel

**birchy**

[@U01A64T6DJQ](@U01A64T6DJQ) as suggested above, try it live with minimum flat stakes ($1 or whatever). More importantly, make sure you have setup bet logging as that will give you "real world" data that you can analyse later.

[https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py|https://github.com/betcode-org/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

---

## 2022-06-13 12:10:41 - betfair-news channel

**Oliver Archer**

Thanks, very helpful, will do some research on AWS :+1::skin-tone-3:

---

## 2022-06-13 12:02:46 - betfair-news channel

**Mo**

I don't know the details of the Kaggle notebook infrastructure or PythonAnywhere but best practice would be to host in AWS Dublin given proximity to Betfair servers

---

## 2022-06-13 11:54:23 - betfair-news channel

**Oliver Archer**

Hi all, potential noob question incoming.. I'm looking to write an automated trading script in Python that uses a scheduler to run every couple of minutes. I could run it on my local machine but ideally don't want to have to have my laptop on 24/7 so looking to move it to the cloud. I've previously written a similar automated script trading crypto through a Binance API using a combination of Kaggle notebooks and PythonAnywhere (followed a guide on Towards Data Science) but when I try to do the same using betfairlightweight I get a 403 error when logging in which I'm assuming is a result of the servers being based in the US. So question is, how would people recommend I move my script to the cloud? Would ideally like to use the Kaggle/PythonAnywhere route if at all possible given I'm paying for that already, but can sign up to something else if I have to! Thanks in advance!!

---

## 2022-06-12 08:25:49 - issues channel

**Rudeger Jamison**

I have a feeling it's an issue on my aws end. Sorry to bother you Liam. Leave it with me.

---

## 2022-06-10 21:09:50 - general channel

**foxwood**

The proform data is in SQL server (last time I subscribed some years ago) so if you are on Windows then no need to export - just run queries to suit. Install SSMS (free) and explore with that. Use pyodbc for real time queries if you are a Python user. You can also run SQL queries from Excel.

---

## 2022-06-10 20:46:06 - issues channel

**Brøndby IF**

Good afternoon everyone!

At certain times of the day, when I pull up the list of live football games ids that contain certain markets:



`markets_base = [`

        `'FIRST_HALF_GOALS_05','FIRST_HALF_GOALS_15','FIRST_HALF_GOALS_25',`

        `'OVER_UNDER_05','OVER_UNDER_15','OVER_UNDER_25','OVER_UNDER_35','OVER_UNDER_45','OVER_UNDER_55','OVER_UNDER_65','OVER_UNDER_75','OVER_UNDER_85'`

        `]`



`market_filter = betfairlightweight.filters.market_filter(`

        `event_type_ids=['1'],`

        `in_play_only=True,`

        `market_type_codes=markets_base`

    `)`

`soccer_events = trading.betting.list_events(filter=market_filter)`



Like for example: 31518957



And I immediately use these ids to collect the game's market data:



`filter_catalog_markets = betfairlightweight.filters.market_filter(`

            `event_ids=[event_id],`

            `market_type_codes = markets_base`

            `)`



        `catalog_markets = trading.betting.list_market_catalogue(`

            `filter=filter_catalog_markets,`

            `max_results='100',`

            `sort='FIRST_TO_START',`

            `market_projection=['EVENT','COMPETITION','MARKET_START_TIME','RUNNER_METADATA']`

            `)`



One or other sporadically returns `catalog_markets` as empty → `[]`.

Is this something 'normal' to happen?



When I say sporadically, I tested run every 1 minute all day and this happened 13 times, one time for each different games.



My question is just to know if this is common to analyze if I can just ignore such an empty return or if I should worry about some structural flaw in the way I'm researching.

---

## 2022-06-09 10:55:45 - random channel

**Ruben**

The storage of my aws ec2 instance seems to be filling up for no apparent reason, any ideas why that might happen? For context, I'm running a flumine instance, a redis instance, and also have anaconda with 1 environment that I use

---

## 2022-06-07 11:24:26 - random channel

**Newbie99**

I'm hoping this is a dumb question and I'm missing the blindingly obvious answer....



When backtesting how would I go about looking back x seconds on an in-play market?



In live of course I can simply compare the publish time to now, but when back testing of course that won't work.



(I appreciate I can look back y price updates as an alternative, but just wondered if this approach was possible)?

---

## 2022-05-26 11:58:05 - issues channel

**LM**

Any thoughts on below error. Seems to occur sporadically after flumine has been running for a while

```{"asctime": "2022-05-26 10:56:10,717", "levelname": "ERROR", "message": "_get_cleared_market error", "exc_info": "Traceback (most recent call last):\n  File \"/home/ec2-user/.local/lib/python3.8/site-packages/flumine/worker.py\", line 229, in _get_cleared_market\n    cleared_markets = betting_client.betting.list_cleared_orders(\n  File \"/home/ec2-user/.local/lib/python3.8/site-packages/betfairlightweight/endpoints/betting.py\", line 434, in list_cleared_orders\n    (response, response_json, elapsed_time) = self.request(method, params, session)\n  File \"/home/ec2-user/.local/lib/python3.8/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 55, in request\n    self._error_handler(response_json, method, params)\n  File \"/home/ec2-user/.local/lib/python3.8/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 81, in _error_handler\n    raise self._error(response, method, params)\nbetfairlightweight.exceptions.APIError: SportsAPING/v1.0/listClearedOrders \nParams: {'betStatus': 'SETTLED', 'marketIds': ['1.199541139'], 'customerStrategyRefs': ['ip-172-31-43-15'], 'settledDateRange': {'from': None, 'to': None}, 'groupBy': 'MARKET'} \nException: None \nError: {'code': -32099, 'message': 'ANGX-0003', 'data': {'APINGException': {'requestUUID': 'ie1-ang03a-prd-05180823-000f838978', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0003', 'data': {'APINGException': {'requestUUID': 'ie1-ang03a-prd-05180823-000f838978', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}", "trading_function": "list_cleared_orders", "response": "SportsAPING/v1.0/listClearedOrders \nParams: {'betStatus': 'SETTLED', 'marketIds': ['1.199541139'], 'customerStrategyRefs': ['ip-172-31-43-15'], 'settledDateRange': {'from': None, 'to': None}, 'groupBy': 'MARKET'} \nException: None \nError: {'code': -32099, 'message': 'ANGX-0003', 'data': {'APINGException': {'requestUUID': 'ie1-ang03a-prd-05180823-000f838978', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0003', 'data': {'APINGException': {'requestUUID': 'ie1-ang03a-prd-05180823-000f838978', 'errorCode': 'INVALID_SESSION_INFORMATION', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}"}```

---

## 2022-05-26 08:08:57 - issues channel

**Oliver Varney**

very very early to bed, early up :joy: I think the use case is strategy specific, alot of my strategy is based of the process of making predictions then creating trades off the back, rather then specific signals that maybe is more conventional. I dont think theres a correct / best practice. The two bullet points youve described above are logical I believe, as they were my similar thoughts when having code in both sections.  have you confirmed that the code is doing what youve listed above in live?

---

## 2022-05-25 00:22:27 - issues channel

**Jonjonjon**

I worked it out... In Flumine 2.0, strategy.process_orders only gets called if the strategy already has live orders. My code was relying on it being called more frequently.

---

## 2022-05-23 07:32:32 - issues channel

**mandelbot**

So I've recently ported my marketrecorder to a linux ec2 but it keeps getting shut down for some reason. Can someone shed light on this for me please?



```{"asctime": "2022-05-23 05:16:35,115", "levelname": "INFO", "message": "Market removed", "market_id": "1.199445646"}

{"asctime": "2022-05-23 05:16:35,119", "levelname": "INFO", "message": "Market closed", "market_id": "1.199439954", "clients": {"Betfair": {"d": {"username": "d", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fcb7bddd190&gt;"], "order_stream": false, "best_price_execution": true, "paper_trade": false}}, "Simulated": {}, "BetConnect": {}}, "markets": {"market_count": 728, "open_market_count": 670}, "streams": ["&lt;DataStream(DataStream, started daemon 140511599916800)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140512124557120)&gt;", "&lt;BackgroundWorker(keep_alive, started daemon 140511835891456)&gt;", "&lt;BackgroundWorker(poll_market_catalogue, started daemon 140511825401600)&gt;", "&lt;Thread(WIN_load_processor, started daemon 140511814911744)&gt;", "&lt;DataStream(DataStream, started daemon 140511599916800)&gt;"]}

{"asctime": "2022-05-23 05:16:35,123", "levelname": "INFO", "message": "Removing market 1.199445645", "clients": {"Betfair": {"d": {"username": "de", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fcb7bddd190&gt;"], "order_stream": false, "best_price_execution": true, "paper_trade": false}}, "Simulated": {}, "BetConnect": {}}, "markets": {"market_count": 728, "open_market_count": 670}, "streams": ["&lt;DataStream(DataStream, started daemon 140511599916800)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140512124557120)&gt;", "&lt;BackgroundWorker(keep_alive, started daemon 140511835891456)&gt;", "&lt;BackgroundWorker(poll_market_catalogue, started daemon 140511825401600)&gt;", "&lt;Thread(WIN_load_processor, started daemon 140511814911744)&gt;", "&lt;DataStream(DataStream, started daemon 140511599916800)&gt;"]}

{"asctime": "2022-05-23 05:16:35,123", "levelname": "INFO", "message": "BackgroundWorker keep_alive shutting down", "worker_name": "keep_alive", "function": "&lt;function keep_alive at 0x7fcb7967f5f0&gt;"}

{"asctime": "2022-05-23 05:16:35,426", "levelname": "INFO", "message": "/home/ec2-user/.local/lib/python3.7/site-packages/flumine/historical/ALL/1.199439954.gz successfully loaded to s3"}

{"asctime": "2022-05-23 05:16:35,452", "levelname": "INFO", "message": "1.199439954 successfully loaded marketCatalogue to s3"}

{"asctime": "2022-05-23 05:16:35,452", "levelname": "INFO", "message": "Removing: /home/ec2-user/.local/lib/python3.7/site-packages/flumine/historical/ALL/1.199445646.gz, age: 3669.27s"}

{"asctime": "2022-05-23 05:16:35,453", "levelname": "INFO", "message": "Removing: /home/ec2-user/.local/lib/python3.7/site-packages/flumine/historical/ALL/1.199445646, age: 3669.29s"}

{"asctime": "2022-05-23 05:16:35,453", "levelname": "INFO", "message": "Removing: /home/ec2-user/.local/lib/python3.7/site-packages/flumine/historical/ALL/1.199445645.gz, age: 3659.23s"}

{"asctime": "2022-05-23 05:16:35,453", "levelname": "INFO", "message": "Removing: /home/ec2-user/.local/lib/python3.7/site-packages/flumine/historical/ALL/1.199445645, age: 3659.26s"}

{"asctime": "2022-05-23 05:16:39,124", "levelname": "INFO", "message": "BackgroundWorker poll_market_catalogue shutting down", "worker_name": "poll_market_catalogue", "function": "&lt;function poll_market_catalogue at 0x7fcb78031440&gt;"}

{"asctime": "2022-05-23 05:16:43,124", "levelname": "INFO", "message": "Shutting down Execution (SimulatedExecution)"}

{"asctime": "2022-05-23 05:16:43,124", "levelname": "INFO", "message": "Shutting down Execution (BetfairExecution)"}

{"asctime": "2022-05-23 05:16:43,196", "levelname": "INFO", "message": "Client logout", "username": "d", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fcb7bddd190&gt;"], "order_stream": false, "best_price_execution": true, "paper_trade": false}

{"asctime": "2022-05-23 05:16:43,197", "levelname": "INFO", "message": "Exiting flumine", "clients": {"Betfair": {"d": {"username": "d", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x7fcb7bddd190&gt;"], "order_stream": false, "best_price_execution": true, "paper_trade": false}}, "Simulated": {}, "BetConnect": {}}, "markets": {"market_count": 727, "open_market_count": 670}, "streams": ["&lt;DataStream(DataStream, started daemon 140511599916800)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140512124557120)&gt;", "&lt;BackgroundWorker(keep_alive, started daemon 140511835891456)&gt;", "&lt;BackgroundWorker(poll_market_catalogue, started daemon 140511825401600)&gt;", "&lt;Thread(WIN_load_processor, started daemon 140511814911744)&gt;", "&lt;DataStream(DataStream, started daemon 140511599916800)&gt;"]}

Traceback (most recent call last):

  File "/home/ec2-user/.local/lib/python3.7/site-packages/flumine/strategies/marketrecorder.py", line 54, in &lt;module&gt;

    framework.run()

  File "/home/ec2-user/.local/lib/python3.7/site-packages/flumine/flumine.py", line 43, in run

    self._process_close_market(event)

  File "/home/ec2-user/.local/lib/python3.7/site-packages/flumine/baseflumine.py", line 350, in _process_close_market

    self._remove_market(market)

  File "/home/ec2-user/.local/lib/python3.7/site-packages/flumine/baseflumine.py", line 211, in _remove_market

    self.markets.remove_market(market.market_id)

  File "/home/ec2-user/.local/lib/python3.7/site-packages/flumine/markets/markets.py", line 31, in remove_market

    self.events[market.event_id].remove(market)

ValueError: list.remove(x): x not in list```

---

## 2022-05-20 08:51:40 - issues channel

**mandelbot**

Not sure why i get the following error when trying to stream FR horseracing markets

```{"asctime": "2022-05-20 07:50:39,625", "levelname": "ERROR", "message": "MarketStream 8006 run error", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flumine\\streams\\marketstream.py\", line 44, in run\n    self._stream.start()\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 67, in start\n    self._read_loop()\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 226, in _read_loop\n    received_data_raw = self._receive_all()\n  File \"C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 256, in _receive_all\n    raise SocketError(\nbetfairlightweight.exceptions.SocketError: [Connect: 8007]: Connection closed by server"}```

---

## 2022-05-18 11:22:54 - issues channel

**Neil T (Betfair)**

Hi Liam, not easily and quickly as session related queries need to be raised with a separate team.  Assuming the sessions are being handled correctly by Flumine, likely cause (based on what I've mentioned above) is that the session is expiring before the Keep Alive call is made.  Its probably worth experimenting with a slightly shorter time period within which the keep alive call is made

---

## 2022-05-18 10:15:49 - issues channel

**Neil T (Betfair)**

Morning [@UG03WDCHH](@UG03WDCHH) - please can you confirm what error is thrown by the Keep Alive request?  Is it still NO_SESSION?

---

## 2022-05-16 18:19:41 - random channel

**liam**

Ah great [https://health.aws.amazon.com/phd/status|https://health.aws.amazon.com/phd/status](https://health.aws.amazon.com/phd/status|https://health.aws.amazon.com/phd/status)

---

## 2022-05-16 18:07:52 - random channel

**liam**

Anyone else getting lots errors from AWS? Looks like it might be APIGateway related 

---

## 2022-05-16 10:01:30 - issues channel

**liam**

20mins after starting do you get any error or warnings logs from the keep_alive worker?

---

## 2022-05-15 16:37:53 - betfair-news channel

**PATTY BET**

Order stream returns real live data of my current orders? My needs is to adapt the examplestreamingerrhandling.py to subscribe to new markets where i place bet on. if now i place a limit that excute correctly i need to track real time data to protect or cashout. i don't know if i'm clear enogh. thank you anyway for trying to understand.

---

## 2022-05-15 06:34:40 - issues channel

**Rudeger Jamison**

I deployed it yesterday to AWS and it appeared to work the whole day and then these errors started today, so after the container had been running for about 24 hrs

---

## 2022-05-15 06:17:16 - issues channel

**Rudeger Jamison**

```

Traceback (most recent call last):

File "/usr/local/lib/python3.9/site-packages/flumine/clients/betfairclient.py", line 44, in keep_alive

return self.betting_client.keep_alive()

File "/usr/local/lib/python3.9/site-packages/betfairlightweight/endpoints/keepalive.py", line 30, in __call__

(response, response_json, elapsed_time) = self.request(session=session)

File "/usr/local/lib/python3.9/site-packages/betfairlightweight/endpoints/keepalive.py", line 60, in request

self._error_handler(response_json)

File "/usr/local/lib/python3.9/site-packages/betfairlightweight/endpoints/keepalive.py", line 67, in _error_handler

raise self._error(response)

betfairlightweight.exceptions.KeepAliveError: API keepAlive FAIL: NO_SESSION```

---

## 2022-05-15 06:14:11 - issues channel

**liam**

Any errors would have keep_alive in them 

---

## 2022-05-15 06:13:20 - issues channel

**liam**

It’s the keep_alive worker [https://github.com/betcode-org/flumine/blob/master/flumine/worker.py|https://github.com/betcode-org/flumine/blob/master/flumine/worker.py](https://github.com/betcode-org/flumine/blob/master/flumine/worker.py|https://github.com/betcode-org/flumine/blob/master/flumine/worker.py)

---

## 2022-05-13 06:40:08 - general channel

**Rudeger Jamison**

because I have loads of different strategy names and I use that to differentiate how they perform in my logging.



I send all the logs to athena in aws, and that is the way I can differentiate an order from one strategy to the next

---

## 2022-05-12 16:40:34 - issues channel

**foxwood**

Not worth changing imho even though it is a dirty fix - presumably been there a long time and not been a problem generally it seems. Use functions like I listed above if you need true local/event time - eg I used dateAsVenueTimezone() to get the correct hour after the UK clocks changed to avoid events at certain times. That worked without change on both backtest and live when checking "market.market_start_datetime". I think when I checked my recorded files they contained "naive" utc dates (not sure about official BF files ?). Changing may risk breaking too much proven code that people already working with ? My 2p for what it's worth  - you can always hide behind the Microsoft get-out of "feature - by design" :)

---

## 2022-05-12 09:06:31 - general channel

**liam**

You are just missing a while loop, notice the use of `is_alive` so it can end cleanly and a sleep which I have used the streaming_timeout



```import time

import requests

from flumine.events.events import CustomEvent

from flumine.streams.basestream import BaseStream





class CustomStream(BaseStream):

    def run(self) -&gt; None:

        while self.is_alive():

            # connect to stream / make API requests etc.

            response = self._get_random_data()



            # callback func

            def callback(framework, event):

                for strategy in framework.strategies:

                    strategy.process_my_event(event)



            # push results through using custom event

            event = CustomEvent(response, callback)



            # put in main queue

            self.flumine.handler_queue.put(event)



            time.sleep(self.streaming_timeout)



    def _get_random_data(self):

        response = requests.get("[https://random-data-api.com/api/cannabis/random_cannabis?size=2](https://random-data-api.com/api/cannabis/random_cannabis?size=2)")

        return response.json()```

---

## 2022-05-11 10:06:38 - issues channel

**Jorge**

I just moved to AWS ireland so network should be OK. It must be the CPU, I only have 1 CPU

---

## 2022-05-11 07:17:25 - issues channel

**Jorge**

The AWS CPU utilization plot looks OK, reaches 40% CPU utilization only.

I run 1 instance of flumine and 1 instance of betfairlightweight for market and order streaming + separate python processes for trading (around 5 processes).

---

## 2022-05-09 16:23:16 - issues channel

**Beginner**

Hi [@U01EZ613ZCZ](@U01EZ613ZCZ) I tried to do this, but unfortunately my knowledge is very scarce and I don't have time to learn, so I will need to outsource, unfortunately because I wanted to learn the correct form, but I couldn't.



My codes are huge, with several flaws and I didn't understand how to do it right.

---

## 2022-05-07 15:47:27 - issues channel

**Marco**

Yeah, I agree with you: Flumine is very powerful! I wrote that phrase incorrectly; what I meant is I've changed the filters of the strategies based on the run-type (backtest or live)

---

## 2022-05-07 15:38:42 - issues channel

**Peter**

[@U03EF29QFS4](@U03EF29QFS4) You don't need to separate the strategies. One of the strengths of Flumine is that you can use the same strategy in production as you used in backtesting. It's only the market filter than changes.

---

## 2022-05-07 10:48:40 - issues channel

**Peter**

I think the problem may be more fundamental. You're creating a dictionary with snake case  keys, but Betfair is expecting camel case, so all your parameters are being ignored and you're trying to subscribe to every live market.

---

## 2022-05-06 13:24:27 - general channel

**Peter**

I use them. Data quality is pretty good and they're super-responsive to corrections in their historic data.



I don't know what the delay is, but I would expect it to be somewhat variable depending on the speed of the feed the data analysts receive, how far ahead of the "live" broadcast they are receiving it, how much of the encoding is automated and how quickly specific analyst is able to encode the non-automated events. Overall however, I would expect it to be dwarfed by the impact of polling for new data.



I'm not aware of a mapping between Betfair team names and Sporting Monks. I've done that myself but found a fair number of inconsistencies in team names on the Betfair making, even in major competitions, rendering a mapping a bit problematic. My preference instead is to use fuzzy logic on the team names to find best matches. I've found that more reliable (i.e. less brittle).

---

## 2022-05-06 12:22:36 - random channel

**Oliver Varney**

Is there any optimisation just to get those that have been updated? i.e. just those trades that have had updates? I guess why isnt strategy_orders called with LIVE_STATUS ?

---

## 2022-05-06 08:36:13 - general channel

**Mo**

How do you think betfairlightweight knows what server to connect to? Or you could just read the API documentation 

---

## 2022-05-05 15:47:24 - general channel

**Ivan Zhou**

Whats the best way to grab a log of your bets at the end of the day? Having issues using the blotter to collect results on a live strategy the same way as you would for a simulation

```# Collect results

results = []

for market in framework.markets:

    print("Profit: {0:.2f}".format(sum([o.simulated.profit for o in market.blotter])))

    for order in market.blotter:

        results.append(

            [

                order.market_id,

                order.selection_id,

                order.responses.date_time_placed,

                order.status,

                order.order_type.price,

                order.average_price_matched,

                order.size_matched,

                order.simulated.profit,

            ]

        )```

---

## 2022-05-04 07:28:34 - general channel

**Steve Roach**

hmm, my bad - brain fart.



As to the second part of my question; what would the expected data gathering rate be in a live market? Or, I suppose, what are the limiting factors?

---

## 2022-05-04 04:11:06 - general channel

**Steve Roach**

Hi Guys,



I've implemented the datacollectwom.py strategy from GitHub: flumine-strategy-development and I've run a few PRO files through it. The resulting datapoints are about 5 seconds apart. Is this about right? Is this what I would expect running Flumine against a live market?



Cheers

---

## 2022-05-03 22:53:04 - issues channel

**AndyL**

At least i know how to handle it now live.

However I think Flumine simulation doesn't do that [@U4H19D1D2](@U4H19D1D2)? It adjusts order liability but it doesnt complete the order at the conversion point?

---

## 2022-05-03 11:48:03 - random channel

**Unknown**

[@UN1497LJC](@UN1497LJC) You asked about EC2 Micro instances recently…I was just in my AWS console and noticed that there are some free cheap as chips Lightsail instances avail free, check it out (This is for windows by the way, linux avail too)

---

## 2022-05-01 14:01:56 - random channel

**Jonjonjon**

If I used an Amazon EC2 micro instance to run a single instance of Flumine that trades around 100bets per week across all horse and dog markets, would I expect to pay any data charges? When using EC2, is there a risk of unexpected high data charges, or are fees predictable?

---

## 2022-04-29 12:50:17 - random channel

**river_shah**

gross revenue higher for ’22 but higher rate PC this year so net flat to last year. having to deploy increasingly sophisticated research infra to capture edge but it is also clearly apparent that total scale is very constrained (for what I am doing). the biggest disappointment I have faced is lack of major scalability. i have been unable to crack football consistently (but trying). time management issues at play as well, baby on the way. [@U4H19D1D2](@U4H19D1D2) please make `flumine` for crypto.

---

## 2022-04-29 10:15:58 - issues channel

**foxwood**

Ah ok - had a look at Cloudflare and they have a data centre in Dublin so presume that's where I must have been connecting to. Out of interest, any idea what sort of ping time and hop count you are getting from your Dublin based AWS ?

---

## 2022-04-29 09:31:06 - issues channel

**foxwood**

[@UBS7QANF3](@UBS7QANF3) considered that and made an AWS in Ireland but never used it yet - use Tagadab currently. Keep seeing comments elsewhere about traffic routing first through London servers (Cloudflare ?) for front end protection. Just checked my desktop connection and the BF connection is to a Cloudflare server at 104.16.x.x. 9 hops (!) and 7-8ms ping time

---

## 2022-04-29 08:14:48 - issues channel

**Jorge**

Ah, I would love to be able to use AWS servers in Ireland. I may be wrong, I will contact Betfair

---

## 2022-04-29 08:12:57 - issues channel

**Mo**

This is not broadly correct. Like many users I live in the UK but use AWS servers in Ireland

---

## 2022-04-28 17:54:39 - issues channel

**Vadym Zh**

Hey! My client runs the bot on Australian VPS and it works well. But then he decided to change the server (location - UK) and started getting an error 'MULTIPLE_USERS_WITH_SAME_CREDENTIAL'. He runs only 1 bot simultaneously, not both servers. Interactive login. Are there any methods to 'reset' the session? Or what would you recommend to fix this error? The goal is to just run it on the UK server.

---

## 2022-04-26 08:23:35 - random channel

**Newbie99**

A significant improvement...unsurprisingly perhaps it was mostly due to inefficient looping, but I re-wrote a few other bits too.



One thing that is confusing me however, is that occasionally I do get a:



```"status": "Violation", "status_log": "Violation", "violation_msg": "Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failed: live_trade_count (1) &gt;= max_live_trade_count (1)"```

But I perform this check before trying to place an order:



```live_trades = market.blotter.strategy_selection_orders(self, runner.selection_id, runner.handicap, order_status=[OrderStatus.EXECUTABLE])



    if len(live_trades) == 0:```

---

## 2022-04-23 15:58:00 - issues channel

**Unknown**

I keep getting a BET_TAKEN_OR_LAPSED error when cancelling, my cancel logic is as follows:

```orders = [order for order in market.blotter.strategy_orders(self) if

        (order.selection_id, order.handicap) == (runner.selection_id,runner.handicap)]```

```for order in orders:

    runner_exposure = get_runner_exposure(self, market, selected_runner=order)

    if order.status == OrderStatus.EXECUTABLE:

        if order.side == 'BACK':

            if positive_runner_exposure &lt; runner_exposure['if_win']:

                logging.info('Cancelling {0} due to positive exposure breach, runner exposure is: {1} and limit is: {2}.'.format(order.bet_id, runner_exposure, negative_runner_exposure))

                market.cancel_order(order)

            elif validate_current_order(order, time_to_cancel_orders) is True:

                logging.info('Cancelling {0} as order has been live for {1} seconds.'.format(order.bet_id, time_to_cancel_orders))

                market.cancel_order(order)

        else:

            if negative_runner_exposure &gt; runner_exposure['if_win']:

                [http://logging.info|logging.info](http://logging.info|logging.info)('Cancelling {0} due to negative exposure breach, runner exposure is: {1} and limit is: {2}.'.format(order.bet_id, runner_exposure, negative_runner_exposure))

                market.cancel_order(order)

            elif validate_current_order(order, time_to_cancel_orders) is True:

                [http://logging.info|logging.info](http://logging.info|logging.info)('Cancelling {0} as order has been live for {1} seconds.'.format(order.bet_id, time_to_cancel_orders))

                market.cancel_order(order)```

I don't understand how this error could (repeatedly) happen when I check to ensure the order is EXECUTABLE before cancelling (logs for one example attached, I have had multiple instances of this today)?



(I do appreciate it will be an error on my side, I'm just completely stumped as to what it could be), so any clues would be greatly appreciated!

---

## 2022-04-19 15:35:33 - general channel

**AndyL**

When running multiple live strategies, what's the best practice of single vs multiple Flumine instances/processes ?

---

## 2022-04-17 11:34:14 - random channel

**D C**

If you place a bet using the async flag on the API placeOrders operation, can you get a FAILURE state in the response? Docs are somewhat unclear here. For example, if I have a bug and I am trying to place something with async flag at an INVALID price will I still get a PENDING status or does the server filter out my error immediately?

---

## 2022-04-17 09:31:06 - issues channel

**Ruben**

good morning everyone; hoping you can give me some pointers with this...basically every saturday my aws instance (t3.small) has a spike in CPU usage and after that it dies. When that happens, I'm usually subscribed to about 50 soccer markets which doesn't seem like that much compared to what I often hear around here. Around that time I often hit the 5000 max transaction limit as well. Only running 1 strategy, which admittedly is quite CPU intensive since after every update it has to do work to keep a clear picture of the market book

---

## 2022-04-13 23:34:02 - random channel

**Paul**

There's a service where you can get a lorry full of hard drives. Snowball edge is the more common variant though: [https://aws.amazon.com/snowball/|https://aws.amazon.com/snowball/](https://aws.amazon.com/snowball/|https://aws.amazon.com/snowball/)

---

## 2022-04-13 16:24:42 - issues channel

**Brøndby IF**

I'm trying to find in the mapping how I can collect the value that shows me what was the last match update, because the free version of the Betfair API has a delay, correct?



When trying to collect like this:

`'DateTimeUpdate': [obj_evento._datetime_updated for obj_evento in soccer_events]`



It delivers the value of the moment I made the call, could someone tell me if there is a way to get the time of the last update of the match itself so I can follow how much is updated?

---

## 2022-04-13 12:17:24 - issues channel

**foxwood**

I'm running 2 live frameworks on same server for the first time - one for "ok" strategies and another for suspect ones and debugging.



As they both place bets it ends up with cross-talk warnings in the other framework's log log saying "Strategy not available to create order" and "Order %s not present in blotter". Was a bit worrying at first sight lol.



Presume this is in part caused by both frameworks having the socket hostname as the customer order ref since this does not happen when running the two frameworks on separate machines.



Suggestions: if the common hostname is the cause then extend the hostname with python PID (poss problems with multi proc) or an instance specific framework uuid property (logging does a short uuid for example) ?

---

## 2022-04-13 09:54:24 - random channel

**D C**

I think the problem is coming from too many calls and I possibly need to add some kind of throttle or minimum delay between successive calls to see if it improves things. 12 second delay between transmission of request and receipt of response in a live horse race is ridiculous

---

## 2022-04-13 08:57:32 - issues channel

**Jorge**

It may be my crappy server provider, I also observed higher "Latency high" logs in general

---

## 2022-04-13 08:23:15 - issues channel

**Jorge**

Hi, my streaming_market_data process is suddenly using a lot of memory RAM. For the first couple of days it is completely OK but then suddenly it starts accumulating more RAM and in a couple of days it takes all the available RAM in the Server, making the process freeze. Yesterday it took 80MB RAM (normal) and today it went up to 250MB RAM. Has anyone experienced something similar? This is basically my code for the process:



```trading = betfairlightweight.APIClient(... , lightweight=True)

trading.login()



market_filter = streaming_market_filter(event_type_ids=event_type_ids, market_types=market_type_codes, turn_in_play_enabled=True, country_codes=country_codes)

market_data_filter = streaming_market_data_filter(fields=["EX_BEST_OFFERS_DISP", 'EX_TRADED_VOL', "EX_MARKET_DEF"], ladder_levels=1)

streaming = Streaming(trading, market_filter, market_data_filter)

streaming.start()



while True:

    try:

        new_market_books = streaming.output_queue.get()

        publish_to_redis(db_0, new_market_books)

        log_debug('Total markets saved in Redis: {}'.format(len(db_0.keys())))



    except Exception as e:

        log_exception(e)```

I don't see any ERROR logs. Only some WARNINGs here and there about "Latency high", but I had those long time ago and the same process did not accumulate RAM memory...

---

## 2022-04-09 16:24:23 - general channel

**JFP**

Hi all, I'm trying to filter out Novice races for a live strategy. For back testing the below code works:



```if "Nov" in market_book.market_definition.name:```

However I get NoneType error when running live. I have tried using the catalogue:



```if "Nov" in market.market_catalogue.market_name:```

This does not work either (I am trying to use this in the process market_book function). Would really appreciate it if someone could point me in the right direction. Thanks

---

## 2022-04-06 17:33:41 - random channel

**Alessio**

[https://aws.amazon.com/snowball/pricing/](https://aws.amazon.com/snowball/pricing/)

---

## 2022-04-06 17:27:54 - random channel

**Mo**

I remember AWS had a service where you could mail them a hard drive. Unfortunately it no longer seems to exist

---

## 2022-04-06 17:19:15 - random channel

**Oliver Varney**

ended up going with aws cli which seems to be faster then boto3 threading but as youve said I think network is the main issue

---

## 2022-04-06 16:03:29 - random channel

**D**

I tend to use the aws wrangler library from aws. Not sure if it's the fastest but is very convenient.

---

## 2022-04-04 15:07:35 - issues channel

**liam**

you are just checking the live_trade_count so once matched or cancelled or it will place another, the logs will tell you exactly what it's doing.



You should be simulating or paper_trading as well if coding up a strategy

---

## 2022-04-04 15:06:27 - issues channel

**Jon K**

And the output comes out as

starting strategy 'ExampleStrategy'

0

Windsor 4th Apr

Placing bet on  24244533

TradeStatus.LIVE

TradeStatus.LIVE

0

Windsor 4th Apr

Placing bet on  24244533

TradeStatus.LIVE

TradeStatus.LIVE

---

## 2022-04-04 13:31:11 - issues channel

**liam**

[https://github.com/betcode-org/flumine/blob/049caf051593d301e0ae6eaba64aed5e9e489f26/examples/strategies/lowestlayer.py#L43|runner_context](https://github.com/betcode-org/flumine/blob/049caf051593d301e0ae6eaba64aed5e9e489f26/examples/strategies/lowestlayer.py#L43|runner_context) is the easy way to see live trades 

---

## 2022-03-30 09:35:25 - issues channel

**Jonjonjon**

I worked it out. It was a typical case on user error. In my check_market_book I was excluding NSW markets for live trading. But not for backtesting. Apologies for wasting everyone's time.:face_palm::skin-tone-6:

---

## 2022-03-30 07:36:14 - issues channel

**liam**

Very tricky for me to debug this, are you simulating with the same data you placing live, something must be different 

---

## 2022-03-29 21:50:49 - issues channel

**Jonjonjon**

I'm still trying to debug the problem of missing bets (I'm getting lots of placed bets in the backtest, but they're not being place when live). After checking my live latency is fine, and then increasing the backtest latency to 1 second (where it still has lots of placed bets) I'm totally flumoxed.:disappointed:

---

## 2022-03-29 14:37:31 - betconnect channel

**Joel Beasley**

`market_type_id` has been fixed in Staging now [@U01EZ613ZCZ](@U01EZ613ZCZ), will update Production in the morning :thumbsup:

---

## 2022-03-29 12:09:24 - betfair-news channel

**Mo**

Have you ever considered more sophisticated security settings? For example:



• Only allowing API access from specific IP addresses

• Generating access keys with specific permissions. e.g. generate a key that only allows read access to the account statement endpoint. This key can then be used by a reconciliation application without it being an attack vector for placing bets

The use of certificates seems an oddity compared to other APIs both financial (e.g. Binance, IG Index) and not (e.g. AWS). They seem to be a bit of a pain point with new users, especially those on Windows

---

## 2022-03-29 11:35:03 - strategies channel

**Ruben**

was asking because I chose to go live with my first strategy, which happens to be a making strategy, and I had not backtested (mainly because at the time I was not recording anything) and I see that it has quite a large variance and it will take a while to tell if the strategy makes money or not

---

## 2022-03-29 10:44:02 - strategies channel

**liam**

Making is more realistic in matching (can prevent double counting of liquidity) but discounts impact, taking theoretically is impacted less by impact but doesn't handle double counting of liquidity.



However if you have reached this point you should be doing the analysis live, simulating is more for confirming a model/backtest and as a final integration test although I am very guilty of using it too much as it works so well..

---

## 2022-03-28 09:33:28 - random channel

**Jorge**

I use a AWS Instance with Tenancy=Default, I haven't had any problem with it. Tenancy=Dedicated is more expensive

---

## 2022-03-28 08:18:00 - issues channel

**Mo**

The important question is why do you need a desktop environment on a remote server?

---

## 2022-03-28 08:14:29 - issues channel

**Unknown**

Just for your reply :grin: Yes I am using a Remote Desktop to connect to a Linux ec2 instance but it’s very slow and lags 

---

## 2022-03-28 08:05:38 - issues channel

**Kunal Maneck**

Thanks Mo! Can I get a Linux box on Aws?

---

## 2022-03-28 07:55:04 - issues channel

**Kunal Maneck**

Any ideas how I can address lagging issues on my Linux ec2 instance? I have a 8cpu instance but still very slow when using Microsoft Remote Desktop on my Mac 

---

## 2022-03-27 23:39:09 - random channel

**Jonjonjon**

When using Flumine to run live trading on cloud servers, do people tend to to for shared or dedicated CPUs?

---

## 2022-03-26 16:56:02 - issues channel

**Peter**

I'd be disappointed if pedantic was anything but ... However, I didn't have that in my code, so I suspect that pedantic is simply catching early a problem that would have happened anyway when the request hit the BetConnect server.

---

## 2022-03-26 10:43:15 - issues channel

**Jonjonjon**

In the order logs that Flumine places for me, I see that I placed some orders 8 days ago. However, they aren't in my cleared orders on the website. So Iel assume they were voided. But voided orders only go back 7 days. I think my strategy might be subject to this problem quite a lot as my live bet count is a lower than what I get if I subsequently run the same market through a backtest.



What is the best way to monitor this situation?

---

## 2022-03-25 09:59:02 - general channel

**liam**

flumine v2.0.0 now released, this has a few breaking changes 99% around naming (Backtest-&gt;Simulated) hence the major version bump (over 2yrs since v1!) but now allows multi clients/exchanges, see [https://betcode-org.github.io/flumine/clients/|docs](https://betcode-org.github.io/flumine/clients/|docs) on how to use. Also added a rough [https://github.com/betcode-org/flumine/blob/master/examples/example-betconnect.py|example](https://github.com/betcode-org/flumine/blob/master/examples/example-betconnect.py|example) on using the BetConnect client.



I have been testing for a few weeks now and I believe it to be bug free, any issues are likely to be around simulation rather than live due to the changes but let me know if you spot anything.



Future work will involve opening up the framework to be more exchange/API agnostic in terms of execution/market/orders etc.

---

## 2022-03-24 16:06:16 - issues channel

**liam**

From Neil



```The service has now been restarted.  The problem was caused by an issue at our side with the related server, apologies for the inconvenience caused!```

---

## 2022-03-23 00:14:45 - random channel

**James T**

I'd test with more data, and see whether you have a decent edge betting outright or not (no hedge). But that's a riskier route. You'd want a few months of data.



If your strategy is purely price action based then you're never going to get a massive margin when taking volume anyway. Offering may be better, but that's harder to test.



I'd recommend just trying it live with small stakes and see whether you can realise that 0.6% or not. If your backtest says you should be getting 95% positive days when hedging out, but the first day you try you lose money, then you can quickly just say it's back to the drawing board.



You're probably not far off having something though.

---

## 2022-03-22 22:31:18 - random channel

**James T**

For pre-play I would generally want more than 1% profit on stakes. 0.6% is a little low, since you're likely to loose a little to slippage / moving the market if you are taking volume once you go live. Or if you are offering then you'll have unmodelled adverse selection. 0.6% is just a bit too low on margin for it to be realistic and scalable. Might depend on when you are trading though, and how much you are trading (combining backs and lays) vs taking large outright bets.

---

## 2022-03-22 10:35:52 - general channel

**Eamonn**

[@UUE6E1LA1](@UUE6E1LA1) Im happy to have a call to answer all your questions if you would like rather than clogging up the thread. Feel free to DM me.



Yes Oliver you are quite right there

---

## 2022-03-19 16:12:57 - general channel

**Peter**

Like [@U4H19D1D2](@U4H19D1D2) I run most (by number) of my backtests locally. But when I have a good prospect and want to run a lot more markets, I use either a cheap dedicated VPS if I can wait, or a powerful EC2 instance if I want quick results.



My local backtests run in a container, but I've never built the pipeline to deploy them remotely in containers. I did look at running on Lambda a while back but retreated rapidly. Too many gotchas.

---

## 2022-03-19 14:15:36 - general channel

**Newbie99**

So you can run both from a logging control and then bundle up as you see fit (I create a .json for example at each market close, but equally you can just insert directly into a db if easier...my db is local not on AWS which is the only reason I don't do it that way)

---

## 2022-03-19 08:26:58 - general channel

**Jonjonjon**

[@U9JHLMZB4](@U9JHLMZB4)  do you use EC2, containers or lambdas to run the backtests?

---

## 2022-03-17 20:50:08 - general channel

**Jonjonjon**

Is transferring the results from AWS to local machine a pain? I usually use a loggingcontrol to write my backtest orders to disk. And then use Jupyter or Spyder to load them into a dataframe for analysis

---

## 2022-03-17 20:49:08 - general channel

**Paul**

Agreed. I suggest using a subset of markets for quick local testing and do AWS for bigger validation

---

## 2022-03-17 20:44:45 - general channel

**liam**

I have tried a few ways of doing it on AWS but always fallen back to backtesting locally, although slower it is worth the cost in being able to very quickly analyse the results / make changes / rerun etc 

---

## 2022-03-17 20:30:35 - general channel

**Paul**

Get your files onto s3 (flumine helps), then go down container or ec2 route. Lambda not a good route IMO. YMMV

---

## 2022-03-17 11:37:48 - general channel

**Paul**

That's a very broad and open question. Do you mean EC2 vs Containers? How to get your code in there? How to move data around?

---

## 2022-03-17 08:46:01 - general channel

**Jonjonjon**

What would be the recommended way of running Flumine backtests on AWS?

---

## 2022-03-16 09:32:38 - general channel

**anomaly**

Can do! So essentially a typical situation is this:



1. you have a strategy name. say it's called "back_strat_42". it's betting on UK horseracing.

2. you have a single market_id and a single selection_id that you want to place a bet on

3. you have the liability to stake (back or lay doesn't matter).

4. you want to place just a single bet on the selection at anytime and leave it in the market. so for simplicity say a OrderType=MARKET_ON_CLOSE with PersistenceType=MARKET_ON_CLOSE

5. you want a log/trace of the result of the bet that you placed for post-race analysis (typically with a database dump)



This is to be repeated multiple times from a few different strategies but the logic is essentially as above and the current system design is to place these bets from individual threads or processes.

A follow up question is if you have dozens of these happening in parallel will it be an issue from a betfair api perspective? I'm connecting with a live api key with bot login (certs) and it seems you're only allowed 9 connections. Would this be an issue with the above setup?

---

## 2022-03-15 08:27:31 - issues channel

**Paul**

It's not turned on by default as it means giving AWS visibility of OS level metrics rather than host level metrics, but this should help you get it moving: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html|https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html|https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/mon-scripts.html)

---

## 2022-03-14 18:33:17 - issues channel

**Ruben**

is there any way to monitor RAM usage of an ec2 instance? in the same way we can see graphs with cpu &amp; network usage

---

## 2022-03-13 20:08:44 - general channel

**VT**

Guys, noob statistics question.



Assuming the probability of 1 more goal in a live event is 50% (@2.00), how do bookies calculate upcoming markets? Is it just a Poisson formula (or something like that)?  It seems too simple to use 50% as the event probability and just make the progression.

---

## 2022-03-12 19:13:36 - random channel

**Mo**

Have production and test servers both running live trading code. New features get live tested before going into production

---

## 2022-03-12 18:10:02 - random channel

**thambie1**

I spend ~$100 a month on a single 8 core dedicated  machine for production. And maybe an avg of ~$1k a month on analysis computations done in the cloud

---

## 2022-03-12 17:15:02 - random channel

**liam**

About 10 ec2 instances micro/small, RDS aurora (master/replica) I think it's two mediums, few £ on s3, cloudwatch and lambda 

---

## 2022-03-12 13:22:01 - random channel

**D**

My costs are about 10% of Mo's - spread mainly across a single ec2 server, S3/Athena data storage, sagemaker usage. I aspire to Mo's level of expenditure though.

---

## 2022-03-12 13:14:38 - random channel

**Mo**

Very roughly I would say that's split 3 ways between database, storage and servers

---

## 2022-03-11 07:23:47 - issues channel

**LM**

Which worked fine in back testing, but in production this afternoon

---

## 2022-03-08 09:42:56 - issues channel

**Unknown**

Anyone ever had its amazon ec2 instance run out of CPU credits? I think that is making one of my instance checks fail, curious to see if anyone had it happen. I can no longer SSH into the instance

---

## 2022-03-07 10:09:40 - general channel

**Nacho Uve**

If I want to save in live every placed order and its result (matched, cancelled, etc without wait to be cleared the market, how would be the best approach?



I think it could be a good idea to do that to avoid any problem with the connection, a unwanted restart of the machine, etc.

---

## 2022-03-04 11:07:07 - strategies channel

**AndyL**

Thanks. in going to restart one of my live strategies with info logging to check the flumine startup polls

---

## 2022-02-28 11:52:06 - general channel

**Tony**

Hi all :wave:, i am new to the channel. I am going to try create a  tennis trading bot as i already trade it daily but would like to try automate certain scenarios. I have a delayed API key but from reading around it doesn’t feel like it would allow me to get an accurate picture and also i would like to stream some data for backtest purposes but the betfair free files don’t seem to have what i would like either. What is the best place way to get the live key activated?

---

## 2022-02-27 11:31:07 - general channel

**Peter**

[@U033HM6Q8BU](@U033HM6Q8BU) You don't have to lose money while you're learning. Checkout the paper trading option in the Flumine documentation. You can build a model that works with live data but doesn't place live bets, just measures what would have happened if you had. Make sure to place some real bets though, so that Betfair doesn't think you're just stealing their data.

---

## 2022-02-24 13:21:38 - issues channel

**liam**

a simple integration tests where you simulate/process a market or two would have caught this, most valuable tests I have in production

---

## 2022-02-22 19:02:23 - issues channel

**AndyL**

[@UPMUFSGCR](@UPMUFSGCR) i only restart scripts when needed for strategy upgrade

I think its betfair server glitches as [@UEA14GBRR](@UEA14GBRR) is seeing them at exactly same times

---

## 2022-02-21 09:31:08 - random channel

**D C**

Do people use multiple github accounts for extra security on their code base for their production level code? Or is it generally accepted that a private github repo is sufficient for the task?

---

## 2022-02-19 10:37:41 - general channel

**Mo**

```import logging

import queue

import threading



import betfairlightweight



# setup logging

logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))  # change to DEBUG to see log all updates



# create trading instance (app key must be activated for cricket stream)

trading = betfairlightweight.APIClient("username", "password", app_key="appKey")



# login

trading.login()



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener, host="sports_data")



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()



# subscribe

streaming_unique_id = stream.subscribe_to_cricket_matches()



# check for updates in output queue

while True:

    updates = output_queue.get()

    for update in updates:

        print(update.json())```

---

## 2022-02-18 14:10:54 - general channel

**Alex Alex**

Thinking of moving my live model code into the flumine framework. I quite like that it seems to have the option to simulate execution and run backtest/live interchangeably. I want to 1) get the current score, and 2) do some calculations on all matches within an event to generate a return forecast. From the `tennsiexample.py` I gather that the correct way to do this within flumine is to add a background worker that generates a custom event once it is done doing all its calcs. Then move the return forecast into market context when processing custom event. Or is there a more straight forward way to do this within flumine?

---

## 2022-02-18 07:58:19 - random channel

**bb**

I assume you are running it on EC2. One thing I have found is that the default swap is a lot lower than on my laptop and any memory spike above the limit caused crashes. 

---

## 2022-02-16 14:32:24 - random channel

**Jonjonjon**

Suppose I ran a script locally and it had no problems, but running it on AWS caused memory issues. How would you try to debug it? I have checked that I am running the same version of the script.

---

## 2022-02-16 14:07:53 - random channel

**liam**

in the docs



```Performance tips



A single market subscription &amp; a subscription to all markets have an identical latency:

Cost is identical as the two subscriptions above would evaluate in sequence and thus with the same average latency.

Initial image is more costly to send than extra updates.

Limiting data with appropriate filters reduces initial image time

Segmented data will always out perform non-segmented data:

You will be processing a buffer while another is in-flight and another is being prepared to send

Writes to your connection are directly effected by how quickly you consume data &amp; clear your socket's buffer

Consuming data slowly is effectively identical to setting conflation.

If you receive con=true flag on a market - then you are consuming data slower than the rate of delivery.```

---

## 2022-02-14 11:55:57 - random channel

**Fab**

I roughly follow these steps to create a dedicated IAM with programmatic access to a bucket (replace `new_user` with your friend name):



• Log into AWS using your root account.

• Top right, a dropdown has an option called My Security Credentials, click on it.

• On the left menu, go to Access Management &gt; Users.

• Create a new user (e.g. `new_user`) with Programmatic Access.

• Do not add it to a group nor assign any permissions for now.

• Towards the end of the creation steps, it shows you the user’s KEY/SECRET. Write it down.

• After creation, in the user list, click on the user and note down his ARN, it looks like:

*arn:aws:iam::168242217730:user/new_user*

• On the top menu, go to Services &gt; S3, then get into the desired bucket.

• Go to Permissions &gt; Bucket Policy.

• Enter a policy similar to the json below, you need to replace the user ARN (inside AWS) and the bucket name (inside Resource).

```{

    "Version": "2012-10-17",

    "Statement": [

        {

            "Sid": "AllowAccessToNewUser",

            "Effect": "Allow",

            "Principal": {

                "AWS": "arn:aws:iam::168242217730:user/new_user"

            },

            "Action": "s3:*",

            "Resource": [

                "arn:aws:s3:::thebucket",

                "arn:aws:s3:::thebucket/*"

            ]

        }

    ]

}```

---

## 2022-02-12 08:20:41 - strategies channel

**AndyL**

Ive being running an order intensive backtest overnight on a EC2 instance, and twice now the python process has been killed after a couple of hours, no exit msgs or backtrace, indicative of kill -9?

Has anyone seen this sort of issue?

Does EC2 have any rogue process detection maybe?

---

## 2022-02-11 15:43:25 - issues channel

**foxwood**

[@U016TGY3676](@U016TGY3676) It's on a Windows server so was easier to replicate my devenv and run it that way for now rather than setting up for command line. Early days with flumine and it has helped with some needed debugging and quick fixes. PyCharm is certainly a memory hog though which with other stuff that runs is pushing the hardware.

---

## 2022-02-11 05:44:33 - general channel

**J**

Did you find a good solution [@U0266A8579B](@U0266A8579B)? I'll be in the same situation soon, moving from workstation with a 3070 on CUDA for ML to AWS

---

## 2022-02-10 19:09:17 - general channel

**Laybot McBackbot**

I have too many different values for one hot encoding. Also, the algorithm it uses for categorical data somehow is working great for me. So I would rather stick with it because between screwing around with what is working I prefer to choose the easier solution -- either get a gpu machine on AWS or continue running from home for now.



My return from this model should be able to cover the AWS cost but I would still like to save money wherever possible.

---

## 2022-02-10 14:15:00 - general channel

**Laybot McBackbot**

Does anyone know what's the cheapest GPU instance on AWS and how much it costs per hour?

---

## 2022-02-09 12:39:21 - random channel

**Unknown**

last 3 days of EC2

---

## 2022-02-09 12:23:40 - random channel

**Jonjonjon**

Recently, [@U4H19D1D2](@U4H19D1D2) posted a screenshot of his CPU utilisation on AWS. I can't find it. Does anyone have a link to it?

---

## 2022-02-08 21:51:13 - general channel

**Dan Q**

So there's the issue where atb and atl are non-virtualized and thus not very useful comparisons to the live pulled atb/atl data. The LTP is usually pretty close to both though, when there's decent liquidity (which there usually is), so it's fine. However, there are intense volatility spikes in LTP that I can't explain and had to take great pains to treat. Stuff like a team being comfortably in the lead at the very end of the game with 1.01 odds and then suddenly updates will come in at much higher odds, like 2.4 in the middle of the more sensible 1.01 prices. Stuff like that. It got to the point where for the historicals I need to calculate local volatility in the LTP time series and take the rolling median as the actual LTP and the rolling min as the best_atb just to be safe, otherwise my models perform way worse because there's so much noise.



It could be an issue of me making some incorrect assumption early on when reading in the data as I had to kind of blindly feel my way through rather than using something like BFLW to read them in properly, but I don't think so, I'm pretty sure I did it right as I've reviewed it a lot.

---

## 2022-02-08 20:40:41 - general channel

**liam**

Certainly recommend streaming, much lighter on CPU and reduces the complication. 



What issues are you seeing the historical data? Thought the pro stuff was good.



flumine can do whatever you want, the selling point is the switch to backtest / paper / live with no changes to your code.  Switching wouldn’t make sense depending on how advanced your current setup is and/or you want some of the features. 

---

## 2022-02-08 20:17:35 - general channel

**Dan Q**

Hey, just wanted to share my process and do a quick sanity check since I'm not super familliar with BFLW/Flumine. This is my process right now:

• Container on AWS ECS recording Betfair odds data every 5 seconds - using the API via BFLW - for the markets I'm interested in (NBA) and writing to a DB

• Model trained on historical data from Betfair pro files (lots of cleaning/processing as there are very weird behaviors in the historicals that don't reflect the live environment)

    ◦ All historical reading and treatment done with code I wrote myself, no BFLW here

• External data read from other sources and also fed into a DB. Historicals are used along with BF historicals to train the model.

• Live, constantly running betting bot reads the most recent data from the DB for live games joined with external data, manipulates it to the format the model expects, and makes a decision based on model output and strategy parameters

    ◦ Again a custom class/process, BFLW used only to connect to the API to collect bankroll info and place orders

• If bet decision is made, sends a fill or kill limit order through BFLW client

• Strategy parameters are determined through intensive backtesting/bootstrapping on the live data collected so far

    ◦ Also a custom backtesting suite I wrote to calculate expected profitability and other stats over n games in the season based on resampling previous games

This is working well and is thankfully profitable so far, but I feel like I may have reinvented the wheel at least five times during the whole thing considering BFLW/Flumine exists. I've seen a lot of mention of backtesting on Flumine, would it also allow more complex operations like I described with using external data and output from a model? What is the benefit of doing so vs. the solution I've described? One thing I'm not doing which I know I could be is streaming and recording the data instead of the 5-second snapshots I'm using right now, but so far it's served me well and I haven't seen a reason to switch.

---

## 2022-02-06 10:23:31 - issues channel

**Peter**

Very basic:



```import os

import logging

import queue

import threading



import betfairlightweight



# setup logging

# logging.basicConfig(level=logging.DEBUG)  # change to DEBUG to see log all updates

logging.basicConfig(

    level=logging.DEBUG,

    filename='tpd/race-subscription.log',

    filemode="a",

    format='%(asctime)s - %(levelname)s - %(message)s',

)



# create trading instance (app key must be activated for streaming)

workspace_prefix = os.getenv("WORKSPACE_PREFIX")

trading = betfairlightweight.APIClient(os.getenv("BETFAIR_USERNAME"), os.getenv("BETFAIR_PASSWORD"), app_key=os.getenv("BETFAIR_LIVE_KEY"), certs=workspace_prefix + "certs")



# login

trading.login()



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener)



# subscribe

streaming_unique_id = stream.subscribe_to_races()



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()



# check for updates in output queue

while True:

    update = output_queue.get()

    print(update)```

---

## 2022-02-03 16:14:05 - issues channel

**foxwood**

Solved - bit brain bendy. In check_market_book I was using market.seconds_to_start to ignore unwanted early packets. When used in backtest that field is calculated using the packet "published time" but for live usage is calculated using "now". Since live always has some latency, at the time it considered it needed to start, the current packet to be processed was actually timestamped 200ms earlier than the wanted start time. Proved it by winding the stream back by 1 packet and it did exactly as the live did and eventually bet on the same runner. QED



My solution is therefore to use the "published time" as the processing trigger and not market.seconds_to_start which is a true real time value. The backtest and live "should" then be in sync I hope.



Be interesting to find out if we do all get the same timestamps - thanks for comments [@UBS7QANF3](@UBS7QANF3)

---

## 2022-02-03 12:24:13 - issues channel

**foxwood**

Puzzled by differences between live and backtest that I'm not expecting. Yesterday recorder and live betting were running on same VPS as separate processes under PyCharm.  Placed 28 live bets. Running the backtest later on it came up with bets on different runners on 4 of the races compared to what the live had done - sufficient to show a significantly different PL.



Looked at one in detail - no latency issues in log - recorder saw bet placed 187ms after logged by bet process. EX_BEST.. not used. Strategy source code the same for backtest/live. Betting triggering on EX_ALL.. price movements.



I expected the backtest to trigger on the same runner at the same time as the live did.



The recorder packets seem to be 4, 5 or 6 per second - could it be dropping packets that the live betting is seeing ? I thought they were all kept and cached if the app couldn't keep up ie excess latency compared to "pt" - ie not possible to miss any packets that BF push out ?



Any suggestions as to cause or where to look next ?

---

## 2022-02-02 07:53:20 - general channel

**Dave**

Free tier AWS should be sufficient to validate the above.

---

## 2022-02-02 05:47:16 - general channel

**Mo**

Firstly, you are being paranoid, this has nothing to do with you being blocked



It's a simple network error and the fact you have 200 mb fibre is irrelevant. Your bandwidth has nothing to do with latency and the packets have a very long way to travel to you from Dublin



I don't speak Brazilian Portugese but I believe it's saying the remote host (i.e. Betfair) has closed the connection. Possibly at the TCP level your ACK packets are not getting back to them so they believe the connection has been lost. You might be able to keep the connection from dropping by tweaking some of your network settings but I don't use Windows so can't offer any specific advice there



The simple solution is what you're already doing which is to use a server (much) closer to Betfair

---

## 2022-02-01 23:12:06 - general channel

**VT**

Hi [@U030L8PPSHM](@U030L8PPSHM), VM stands for Virtual Machine? If so, I have no problems with the VPS, I never had an error in the market stream hosted on AWS, the error is on my local machine and has nothing to do with CPU or memory.

---

## 2022-02-01 23:08:43 - general channel

**Alex Alex**

Never encountered this kind of error before but my guess would be that it takes some time to get a response for your request reaching the server. `Latency high` sounds like it. What is the CPU load on your VM when you encounter this error?

---

## 2022-02-01 22:45:22 - general channel

**VT**

Guys, if anyone can help, I'm not able to proceed with my tests, every 5 minutes the error stream.



I ran the example code for examplestreamingerrhandling.py errors and the answer is below.



I even thought it could be due to data consumption, but the account I currently use must have already paid more than 20 thousand pounds just in commissions. I use a VPS on AWS and I leave the stream market running there, I believe it has nothing to do with it, I connect to the server and my computer at the same time.



I'm going to stop testing because I'm afraid my account will be blocked.



+++++++++++++++++++++++++++++++++++++++++++++++++++++++



The latency error appears 100% of the time, but it used to be between 0.6 and 0.9, this value above 1 had never seen. I'm in Brazil and this is the best we can get, 200mb download and upload, fiber optics.



WARNING:betfairlightweight.streaming.stream:[MarketStream: 1003]: Latency high: 1.6549324989318848



+++++++++++++++++++++++++++++++++++++++++++++++++++++++



ERROR:__main__:MarketStreaming run error



Traceback (most recent call last):

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 230, in _receive_all

    part = self._socket.recv(self.buffer_size)

  File "C:\Users...\anaconda3\lib\ssl.py", line 1226, in recv

    return self.read(buflen)

  File "C:\Users...\anaconda3\lib\ssl.py", line 1101, in read

    return self._sslobj.read(len)

ConnectionResetError: [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "C:\Users...\AppData\Local\Temp/ipykernel_6984/859485830.py", line 61, in run

    self.stream.start()

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 60, in start

    self._read_loop()

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 212, in _read_loop

    received_data_raw = self._receive_all()

  File "C:\Users...\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 234, in _receive_all

    raise SocketError("[Connect: %s]: Socket %s" % (self._unique_id, e))

betfairlightweight.exceptions.SocketError: [Connect: 1003]: Socket [WinError 10054] Foi forçado o cancelamento de uma conexão existente pelo host remoto

INFO:__main__:Starting MarketStreaming

---

## 2022-01-30 16:24:07 - issues channel

**birchy**

Has something changed in the Flumine order validation, because, for example, a LAY bet of £1@10.0 is now throwing:

`Order has violated: ORDER_VALIDATION Error: Order size is less than min bet size (2) or payout (10) for currency`

This is a 6+ months running strategy (live) which is still working ok, but fails when backtesting.

If I change the size to £2, validation passes.

FYI: live is running on Flumine 1.20.13 and backtesting is 1.21.2

Ok, so this seems to be related to PR #546:

[https://github.com/liampauling/flumine/blob/770fcf967dab590cba63350dd2ee46839363f8d1/flumine/controls/tradingcontrols.py#L71|https://github.com/liampauling/flumine/blob/770fcf967dab590cba63350dd2ee46839363f8d1/flumine/controls/tradingcontrols.py#L71](https://github.com/liampauling/flumine/blob/770fcf967dab590cba63350dd2ee46839363f8d1/flumine/controls/tradingcontrols.py#L71|https://github.com/liampauling/flumine/blob/770fcf967dab590cba63350dd2ee46839363f8d1/flumine/controls/tradingcontrols.py#L71)

As far as I can tell, £1@10.0, £0.10@100, etc should still be valid?

[https://docs.developer.betfair.com/plugins/servlet/mobile?contentId=3833942#placeOrders-Abilitytoplacelowerminimumstakesatlargerprices|https://docs.developer.betfair.com/plugins/servlet/mobile?contentId=3833942#placeOrders-Abilitytoplacelowerminimumstakesatlargerprices](https://docs.developer.betfair.com/plugins/servlet/mobile?contentId=3833942#placeOrders-Abilitytoplacelowerminimumstakesatlargerprices|https://docs.developer.betfair.com/plugins/servlet/mobile?contentId=3833942#placeOrders-Abilitytoplacelowerminimumstakesatlargerprices)

---

## 2022-01-28 17:39:17 - general channel

**VT**

I would also like to say that I am very grateful for everyone's help, even with my primitive knowledge *I can send and close orders (market stream hosted in the UK) faster than any program I've ever used*. Having access to the stream also opens up many possibilities for semi-automated trading in live football.

---

## 2022-01-28 17:33:01 - general channel

**VT**

yes, I've known this for some time, I'm using python to create my strategies, commands, shortcuts and after everything is tested I intend to invest in someone to reprogram in C.



But only after everything is tested because I use other APIs and database to 'predict' the price of goals live in football.

---

## 2022-01-28 15:00:41 - random channel

**James T**

I don't think I would have contributed anything more / less if it had Py in the name actually. I might have joined earlier though if it didn't have Py in the name. I'd probably contribute to C# code if there was a starting base already, but there's probably no demand for a C# base to start with. 



Betfair in the name draws me in because that is my primary platform. I guess maybe the question is do people choose Betfair and then look for a library / community, or does someone want to learn how to bet professionally and then look for a library to see what it provides and how others do it?

---

## 2022-01-26 09:40:36 - issues channel

**foxwood**

Don't know if it's common but had a couple of these overnight on OrderStream which rebuilt connection ok "betfairlightweight\\streaming\\betfairstream.py\", line 242, in _receive_all\n    raise SocketError(\nbetfairlightweight.exceptions.SocketError: [Connect: 1003]: Connection closed by server" -  anything to worry about ?

---

## 2022-01-26 08:58:39 - issues channel

**liam**

Ah, unless you have experience in building a trading system I recommend having a read of some code. Few schools of thought on how one should be built but the central idea is to consume market/order data and make decisions. 



flumine uses a FIFO queue with strategies, threads for streams and workers and a thread pool for execution. Its lightweight and fast whilst still being adapted per use case, as well as being very easy to get started.



Mo's design gives you a highly available / scalable (complicated) system that can do whatever you want and at a guess very easy to deploy upgrades etc. But I wouldn’t recommend this to someone starting out.

---

## 2022-01-26 08:50:37 - issues channel

**Jorge**

I use betfairlightweight to stream the orders and data. My live bots use this data but they do not trade with flumine

---

## 2022-01-25 19:14:14 - strategies channel

**foxwood**

Well, well, well. Changed the time server from [http://time.nist.gov|time.nist.gov](http://time.nist.gov|time.nist.gov) (win default) to [http://ntp1.npl.co.uk|ntp1.npl.co.uk](http://ntp1.npl.co.uk|ntp1.npl.co.uk) and the log is sitting there nicely empty (no flumine logging though ??). That is a stratum 2 ntp server ie more accurate than most. Maybe tomorrow I will get to run live properly :joy:

---

## 2022-01-25 17:48:36 - strategies channel

**birchy**

Yeah, check that clock is syncing to an NTP server. Should be automatic on Linux but can be flakey on Windows. Also in PyCharm, if you run in debug mode, there's extra overheads.

---

## 2022-01-25 16:17:28 - strategies channel

**foxwood**

VPS running 2 x mktrec + 1 x BA fine today - 5 connections. Tried strategy live locally today (simply removed paper_trade=True). Seemed to log in ok but after 20 mins log had about 18k lines all like this `{"asctime": "2022-01-25 10:21:47,343", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.8610439300537109"}` No other message about set up or starting of flumine which appears normal in mktrec. Message printed "starting strategy xxx"  when strat class started so it was starting off ok but that was all. Killed it after 20 mins and went for long walk lol. Any suggestions/clues - I can see the time it gives exceeds max latency of 0.5 but presume things should still function but quite slowly.

---

## 2022-01-25 09:42:52 - strategies channel

**foxwood**

[@U4H19D1D2](@U4H19D1D2) Was a dead connection I believe. I'd think I'd bust the connection limit - I had 5 open on a VPS (2 x mktrecord + 1 BA) and then my paper testing crashed or I thought of something to change so killed it to make a quick change. That meant 2 connections were abandoned each time and I read here somewhere that it takes about 15 mins for them to clear them. I know I killed it about 10 times in 30 mins lol. End result was BF pulled the plug on the earliest connections or maybe all connections for a short time. Killed all the server jobs including BA. The latency message was on server with a dead connection. Must find a way of doing a graceful terminate that closes connections - running under PyCharm currently. Thanks anyway - see what today brings lol.

---

## 2022-01-24 21:55:24 - issues channel

**birchy**

While on the subject of RAM, how do you guys handle "big" backtests/signal processing with Flumine? I've found that if I run a strategy that produces 200k+ bets, the Framework eventually falls over because my 4GB AWS machine falls over with OOM errors. I know that increasing the amount of RAM is a solution (as is running on my local 8GB machine), but am wondering if there's a way to break up the framework into smaller chunks so that it can run on lower spec hardware?

---

## 2022-01-24 21:22:02 - strategies channel

**foxwood**

Was worth doing as [@U4H19D1D2](@U4H19D1D2) said lol. Many confusing issues. My connection to BF from server stopped late afternoon. At that same time I had logged in to test and live at least 7 times in 20 mins after fixing issues from a local PC and also started a BA instance. Think I blew the connection limits since no graceful termination - just killed flumine. Local ran for the rest of the day but server didn't get any live data it seems and stuck cycling non-stop with Latency high: 3317.0....."". Not sure if they simply killed that IP. See what tomorrow brings.

---

## 2022-01-23 23:11:43 - strategies channel

**foxwood**

I want to shift a backtest strategy to live - my first live flumine play. The strategy is based on the DataCollectWOM strategy example at [https://github.com/liampauling/flumine-strategy-development/blob/master/main.py](https://github.com/liampauling/flumine-strategy-development/blob/master/main.py)



Obvious things I need to do is change the wrapper to use live client login, remove mock_patch and provide a bflw streaming market filter.



There are 2 possible gotchas in my strategy where I'm not sure if things exist in live ie will it crash if these references are left in:

a) market_catalogue is used to get the runner name - is this just a backtest feature and should this middleware be removed if a catalogue is constructed during live streaming ?

b) in function process_closed_market the field "order.simulated.profit" is printed and logged - will that field exist or will this crash things if the reference is left in ?



Any advice or thoughts about other things I might not think of first time more than welcome

---

## 2022-01-23 11:36:44 - random channel

**Peter**

I think that AWS's philosophy is to give you the tools to produce alerts to your specific requirements. If they offered buttons for this stuff, people would come up with 1001 different use cases and their admin pages would be an unintelligible mess of buttons.

---

## 2022-01-23 10:01:35 - random channel

**Jorge**

I'd have expected AWS to include a button to do this in the Website, though

---

## 2022-01-23 10:00:53 - random channel

**Jorge**

Thanks, I created a Disk and RAM alarms with CloudWatch following this tutorial: [https://kumargaurav1247.medium.com/aws-cloudwatch-agent-installation-for-memory-metric-integrate-with-grafana-365404154](https://kumargaurav1247.medium.com/aws-cloudwatch-agent-installation-for-memory-metric-integrate-with-grafana-365404154)

---

## 2022-01-23 08:34:04 - random channel

**Ruben**

I created an alert for when my estimated billing costs went above a threshold using cloudwatch, following this tutorial: [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html) I imagine creating alerts for other metrics would be very similar

---

## 2022-01-22 11:13:19 - random channel

**Jorge**

Hey guys, I finally moved from my crappy server provider to AWS but I am having trouble configuring an Alarm for Disk Space and RAM usage. Anyone knows a simple way to do this?

---

## 2022-01-18 19:59:21 - strategies channel

**liam**

Context is your safe place, just the flumine way of storing your own data in flumine objects that won’t be cleared etc.



Not naughty but not very pythonic and your IDE might struggle with hints 



No, only on a restart of the python process would the data be cleared. There would be a case of this if betfair closed a market and then recreated after 1hr but not sure that has ever happened



Designed so that strategies can be used live, paper and simulated with no changes 

---

## 2022-01-18 19:54:32 - strategies channel

**foxwood**

I have built a strategy using "datacollectwom.py" as a template. In that example it adds a new dictionary variable named "data" to market.context the first time any new market is passed to check_market_book. I've copied that approach for a strategy that watches market movements and prices and bets when appropriate. That's testing out reasonably so far with history files but I've hit some questions / doubts ...



1) is there a special reason for using market.context for the "data" variable or could I do something safely at a higher level without interfering with bflw/flumine eg market.mydatavar = {} ?

2) is it common / naughty in Python to add a variable dynamically like that ?

3) if the "enhanced" market object is deleted then any data in that variable would be lost. In bflw or flumine is there a case of a market object being deleted and recreated from scratch ie some sort of mcm case where the approach is to discard what was held and rebuild the market object from scratch ?

4) the main.py for the datacollectwom is processing history files - would the same structure /code for a strategy (less mock_patch) work with live streaming data ie would it just be simply a change of input source for the json ?



Sorry if some of these questions seem basic but trying to fill in some of my python and flumine gaps

---

## 2022-01-18 07:46:55 - general channel

**liam**

Looks to be network or server side, quite common, are you using the error handling example?

---

## 2022-01-16 05:45:03 - general channel

**Colin**

Does anyone have any experience with developing their own bot and using it with a stake that is below the default minimum amount in your region/country for a prolonged period of time and for betting types that were not classed as closing out? If so, did you receive a warning from Betfair quite swiftly?

I note the 4th bullet point here, [https://developer.betfair.com/en/exchange-api/faq/](https://developer.betfair.com/en/exchange-api/faq/)

I'm keen to production-ise my bot soon, but I also want to start slow given it is not feasible for me to use Betfair's historical data to back test the algorithm I am using.

---

## 2022-01-15 12:33:56 - strategies channel

**foxwood**

I'm developing some analysis based on the example datacollectinwom.py and am gathering lists of data per runner for graphing etc. I hadn't allowed for getting a None price from non-runners so it blew up (expected behaviour lol). However, that got me thinking about some key assumptions I had made and I wonder if they were safe, particularly with non-runners - maybe those with more knowledge could comment.

Assumptions:

1) the strategy callback argument "market_book" is the same object as could be accessed at "market.market_book"

2) the strategy callback "check_market_book" is made for every "mcm" streaming packet received and if that returns True then "process_market_book" is called - this means the strategy is offered every packet

3) for each callback all the data from the supplied market/market_book pointers has been updated to reflect the changes implied by the packet

4) the list of runners in the market_book and their index order is invariant ie runner Fred will be at the same index in "market_book" from first recording through to closed market



I think the first three are correct but not sure relying on the index position is safe - perhaps selection_id has to be used as a dict key for my saved lists which would slow things down quite a bit - important if it was a live betting strategy ?



I have noticed that BF change the "sort_priority" field for runners when there are withdrawals but not sure how/if that impacts flumine.



One area of difference I have found is with "market.market_book.market_definition.runners" which appears to be in BF sort order with N/R at the end whereas "market.market_catalogue.runners" and "market.market_book.runners" appear to remain in fixed positions regardless of withdrawal.

---

## 2022-01-15 11:42:26 - random channel

**Newbie99**

I guess the problem then is you might run out of connections (as presumably if you are collecting data using the same filters as a live strategy flumine shouldn't open up a new connection). But if that is the best approach then I can revert back to that, in a sense its easier to record locally as it saves transferring everything and then can test on the same machine!

---

## 2022-01-15 11:36:37 - random channel

**Peter**

Last conversation I remember here o this topic said the opposite, i.e. keep data collection separate from strategy execution. However it's very possible to combine the execution of multiple strategies on a single server and this is generally what people advise.

---

## 2022-01-15 09:30:23 - random channel

**Newbie99**

Apologies as this is a bit open ended, but I'm not understanding what went wrong here, last night around 1 am, out of (seemingly) no-where it looks like I suddenly got latency warnings and then eventually the process killed:



```{"asctime": "2022-01-15 01:00:05,664", "levelname": "INFO", "message": "Placing new LAY order for runner ('Jaime Jamel', 35971913, 0), runner exposure is: {'selection_id': 35971913, 'handicap': 0, 'if_win': 0.539, 'runner_matched_stake': 0.0, 'runner_matched_exposure': 0.0, 'min_market_exposure': -18.509999999999998} and limit is: 0.3."}

{"asctime": "2022-01-15 01:00:05,664", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.193372878", "selection_id": 35971913, "handicap": 0, "id": "138615012056645370", "customer_order_ref": "7c44d7544ce40O138615012056645370", "bet_id": null, "date_time_created": "2022-01-15 01:00:05.664573", "publish_time": "2022-01-15 01:00:05.647000", "market_version": null, "async": false, "trade": {"id": "7a2c3bee-759e-11ec-b096-02054918c7c6", "strategy": "betting_market_lay", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138615012056645370"], "offset_orders": [], "notes": "R3 5f Allw,2022-01-15 01:02:00,WIN,Flat,US,Charles Town,False,False,None,False,Jaime Jamel,2.64,2479.75,8,1,False,None,0,Charles Town (US) 14th Jan,31175177,7,{'strategy_name': 'betting_market_lay', 'selection_id': 35971913, 'handicap': 0, 'price_percentage': 0.768, 'min_valid_external_value': 0.8, 'current_back_price': 48, 'current_mid_price': 56.5, 'current_lay_price': 65, 'previous_back_price': 50, 'previous_mid_price': 62.5, 'previous_lay_price': 75},1", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 50.0, "size": 0.31, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 0.31, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Pending", "status_log": "Pending", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "R3 5f Allw,2022-01-15 01:02:00,WIN,Flat,US,Charles Town,False,False,None,False,Jaime Jamel,2.64,2479.75,8,1,False,None,0,Charles Town (US) 14th Jan,31175177,7,{'strategy_name': 'betting_market_lay', 'selection_id': 35971913, 'handicap': 0, 'price_percentage': 0.768, 'min_valid_external_value': 0.8, 'current_back_price': 48, 'current_mid_price': 56.5, 'current_lay_price': 65, 'previous_back_price': 50, 'previous_mid_price': 62.5, 'previous_lay_price': 75},1", "market_notes": null}

{"asctime": "2022-01-15 01:00:30,284", "levelname": "INFO", "message": "1.193373059: 0 cleared orders found, more available: False"}

{"asctime": "2022-01-15 01:01:13,244", "levelname": "WARNING", "message": "[FlumineStream: 4002]: Latency high: 0.9758524894714355"}

{"asctime": "2022-01-15 01:01:13,245", "levelname": "WARNING", "message": "[MarketStream: 3001]: Latency high: 0.9100987911224365"}

{"asctime": "2022-01-15 01:01:13,245", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 0.8022336959838867"}

{"asctime": "2022-01-15 01:12:41,802", "levelname": "INFO", "message": "Deleting requests.Session", "sessions_created": 36, "session": "&lt;requests.sessions.Session object at 0x7f2c6a5195d0&gt;", "session_time_created": 1642208309.3769233, "session_time_returned": 1642208405.607362, "live_sessions_count": 1, "err": true}

{"asctime": "2022-01-15 01:12:41,813", "levelname": "WARNING", "message": "[MarketStream: 3001]: Latency high: 689.2287058830261"}

{"asctime": "2022-01-15 01:12:41,814", "levelname": "WARNING", "message": "[FlumineStream: 4002]: Latency high: 689.5181279182434"}

{"asctime": "2022-01-15 01:12:41,814", "levelname": "WARNING", "message": "[MarketStream: 2001]: Latency high: 689.121832370758"}```

The only change I've made (and so at this stage I'm assuming is the culprit) is to run the market recorder on my live instance, as opposed to my local test instance (previously I recorded locally and kept my live stuff separate), so I added this just before framework.run() on my live code in AWS:



```recorder = MarketRecorder(

            name="MR_horse_greyhounds",

            market_filter=streaming_market_filter(

                    event_type_ids=[7],

                    country_codes=['GB','IE','US','FR', 'AU'],

                    market_types=['WIN', 'PLACE', 'EACH_WAY'],

                ),

            stream_class=DataStream,

            context={

                "local_dir": ai.unprocessed_data,

                "bucket": "fluminetest",

                "force_update": False,

                "remove_file": True,

            },

        )



        framework.add_strategy(recorder)```

Am I potentially trying to record too much in one go and/or from the error am I somehow creating too many sessions (and if so how might I stop this)?



Again apologies for the open ended nature, I'm clearly not understanding something about sessions I feel!

---

## 2022-01-14 08:21:12 - issues channel

**Colin**

This appears to be the instigating exception,



Traceback (most recent call last):

  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/connectionpool.py", line 382, in _make_request

    self._validate_conn(conn)

  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/connectionpool.py", line 1010, in _validate_conn

    conn.connect()

  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/connection.py", line 411, in connect

    self.sock = ssl_wrap_socket(

  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/util/ssl_.py", line 449, in ssl_wrap_socket

    ssl_sock = _ssl_wrap_socket_impl(

  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/urllib3/util/ssl_.py", line 493, in _ssl_wrap_socket_impl

    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)

  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py", line 500, in wrap_socket

    return self.sslsocket_class._create(

  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py", line 1040, in _create

    self.do_handshake()

  File "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py", line 1309, in do_handshake

    self._sslobj.do_handshake()

socket.timeout: _ssl.c:1112: The handshake operation timed out

---

## 2022-01-13 11:14:21 - general channel

**rob smith**

`import betfairlightweight`

`import datetime`

`import pandas as pd`

`from betfairlightweight import filters`

`from time import sleep`

`from datetime import datetime`

`from datetime import timedelta`



`username = ""`

`password = ""`

`api_key = ""`

`pd.set_option('display.max_rows', 1000)`



`def current_time(message):`

    `current_time = datetime.now()`

    `current_time = current_time.strftime('%H:%M:%S:%f')`

    `print(f'{current_time}: {message}')`



`def get_todays_games():`



    `trading = betfairlightweight.APIClient(`

            `username, password, app_key=api_key`

        `)`

    `trading.login_interactive()`

    `print("Logged in")`



    `trading.keep_alive()`



    `t20_filter = betfairlightweight.filters.market_filter(`

        `event_type_ids=[4],`

        `market_type_codes=["MATCH_ODDS"],`

        `market_start_time={`

            `'to': (datetime.utcnow() + timedelta(days=1)).strftime("%Y-%m-%dT%TZ")`

        `}`

    `)`

    `t20_events = trading.betting.list_events(`

        `filter=t20_filter`

    `)`



    `t20_events_today = pd.DataFrame({`

        `'Event Name': [event_object.event.name for event_object in t20_events],`

        `'Event ID': [event_object.event.id for event_object in t20_events],`

    `})`



    `print(t20_events_today)`



    `print()`

    `event_id = input("Input Event ID:  ")`

    `print()`



    `market_catalogue_filter = betfairlightweight.filters.market_filter(event_ids=[event_id])`



    `market_catalogues = trading.betting.list_market_catalogue(`

        `filter=market_catalogue_filter,`

        `max_results='100',`

        `sort='FIRST_TO_START'`

    `)`



    `market_types_mooney_valley = pd.DataFrame({`

        `'Market Name': [market_cat_object.market_name for market_cat_object in market_catalogues],`

        `'Market ID': [market_cat_object.market_id for market_cat_object in market_catalogues],`

        `'Total Matched': [market_cat_object.total_matched for market_cat_object in market_catalogues],`

    `})`

    `print(market_types_mooney_valley)`



    `print()`

    `market_id = input("Input Market ID:  ")`

    `print()`



    `market_books = trading.betting.list_market_book(`

            `market_ids=[market_id],`

            `price_projection=filters.price_projection(`

                `price_data=filters.price_data(ex_all_offers=True)`

            `),`

        `)`



    `while True:`

        `for market_book in market_books:`

            `print(` 

                `market_book.market_id,`

                `market_book.inplay,`

                `market_book.status,`

                `market_book.total_matched,`

            `)`

        `sleep(1)`



        `if market_book.inplay == True:`



            `while True:`

                `for runner in market_book.runners:`

                    `print(`  

                        `runner.selection_id,`

                        `runner.status,`

                        `runner.last_price_traded,`

                        `runner.total_matched,`

                    `)`

                `sleep(1)`



`get_todays_games()`

---

## 2022-01-12 16:42:34 - general channel

**Stefan**

Of course if you work with historical data from different sources, you could have problems, but your question was about live data, right?

---

## 2022-01-10 22:07:59 - random channel

**Jonjonjon**

Is there a way to check how many streaming connections each of my servers is using?

---

## 2022-01-10 15:12:38 - random channel

**Stefan**

[@UPMUFSGCR](@UPMUFSGCR) My connection latency to betfair servers is around 120ms. I wrote my test code for 4 combination using rest api or streaming api, and placing 10 bets in one api call, or bet by bet in succession checking whether rest or order streaming api updated market bets.  Results are following:



                            | RestApiInOneApiCall -&gt; 0.504 seconds

                            | RestApiBetByBet -&gt; 3.134 s

                            | StreamingApiInOneApiCall -&gt; 0.332 s

                            | StreamingApiBetByBet -&gt; 2.308 s



[https://github.com/StefanBelo/Bfexplorer-BOT-SDK/blob/master/TestBetPlacementSuccession/Program.fs](https://github.com/StefanBelo/Bfexplorer-BOT-SDK/blob/master/TestBetPlacementSuccession/Program.fs)

---

## 2022-01-09 19:11:37 - general channel

**Jimmy**

Are there any docs on tying the data/any live score data into Flumine?

---

## 2022-01-08 09:18:32 - strategies channel

**birchy**

I was just thinking that after I'd posted. :man-facepalming: So in a nutshell, the only way to separate orders is to run a separate AWS instance per strategy? OR...use one instance with logging control for each strategy (which I do anyway) and only use the polled listClearedOrders data for basic analysis at account level.

---

## 2022-01-08 08:42:51 - strategies channel

**birchy**

[@U4H19D1D2](@U4H19D1D2) I'm a bit confused.... The reason I changed the customer_strategy_ref was because I have one deployment server which runs 2 independent frameworks (one per betfair account), each of which has 1+ independent strategies. Obviously they're all on the same hostname, so all of the orders had the same reference which was a PITA. So what's best practice for this kind of setup?

---

## 2022-01-07 18:02:44 - issues channel

**Ke**

not sure if market.blotter.strategy_selection_orders(self, runner.selection_id, runner.handicap) is the correct way to get the live status of my order. The order i get from this method is telling me that my order still 100% unfilled, but it is actually filled already when i check on web

---

## 2022-01-07 14:52:07 - random channel

**Jonjonjon**

Has anyone here used ray?[https://www.ray.io/|https://www.ray.io/](https://www.ray.io/|https://www.ray.io/)



I'm wondering if that's what I need to do my backtesting on AWS. Though I'm not yet sure how to get all my Betfair historical files up there...

---

## 2022-01-07 13:40:53 - random channel

**Jonjonjon**

Suppose you didn't have access to a local machine with several CPU cores and wanted to run a Flumine backtest using python multiprocessing. Is it practical to launch an aws instance to run the cpu-intensive task. Then stop it and analyse the results. Then rinse and repeat several times? Is there much overhead in starting/stopping an aws?

---

## 2021-12-30 19:07:18 - random channel

**mandelbot**

Problem with it is you can't offload your share in an artwork unless you live in the US and have a buyer. You'd have to wait till the fund sells the artwork. So it's a long term commitment

---

## 2021-12-30 18:44:13 - random channel

**James T**

I think it can make sense to have a regional bias to where you live though. I believe the UK Vanguard LifeStrategy 100% equity fund is global equities, but has a higher weighting towards the UK for example. 

---

## 2021-12-28 00:02:58 - general channel

**VT**

I searched the Betfair API documentation and it seems to me that there is really only one way to send orders (via placeOrders) but here in Python it takes almost 1 second to place bets (on pre-live market), a little slow for me.

---

## 2021-12-27 00:41:20 - general channel

**VT**

I'm getting this error when I run the basic BFLW code for stream market, does anyone know why?

The error happens when I send an order to the market (by an external software).



import queue

import threading

import betfairlightweight

from betfairlightweight.filters import streaming_order_filter



trading = betfairlightweight.APIClient("username", "password", app_key="appKey")

trading.login()



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(output_queue=output_queue)



# create stream

stream = trading.streaming.create_stream(listener=listener)



# create filters

order_filter = streaming_order_filter()



# subscribe

streaming_unique_id = stream.subscribe_to_orders(

    order_filter=order_filter,

    conflate_ms=1

)



# start stream in a new thread (in production would need err handling)

t = threading.Thread(target=stream.start, daemon=True)

t.start()



# check for updates in output queue

while True:

    current_orders = output_queue.get()

    print(current_orders)



===================================================



Exception in thread Thread-10:

Traceback (most recent call last):

  File "C:\anaconda3\lib\threading.py", line 932, in _bootstrap_inner

    self.run()

  File "C:\anaconda3\lib\threading.py", line 870, in run

    self._target(*self._args, **self._kwargs)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 60, in start

    self._read_loop()

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 219, in _read_loop

    self._data(received_data)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\betfairstream.py", line 258, in _data

    if self.listener.on_data(received_data) is False:

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\listener.py", line 151, in on_data

    self._on_change_message(data, unique_id)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\listener.py", line 194, in _on_change_message

    self.stream.on_update(data)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\stream.py", line 73, in on_update

    img = self._process(data[self._lookup], publish_time)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\stream.py", line 209, in _process

    order_book_cache.update_cache(order_book, publish_time)

  File "C:\anaconda3\lib\site-packages\betfairlightweight\streaming\cache.py", line 587, in update_cache

    runner.matched_backs.update(order_changes["mb"])

TypeError: update() missing 1 required positional argument: 'active'

[&lt;CurrentOrders&gt;]

---

## 2021-12-26 20:37:46 - general channel

**S G**

Based on the below link,

[https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1635427333064000?thread_ts=1635415964.059500&amp;cid=C4HL6EZTQ](https://betfairlightweight.slack.com/archives/C4HL6EZTQ/p1635427333064000?thread_ts=1635415964.059500&amp;cid=C4HL6EZTQ)



I tried to create the below strategy, but failed. Any ideas?

```strategy = MarketRecorder(

    name=recorder_name,

    market_filter=[

        streaming_market_filter(

            event_type_ids=[2],

            market_types=["SET_BETTING", "MATCH_ODDS"]

        ),

        streaming_market_filter(

            event_type_ids=[2],

            market_types=["SET_WINNER", "NUMBER_OF_SETS"]

        ),

    ],

    stream_class=DataStream,

    context={

        "local_dir": DATA_DIR_TENNIS,

        "force_update": False,

        "remove_file": True,

        "remove_gz_file": False,

    },

)```

I get the below error:

{"asctime": "2021-12-26 20:26:04,798", "levelname": "ERROR", "message": "DataStream 3001 run error", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\sg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\flumine\\streams\\datastream.py\", line 143, in run\n    self._stream.start()\n  File \"C:\\Users\\sg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 60, in start\n    self._read_loop()\n  File \"C:\\Users\\sg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 212, in _read_loop\n    received_data_raw = self._receive_all()\n  File \"C:\\Users\\sg\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\betfairlightweight\\streaming\\betfairstream.py\", line 242, in _receive_all\n    raise SocketError(\nbetfairlightweight.exceptions.SocketError: [Connect: 3002]: Connection closed by server"}

{"asctime": "2021-12-26 20:26:06,813", "levelname": "INFO", "message": "Starting DataStream 3001", "stream_id": 3001, "market_filter": {"eventTypeIds": [2], "marketTypes": ["SET_WINNER", "NUMBER_OF_SETS"]}, "market_data_filter": {"fields": ["EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP", "EX_MARKET_DEF", "SP_TRADED", "SP_PROJECTED"]}, "conflate_ms": null}

---

## 2021-12-24 18:53:14 - issues channel

**Jimmy**

Hello! I’m using a LoggingControl for live but realised I’ve left the orders profit as simulated as per the backtest example. I’ve gone to change it and thought removing the simulated would do it but it’s not a property on the object. I’m certain I’m being a bit daft and missing the obvious — can anyone steer me towards what I’m missing and how to record the order profit? Thanks in advance and Merry Christmas! :slightly_smiling_face:

---

## 2021-12-22 23:04:14 - random channel

**Jonjonjon**

[@U4H19D1D2](@U4H19D1D2) I'm looking at transactions again, using something like:



`with market.transaction() as t:`

    `t.place_order(order1)`

    `t.place_order(order2)`



Is there a way to get the full transaction to fail if either `order1` or `order 2` fails to validate? From what I know, Betfair will fail both orders if either of them is invalid (please correct me if I'm wrong!). However (and perhaps I'm not using it properly), I am seeing one of the orders being matched (in a backtest) if the other fails.



As a hack, I am using this code but it feels like a bit of an abomination:



```                        with market.transaction() as t:

                            all_valid = all(

                                t._validate_controls(order, OrderPackageType.PLACE)

                                for order in entry_trade.orders

                            )

                            if all_valid:

                                if runner_context.live_trade_count+len(entry_trade.orders)&lt;self.max_live_trade_count:

                                    for order in entry_trade.orders:

                                        t.place_order(order)

                                else:

                                    logger.error('This would invalidate the max trade count')```

---

## 2021-12-22 19:58:06 - issues channel

**Paul**

So now i'm confused. I have a live key. I've placed bets with it using BFLW but without streaming. First time I've used streaming - is there a further activation step for streaming?

---

## 2021-12-21 20:10:45 - issues channel

**Aaron Smith**

[@U4H19D1D2](@U4H19D1D2) you mean the path where its supposed to put the file? the path to the file in the s3bucket is the same.  I dont think it ever gets to the part where it checks the path to where its supposed to download to, but those are also the same (locally its mac, ec2 is ubuntu, path exists on both as its all within the project)

---

## 2021-12-21 18:23:16 - issues channel

**Mo**

Because your credentials are correctly configured locally but not on the EC2 instance?

---

## 2021-12-21 18:22:29 - issues channel

**Aaron Smith**

(not directly bflw/flumine related): does anyone have a clue why i would be able to download s3objects from local, but running same code on ec2 gives a FileNotFoundError ?

---

## 2021-12-21 08:46:19 - issues channel

**JFP**

Here is my backtest code



```client = clients.BacktestClient()



framework = FlumineBacktest(client=client)



root_folder = 'C:/BF_PRO_DATA/Nov/'

folders = os.listdir(root_folder) #list of folders

markets = []



for folder in folders:

    folder = os.path.join(root_folder, folder) #dir to open folder in list of folders

    _markets = os.listdir(folder) # list of files inside folder inside list of folders

    _markets = [x for x in _markets if x.endswith('.bz2')] #adds file names to list

    _markets = [os.path.join(folder, x) for x in _markets] #adds file names to a dir string

    markets.extend(_markets) #adds exact dir for each file to market list





spList=[]



class Test1(BaseStrategy):





    def check_market_book(self, market, market_book):

        if (market.seconds_to_start &lt;= 360) and (market_book.status != "CLOSED") and not market_book.inplay and market.market_type == "WIN":

            return True



    def process_market_book(self, market, market_book):

        

        if (ABC):



            for runner in market_book.runners:

                

                runner_context = self.get_runner_context(market.market_id, runner.selection_id, runner.handicap)

                

                back1 = get_price(runner.ex.available_to_back, 0)

                back2 = get_price(runner.ex.available_to_lay, 0)

                

                #exit current selection itteration if back1 = None

                if back1 == None or back2 == None:

                    continue



                

                if (XYZ):

                        

                    trade = Trade(

                        market_id=market_book.market_id, 

                        selection_id=runner.selection_id,

                        handicap=runner.handicap,

                        strategy=self

                    )

                    

                    order = trade.create_order(

                        side= Side, 

                        order_type=LimitOrder(price=back1, size= sizeB1),

                        notes=OrderedDict(market_vol=round(totalVol, 4), sel_volume = selTradedVol, event_details = market_book.market_definition.name, order_num = 1)

                    )

                    

                    market.place_order(order)

                        

    def process_closed_market(self, market, market_book) -&gt; None:



        for runner in market_book.runners:

            spList.append([runner.selection_id, runner.status, runner.sp.actual_sp])





with mock_patch("builtins.open", smart_open.open):

    strategy = Test1(

        market_filter={"markets": markets,"listener_kwargs": {"seconds_to_start": 360}, "market_types":["WIN"]},

        max_trade_count = 4,

        max_live_trade_count = 4,

        max_order_exposure = 10000,

        max_selection_exposure = 20000

    )

    framework.add_strategy(strategy)

    framework.run()



dfList=[]



for market in framework.markets:



    for order in market.blotter:

        dfList.append(

        [

            market.venue,

            market.country_code,

            market.event_name,

            market.market_start_datetime,

            order.market_id,

            order.selection_id,

            order.notes['event_details'],

            order.notes['market_vol'],

            order.notes['sel_volume'],

            order.notes['order_num'],

            order.responses.date_time_placed,

            order.status,

            order.order_type.price,

            order.average_price_matched,

            order.size_matched,

            order.simulated.profit,

        ]

        )



df = pd.DataFrame(dfList, columns = ['market_venue', 'country_code', 'event_name', 'market_start_datetime', 'market_id', 'selection_id', 'event_details', 'market_vol', 'sel_volume', 'order_num', 'time_placed', 'status', 'price', 'average_price_matched', 'size_matched', 'simulated_profit'])

df['weekday'] = df['time_placed'].dt.dayofweek

df['month'] = df['time_placed'].dt.month



#add bsp

df['bsp']=df['selection_id']

df['selection_status'] = df['selection_id']



for L in spList:

    df.loc[df['selection_id'] == L[0], 'bsp'] = L[2]

    df.loc[df['selection_id'] == L[0], 'selection_status'] = L[1]```

---

## 2021-12-20 13:55:44 - general channel

**mandelbot**

an ec2 from aws, you can get one free for a year

---

## 2021-12-20 13:42:34 - general channel

**Shashi Khaya**

Hey folks, this might be a silly question but could anyone please advise on what the options are for deploying flumine if I dont have a local machine available to run 24/7?

---

## 2021-12-19 18:54:18 - strategies channel

**liam**

How many markets? CPU intensive strategy? Spikes of warnings or gradual increase? Look at monitoring in AWS 

---

## 2021-12-19 18:53:05 - strategies channel

**Ruben**

hello everyone, I am experimenting with running my first strategy live, and I am running it in an EC2 t3.micro instance (2 vCPUs, 1gb memory). However, looking at the logs, I am seeing a lot of latency warnings, some with very high latency, like 60s. Could this be due to the CPU of the instance not being fast enough to process updates at the rate they come in?

---

## 2021-12-19 10:23:14 - random channel

**Ruben**

I'm thinking about using cloudwatch to start monitoring logs as opposed to logging into my EC2 instance and using grep like a maniac. Anyone here using it? What do you guys use to monitor logs?

---

## 2021-12-17 20:27:09 - general channel

**Aaron Smith**

I am a tech noob in general. I chose RDS (well, i didnt think much about it, aws recommended it :smile: ) and i didnt struggle to much with it. I feel it will come easier to you than you may expect.

---

## 2021-12-17 20:19:23 - general channel

**Beeblebrox**

Thanks. I'm definitely going to use aws to host my db - learnt my lesson having it local and crashing!



What's the reason you wouldn't use Mongodb over and RDS? To my mind it's simpler as you just have an object that you bung into the db, whereas with and RDS you have to design the tables and try and normalise the db.  Like I say though, my db knowledge is limited, so maybe it's not so hard.

---

## 2021-12-17 20:02:40 - general channel

**liam**

Yes AWS yes 

---

## 2021-12-17 19:17:06 - general channel

**Beeblebrox**

Database questions... I currently have a single MongoDB database, running locally on the same machine as Flumine is running, to store all my orders with each strategy in a separate collection. I chose MongoDB as I know next to nothing about databases and the fact it could easily store JSON data seemed like the easiest solution just to get something working.



However...



1. Getting data from the db for analysis is pretty slow

2. It's crashed a couple of times and lost a load of orders

So I'm thinking there's probably a better choice of db I could chose, but I'm not really sure what and I'm not sure how to design it.



• Is an RDS db a better choice?

• Do people host their db's locally or do you use AWS to host it?

• Do you have one big db with all your orders (and separate tables for each strategy) or separate dbs to store orders for different strats?

I guess the general question is: what do people use and why?

---

## 2021-12-16 12:44:02 - general channel

**Peter**

I thought "that can't be too difficult" then took a look at the now very old script I have running as a daily cron job to do it. Hmmmm.



I'll stick it up here in case it helps. Ignore references to workspaces and the database stuff that uses my custom helper. Also my "trading" == your "client". Hope it helps ...



```import os

import sys

import json

import betfairlightweight

from datetime import datetime, timedelta

from helpers.DatabaseHelpers import database



WORKSPACE_PREFIX = os.getenv("WORKSPACE_PREFIX")

DATABASE_TABLE = "betfair_statement_lines"

SQL = """

    INSERT INTO {}

    (refId, amount, avgPrice, betSize, betType, betCategoryType, commissionRate, eventId, eventTypeId, fullMarketName, grossBetAmount, marketName, marketType, placedDate,

     selectionId, selectionName, handicap, startDate, transactionType, transactionId, winLose, deadHeatPriceDivisor, avgPriceRaw)

    VALUES ({})

""".format(DATABASE_TABLE, ('%s,' * 23)[:-1])



def processing_date():

    if len(sys.argv) &gt; 1:

        return sys.argv[1]

    else:

        return str(datetime.today() - timedelta(days=1))[:10] # returns date as a string without time



def download_account_statement(date_str):

    db = database()

    existing_lines = db.fetch_all('SELECT refId, winLose FROM {} where left(placedDate, 10) = "{}"'.format(DATABASE_TABLE, date_str))

    process = True

    from_record = 0

    while process:

        print(date_str, from_record, "-", from_record + 99)

        start_time = date_str + "T00:00:00Z"

        end_time = date_str + "T23:59:59Z"

        time_range = betfairlightweight.filters.time_range(from_=start_time, to=end_time)

        ast = trading.account.get_account_statement(

            from_record=from_record,

            item_date_range=time_range,

            include_item="ALL",

        )

        txns = []

        for a in ast.account_statement:

            line = json.loads(a.item_class_data["unknownStatementItem"])

            if (int(a.ref_id), line["winLose"]) not in existing_lines:

                txns.append(list({**{"refId": a.ref_id, "amount": a.amount}, **line}.values()))

        result = db.execute_many(SQL, txns)

        if result: print(result)

        from_record += 100

        process = ast.more_available

    db.close()



trading = betfairlightweight.APIClient(os.getenv("BETFAIR_USERNAME"), os.getenv("BETFAIR_PASSWORD"), app_key=os.getenv("BETFAIR_LIVE_KEY"), certs=WORKSPACE_PREFIX + "certs")

trading.login()



download_account_statement(processing_date())```

---

## 2021-12-16 08:26:10 - general channel

**Paul**

On another point last night I broke my remote code-server setup I used to write bot code on my iPad. Gave AWS Cloud9 (a cloud native IDE), a go, and was pleasantly surprised by it. Think over Christmas I can see myself getting a setup where I use sagemaker for jupyter, and cloud9 for coding. But then, I'm on the Kool Aid, so…

---

## 2021-12-15 18:51:21 - general channel

**birchy**

Probably, the article I read was from a typical rag newspaper that insinuated that ALL of AWS had crashed. :grinning:

---

## 2021-12-15 18:31:18 - issues channel

**Mo**

If using environment variables I will set them as part of running the command a la `AWS_PROFILE=xxx python -m ...`

---

## 2021-12-15 18:13:01 - issues channel

**birchy**

Interestingly, I use `.aws/credentials` and also `.betfair/credentials`, which is something I setup when I first started using Flumine ~12 months ago. I've never used the environment variables for username, password, etc. Just wondering how you guys normally SET these values? I know it can be done from the terminal but am wondering if you're automating it or entering manually?

---

## 2021-12-15 18:00:12 - general channel

**birchy**

Just read a news article saying that AWS went to shit again this morning. Coincidence?

---

## 2021-12-15 14:28:57 - issues channel

**Peter**

Or if you want to make it more explicit, you could start your S3MarketRecorder like this:

```class S3MarketRecorder(MarketRecorder):

    def __init__(self, *args, **kwargs):

        MarketRecorder.__init__(self, *args, **kwargs)

        self._bucket = self.context["bucket"]

        self._data_type = self.context.get("data_type", "marketdata")

        self.s3 = boto3.Session(

            aws_access_key_id=os.getenv("AWS_ACCESS_KEY"),

            aws_secret_access_key=os.getenv("AWS_SECRET")

        ).client("s3")

        transfer_config = TransferConfig(use_threads=False)

        self.transfer = S3Transfer(self.s3, config=transfer_config)```

---

## 2021-12-15 14:27:16 - issues channel

**Peter**

boto3 can also pick them up automatically from

```.aws/credentials```

---

## 2021-12-15 14:24:25 - issues channel

**Ruben**

maybe is it supposed to be run from an ec2 instance that has been given access to the bucket and hence nothing else is needed?

---

## 2021-12-15 13:14:51 - random channel

**birchy**

I'm a bit of a simpleton and simply use S3 for storage and Lightsail for deployment, which is nothing beyond `python3 launchbots.py &amp;` :man-shrugging:

---

## 2021-12-15 13:10:39 - random channel

**Mo**

Yes. Need an extra server? You can have one in seconds, no contract to sign, no need to wait for your hosting provider to provision it or - God forbid - order the hardware. And there are all kinds of niche configurations available e.g. GPU or FPGA instances. If you want to relocate your infrastructure half way around the world, you can do it in seconds. You can scale down just as easily as scaling up. You're not tied into a contract or stuck with last gen hardware. And these are the benefits of just the EC2 product

---

## 2021-12-15 11:59:12 - random channel

**liam**

For me its about having everything in one place and a nice simple terraform file, ec2/ecs/rds/s3/cloudwatch

---

## 2021-12-15 11:58:30 - random channel

**Paul**

The high-level benefits (beyond reliability), are briefly touched on here: [https://aws.amazon.com/what-is-aws/|https://aws.amazon.com/what-is-aws/](https://aws.amazon.com/what-is-aws/|https://aws.amazon.com/what-is-aws/)

---

## 2021-12-15 11:49:35 - random channel

**D C**

On the topic of AWS or other cloud approaches, what do people consider the main benefits? What does it give you over a good old fashioned server hire other than (presumably) lower cost. I've been working on dockerising my stuff ready for AWS deployment and reading about various types. There is a bewildering array of options but cost aside, the only reason I will be going down this route is more reliable internet connectivity but I could get that getting a dedicated server hire (although admittedly more expensive). I must be missing something because I can't see the allure of AWS other than improved connection reliability. Can someone educate me?

---

## 2021-12-15 08:58:12 - general channel

**liam**

I have experimented with a few things on AWS lambda/spot instances/fargate/ecs and they all work well but there is always the upfront time cost in getting them setup and without a sophisticated framework on top it isn't exactly simple to quickly run something.



I agree with [@UBS7QANF3](@UBS7QANF3) as since purchasing a decent laptop (M1) the amount of backtests I carry out has skyrocketed as I can just press f5 and forgot about it

---

## 2021-12-15 08:43:17 - general channel

**Mo**

To be clear, didn't mean to suggest the local workstation was _better_ just an alternative to weigh up against the pros and cons of AWS

---

## 2021-12-15 08:37:36 - general channel

**Paul**

That works if you have the upfront capital and can keep it busy. One thing I would say on the AWS stack is see if you can use spot instances if you're doing a lot of this: reduces costs a lot, and obviously upfront costs disappear. I am biased as it's my employer, but I'd agree that if you can do the upfront cost and keep it busy and don't mind the occasional upgrade cycle, Mo’s suggestion can be cost effective.

---

## 2021-12-14 17:29:46 - general channel

**Paul**

If you're used to Jupyter, search for Sagemaker in the console. You can start a notebook instance and start there. If you really mean you want an ide, look at AWS cloud 9

---

## 2021-12-14 14:26:14 - general channel

**C0rnyFlak3s**

Has anyone deployed their backtesting engine to an AWS service? I am planning on porting my data and engine to a service that allows me to load files very fast and offers me enough performance to run parallel backtests on given data. For this I figured, I need 1.) enough fast read/write storage (more so the read speed should be maximized). 2.) enough CPUs to start multiple backtesting processes at once. For this I am looking for a suited AWS service, where I can run my IDE and have my data stored on fast memory. Can you guys point me in the right direction on this one? I am not so experience with cloud computing and finding the right configuration for a given purpose. Would gladly appreciate any pointers. :slightly_smiling_face:

---

## 2021-12-13 16:17:23 - general channel

**Aaron Smith**

can i use [https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py](https://github.com/liampauling/flumine/blob/master/examples/controls/backtestloggingcontrol.py)

live (instead of backtest) and if so, is there anything i need to take care of except for getting rid of order.simulated.profit?

---

## 2021-12-12 09:44:57 - general channel

**captainonionhead**

Following up on the comment about getting the marketrecorder example going - if you're using a live key, you'll need a strategy to run with it to stop BetFair being upset that you're just scraping data.  I'd recommend posts like: [@U016TGY3676](@U016TGY3676)'s:

[https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1623458534154600](https://betfairlightweight.slack.com/archives/CTPL3R3FU/p1623458534154600)

Which amongst many other things has some useful ways to get a strategy started at the bottom.

I also found a lot of useful discussions in [#CTPL3R3FU|strategies](#CTPL3R3FU|strategies) from around late-June/early-July this year.  It's well worth browsing through these channels as there's lots of super helpful people in here and this is a fabulous resource to get started!

---

## 2021-12-11 22:06:24 - strategies channel

**Ruben**

Good night everyone. I am beginning to test my first strategy live, and I am seeing that it matches about 3% of orders it offers. It offered ~1700 orders and only 50 orders were matched. Is this too low for Betfair's standards?

---

## 2021-12-11 21:59:18 - random channel

**birchy**

Or you can just open multiple terminals on your client/local machine and SSH into the AWS instance from each.

---

## 2021-12-11 20:00:16 - random channel

**Beeblebrox**

[@U016TGY3676](@U016TGY3676) I had something similar today, in my case it was because I was maxing out the cpu.  It had been fine for months, but I started running an extra flumine instance with a new strategy a couple of days ago and it couldn't cope.



My Lightsail instance was Windows (yes, yes I know! I just used that as I'm more used to Windows), but I'm going to try setting up some Unix EC2 instances instead.  As a novice Unix user has anyone got a guide on things like which flavour of Unix to use, which instance types to use (I'm thinking t3.micro), how to install Python, Git, etc, and any other tips?  I'm sure I can work it out myself, but any guidance would be appreciated.

---

## 2021-12-11 16:27:39 - random channel

**Aaron Smith**

[@U016TGY3676](@U016TGY3676) few days ago my ec2 instance would only pass 1/2 status checks and i was unable to do anything with it. Probably there is some smart solution i didnt find, i ended up terminating and remaking it :smile:

---

## 2021-12-11 16:17:04 - random channel

**birchy**

Anyone here ever been locked out of an AWS VPS instance? In my case, it's a Lightsail instance, which is basically an idiot's version of EC2. Can't login via SSH from any of my machines OR from the AWS management web page/console. Tried to reboot from AWS console but it failed. Ended up having to "stop" the instance from AWS but that also hung for a good 10+ minutes. Have been using this server for 6+ months and not had anything like it happen before.

---

## 2021-12-10 18:30:10 - general channel

**C0rnyFlak3s**

In general what would be the most reliable way to detect start/end of half times in both backtesting data but also live data?

---

## 2021-12-07 09:57:17 - strategies channel

**Aaron Smith**

surely given you find and analyse the right parameters, you can make a profit from laying the draw. That is kinda true for any market. But why are you limiting yourself to laying the draw? why not back the draw? Feels like there is an underlying assumption that draws are generally underpriced and that sort of assumption surely needs a decent foundation if you want to build a strategy around it.

---

## 2021-12-06 21:31:09 - random channel

**James T**

There's an unofficial API which provides scores for the Betfair website. Can't remember the url off the top of my head. I recall that there is an example in the bflw library. 



For backtesting I just use the same code as I would in live. Decipher the score from correct score, O/U and score API. 

---

## 2021-12-06 20:24:05 - issues channel

**Newbie99**

This error has come up a few times recently, when the market cleared event is processed:



```

{"asctime": "2021-12-06 14:30:14,617", "levelname": "ERROR", "message": "_get_cleared_market error", "exc_info": "Traceback (most recent call last):\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/worker.py\", line 234, in _get_cleared_market\n    group_by=\"MARKET\",\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/betfairlightweight/endpoints/betting.py\", line 434, in list_cleared_orders\n    (response, response_json, elapsed_time) = self.request(method, params, session)\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 55, in request\n    self._error_handler(response_json, method, params)\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 81, in _error_handler\n    raise self._error(response, method, params)\nbetfairlightweight.exceptions.APIError: SportsAPING/v1.0/listClearedOrders \nParams: {'betStatus': 'SETTLED', 'marketIds': ['1.192009763'], 'customerStrategyRefs': ['ip-172-31-4-105'], 'settledDateRange': {'from': None, 'to': None}, 'groupBy': 'MARKET'} \nException: None \nError: {'code': -32099, 'message': 'ANGX-0006', 'data': {'APINGException': {'requestUUID': 'ie1-ang11b-prd-10290844-0055b9d69e', 'errorCode': 'UNEXPECTED_ERROR', 'errorDetails': ''}, 'exceptionname': 'APINGException'}} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0006', 'data': {'APINGException': {'requestUUID': 'ie1-ang11b-prd-10290844-0055b9d69e', 'errorCode': 'UNEXPECTED_ERROR', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}", "trading_function": "list_cleared_orders", "response": "SportsAPING/v1.0/listClearedOrders \nParams: {'betStatus': 'SETTLED', 'marketIds': ['1.192009763'], 'customerStrategyRefs': ['ip-172-31-4-105'], 'settledDateRange': {'from': None, 'to': None}, 'groupBy': 'MARKET'} \nException: None \nError: {'code': -32099, 'message': 'ANGX-0006', 'data': {'APINGException': {'requestUUID': 'ie1-ang11b-prd-10290844-0055b9d69e', 'errorCode': 'UNEXPECTED_ERROR', 'errorDetails': ''}, 'exceptionname': 'APINGException'}} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0006', 'data': {'APINGException': {'requestUUID': 'ie1-ang11b-prd-10290844-0055b9d69e', 'errorCode': 'UNEXPECTED_ERROR', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}"}```

All I'm doing is the following (using the Closed Market Logging example):



```    def _process_cleared_markets(self, event):

        for market in event.event.orders:

            market_id = market.market_id

            file_path = market_closed_report_path + '/' + str(market_id) + '_cleared_market_data.json'

            cleared_market_data = {'market_id': market_id,

                                  'market_profit': market.profit,

                                  'market_com': market.commission

                                 }



            check_and_create_json(file_path, cleared_market_data)```

But the error looks like Flumine is making an incorrectly constructed API call to Betfair if I'm reading that correctly?

---

## 2021-12-05 22:05:51 - general channel

**Jeff Waters**

"I’ve learned a lot about using R, Python, Stan, JAGS, and other software platforms that can be used to create considerably more advanced machine learning sports models [than using Excel-based models]. In all seriousness that’s where the future of sports modelling lies.



Even Marco Blume, the head of trading at Pinnacle, has come out openly saying Pinny’s traders are using machine learning models deploying the Caret package in R and the

Scikit Learn package in Python. Someday soon, these platforms are likely to be the only path forward for attacking major sports betting markets."



Mack, Andrew (2019-07-09T23:58:59). Statistical Sports Models in Excel



Agree/disagree?

---

## 2021-12-03 06:25:03 - general channel

**Peter**

I'm a keep it simple kind of a guy, so even though the market recorder is very lightweight, I run it on it's own ec2 instance to remove any risk of interference. Or more accurately I run them (as I run five of them collect all the markets that interest me).

---

## 2021-12-02 23:02:42 - general channel

**D**

1 ec2 instance, use tmux to run 2 (or more) separate python processes.

---

## 2021-12-02 21:20:18 - general channel

**TT**

How do people deploy the market recorder alongside strategies? My initial thoughts are to try and keep them separate but I can see a few options:

 - Deploy two separate ec2 instances (one for the market recorder and one for running strategies)

 - Use one ec2 instance but run 2 separate python processes

 - Use one instance and just add both the market recorder strategy to the framework alongside the other strategies



 How do other people approach this? And are there any other things to consider i.e latency/connection limit issues etc?

---

## 2021-12-02 10:57:13 - strategies channel

**Peter**

The numbers were clearly made up :wink:. If you had a strategy capable of that, you'd be quietly raking in the profits rather than seeking advice here :grinning:.



I think that by number of winners in this context, you're referring to hit rate, and that's generally at the heart of whether a strategy has value, i.e. whether the odds at which you can match and the frequency with which they come in, are good enough for the winnings to overcome the losses, because, unlike gamblers, we carefully balance the two.



I believe that Liam's earlier point was that as you get closer to the start time the market gets more efficient. So by moving further away from the start and being selective, you can hope to find more mispriced runners with better odds and increase your hit rate.



After the event you can compare the price you got with the BSP as a proxy for the fair price to see whether the odds that you though were good actually had value, or not, as you can't tell that from the outcome of the race itself.



I very much doubt that you will find a strategy that delivers consistent value, i.e. over a large number of races, but has prices that, on average, are worse than BSP.

---

## 2021-12-01 13:59:29 - issues channel

**Peter C**

I'm running a new strategy file today on inplay horses. It uses the field

```runner.last_price_traded```

but I'm running into trouble because this is always None. Below is the streaming filter I'm using

```market_data_filter=streaming_market_data_filter(fields=["EX_MARKET_DEF", "EX_ALL_OFFERS", "EX_TRADED"])```

I can't for the life of me work out why runner.last_price_traded never has a value when live (worked in backtests) - Does anyone know why this might be?

---

## 2021-11-30 20:56:45 - issues channel

**Aaron Smith**

so, given cpu is at 6%, its got to be network which i would have to solve on aws level? get an instance with better bandwidth?

---

## 2021-11-30 20:43:27 - issues channel

**Aaron Smith**

Hey ppl, last few days i m getting flooded with both

```"WARNING", "message": "[MarketStream: 2001]: Latency high: 9.45749306678772"}```

and

```"WARNING", "message": "High latency between current time and MarketBook publish time", "market_id": "1.191643732", "latency": 13.961846351623535, "pt": "2021-11-30T18:37:48.781000"}```

First, i would like to understand the difference between the two. Both compare current time to publish time, but i recall them being different in some way.

Also any leads on how to approach this problem are appreciated. I ve got flumine running on aws (eu-west-1b), newly set up the servers (but also was on aws before). I m running t2-micro instances. CPU utilization seemingly never goes higher than 6%. The strategies are also very light in terms of computing time. Could the internet connection/bandwidth of the ec2-instances be a limiting factor?

Also, i realized, instead of piling up latency (as in getting more and more behind on each update), it seems to jump from 0 to 20 seconds (and then gradually gets lower), which seemed like an odd behaviour to me.

---

## 2021-11-28 16:04:47 - general channel

**Amit Patel**

hey guys! is there any way to get the runner name inside a strategy when running live? I can see its there for backtests but can't figure out if its possible otherwise

---

## 2021-11-28 13:58:49 - general channel

**D C**

I see what you mean. I was just thinking about if you had 2 order stream subscriptions - one on your home machine where you are placing the bet and another on AWS, both order streams will push the bet ID when it is created and you can compare arrival times for latency because they will both have the same publish time. But as you say, that only tests a one way performance on the stream side, not the actual lag in the whole bet placement timeline. I don't know enough about this stuff other than working on real time embedded stuff where you can get more accurate timings on things. A bit different to comms between machines running full OS with multiple processes.

---

## 2021-11-28 13:24:21 - general channel

**PeterLe**

Thanks DC for your thoughts. So to answer the first part of that, Im probably 180 miles to Dublin, so there will probably be a slight variation when the inplay signal is received. If it was found that my home PC was consistently slower to receive the data that the instance in AWS, I wouldn't be concerned as Im not measuring the first part of the leg, but the whole trip, ie the ability to receive the data , act upon it; place an order and receive a Betid.

Re the second part, Im not as technical as you guys DC :grinning:, so I wouldn't know how to test that (I actually deleted the Lightsail instance last night anyway)

---

## 2021-11-28 09:19:18 - general channel

**Mo**

Oh, it's an AWS product?

---

## 2021-11-28 09:11:25 - general channel

**PeterLe**

So for what it is worth, the home PC was quicker to receive betid on every market i tested yesterday.

I was using lightsail rather than EC2, which has a smaller bandwidth than EC2 if i remember correctly. (there is also some throttling (bustable zones) but I wouldn't have thought that would have come into play at the beginning of the race etc

The differences were much bigger too than the screenshot I posted above, ie in a couple of cases, betid was up to 50K.

Anyway make of it what you will, only posted as it may help someone. For me, Ill probably just run locally.

---

## 2021-11-27 13:35:29 - general channel

**birchy**

[@UQL0QDEKA](@UQL0QDEKA) so I guess the logical solution is to run several servers, i.e. AWS, Simply (Tagadab), home pc, etc and let it average out over time?

---

## 2021-11-26 19:52:36 - general channel

**D C**

Surely latency is not the only concern is it? Internet down time at home is a lot more frequent than it is if you use AWS surely?

---

## 2021-11-26 19:31:02 - general channel

**Peter**

“Sports betting is a game where finding edges is fairly easy but _getting matched_ for sizable sums is near impossible”



This is exactly the problem and why sports hedge funds don’t work. Capital isn’t the problem, deploying it is.



I’ve almost lost count of the number of firms that have started and failed in similar manners. In aggregate they have lost millions.



I really wish the markets were ten times bigger, but they are not. So I’ve never felt the need to raise capital and neither has any other successful trader I’ve met.



It’s a self defeating proposition in my mind. You give away an edge and get a fraction of the return in lieu of using money you don’t need.

---

## 2021-11-26 17:42:10 - issues channel

**Mo**

And you will generate them on AWS using the newer version of OpenSSL

---

## 2021-11-26 17:42:07 - issues channel

**birchy**

Ah, yessss, that was the issue I had. It was OpenSSL being newer on AWS. Update your local OpenSSL, regenerate the certs and upload to Betfair.

Actually, update OpenSSL first and run the code as-is to prove that's the issue.

---

## 2021-11-26 17:37:58 - issues channel

**Lee**

sounds like you have a newer ssl version on aws

---

## 2021-11-26 17:36:47 - issues channel

**Aaron Smith**

on aws i get:

url: (58) could not load PEM client certificate, OpenSSL error error:140AB18E:SSL routines:SSL_CTX_use_certificate:ca md too weak, (no key found, wrong pass phrase, or wrong file format?)

---

## 2021-11-26 17:27:00 - general channel

**PeterLe**

Yes think I knew you was [@UBS7QANF3](@UBS7QANF3) :smile:. when I first started using AWS years ago there was a significant advantage I recall, anyway I’ll see how it plays out

---

## 2021-11-26 17:21:31 - issues channel

**Mo**

Are you running Linux in AWS?

---

## 2021-11-26 17:20:30 - issues channel

**birchy**

What about the curl command via AWS?

---

## 2021-11-26 17:19:56 - issues channel

**Aaron Smith**

client.login() fails on aws but succeeds locally

---

## 2021-11-26 17:13:29 - issues channel

**birchy**

Weird, something must be different on the AWS setup. Probably something simple like a slightly different directory structure. Can you ping the API server from AWS? Or even load the website with `wget`?

---

## 2021-11-26 17:10:45 - issues channel

**Aaron Smith**

i just checked and on my local pc i actually can log in (same code), its just on my aws ec2 where it fails.

---

## 2021-11-26 17:00:59 - general channel

**ShaunW**

4 does seem tiny given it can vary quite a bit from one order to the next .... I ran mine all day and averaged, running on both local and aws simultaneously to rule out any daily variations in the backbone in general. I couldn't find any tangible benefit to running locally so for me, the pros of being on aws outweigh the cons and performancewise it's much of a muchness.

---

## 2021-11-26 16:00:16 - general channel

**Unknown**

I was just testing connection time to the API..comparing my home PC (Quite powerful system) v AWS eu-west-1a (Low powered but adequate : 2Gb 4 vCpu's)

from previous threads on here i know that the connection to betfair is asymmetric (as it goes via prolexic)

So I thought i check it out...

Same strategy on each system, simply fires in a lay order at 1.01 as the market signals it is in-play

In 3 out of 4 tests the home PC was able to receive the price data, and obtain a bet reference faster than AWS(as my strategies react to market prices, I thought this was the best way to measure and not worry about the asymmetry)

Would you say this was a reasonable test?

Just putting aside the other benefits of running on AWS, If speed to market is the main important factor is AWS worth it?

---

## 2021-11-26 07:08:25 - issues channel

**liam**

Are you able to share the full cprofile from ec2? How many orders? CPU seems very high 

---

## 2021-11-26 00:53:48 - issues channel

**Tom**

Recently tried deploying a flumine strategy (which works well locally) to an AWS machine and was experiencing some of the same latency warnings I've seen posted above: `"... High latency between current time and MarketBook publish time", "market_id": "...", "latency": 53.55.."` which sort of continually escalates to the point the strategy was acting on market book updates from 30 seconds prior. CPU utilization was pretty high (40-50%) on a EC2 t3a.medium but not enough to cause this huge bottleneck I thought but i tried it on a EC2 t3a.2xlarge and experienced the same behaviour.



Took the advice to profile the code using cProfile and the biggest difference was the `process_orders` calls [ `baseflumine.py:220(_process_current_orders)` and `process.py:33(process_current_orders)` ] were completing in approx 10ms per call locally but 350ms on the AWS machine. Any ideas why this might be happening? Or anything i can do to clean out + speed up `process_orders` generally? Strategy is pretty order heavy so the partial fills and price updates etc start adding up to a big order list for each market but flumine seems to barely feel it locally.

---

## 2021-11-25 16:22:34 - general channel

**Michael**

I will also watch this with an interested but sceptical eye. It's almost a standing joke amongst successful players that we get used to politely explaining to friends and acquaintances why we don't need their investment. Even a modestly successful player will have more funds than they can deploy and a frequent question here is _"how can I scale up - it doesn't seem to work."_



We've also got used to seeing "professional traders" advertising that they need investment but that's always met with derision. _"System working, send more money"_. Yeah.



So it seems like this lot are looking for that lot - the winners who haven't won. Why? Are there really investors queueing up to back low ranking gamblers recruited online who couldn't cut the mustard with a couple of grand but can safely be trusted with (presumably) a much bigger fund? People who see a website with literally nothing on it except a claim to be a "millionaires factory" and say _"yeah, that's for me"._ 



I can think of a few ways this can be made to work for the business owners if they have something slightly different in mind to what they're advertising. They wouldn't be the first people to have that idea, but this as a straight proposition? I wouldn't bet my own money on it - but hey, maybe I could use theirs.....?

---

## 2021-11-25 06:40:09 - general channel

**Mike Walpole**

Hi guys &amp; gals, not sure if this is the right channel for that. 



I'm running Randwick Group, a sports &amp; racing trading company in Sydney. Founders come from hedge fund and prop FX trading background. We treat sports trading as just another asset class &amp; run the business just like any other high quality hedge fund. 

We currently have 4 traders across racing, basketball and esports. 

We’ve raised additional capital from our investors and are now looking to hire more traders (ideally trading soccer, cricket, UK racing and NFL) to allocate capital to. 



We don't need you to work on this full time, and we're happy for you to trade on personal account as well. We offer market leading profit share of your trading. We respect your IP and don't require you to share your secret sauce. We provide you with a funded account and you trade any way you like (within risk parameters!) 



What we need is a verifiable track record (ideally on BetFair) of deploying money - so no backrests please. DM for a job description or if you have any questions. 

[http://www.RandwickGroup.com|www.RandwickGroup.com](http://www.RandwickGroup.com|www.RandwickGroup.com)

---

## 2021-11-20 12:48:33 - general channel

**mandelbot**

just an AWS question here, do stopped instances incur charges? Can they be restarted with files intact?

---

## 2021-11-16 03:42:31 - general channel

**VT**

Hi everyone, I've noticed that most people are using Flumine, and apparently it's a more advanced tool. I'm a beginning programmer, I understand Python for working with data science, statistically analyzing data and things like that. I have a betting system using the platform before betfairlightweig and everything works fine, it's a simple system that I was able to develop just using the documentation.



Anyway, I'm trying to learn Flumine but it's challenging for me, I haven't found any real and complete tutorial or practical example. The content of the documentation is quite confusing for a beginning programmer, without practical and simple examples, at least for me.



I would like to know where to start, what I need to study for how to understand the system because I don't even know where to start.



Someone has a basic code, like for example checking the odds prices for the over 2.5 goals of Liverpool's next EPL game and placing a bet.



Thanks

---

## 2021-11-11 11:00:43 - issues channel

**captainonionhead**

It's a fresh install a few days ago in a newly built VPS whilst I work out how to get AWS up and running so I thought I was up-to-date with everything.

---

## 2021-11-11 10:20:51 - issues channel

**captainonionhead**

Yes, I thought the keepalive thread should stop this...  My client init looks like:

```  trading = betfairlightweight.APIClient(

    creds["username"],

    creds["password"],

    creds["app_key"],

    creds["certs"]

  )



  client = clients.BetfairClient(

    trading,

    order_stream=False,

    market_recording_mode=True

  )



  framework = Flumine(client=client)```



---

## 2021-11-10 04:04:07 - general channel

**VT**

Thanks for the code but it didn't work here, besides being a solution that seems inefficient to me, I still received:



'errorCode': 'TOO_MUCH_DATA'



I would like to list all football games that are in-play, live with the ball in play. I would like to get the ID of these events only.



It would be the same events found on the Betfair website when we click on in-play &gt; football



If anyone can help, I still haven't found a solution.



Thanks

---

## 2021-11-07 19:30:35 - random channel

**D C**

yeah. I believe that the clocks change in US today (yesterday) too which may be related. Clutching at straws but I had issues with the feed last night for US too

---

## 2021-11-07 09:51:57 - general channel

**liam**

I am hybrid, recorders always running along with my strategies that work across all countries however I start UK/IE up each morning via AWS lambda / portainer. Reason being that I use quite a few instances, some are heavy on CPU so would prefer to limit markets / time running but tbh I could probably just keep them all running for a few £ each month and less complication

---

## 2021-10-28 23:26:58 - random channel

**Paul**

Very much enjoyed this evening. Big thanks to [@UBS7QANF3](@UBS7QANF3) for organising, lots of thoughts pinging in my head. Also, enjoyed my chats with Betconnect and Betfair people present - found that I didn't want to spray them with holy water or drive a stake in their hearts as they were mostly interested in how to make our lives easier. Great stuff.

---

## 2021-10-28 16:21:36 - strategies channel

**Aaron Smith**

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) regarding your strategy that lost money on live: just as a strategy can be lucky during a backtest and indicate that its good even though its not, it may just be unlucky when live and be good even though its current results arent. It may be worth to keep that strategy in mind and once you got another good batch of data, throw another backtest at it. If it performs well again in that backtest you may give it another shot at live, or in case you want to be more cautious, ask yourself where and why it could perform better during the backtest than live.

---

## 2021-10-28 15:40:11 - strategies channel

**Aaron Smith**

I dont agree here. The goal of backtesting is to replicate as good as possible what would have happened if you had the strategy running live. There is no need for it to be very unique. I m rather sure goats problem was (and the problem he is warning of) that he just threw so many strategies into the backtest machinery until he got a positive results by chance. Even if you were to throw the same strategy in the backtest machinery and always give it a different patch to backtest on, assuming its not horribly bad, it will eventually show you good results, even though its not a good strategy.

---

## 2021-10-28 11:10:11 - strategies channel

**JazzMan**

To make money from a live strategy you have to be doing something unique. If you’re doing the same as everyone else then you’re all waiting in line to place the same bet. This maybe the problem with everyone using the same backtesting program. Backtesting works fine for me but I’ve taken a long time writing my own.

---

## 2021-10-27 22:09:45 - strategies channel

**AndyL**

[@U01S1VB9X9P](@U01S1VB9X9P) just being reading your posts, you sound like you're doing what i was, ie.making a pure backtest created strategy work, they don't!!

I had a backtest dog betting strategy, worked great over the 3months/3000selections, 6.5%ROI, went live.....lost money....

I think what works is come up with a strategy based on your experience of how the market works, then backtest to validate

---

## 2021-10-26 16:11:50 - general channel

**Lennart**

Hi everyone, wondering if anyone here can recommend a live data provider for NBA games or Basketball in general? Of course, ideally, I'd love a free one. Also wondering - anyone else here trading on Basketball markets?

---

## 2021-10-25 16:21:09 - betfair-news channel

**Neil T (Betfair)**

Hi [@UCQB6S222](@UCQB6S222)



Thanks for clarifying your points. You can measure network latency to the edge of the Betfair infrastructure using well-known techniques (traceroute, ping, etc) based on the endpoints that we provide ([http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)). For any API request, the difference between the HTTP request and response time (available as an HTTP header) will provide an indication of the total round trip time. 



placedDate is the time that the order arrived at the Betfair servers for processing at which point the bet will be placed (matched/unmatched) or subject to the in-play delay, if applicable, before being placed. Within the above parameters, you only have the potential to control the latency between your machine and our infrastructure. Once your request is received, you are subject to the speed at which the bet is processed within our matching engine. This itself may vary around the ~100m/s processing time.

---

## 2021-10-25 11:03:58 - random channel

**PeterLe**

No its my own stuff Oliver,I use Flumine too

---

## 2021-10-23 10:01:00 - strategies channel

**Adrian**

Thanks Oliver. I don't really know how to approach historical data analysis. My only skill right now is backtesting, then iterating the backtest for better performance. THere must be a way to make it work

---

## 2021-10-22 16:22:49 - betfair-news channel

**George**

Hi [@UNW8Q88EL](@UNW8Q88EL), thanks very much for getting back to me with this impressively comprehensive response, which is much appreciated, and thanks again for this initiative which is brilliant I think we all agree.

A few things come to mind:

• I probably phrased question 1 in a misleading way. I didn't mean to sound as if I was asking about asynchronous orders. I am really just interested to know how best to measure latency from door to door - and I am sure others would be too. Your thoughts on that would be greatly appreciated.

• Something like placedDate is _*possibly*_ what I am looking for, and I should have mentioned it as a possible solution. However, it currently doesn't help because it seems to be rounded to the nearest second or at least doesn't display any precision further than that at all, which is frustrating.

• Furthermore, even if placedDate was a millisecond-precise quantity it's not entirely clear (to me) what the meaning of this value actually is. Does it describe the time that the order arrived at Betfair's servers, or does it represent the time that the order was considered by the matching engine perhaps?

• In terms of question 2, I suppose what I am really after is a way to figure out exactly how much slower my order was than the one which got the fill that I wanted. Let's say a horse falls and I want to be the first to lay it. So I send a LAY order to Betfair, but it doesn't get filled, because someone else was faster. It's pretty hard in this situation for me to know whether I "lost" the race for liquidity by 1 millisecond or 200 milliseconds. I can look at the publishTime of the stream update which shows the successful order, but there's no way to track that successful order backwards in time to know what time it arrived at Betfair's server compared to what time my order arrived. I was just thinking that, if I knew how long the specific matching cycle in question had taken, that would enable me to work backwards and make a good estimate of when the successful order arrived. And, if I also had a good idea of what time my order arrived, then the difference between the two represents how much I lost the race by. 

• If I'm going to lose the race by 200ms then I may as well give up; if I'm mostly losing by 20ms then I should probably carry on trying and maybe invest in some better technology!

---

## 2021-10-22 15:40:30 - betfair-news channel

**Newbie99**

A while back there were a few users on here that got blocked and didn't seem to know why, obviously Betfair have certain legal obligations that can't be avoided and have to adhere to these, but aside from that, when permissible by local laws (i.e. when its not AML related basically), it would be good if users were given a warning if their betting behaviour would be likely to cause them to be blocked (e.g. an e-mail warning saying you are making an excessive number of REST API calls without placing sufficient bets or something to that effect).



In essence it would hopefully be mutually beneficial to allow people to change behaviour, rather than taking the nuclear option and closing down accounts without warning.



I should stress I have no idea what actually happens/happened in these situations personally, I'm just going on what people have posted.



So I guess the question is, would it be possible to have a slightly more formalised / structured approach to this type of occurrence to provide piece of mind that no-one is going to get shut down without just cause?

---

## 2021-10-22 10:47:39 - betfair-news channel

**George**

Would Betfair be willing to provide accurate(ish - down to millisecond precision) internal timestamps in responses and in the stream?

1. When I send a bet in-play, for example in horse racing, I get a 'pending' response, but this response doesn't include the timestamp that the order was received at the Betfair server. This timestamp must exist in order for Betfair's tech to know in which sequence all orders were received. It would be great to have this timestamp included in the pending response, so that I can measure my latency "from door to door" accurately.

2. On the market data stream we receive a 'publishTime' which is presumably the time at which the matching engine cycle finished. It would also be useful to have a field that represents the time at which the matching engine cycle began. This would enable me to look back at the logs and say: OK, my order didn't arrive in time to make it into this matching cycle which began at T1, so it was delayed a bit longer and had to wait for the next one, which began at time T2.

---

## 2021-10-21 13:53:12 - betfair-news channel

**Neil T (Betfair)**

Hi [@UBS7QANF3](@UBS7QANF3) - we wouldn't share the production code for this functionality.  We have a couple of articles that explain how we &lt;http://(https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Additional+Information#AdditionalInformation-VirtualBets)|virtualize bets&gt; and how [https://promo.betfair.com/betfairsp/FAQs_detailedWorkings.html|Near and Far prices are calculated](https://promo.betfair.com/betfairsp/FAQs_detailedWorkings.html|Near and Far prices are calculated) (appreciate that you may be already of both of these.

---

## 2021-10-21 11:39:06 - betfair-news channel

**Mo**

Would you be open to sharing actual production code for certain pieces of functionality? Specifically I am thinking:



1. Order book virtualisation

2. Projected SP calculation (near and far)

I'm working on the virtualisation myself and even knowing the answer I should be getting by cross-refencing the virtual and non-virtual price streams, it is very challenging to reproduce exactly. There are too many questions around e.g. how you handle rounding of stakes at various steps in the process

---

## 2021-10-20 12:19:16 - general channel

**ThomasJ**

[@U01UN2R5LM8](@U01UN2R5LM8) You said "But if doing so does not speed up the backtesting, i guess that is pointless." ... well it might speed things up if you summarize data...but what to summarize? You'll find that over even a short time of backtesting you'll want to look at data in many different ways and you'll keep on producing different summaries. So the production of the summaries and then testing on them will probably be way longer than just reading from the raw data, putting it into memory via, as liam mentioned a Pandas DataFrame (I presume it's Pandas), and then you can look at the data in an infinite number of ways directly, and super fast.

---

## 2021-10-19 13:32:55 - issues channel

**JC**

hmm damn, let me know if you think of an easy/quick way around this!! Or how to pass a different datetime to AWS

---

## 2021-10-18 06:28:28 - random channel

**Mo**

What do you do for AWS if you don't mind sharing?

---

## 2021-10-17 13:24:38 - random channel

**Paul**

Disclaimer: I work for AWS, but do not speak for them here in an official capacity. Customers tend to think it's expensive but reserved instances, spot instance if you're able to use them, moving to Lambda if you can (so not paying for idle), can all cut costs.

---

## 2021-10-17 11:52:56 - random channel

**liam**

AWS isn’t the cheapest compared to some other providers but it has everything 

---

## 2021-10-17 09:48:03 - random channel

**Mo**

AWS maybe has an advantage in the number of users we have here that have experience with it. I’m sure there are more cost effective options out there but personally I’m too invested in AWS already and I don’t think they have the same track record (a few days ago there was a global Azure outage). Cheaper and better drop in replacements for AWS services would be ideal. Someone mentioned Cloudflare R2 recently which is supposed to do this for AWS S3

---

## 2021-10-17 09:37:59 - random channel

**Mo**

AWS Dublin is close to the Betfair servers but it doesn’t help with the way their infrastructure is currently set up 

---

## 2021-10-17 09:36:28 - random channel

**Mo**

Betfair aren’t in AWS

---

## 2021-10-17 08:49:50 - random channel

**Oliver Varney**

Morning, im looking for some advice on hosting platforms, mainly AWS vs the rest. I am just about to start re-writing some of my services and was just wondering what peoples thoughts are on the best way to go. From what I understand if I go with AWS (rather then something like linode) im getting a large product/ feature set but in return paying higher costs and potentially slightly slower performance (from what ive read thus far). Is there any specific things around AWS and Betfair/betting that are also advantages of using it. From my understanding betfair are hosted on AWS? im pritty clueless when it comes to this stuff so would be interested in the techy guys thoughts.

---

## 2021-10-16 10:48:07 - strategies channel

**PeterLe**

Morning

Im just looking to relax some of these params now Im getting more confident..



max_order_exposure: float = 10,

max_trade_count: int = 300,

max_live_trade_count: int = 2,

multi_order_trades: bool = False,



So (and I apologise for the basic question!) ...assuming the settings above am I correct in thinking that using the exposure setting and max live trades, my exposure would be £20 (Each time my trigger is activated, ie 2 orders at £10 exposure each)?

Also, if the multi_order_trades bool was set to False this would only ever allow one live trade anyway?

Essentially I want to send more orders but still have control on max exposure. Thanks in advance...

---

## 2021-10-15 12:58:41 - random channel

**Jack B**

Security Group - [https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)

---

## 2021-10-15 10:35:50 - random channel

**Jack B**

How have people setup their network security when running on AWS? Public/Private subnets, ssh tunnels etc?

---

## 2021-10-05 14:53:42 - general channel

**EJono**

So the most recent time was due to market suspension so nothing suspicious there but there was an occasion where it looked as if there was a small amount ~70p tied up in a partially cancelled bet of status VIOLATION, partially matched £24.30. This then filled the maxlivetradecount=1 limit and I could not move on as I was unable to cancel the remaining amount. I was wondering if this was something expected to be encountered or if it is more likely a one off fuck up my side (misinterpreting my logs). I've made changes over the past week and haven't witnessed this problem since but am curious if i should have violation checks in place for bets that cannot be altered but have small remaining amounts

---

## 2021-10-05 11:20:53 - general channel

**liam**

both, for live you will have a single `Market` object with two different MarketBooks being sent through it (same marketId but different data because of the data filter) something to be aware of that might not impacting anything and tbh its on my list to isolate and fix the potential issue

---

## 2021-10-04 05:29:51 - random channel

**Alex A**

I don’t use flumine, but I always use the timestamp on the most recent message I received from BetFair as the current time, so all my logic around comparing times should be the same in a backtest as it is live.

---

## 2021-10-01 17:03:47 - issues channel

**Jack**

Betfair have temporarily upped my connections to 15 so I can get my strategy running over the weekend.



I don't see how I could have - I even destroyed my old digital ocean server yesterday and moved everything over to AWS so it should all be in one place now. :confused:

---

## 2021-10-01 16:52:18 - issues channel

**Jack**

Hi all :wave:



I am still having issues bringing my live strategy back up. Logs are telling me that I have too many connections open (10 apparently).

I know that I shouldn't be using any more than 4 (5 with this strategy) but old unused connections seem to be persisting forever.



Does anyone know of a way to see what connections are open and terminate them manually?

Is manual termination of connections something that people do when they pull strategies/streams down?

---

## 2021-09-30 11:54:16 - strategies channel

**liam**

strategy dependant and part of the art, anything I deploy live would have been through backtesting yes

---

## 2021-09-30 11:42:55 - strategies channel

**liam**

default strategy [https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/strategy/strategy.py#L52|settings](https://github.com/liampauling/flumine/blob/cd1e981c57baef58a529b624a17a7877cc67c676/flumine/strategy/strategy.py#L52|settings) are one live order per runner, you can bump or remove this, but you also have the following on the trade/order itself:



```place_reset_seconds: float = 0.0,  # seconds to wait since `runner_context.reset` before allowing another order

reset_seconds: float = 0.0,  # seconds to wait since `runner_context.place` before allowing another order```

---

## 2021-09-29 14:49:48 - general channel

**Peter**

Recommended method is to persist live market streams, including price data, to files and then re-stream from there as many times as you need to test different placing strategies, at which time you could use logging control to write to a database - though I suspect many will write to a csv file or similar for ingestion into a Pandas dataframe.

---

## 2021-09-29 13:35:57 - general channel

**Jono**

sorry i dont have anything particularly insightful to hand log wise other then times of different market transactions taking place and a list of the different orders placed . Need to make better use of the logging worker. the documentation has a comment inside the OrderStatus class stating beside VIOLATION definition that # order never placed due to failing controls

could you possibly elaborate on what this means? Is it something that is generated from both failed placings on betfair due to incorrect instructions sent to the api and/or breaches of the strategy variables ie max_live_trade_count?

---

## 2021-09-29 13:12:20 - general channel

**Jono**

Occasionally when altering an order either by cancelling entirely or adjusting the odds in my strategy, something causes the instruction to not run as expected and result in an order of status VIOLATION being produced. It seems to me to be indicative of the original bet being partially matched as i try to submit the cancel request but could very well be something else im not aware of. The new order correlating to the remaining unmatched amount of the original stake tends to be quite small, usually no more than a few tens of pence but then is unable to be cancelled in order to start the strategy cycle successfully again. The strategy heavily relies on making use of the max_live_trade_count = 1 in order to stay in check and not over place but orders of status VIOLATION contribute to this count resulting in progress stagnating until the remaining amount is matched as i cannot seem to cancel it.



Is there a way to deal with orders of status VIOLATION either by forcing cancellation or ignoring them in the max_live_trade_count or some other way i'm missing?



Additionally what tends to cause them so i can avoid generating these awkward orders?



Thank you

---

## 2021-09-29 09:07:19 - strategies channel

**Jack**

Wanted to go live with a new strategy and protect my main balance on case it starts throwing money away. 

I suppose when it was mentioned before the suggestion was actually just to move money across from main to games wallet to keep it safe.

---

## 2021-09-28 13:29:25 - general channel

**JC**

Thanks so much, did you run this on a decent EC2 instance for speed?

---

## 2021-09-28 13:17:32 - general channel

**JC**

Hi everyone, I've been running an old version of the S3 Market Recorder and am trying to convert all of my old recorded zip files to gzip on S3. EC2 bash script seems like potentially the best option, but will need a larger instance as inflating and zipping takes ages. Anyone had any experience with this or got an easy/serverless way to do it? Cheers, Joe

---

## 2021-09-25 16:19:46 - general channel

**Unknown**

I was just doing a sanity check over my live strategy to check it was on-course and I just wanted to give a shout-out to how solid the back-testing is in flumine :chart:.

Top graph is actual behaviour over the last couple of weeks, and bottom graph is the results over the same time using back-testing.

---

## 2021-09-24 11:25:03 - issues channel

**Aaron Smith**

I am doing a backtest on InPlay horses and i am getting weird results. I m placing terrible bets on purpose to check its sanity, but i m barely getting matched. I had this strategy running live (with more reasonable prices) and i was getting matched more. Anyone has an idea in what direction to look here?

---

## 2021-09-23 15:13:37 - issues channel

**Peter C**

I'm a bit unsure of how to add multiple strategies to a single instance of flumine and would appreciate going over it. I'm under the impression that I can import my two strategy files, set up two strategy objects as such:

```strategy_1 = strategy_1(

  market_filter=streaming_market_filter(

        event_type_ids=["7"],

        country_codes=["GB", "IE"],

        market_types=["WIN"],

    ),

      market_data_filter=streaming_market_data_filter(fields=["EX_MARKET_DEF", "EX_ALL_OFFERS", "EX_TRADED"]),

      max_order_exposure=1,

      max_selection_exposure=1,

      max_live_trade_count=1,

      conflate_ms=0,

      context={"stake": 2},

)```

     and then call:

```framework.add_strategy(strategy_1)

framework.add_strategy(strategy_2)

framework.run()```

Then I am under the impression these two strategies will run independently on the same market stream. Is this right, and are there any gotchas? Thanks!

---

## 2021-09-22 16:02:22 - general channel

**S G**

Hi All 

How are you maintaining cert files required for betfair login in cloud? Considering the boxes are choosen at random/or a free box is choosen. For example in aws

---

## 2021-09-20 09:03:25 - general channel

**liam**

It will be due to the runner_context stopping the second order (the logs would tell you this), you can let two orders go through by



```strategy = LowestLayer(

    market_filter={"markets": markets, "listener_kwargs": {"inplay": True}},

    max_live_trade_count=2

)```

Or putting them under the same trade

---

## 2021-09-19 09:46:57 - issues channel

**liam**

1.20.1 deploying now [https://github.com/liampauling/flumine/actions/runs/1250358231](https://github.com/liampauling/flumine/actions/runs/1250358231)

---

## 2021-09-19 09:19:09 - issues channel

**Oliver Varney**

[https://github.com/liampauling/flumine/blob/ec060b7d98a9f15ec229fab73644d5ad970e05fe/flumine/clients/baseclient.py#L32](https://github.com/liampauling/flumine/blob/ec060b7d98a9f15ec229fab73644d5ad970e05fe/flumine/clients/baseclient.py#L32)

---

## 2021-09-16 11:41:54 - issues channel

**liam**

hmm this is because `market.market_book` isn't updated when the status is closed (`market()` not called), open to a PR to change this, logic [https://github.com/liampauling/flumine/blob/ec060b7d98a9f15ec229fab73644d5ad970e05fe/flumine/backtest/backtest.py#L118|here](https://github.com/liampauling/flumine/blob/ec060b7d98a9f15ec229fab73644d5ad970e05fe/flumine/backtest/backtest.py#L118|here)

---

## 2021-09-15 10:09:33 - strategies channel

**Jonjonjon**

It's due to this bit:

[https://github.com/liampauling/flumine/blob/ec060b7d98a9f15ec229fab73644d5ad970e05fe/flumine/config.py#L8-L12](https://github.com/liampauling/flumine/blob/ec060b7d98a9f15ec229fab73644d5ad970e05fe/flumine/config.py#L8-L12)

---

## 2021-09-14 14:31:52 - general channel

**EJono**

Occasionally I'm missing executable orders in flumine upon restarting my strategy. I believe this is because the "config.hostname"  changes for me each time I rebuild (I'm running flumine on a cloud service VM and each deployment it changes). I'm wondering if I could use the same middleware method for accessing EXECUTION_COMPLETE orders but substitute in EXECUTABLE to the order_projection field of list_current_orders, would this be viable? I'm also filtering list_current_orders on market id. Other strategies bets being pulled through are not a problem as I'm only placing on this one strategy at the moment



Additionally should the market.blotter be filled with Strategy relevant executable orders at the time of add_market_middleware running?

---

## 2021-09-13 13:25:46 - strategies channel

**Unknown**

All my bot strategies are implemented with different set of parameters, this allows me to set different strategies just by setting different entry parameters/values, and of course changing the strategy behavior in runtime when bot is actively running on a market. I do not restart my system at all (my app) app manages any problems with betfair api, and is able to re-login and so on. Well yes I restart the app only when hosting server restarted.   Screenshot better explains what I tried to explain.

---

## 2021-09-12 13:44:59 - strategies channel

**Mo**

There is a Trader Manager component which monitors one or more data streams to identify events to trade. When a new one is encountered it runs a Trader process, specifically Python pointed at a particular module. The code that gets executed is just whatever is in the virtual environment at the moment the process is launched. So if a code update is deployed then it will get picked up by any new processes starting up after that deployment

---

## 2021-09-12 10:11:23 - strategies channel

**S G**

In some production softwares, dynamic configuration is read from DB, and we a add a cache time to queries say 5 seconds, meaning your config changes will take a max of 5 seconds to kick in to your strategy. This also means you read config from DB every 5 seconds

---

## 2021-09-11 17:28:40 - strategies channel

**birchy**

When needing to change some hard-coded parameters for a "live" strategy, is it best to stop -&gt; change -&gt; restart, or use a more dynamic setup, i.e. have an external settings file which is polled each time a market strategy is initiated?

---

## 2021-09-10 12:58:36 - general channel

**liam**

It needs to be above 15 to correctly get the timeout message on connection but this was to help poor networks where the socket was timing out when the server was still happy

---

## 2021-09-07 21:24:19 - general channel

**Peter**

The data isn't really the same shape. When backtesting Flumine processes one market at a time by default (unless you ask it to group markets together e.g. by event). With paper trading you're getting all the open markets captured by your filter, at the same time, so the same structure as your live strategy will see. This is especially useful if your strategy crosses markets.

---

## 2021-09-07 20:48:02 - general channel

**ShaunW**

[@U01SCPDTV5J](@U01SCPDTV5J) The data may be the same shape but the delivery method is different, so if a test is possible then the question becomes why not?  Personally I'd always do it for a while at least as it was a useful part of the belt and braces methodology I introduced at my old 'work'.  I like to treat my money with the same degree of respect I had to show towards other people's.:slightly_smiling_face:

---

## 2021-09-07 19:49:44 - general channel

**liam**

Yep it’s basically a final integration test that is using live data, handy if you are testing new markets where you don’t have data or in my case checking a change/update without accidentally fat fingering 

---

## 2021-09-07 19:31:53 - general channel

**Beeblebrox**

Yeah, in theory it should be the same, and to be fair Flumine already has checks in-built to make it less likely that you'll blow your bank.  So you don't have to do it if you don't want to - up to you.



I've used it sparingly.  To be fair my process is a bit slap dash.  Rather than analysis -&gt; backtest -&gt; paper trade -&gt; live, mine's make something up in my head that I think's logical -&gt; back test it -&gt; live -&gt; analysis to refine it.

---

## 2021-09-07 19:22:22 - general channel

**Beeblebrox**

It's happening in real time and with live data.  In theory it should be the same as backtesting, but who knows what errors you might have made that won't show up with historic data, but will in live data?  It's just a safety net to stop you losing a load of money.

---

## 2021-09-07 19:10:20 - general channel

**Beeblebrox**

I think the standard process for developing a strategy is: analysis -&gt; backtest -&gt; paper trade -&gt; live...

---

## 2021-09-07 14:21:44 - general channel

**Jeff Waters**

I don't know if it's relevant, but when I ran the program I got loads of messages like the following:



*{"asctime": "2021-09-07 10:55:53,215", "levelname": "CRITICAL", "message": "Unknown error can only concatenate str (not \"int\") to str in process_market_book (1.145082711)", "exc_info": "Traceback (most recent call last):\n  File \"C:\\Users\\water\\anaconda3\\envs\\bet-project\\lib\\site-packages\\flumine\\utils.py\", line 204, in call_strategy_error_handling\n    return func(market, market_book)\n  File \"C:\\Users\\water\\PycharmProjects\\bet-project\\teststrategy.py\", line 47, in process_market_book\n    print(\"Number of bets placed so far: \" + len(market.blotter))\nTypeError: can only concatenate str (not \"int\") to str"}*



and





*{"asctime": "2021-09-07 10:55:53,215", "levelname": "INFO", "message": "Order status update: Violation", "market_id": "1.145082712", "selection_id": 17069875, "handicap": 0, "id": "138503049532150426", "customer_order_ref": "d0d1ab6db12de-138503049532150426", "bet_id": null, "date_time_created": "2018-07-01 07:42:37.024000", "publish_time": null, "trade": {"id": "2bac1299-0fca-11ec-91ad-00e62d0401a8", "strategy": "TestStrategy", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138503049532150426"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 42, "size": 10, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 10.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}*



and



*{"asctime": "2021-09-07 10:55:53,230", "levelname": "WARNING", "message": "Order has violated: STRATEGY_EXPOSURE Error: Potential selection exposure (410.00) is greater than strategy.max_selection_exposure (105)", "control": "STRATEGY_EXPOSURE", "error": "Potential selection exposure (410.00) is greater than strategy.max_selection_exposure (105)", "order": {"market_id": "1.145082712", "selection_id": 17069875, "handicap": 0, "id": "138503049532150426", "customer_order_ref": "d0d1ab6db12de-138503049532150426", "bet_id": null, "date_time_created": "2018-07-01 07:42:37.024000", "publish_time": null, "trade": {"id": "2bac1299-0fca-11ec-91ad-00e62d0401a8", "strategy": "TestStrategy", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138503049532150426"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 42, "size": 10, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 10.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": "Order has violated: STRATEGY_EXPOSURE Error: Potential selection exposure (410.00) is greater than strategy.max_selection_exposure (105)", "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}}*



I also got loads of 'market closed' and 'market cleared' 'INFO' messages.



Are they to be expected, or do they hint at a problem with the code?

---

## 2021-09-07 12:01:19 - general channel

**birchy**

I run mine on an AWS Lightsail instance, overnight. Results are usually in by the time I wake up.

---

## 2021-09-05 22:35:15 - general channel

**Jeff Waters**

I've tried to add filters to only include British races and win races (based on filters used in the the example code at [https://github.com/liampauling/flumine](https://github.com/liampauling/flumine)):



```strategy = TestStrategy(

    market_filter={"markets": marketsToProcess},

    country_codes=["GB"],

    market_types=["WIN"],

    max_order_exposure=1000,

    max_selection_exposure=105,

)```

However, I got an error message:



*TypeError: __init__() got an unexpected keyword argument 'country_codes'*



Presumably, that was because I wasn't using market_filter=streaming_market_filter, as in the example (as I was using historical data rather than live data).



Is there any way of filtering out particular types of races when using historical data, other than putting something like

```if market.country_code != "GB":

    continue```

in the process_market_book method?



Thanks



Jeff

---

## 2021-09-05 10:54:35 - general channel

**Jonjonjon**

What do people do if they run over 10 discrete strategies?



Do they have Python scripts that run more than one strategy?



Or do they use some sort of local server, with course filter, and sends the data to individual strategies running on other Python scripts?

---

## 2021-09-04 22:54:53 - general channel

**Jonjonjon**

I use non-standard deployment methodology, as I'm only interested in stuff a few minutes before the race starts.

---

## 2021-09-04 06:19:04 - issues channel

**Peter**

My error logs this morning are full of errors like this:



`ERROR:flumine.worker:_get_cleared_market error`

`Traceback (most recent call last):`

  `File "/home/ec2-user/.local/lib/python3.7/site-packages/flumine/worker.py", line 232, in _get_cleared_market`

    `group_by="MARKET",`

  `File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/endpoints/betting.py", line 434, in list_cleared_orders`

    `(response, response_json, elapsed_time) = self.request(method, params, session)`

  `File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/endpoints/baseendpoint.py", line 55, in request`

    `self._error_handler(response_json, method, params)`

  `File "/home/ec2-user/.local/lib/python3.7/site-packages/betfairlightweight/endpoints/baseendpoint.py", line 81, in _error_handler`

    `raise self._error(response, method, params)`

`betfairlightweight.exceptions.APIError: SportsAPING/v1.0/listClearedOrders`

`Params: {'betStatus': 'SETTLED', 'marketIds': ['1.187240733'], 'customerStrategyRefs': ['ip-172-31-5-190'], 'settledDateRange': {'from': None, 'to': None}, 'groupBy': 'MARKET'}`

`Exception: None`

`Error: {'code': -32099, 'message': 'ANGX-0006', 'data': {'APINGException': {'requestUUID': 'ie2-ang30b-prd-09020828-000312861c', 'errorCode': 'UNEXPECTED_ERROR', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}`

`Full Response: {'jsonrpc': '2.0', 'error': {'code': -32099, 'message': 'ANGX-0006', 'data': {'APINGException': {'requestUUID': 'ie2-ang30b-prd-09020828-000312861c', 'errorCode': 'UNEXPECTED_ERROR', 'errorDetails': ''}, 'exceptionname': 'APINGException'}}, 'id': 1}`



Latest version of Fumine (1.19.12) and BFLW (2.13.2), and the trades are working fine.



I'm wondering whether anybody else has any idea what could be triggering these "unexpected errors" in the __get__cleared_market() method.



It might be connected to the from and to dates not being set in the API call, but as this is happening internally to Flumine, I'm don't know how to ensure that they get set.

---

## 2021-09-03 00:00:22 - strategies channel

**Adrian**

Yes thanks for all the help! I might be falling into the trap of overfitting. I'm running strategies live that I couldn't get lower than 4-5% in the MC and didn't split the data into test/validation as I only have 2 months worth

---

## 2021-09-02 11:55:49 - strategies channel

**Jack**

[@UGV299K6H](@UGV299K6H) wouldn't the logical error be caught by the backtest itself? Or is there an example of an error that would only present itself in a live scenario?

---

## 2021-08-28 07:36:55 - general channel

**Mo**

It's the wrong tool for the job. It takes in either the live stream or historic data files and runs a strategy or strategies against that. It's for live trading or backtesting. It's not for data transformation or analysis

---

## 2021-08-26 08:58:58 - general channel

**liam**

have a look at the [https://github.com/liampauling/betfair/blob/0dbd551272d66ea1e4efe8f8c695b54099567265/betfairlightweight/baseclient.py#L96|code](https://github.com/liampauling/betfair/blob/0dbd551272d66ea1e4efe8f8c695b54099567265/betfairlightweight/baseclient.py#L96|code) every time you set the sessionToken you reset the login time, yes making sure you keep_alive will prevent errors but just don't use that property if you are sharing tokens around trading objects

---

## 2021-08-26 08:56:03 - general channel

**Jono**

Im assuming that is only a problem after 8 hours and i havent ran keep_alive() at any point - as long as i stay on top of running keep_alive() will this be enough to get around any issues this might cause?

---

## 2021-08-25 10:19:05 - general channel

**liam**

`process_orders` is per market but note that if you have live orders in the market this will be called every 0.25s and every 5s regardless, logic [https://github.com/liampauling/flumine/blob/74114ed6c20e90289e5a0d2fac46d0969d46a158/flumine/streams/orderstream.py#L62|here](https://github.com/liampauling/flumine/blob/74114ed6c20e90289e5a0d2fac46d0969d46a158/flumine/streams/orderstream.py#L62|here). So you can assume each event will be a single market but you will need to handle which market if you have specific rules etc.



`RunnerContext` is per market per selection, need more info on any problems regarding this?



Yes the controls will prevent you placing any orders over exposure/count with a violation message.

---

## 2021-08-24 16:44:09 - general channel

**Jono**

hey everyone ive got a strategy built out in flumine that ive got to a stage where i'd like to increase the number of markets i place on to really test it out. So far ive kept it at one market for testing as this is my first flumine strat and very much needed to take the time to familiarise myself with the basics. I was wondering the following after recently attempting to place on two markets:



Does the process_orders function run independently for each market_id placed on or does it just run whenever there is an update on any executable order across all markets? I am looping through the "orders" list in this func currently and have been assuming only orders in the same market correlating to the update are being included with no check of my own. Will this be an issue if i dont include my own market_id check going forward inside process_orders?



Secondly i currently have the "max_live_trade_count" set to 1 and was assuming this would allow for 1 executable order per runner per market as suggested in the docs but im running into a problem that suggests im going over this from time to time. Am i wrong and does this variable actually apply across the whole strategy or should it function as i initially described?



Lastly, should the limits set up through "max_order_exposure= ", "max_live_trade_count=", "max_selection_exposure=" ... actually stop the placing of orders or cancel bets if their set values are exceeded?



Thanks for any help with the above its greatly appreciated! :grin:

---

## 2021-08-24 09:50:33 - general channel

**liam**

On my production code I have an API logging control which sends new markets/trades/orders to an API so that I can analyse later in the db, the control is just an abstraction for exactly this with the framework itself making the calls when required

---

## 2021-08-24 07:04:55 - strategies channel

**Steve Roach**

Ok, that’s no good. I was hoping to be able to build an initial history without having to pay at this stage. Of course, I’ll pay for access when I get closer to a live system but it’s all analysis and modeling at the moment.



Don’t suppose there’s a free way of building a history of pre-start Aus horse racing data?

---

## 2021-08-23 08:59:40 - issues channel

**Adrian**

[@U4H19D1D2](@U4H19D1D2) its not critical, just warning but its "Order status update: Violation" so it's not placing bets, from what I can tell.

And i'm working on the better connection :wink::laughing: just want to salvage the data i have

&gt; {"asctime": "2021-08-23 07:23:18,428", "levelname": "INFO", "message": "Order status update: Violation", "market_id": "1.186624929", "selection_id": 35921733, "handicap": 0, "id": "138489961984286210", "customer_order_ref": "c6e3b6551008c-138489961984286210", "bet_id": null, "date_time_created": "2021-08-22 07:20:46.819000", "publish_time": null, "trade": {"id": "fd07dc9a-03e2-11ec-9864-1e0062368792", "strategy": "Backtest", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138489961984286210"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 6.8, "size": 5.0, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 5.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}

---

## 2021-08-20 16:51:08 - issues channel

**Jack**

I think you are better off looking at this file:

[https://github.com/liampauling/flumine/blob/master/examples/backtest.py](https://github.com/liampauling/flumine/blob/master/examples/backtest.py)



This is where you kick off your backtesting. You pass your markets into the market filter (line #25) as a list of filepaths.



You don’t need to manually call the process_orders() function. It’s going to get called for you as flumine does it’s thing and detects updates to the market stream (whether that is simulated from a file in this case, or from a real stream if you are live)



Try running that backtest file with a market file of your own as a starting point.

---

## 2021-08-20 12:08:08 - issues channel

**Jeff Waters**

No - unless it notices there is a virtual environment and does so automatically. I've tried running it without a virtual environment and get the same error message.



I wonder whether there is a gremlin somewhere on my computer that's causing this issue. It might be worth me using Google Colaboratory and see if I have any more joy running the program on their servers?

---

## 2021-08-18 14:29:26 - strategies channel

**thambie1**

Lots (probably most) people on this slack are. You can rent a sufficient server for between $5 and $20 per month for basic setups. Many use aws ec2

---

## 2021-08-18 13:39:42 - random channel

**Newbie99**

Orders appear to be clearing correctly and I'm just simply trying to verify the event is coming through correctly at the mo, not even trying to do anything clever yet.



Here are the logs:



```INFO:flumine.markets.market:Market 1.186458391 closed

{"asctime": "2021-08-18 12:01:04,265", "levelname": "INFO", "message": "Market 1.186458391 closed", "market_id": "1.186458391", "event_id": "30801039", "event_type_id": "7", "event_name": "Ballarat (AUS) 18th Aug", "market_type": "WIN", "market_start_datetime": "2021-08-18 11:57:00", "country_code": "AU", "venue": "Ballarat", "race_type": "Flat", "orders_cleared": false, "market_cleared": false, "closed": true}

INFO:flumine.baseflumine:Market cleared

{"asctime": "2021-08-18 12:01:04,266", "levelname": "INFO", "message": "Market cleared", "market_id": "1.186458391", "order_count": 31, "client": {"id": "78382e88", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 4, "transaction_count_total": 31, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x0000028CAC6449A0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": true}, "markets": {"market_count": 249, "open_market_count": 240}, "streams": ["&lt;SimulatedOrderStream(SimulatedOrderStream, started daemon 20980)&gt;", "&lt;MarketStream(MarketStream, started daemon 1276)&gt;"], "logging_controls": ["&lt;ClosedMarketLogging(CLOSED_MARKET_LOGGING_CONTROL, started daemon 26460)&gt;"], "threads": ["&lt;_MainThread(MainThread, started 24220)&gt;", "&lt;Thread(Thread-1, started 24048)&gt;", "&lt;BackgroundWorker(get_live_orders, started daemon 12544)&gt;", "&lt;BackgroundWorker(get_price_change, started daemon 22728)&gt;", "&lt;BackgroundWorker(terminate, started daemon 26972)&gt;", "&lt;BackgroundWorker(keep_alive, started daemon 24260)&gt;", "&lt;BackgroundWorker(poll_market_catalogue, started daemon 8088)&gt;", "&lt;BackgroundWorker(poll_account_balance, started daemon 24856)&gt;", "&lt;BackgroundWorker(poll_market_closure, started daemon 25208)&gt;", "&lt;ClosedMarketLogging(CLOSED_MARKET_LOGGING_CONTROL, started daemon 26460)&gt;", "&lt;SimulatedOrderStream(SimulatedOrderStream, started daemon 20980)&gt;", "&lt;MarketStream(MarketStream, started daemon 1276)&gt;", "&lt;Thread(MarketStream_output_thread, started daemon 19184)&gt;", "&lt;Thread(ThreadPoolExecutor-0_0, started daemon 9736)&gt;", "&lt;Thread(ThreadPoolExecutor-0_1, started daemon 11932)&gt;", "&lt;Thread(ThreadPoolExecutor-0_2, started daemon 20068)&gt;", "&lt;Thread(ThreadPoolExecutor-0_3, started daemon 18468)&gt;", "&lt;Thread(ThreadPoolExecutor-0_4, started daemon 27360)&gt;"]}

INFO:flumine.baseflumine:Market closed

{"asctime": "2021-08-18 12:01:04,267", "levelname": "INFO", "message": "Market closed", "market_id": "1.186458391", "client": {"id": "78382e88", "exchange": "Betfair", "betting_client": "APIClient", "current_transaction_count_total": 4, "transaction_count_total": 31, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x0000028CAC6449A0&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": true}, "markets": {"market_count": 249, "open_market_count": 240}, "streams": ["&lt;SimulatedOrderStream(SimulatedOrderStream, started daemon 20980)&gt;", "&lt;MarketStream(MarketStream, started daemon 1276)&gt;"], "logging_controls": ["&lt;ClosedMarketLogging(CLOSED_MARKET_LOGGING_CONTROL, started daemon 26460)&gt;"], "threads": ["&lt;_MainThread(MainThread, started 24220)&gt;", "&lt;Thread(Thread-1, started 24048)&gt;", "&lt;BackgroundWorker(get_live_orders, started daemon 12544)&gt;", "&lt;BackgroundWorker(get_price_change, started daemon 22728)&gt;", "&lt;BackgroundWorker(terminate, started daemon 26972)&gt;", "&lt;BackgroundWorker(keep_alive, started daemon 24260)&gt;", "&lt;BackgroundWorker(poll_market_catalogue, started daemon 8088)&gt;", "&lt;BackgroundWorker(poll_account_balance, started daemon 24856)&gt;", "&lt;BackgroundWorker(poll_market_closure, started daemon 25208)&gt;", "&lt;ClosedMarketLogging(CLOSED_MARKET_LOGGING_CONTROL, started daemon 26460)&gt;", "&lt;SimulatedOrderStream(SimulatedOrderStream, started daemon 20980)&gt;", "&lt;MarketStream(MarketStream, started daemon 1276)&gt;", "&lt;Thread(MarketStream_output_thread, started daemon 19184)&gt;", "&lt;Thread(ThreadPoolExecutor-0_0, started daemon 9736)&gt;", "&lt;Thread(ThreadPoolExecutor-0_1, started daemon 11932)&gt;", "&lt;Thread(ThreadPoolExecutor-0_2, started daemon 20068)&gt;", "&lt;Thread(ThreadPoolExecutor-0_3, started daemon 18468)&gt;", "&lt;Thread(ThreadPoolExecutor-0_4, started daemon 27360)&gt;"]}```

Here is the code:



```import logging

from flumine.controls.loggingcontrols import LoggingControl

from flumine.events import events



logger = logging.getLogger(__name__)



class ClosedMarketLogging(LoggingControl):

    NAME = "CLOSED_MARKET_LOGGING_CONTROL"



    def __init__(self, *args, **kwargs):

        super(ClosedMarketLogging, self).__init__(*args, **kwargs)



    def _process_cleared_markets(self, event: events.ClearedMarketsEvent):

        [http://logging.info|logging.info](http://logging.info|logging.info)(event)

        for cleared_market in event.event.orders:

            [http://logging.info|logging.info](http://logging.info|logging.info)(cleared_market.profit, cleared_market.commission)```

I'm really hoping I'm unintentionally just doing something weird (which lets face it is normally the case)!!!

---

## 2021-08-16 23:10:23 - strategies channel

**admiral**

Would you not need a stream of live data to keep the data your feeding into your model for predictions up to date?

---

## 2021-08-16 11:04:08 - issues channel

**George**

I've started getting a lot of 'Connection reset by peer' errors this morning. I am using a script that logs in to the API at the start. Is it because I am failing to log-out when the script terminates? Or is it just a bad day at the BF servers

---

## 2021-08-15 08:32:37 - issues channel

**ThomasJ**

I will create a GitHub issue. I'm thinking that it might be best in Flumine issues as total_matched is most likely to be used in Flumine where it can be easily calculated only when it is required. Rather than wasting CPU cycles in BFLW. Perhaps just add a comment in BFLW at `runner.total_matched = new_data["tv"]`

So I presume when live streaming `new_data["tv"]` value is sum of volumes of all prices?

---

## 2021-08-15 04:55:41 - issues channel

**ThomasJ**

(I am running Flumine over Historical Data.)



Is the runner traded volume for live streaming different to historical data? I ask because this code `runner.total_matched = new_data["tv"]` in BFLW  `Class MarketBookCache &gt; def update_cache` seems to store only the traded volume at a single price (sometimes for more than 1 price) and not the accumulated volume for all price points.



In "Betfair Developer Program  Exchange API  Historical Data FAQ's - How is traded volume represented within the PRO Historical Data files?" ([https://support.developer.betfair.com/hc/en-us/articles/360002401937-How-is-traded-volume-represented-within-the-PRO-Historical-Data-files-|link](https://support.developer.betfair.com/hc/en-us/articles/360002401937-How-is-traded-volume-represented-within-the-PRO-Historical-Data-files-|link)) it explains that "The runner tv (“tv”) represents the cumulative traded amount at the last price traded (ltp) or cumulative amount of all ‘trd’ amounts in the update if multiple prices are included."



Anyway, regardless of the details of the calculation of `runner.total_matched`, at any point in time should not its value be equal to the sum of all volumes at all prices?



I am digging into this because in Flumine's `class RunnerAnalytics &gt; def _calculate_matched`, the code `return round(total_matched - prev_total_matched, 2)` seems to assume that `total_matched` is always increasing in value when it's not. Numerous times the returned value is a negative (and by large amounts as I recorded them) which is not possible in real life.

---

## 2021-08-13 11:27:38 - general channel

**Rach**

Mmm then I'll consider paying for the live_key soon. Can you use streaming with the delayed key?

---

## 2021-08-12 11:30:31 - general channel

**George**

I think this might be the answer? [https://docs.python-requests.org/en/latest/user/advanced/#keep-alive](https://docs.python-requests.org/en/latest/user/advanced/#keep-alive)

---

## 2021-08-12 07:53:27 - random channel

**Laybot McBackbot**

One thing I learned from losing a lot -- if a couple of mistakes (regardless of the sources) could significantly ruin your P/L -- then it's probably not a good system.  These days I am spending most of my time backtesting and compiling all the worst possible outcome that I came across in my testing/live and whatever strat I come up with must be able to survive those.



Of course,  past data is no indicator of future performance but how could one say a system is working if it could not even do well when backtested? :sweat_smile:

---

## 2021-08-10 12:28:22 - random channel

**Unknown**

A bit of fun..:grinning:, 1st live Flumine strat (week) hangin on by a thread (thanks to [@UUCD6P13J](@UUCD6P13J) lee for building the bare bones and then me making it look like a Frankenstein version, and not a pretty one at that!), thumbs up if you think it will  turn out well, thumbs down if you think no chance :grinning: rocket if it will let me put an offer in for [@UPMUFSGCR](@UPMUFSGCR) yacht early next year...and Ill report back in a month :grinning: (PS my main stuff had an under par week too, so lets see)

---

## 2021-08-09 19:16:00 - general channel

**admiral**

Very noob question but if you wanted to record market data to use in backtest going forwards, would running a OrderDataStream class on an ec2 instance saving to S3 be a generic way of doing this?

---

## 2021-08-08 13:24:45 - strategies channel

**Jonjonjon**

What sort of consistency can those models deliver? Is it possible to be profitable every day of the week?

---

## 2021-08-06 13:45:27 - strategies channel

**Dave**

It's difficult to generalise this I think. For example, in football you have new leagues starting every few weeks in all sorts of regions....I spend a fair bit of time maintaining models for so many leagues, investigating if a model is ready to be deployed for a recently started league etc. Per-league effort is probably quite small but it adds up, and then you need time for execution optimization, and r&amp;d into new features. Maybe racing markets are more stable in this regard (idk). This is coming from a truly fundamental approach.

---

## 2021-08-06 12:49:12 - issues channel

**Mo**

AWS, eu-west-1

---

## 2021-08-06 11:04:24 - strategies channel

**liam**

I think it is very dependant on where your edge lies, my weak link is strategy development so my focus is on execution, infra/ease of deployment, rt analysis and speed (probably in that order)

---

## 2021-08-04 11:46:51 - strategies channel

**Mo**

Bit of a fundamental question here but not something I've ever had to worry about: How do you handle removing your orders from your view of the market when the order stream and price stream may not be aligned? For example, the order stream update comes in first so your order is not actually in the price stream to remove or, vice versa, the price stream update comes in first so you don't realise your order is live yet?

---

## 2021-08-03 08:10:05 - general channel

**Mo**

I wouldn’t recommend someone just starting out to dump thousands on a load of PRO data for backtesting. Just put together a simple strategy so you are actually placing live bets then record the data yourself. This seems to be the canonical way to get started 

---

## 2021-08-03 03:31:54 - strategies channel

**ShaunW**

It was [@U01LD279D16](@U01LD279D16) (i think :slightly_smiling_face:)..but I'm not sure I said much you probably didn't already know.   The trouble with charts is that they can be like fractals, you never quite know what part you're looking at and at what zoom level. Even grim times lived in real-time can be not much more than a ripple after a year or two.  I once saw an interview with a quant, and worked with a few too back in another life, strategy death despite being the subject of plenty of research is very much a finger in the wind job, and that inexplicable 'feel' you get from experience. Taking on a comment from [@UGV299K6H](@UGV299K6H) and my own limitted experience, they tend to fade rather than do an about turn so maybe that's just a bit of random weirdness. Good luck with it, no easy anwsers that I know of.

---

## 2021-08-02 19:16:26 - strategies channel

**Peter**

It had been thoroughly analysed and then backtested before going live, so I was pretty confident that it was a solid strategy and the first 20k of bets rather backed that up. Couldn't see anything in the market that had changed fundamentally, so kept the faith that it was just a bad run, and it's back doing what I would generally expect it to do.

---

## 2021-08-02 18:24:35 - strategies channel

**Jonjonjon**

No. A back-to-lay strategy. But I had a deployment issue and it stopped laying the long odds.

---

## 2021-07-31 07:25:39 - strategies channel

**liam**

I pipe straight to aws cloudwatch 

---

## 2021-07-30 09:15:02 - issues channel

**Adrian**

in future, instead of asking silly questions, how do I find where a function lies in the flumine ecosystem? E.g. you share the bspMarket boolean, which is obviously in market_definition, but beyond that I have trouble tracing back where it lives

---

## 2021-07-30 07:20:05 - general channel

**Jono**

What constitutes a live order in flumine - is it an order that has a unmatched amount on a market yet to resolve? As i want to place a follow up bet each time the previous one is matched in the order back, lay , back, lay, back.... so on i was wondering will i have to adjust the max_live_trade_count to something very high in order to do this?



And are orders retrieved in the process_orders function listed in a p[articular way consistently eg placed time descending? Additionally if there is some consistent ordering does this also apply to selection_orders/strategy_orders orders retrieved via the market.blotter so i know when iterating? Thanks for any help everyone :grin:

---

## 2021-07-27 18:54:17 - general channel

**PeterLe**

Folks, quick question please..First time I ran a live strategy (Whoop!)...but placed a bet deliberately low at 1.03..If I stop the program to edit the code and then restart it, does it somehow keep a live connection (ie am I in danger of  having too many connections if I stop and start it regularly? Thanks

---

## 2021-07-27 13:01:40 - strategies channel

**liam**

yeah, so any update to order will result in the order stream sending an update through and then flumine will [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/streams/orderstream.py#L62|snap](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/streams/orderstream.py#L62|snap) the stream anyway every 0.25s if there are live orders or every 5s regardless, this is to cater for fill/kill etc.

---

## 2021-07-26 14:55:54 - general channel

**Matthieu Labour**

Hi, when running flumine / backtest, do the `runner_context.trade_count` and `runner_context.live_trade_count` get updated?

---

## 2021-07-25 08:54:31 - issues channel

**Mo**

In production I’ve measured an overhead of 300 microseconds between the publisher and subscriber. I’m purely thinking in terms of reducing that number 

---

## 2021-07-24 09:22:41 - general channel

**Jono**

i was wondering the best way/practice to cancel any unmatched bets on a market using flumine? The reason i am asking is that although i am aware that the runner context should only allow 1 active bet per market per selection, upon running the strategy i am currently testing out it seems to place another order. Thus i would like to cancel the unmatched bet placed previously by the strategy so i dont unnecessarily increase my exposure whilst getting to grips with flumine. Below is the codfe i belived should have cancelled any outstanding orders i have but this is causing errors at "market.cancel_order(order)". Any help very much appreciated thank you!



`def process_orders(self, market, orders):`

`        [http://logging.info|logging.info](http://logging.info|logging.info)("Processing orders: %s"%(orders))`

        `for order in orders:`

`    `

            `try:`

`                [http://logging.info|logging.info](http://logging.info|logging.info)("Order: %s"%(order.__dir__))`

                `for key, value in order.__dict__.items():`

`                    [http://logging.info|logging.info](http://logging.info|logging.info)("%s : %s"%(key, value))`

            `except:`

`                [http://logging.info|logging.info](http://logging.info|logging.info)("Couldn't log order dict in method above")`





            `if order.status == OrderStatus.EXECUTABLE:`

`                [http://logging.info|logging.info](http://logging.info|logging.info)("Order executable")`



            `market.cancel_order(order)`                

`            `

`            [http://logging.info|logging.info](http://logging.info|logging.info)("Order cancelled supposedly")`

---

## 2021-07-22 10:52:07 - strategies channel

**liam**

I plug in matching rates for price/win/loss when modelling this so it gets me half way before confirming with backtesting and then live

---

## 2021-07-21 09:13:52 - issues channel

**Adrian**

yes thanks, i've done that for the market recorders. i might just have to work at reducing everything down to 2 flumines and/or get uk server as suggested

---

## 2021-07-21 03:46:52 - general channel

**Greg**

thanks again Oliver. My best mate is a coder and solved it with a get and set(C#/full stack coder he is but limited python) He solved for me in about 30 secs via screen share.Sigh..so much to learn from you lot :slightly_smiling_face: Anyway cheers for the offer.Might have to take advantage another time if that's ok.Cheers

---

## 2021-07-20 14:18:41 - issues channel

**Luka**

[@UPMUFSGCR](@UPMUFSGCR) I am running it on AWS so the errors I am getting are just those things I create print statements for. I have a version of the code that I can run locally, which may help me with debugging

---

## 2021-07-20 11:08:23 - general channel

**liam**

`process_market_book`  will execute if True is retired on the check, the update itself comes from either an update from the market stream or a snap based on the strategy `streaming_timeout` which is handy on low update/liquidity markets



`process_orders` will be executed if there is an update through the order stream or is executed every 0.25s if there are live orders, see logic [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/streams/orderstream.py#L67|here](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/streams/orderstream.py#L67|here)



Order count is controlled by the runner context in the strategy, the default is a single live order per selection per market, var [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/strategy/strategy.py#L52|here](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/strategy/strategy.py#L52|here). And logic for validating that is [https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/strategy/strategy.py#L138|here](https://github.com/liampauling/flumine/blob/a7aff28371b1a7a30122dfd76a1effcee4ddc127/flumine/strategy/strategy.py#L138|here) and executed on each place. Currently designed to be as safe as possible by default to prevent what you are describing. But you have backtesting and paper trading available to validate your strategy won't shit itself before you execute live :slightly_smiling_face:

---

## 2021-07-13 20:16:06 - strategies channel

**thambie1**

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) The first thing I'd suggest doing is verifying the accuracy of your backtest. And understanding all differences between live and backtest performance. If you do that, you can then launch strategies with a high degree of confidence even if they hit an initial downswing

---

## 2021-07-13 10:45:51 - general channel

**J**

Hey, excited to join betfairlightweight. has anyone intergrated historical or live data with xcode or firebase?

---

## 2021-07-10 03:43:53 - general channel

**Andrew**

Hi. I’m a non bflw user at the moment. My coding strength is C# and happily use the API - variations of that provided by Betfair. But I have a question on considering trying some bflw implementation. On RunnerChange trd is aggregate volume at price since start of market. I think I’m right. I can handle that OK but curious if bflw does that already to deliver change in trade volume at each change point? Simulating trading (how I do it) needs to know volume executed at a time point so how does bflw handle this?

---

## 2021-07-08 06:00:21 - general channel

**Adrian**

I'm using the live key. I just wanted to know how flumine queues up the market messages. I think I found the answer but it would be good to confirm. It seems there is a one-to-one ratio between loops and market messages, provided there is a message. Therefore if there is a delay and 5 messages are published, it will take 5 loops of the function to catch up, rather that catching them all up on the next loop.

---

## 2021-07-08 03:33:47 - general channel

**Adrian**

Could someone help me understand the following scenario? If I am retrieving the last price traded from market_book.runners every cycle and there is lag, how are the delayed messages handled? Are they all queued up and delivered once everything resumes, or are the delayed messages skipped over and only the last update sent through? E.g. in process_market_book, i save the ltp into a variable and 5 market updates are created but I don't receive them until the 6th update, do I only get the 6th update or will all 6 come through at once? Thanks

---

## 2021-07-05 08:59:36 - general channel

**Mo**

1. Better to use the raw stream live because of the latency of the virtual stream

2. Can always go raw -&gt; virtual, I would imagine it's impossible to go the other way

---

## 2021-07-05 01:06:56 - general channel

**Adrian**

Hey guys hoping someone could help me interpret this printout from process_market_book (flumine):

```pt: 1625442293716 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442294679 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442294788 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442296790 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442299818 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442300182 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442300901 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442301216 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442301310 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442301638 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}

pt: 1625442301720 m_id: 1.185041121 r_ex_tv: {'price': 7, 'size': 69.59}```

As you can see, these are all from the same market/runner in the same update. But they are spread over 8 seconds. Which I can't wrap my head around, why would the ltp and volume return the same amount for 8 seconds? There should only be one update showing like this and the rest should be blank

(live key)

---

## 2021-07-03 23:22:08 - issues channel

**Adrian**

[@U01DVUAE2G1](@U01DVUAE2G1) no i'm on the live key now.

[@UBS7QANF3](@UBS7QANF3) yeah that's what originally thought to do but i wanted to do it right

[@U4H19D1D2](@U4H19D1D2) are there any workers i can repurpose in bflw/flumine?

---

## 2021-06-29 20:48:43 - random channel

**Newbie99**

(I'm using AWS EC2, eu-west-2c region, are you the same by chance?)

---

## 2021-06-25 18:49:25 - strategies channel

**AndyL**

[@U01LD279D16](@U01LD279D16) fyi as your question is quite apt for what ive experienced in the last couple of days. My greyhound bot had a 41 bet losing streak yesterday in live betting, i then ran the streams last night through the same strategy in bactest and backtest only had a 26 bet streak as it missed 15 matches...

My strategy has a 10 second fill or kill, and from comparing logs live matched quite a bit more often than backtest 

So as [@UGV299K6H](@UGV299K6H) says dont just rely on backtest, just shame I've not worked anything else out yet..!?!

---

## 2021-06-25 15:59:02 - strategies channel

**Unknown**

My question about resampling is assuming that your backtest results match your live results, which is the case for me right now

---

## 2021-06-25 15:05:39 - strategies channel

**birchy**

My personal experience has been that live betting P&amp;L is significantly _better_ than Flumine backtests, i.e. one particular strategy has been matching a lot more bets than the backtests suggested. IMO the _type_ of strategy makes a difference.

---

## 2021-06-25 11:40:27 - strategies channel

**Beeblebrox**

As someone who started less then a year ago, so relatively new, I always assume that any strategy I design will fail when it goes live! It helps to have realistic expectations.



[@UGV299K6H](@UGV299K6H) as someone with a lot of experience what is your success rate like for new strategies you design? Do the majority of them succeed or is it closer to 50/50?

---

## 2021-06-25 10:48:16 - strategies channel

**Michael**

The answer to your question is 'it depends'. However, since you've made this decision: _'Ultimately I will run this strategy live either way and find out for myself!'_ it doesn't really matter does it?

---

## 2021-06-25 10:33:33 - strategies channel

**Peter C**

Apologies, I know this has been discussed before, but I'm searching through the slack and I can't find what I'm looking for. I'm backtesting a strategy that is putting up offers and am pleased with the results. Since this is my first attempt at offering prices I am keen to understand how the flumine backtest might translate into real life betting. I'd really appreciate anyone's insight into how I might expect my strategy to translate from backtest to real life. Ultimately I will run this strategy live either way and find out for myself!

---

## 2021-06-25 06:22:18 - general channel

**Peter**

It's controlled by injecting an appropriate client when you instantiate the framework

`framework = Flumine(client=client)`



If you inject the BetfairClient

`client = clients.BetfairClient(trading)`

that's designed to log you into Betfair and keep the connection alive. But if you're backtesting on previously recorded data, you inject the backtestclient,

`client = clients.BacktestClient()`

for which external login is disabled.

---

## 2021-06-24 15:50:44 - general channel

**Mo**

AWS RDS

---

## 2021-06-24 09:38:40 - general channel

**Adrian**

Function/class for recording live data

---

## 2021-06-24 09:20:28 - general channel

**Adrian**

Yes I've done that, using historical data. It was tricky, since the time intervals are not steady. But now i need live data

---

## 2021-06-23 11:41:26 - random channel

**PeterLe**

Thanks [@UUCD6P13J](@UUCD6P13J) So as a way forward, as Im still learning Python, would a good way forward be :

Record Data

Create Strategies

Test Those Strategies as a BackTest against recorded data before going live (On limited funds :grinning:)

---

## 2021-06-23 09:29:51 - issues channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2) I've noticed something else about the Replace function (I'm guessing most people just cancel and place a new order, which perhaps I should do!), below the replace fails as the bet has already had an action on it, that error makes sense, however after this the order is never removed from the blotter and the status remains as OrderStatus.REPLACING until I kill the process. This of course means no further orders can be placed on this runner as the blotter thinks there is 1 order remaining. This is obviously a pretty rare occurrence, but I've noticed it a few times and I think the below shows the process flow, but at the end presumably a better outcome would be that the order is removed (i.e. set to EXECUTION_COMPLETE) instead?



This is going to be very tricky to replicate, but if you agree its a bug I'll raise an issue (this is in live btw not simulation).



```{"asctime": "2021-06-22 18:50:10,695", "levelname": "INFO", "message": "execute_replace", "trading_function": "replace", "elapsed_time": 0.05340576171875, "response": {"customerRef": "abcf9b20d38a11ebbbac02054918c7c6", "status": "FAILURE", "errorCode": "BET_ACTION_ERROR", "marketId": "1.183248116", "instructionReports": [{"status": "FAILURE", "errorCode": "BET_TAKEN_OR_LAPSED", "cancelInstructionReport": {"status": "FAILURE", "errorCode": "BET_TAKEN_OR_LAPSED", "instruction": {"betId": "236404314713"}}, "placeInstructionReport": {"status": "FAILURE", "errorCode": "RELATED_ACTION_FAILED"}}]}, "order_package": {"id": "abcf9b20-d38a-11eb-bbac-02054918c7c6", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f148822e790&gt;", "market_id": "1.183248116", "orders": ["236404314713"], "order_count": 1, "package_type": "Replace", "customer_strategy_ref": "ip-172-31-4-105", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}}

{"asctime": "2021-06-22 18:50:10,701", "levelname": "INFO", "message": "Trade status update: Pending", "id": "511443a4-d388-11eb-bbac-02054918c7c6", "strategy": "politics_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["236404314713"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Pending", "status_log": "Pending, Live, Pending, Live, Pending, Live, Pending"}

{"asctime": "2021-06-22 18:50:10,707", "levelname": "INFO", "message": "Order status update: Executable", "market_id": "1.183248116", "selection_id": 5191378, "handicap": 0.0, "id": "236404314713", "customer_order_ref": "3203239236f2d-236404314713", "bet_id": "236404314713", "date_time_created": "2021-06-22 18:33:19.422805", "publish_time": null, "trade": {"id": "511443a4-d388-11eb-bbac-02054918c7c6", "strategy": "politics_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["236404314713"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Pending", "status_log": "Pending, Live, Pending, Live, Pending, Live, Pending"}, "order_type": {"order_type": "Limit", "price": 1000.0, "size": 0.09, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 0.09, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Executable", "status_log": "Executable, Replacing, Executable, Replacing, Executable, Replacing, Executable, Replacing, Executable", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}

{"asctime": "2021-06-22 18:50:10,718", "levelname": "INFO", "message": "Order status update: Replacing", "market_id": "1.183248116", "selection_id": 5191378, "handicap": 0.0, "id": "236404314713", "customer_order_ref": "3203239236f2d-236404314713", "bet_id": "236404314713", "date_time_created": "2021-06-22 18:33:19.422805", "publish_time": null, "trade": {"id": "511443a4-d388-11eb-bbac-02054918c7c6", "strategy": "politics_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["236404314713"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Pending", "status_log": "Pending, Live, Pending, Live, Pending, Live, Pending"}, "order_type": {"order_type": "Limit", "price": 1000.0, "size": 0.09, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 0.09, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Replacing", "status_log": "Executable, Replacing, Executable, Replacing, Executable, Replacing, Executable, Replacing, Executable, Replacing", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}

{"asctime": "2021-06-22 18:50:10,718", "levelname": "INFO", "message": "Trade status update: Live", "id": "511443a4-d388-11eb-bbac-02054918c7c6", "strategy": "politics_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["236404314713"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": "Pending, Live, Pending, Live, Pending, Live, Pending, Live"}

{"asctime": "2021-06-22 18:50:10,719", "levelname": "INFO", "message": "Thread pool submit", "trading_function": "execute_replace", "session": "&lt;requests.sessions.Session object at 0x7f144b600050&gt;", "latency": 0.0001, "order_package": {"id": "abdbd200-d38a-11eb-bbac-02054918c7c6", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f148822e790&gt;", "market_id": "1.183248116", "orders": ["236404314713"], "order_count": 1, "package_type": "Replace", "customer_strategy_ref": "ip-172-31-4-105", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}, "thread_pool": {"num_threads": 32, "work_queue_size": 1}}

{"asctime": "2021-06-22 18:50:10,719", "levelname": "CRITICAL", "message": "Execution unknown error", "exc_info": "Traceback (most recent call last):\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/execution/betfairexecution.py\", line 243, in _execution_helper\n    response = trading_function(order_package, http_session)\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/execution/betfairexecution.py\", line 214, in replace\n    instructions=order_package.replace_instructions,\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/order/orderpackage.py\", line 152, in replace_instructions\n    return [order.create_replace_instruction() for order in self]\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/order/orderpackage.py\", line 152, in &lt;listcomp&gt;\n    return [order.create_replace_instruction() for order in self]\n  File \"/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/order/order.py\", line 394, in create_replace_instruction\n    bet_id=self.bet_id, new_price=self.update_data[\"new_price\"]\nKeyError: 'new_price'", "trading_function": "replace", "exception": "'new_price'", "order_package": {"id": "abdbd200-d38a-11eb-bbac-02054918c7c6", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f148822e790&gt;", "market_id": "1.183248116", "orders": ["236404314713"], "order_count": 1, "package_type": "Replace", "customer_strategy_ref": "ip-172-31-4-105", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}}

{"asctime": "2021-06-22 18:50:10,719", "levelname": "INFO", "message": "1 order packages executed in transaction", "market_id": "1.183248116", "order_packages": [{"id": "abdbd200-d38a-11eb-bbac-02054918c7c6", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f148822e790&gt;", "market_id": "1.183248116", "orders": ["236404314713"], "order_count": 1, "package_type": "Replace", "customer_strategy_ref": "ip-172-31-4-105", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0, "async": false}], "transaction_id": 10}

{"asctime": "2021-06-22 18:50:10,722", "levelname": "INFO", "message": "Deleting requests.Session", "sessions_created": 14, "session": "&lt;requests.sessions.Session object at 0x7f144b600050&gt;", "session_time_created": 1624387784.7466655, "session_time_returned": 1624387810.701893, "live_sessions_count": 0, "err": true}```



---

## 2021-06-20 20:44:43 - random channel

**Newbie99**

For live, I use the sep parameter to insert a code (i.e. order type a, b, c etc.), however I noticed in paper trading, it appears to ignore the sep parameter and just appears to have the 2 hashes joined together without a separator. Is there a way to replicate the live functionality here, so I can insert my order code without messing around with flumine source code (or if that is the only option, which bit should I be looking at, as I couldn't work it out)?



All I'm doing is this:



```order = trade.create_order(

        side=order_direction, order_type=LimitOrder(price=price, size=stake, persistence_type=persistence_type), sep=sep

    )```

---

## 2021-06-20 10:39:28 - strategies channel

**PeterLe**

[@U4H19D1D2](@U4H19D1D2) /All, So how might you go about doing this in practice? Lets say you had a laying system and one key variable you wanted to test (fill/kill). 

If the Fill/Kill was too fast, (and it can be) you would miss opportunities and too slow you’d get taken to the cleaners.

Over the years I have reached what I think is near an optimum through trail and error. 

What would be the steps you would take in practice (could you only do this by backtesting?) Or maybe you would use backtesting to get in a range and then fine tune by live testing perhaps?

Thanks in advance

---

## 2021-06-18 20:39:45 - strategies channel

**Oliver Varney**

I guess if the whole process of cross validation , grid searching parameters, model selection and deployment is all automated there no reason why you couldn't do it daily

---

## 2021-06-18 20:27:03 - strategies channel

**Dave**

Yeah, I retrain about once or twice a month. My training set is never older than past 4 months, but I hate retraining the model and re-deploying even though out of sample results are still as good as before. Maybe an unreasonable fear that things won't continue like they did on the older model

---

## 2021-06-17 07:39:01 - strategies channel

**birchy**

To add to the above, that strategy took several months to develop but is built upon market knowledge I've gained over the years and was NOT a "backtest today, go live tomorrow" strategy. As with all strategies, it started with a sensible idea but then I had to implement it and overcome lots of anomalies, execution issues and unforseen bugs. And then, when it worked as I wanted, I simply WAITED. I didn't touch anything, I didn't start "tweaking" parameters and I didn't obsessively check my P&amp;L every 5 minutes. Doing nothing is the hardest part. Learn that level of patience and everything else will fall into place.

---

## 2021-06-16 09:44:34 - issues channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2) it wasn't the 1000 orders (but have built a function to handle that now, based your BFLW example), but I added this line (under line 26 in your example) and it appears to work (have tried locally and on AWS and it seems to pick up orders placed on either and importantly not duplicate, which seemed to be happening previously, it appeared the middleware and flumine were both adding the orders placed via the current host without the line below):



```if current_order.customer_strategy_ref != config.hostname or current_order.status != 'EXECUTABLE':```

---

## 2021-06-15 20:12:02 - issues channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2) is there a tidy way to get current (executable) orders placed on a different host (with Flumine), presumably via middleware?



For example, I've placed orders with Flumine on my local machine and I'm moving this strategy onto AWS, but I want to pick up the existing live orders when I re-start the strategy on AWS.



I've been playing around, adapting your middleware example, trying to pull in ALL order types, but there's a step I'm missing, as it doesn't pick up the orders from the other host:



```order = self._create_order_from_current(current_order, market)

            if order:

                order.update_current_order(current_order)

                if current_order.status == "EXECUTION_COMPLETE":

                    order.execution_complete()

                else:

                    if order.bet_id not in market.blotter:

                        order.executable()```

---

## 2021-06-13 19:19:14 - strategies channel

**liam**

[@UQL0QDEKA](@UQL0QDEKA)  very dependant on what you are doing, for example flumine has a few limitations in terms of doubling matching liquidity and delays etc. Inplay/volatility certainly makes things worse. As long as you factor this in and don’t go mad with stakes I actually find things to be better when live (inplay horse racing) 

---

## 2021-06-12 20:07:39 - strategies channel

**AndyL**

Good point [@UGV299K6H](@UGV299K6H) the criteria for stopping... im not losing money now ive got bflw &amp; flumine setup, at some point ill need to go live with something but im going to set that stop point...

---

## 2021-06-11 15:34:18 - strategies channel

**Scott**

Awaiting my live key [@U9JHLMZB4](@U9JHLMZB4), was going to place a few manual football bets on euros automatically. If that makes sense just so I can get used to what I’m sending and any feedback flumine/bflw gives me. 

---

## 2021-06-11 05:37:27 - general channel

**VT**

Hi, I've been researching how I can get Betfair historical data to backtest, any tips on a good tutorial for beginners in Python? I would like to consult the live football game moneyline markets. I intend to consult the free Basic data, 1 minute intervals, I would like to convert the odds values ​​in each minute to a dataframe.

---

## 2021-06-10 09:51:02 - general channel

**Peter**

There's going to be some degradation, but it's not significant. A few months back [@U4H19D1D2](@U4H19D1D2)  posted information about the number of markets he was streaming on an AWS instance with a single CPU and it blew my mind (and saved me a few bucks at AWS).

---

## 2021-06-09 22:15:29 - strategies channel

**V**

Backtests are important tools, but they should be as realistic as possible. Latency is quite important (depending on what you’re doing) you can quite easily find a great backtest with 0 latency but it’s for good reason… it’s impossible to trade it in real life! Or maybe the strategy is too slow for the message throughout and your queue gets backed up, so you can’t trade on all price updates in practice… the backtest won’t show the latter, production will.

---

## 2021-06-09 07:08:41 - random channel

**Mo**

If you’re starting from scratch then use flumine. You can use it to collect Betfair price data and do backtesting and any strategy you develop using it can be trivially switched from backtest to live

---

## 2021-06-09 07:02:23 - random channel

**Steve Roach**

Thanks for that. So, I’m going to build 2 main systems - one for analysis, the other for managing bets. Is there a preference for bflw or flumine for either of these. Of course, the analysis engine is mainly concerned in collecting and structuring data, largely from betfair but I could imagine other feeds down the track. The bet management system would interact with live markets using strategies invented on the former. Or is this just a personal preference?

---

## 2021-06-08 18:56:39 - general channel

**Peter C**

Is this what I need to be looking at?

```{"asctime": "2021-06-08 17:54:42,433", "levelname": "INFO", "message": "Starting historical market '/recordings/1.183683101'", "market": "/recordings/1.183683101"}

{"asctime": "2021-06-08 17:54:42,435", "levelname": "INFO", "message": "[Register: 1000]: marketSubscription"}

{"asctime": "2021-06-08 17:54:42,438", "levelname": "INFO", "message": "[MarketStream: 1000]: \"MarketStream\" created"}

{"asctime": "2021-06-08 17:54:42,440", "levelname": "INFO", "message": "[MarketStream: 1000]: 1.183683101 added, 1 markets in cache"}

{"asctime": "2021-06-08 17:55:39,023", "levelname": "INFO", "message": "Market 1.183683101 closed", "market_id": "1.183683101", "event_id": "30546561", "event_type_id": "7", "event_name": null, "market_type": "WIN", "market_start_datetime": "2021-05-23 14:15:00", "country_code": "IE", 

"venue": "Curragh", "race_type": "Flat", "orders_cleared": false, "market_cleared": false, "closed": true}

{"asctime": "2021-06-08 17:55:39,028", "levelname": "INFO", "message": "Market cleared", "market_id": "1.183683101", "order_count": 0, "client": {"id": "ac3d1f03", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x000001DC498CB130&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 1, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 14988)&gt;", "&lt;WriterThread(pydevd.Writer, started daemon 14100)&gt;", "&lt;ReaderThread(pydevd.Reader, started daemon 13280)&gt;", "&lt;_TimeoutThread(Thread-4, started daemon 1112)&gt;", "&lt;PyDBCommandThread(pydevd.CommandThread, started daemon 1512)&gt;", "&lt;CheckAliveThread(pydevd.CheckAliveThread, started 11800)&gt;"]}```

---

## 2021-06-08 18:53:02 - general channel

**Peter C**

```{"asctime": "2021-06-08 17:50:21,445", "levelname": "INFO", "message": "Adding strategy PriceRecorder"}

{"asctime": "2021-06-08 17:50:21,449", "levelname": "INFO", "message": "Creating new HistoricalStream (1000) for strategy PriceRecorder", "strategy": "PriceRecorder", "stream_id": 1000, "market_filter": "/recordings/1.183683101", "event_id": "30546561", "event_processing": false}

{"asctime": "2021-06-08 17:50:21,463", "levelname": "INFO", "message": "Starting flumine", "client": {"id": "926cb2e8", "exchange": "Simulated", "betting_client": null, "current_transaction_count_total": 0, "transaction_count_total": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxTransactionCount object at 0x0000015AA2F2A130&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 0, "open_market_count": 0}, "streams": ["&lt;HistoricalStream(HistoricalStream, initial daemon)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 13600)&gt;", "&lt;WriterThread(pydevd.Writer, started daemon 10536)&gt;", "&lt;ReaderThread(pydevd.Reader, started daemon 4432)&gt;", "&lt;_TimeoutThread(Thread-4, started daemon 12796)&gt;", "&lt;PyDBCommandThread(pydevd.CommandThread, started daemon 12004)&gt;", "&lt;CheckAliveThread(pydevd.CheckAliveThread, started 4428)&gt;"]}```

I can't see any mention of it in there - maybe I'm not adding it correctly?

---

## 2021-06-03 10:48:45 - issues channel

**James McKenzie**

I was going to ask about that as well. I have noticed that when I run my flumine code, or a previous betfairlightweight price scraper I have on the HK box I see the RAM usage slowly creep up until the process is killed. I had previously tried to flush the queue every 10 seconds (presuming it was the queue gathering data) and I still saw the RAM usage increase. The same code ran on a UK based AWS instance for my own use didnt have the memory problems.

---

## 2021-06-03 10:38:19 - issues channel

**James McKenzie**

Hi guys, I have a Betfair account that's been set up by a client who is based in Asia. I have been told that all betfair code needs to be ran from a Hong Kong server. I am running into connection problems when I run some flumine based price scraping code from an AWS server in HK. I see a huge ramp up in the latency and then a fairly nasty connection error message:



```{"asctime": "2021-06-02 12:37:54,900", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 0.5828690528869629"}                                                                        

{"asctime": "2021-06-02 12:40:35,503", "levelname": "INFO", "message": "[MarketStream: 2001] 1.184035567 removed, 139 markets in cache"}                                                                    

{"asctime": "2021-06-02 12:45:32,223", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 5.051729917526245"}                                                                         

{"asctime": "2021-06-02 12:46:17,490", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 50.812371015548706"}                                                                        

{"asctime": "2021-06-02 12:46:55,148", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 88.5535352230072"}                                                                          

{"asctime": "2021-06-02 12:47:41,872", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 133.0834095478058"}                                                                         

{"asctime": "2021-06-02 12:48:34,090", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 182.7278220653534"}                                                                         

{"asctime": "2021-06-02 12:50:47,754", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 297.83341121673584"}                                                                        

{"asctime": "2021-06-02 12:56:28,947", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 664.3587987422943"}                                                                         

{"asctime": "2021-06-02 12:57:00,334", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 695.7359499931335"}                                                                         

{"asctime": "2021-06-02 12:58:46,348", "levelname": "ERROR", "message": "get_account_details error", "exc_info": "Traceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/urllib3/connec

tion.py\", line 159, in _new_conn\n    conn = connection.create_connection(\n  File \"/usr/lib/python3/dist-packages/urllib3/util/connection.py\", line 61, in create_connection\n    for res in socket.geta

ddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/usr/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gai

error: [Errno -3] Temporary failure in name resolution\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages

/urllib3/connectionpool.py\", line 665, in urlopen\n    httplib_response = self._make_request(\n  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 376, in _make_request\n    self._v

alidate_conn(conn)\n  File \"/usr/lib/python3/dist-packages/urllib3/connectionpool.py\", line 996, in _validate_conn\n    conn.connect()\n  File \"/usr/lib/python3/dist-packages/urllib3/connection.py\", l

ine 314, in connect\n    conn = self._new_conn()\n  File \"/usr/lib/python3/dist-packages/urllib3/connection.py\", line 171, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionEr

ror: &lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f7c3bde0c10&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution\n\nDuring handling of the above exception,

 another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3/dist-packages/requests/adapters.py\", line 439, in send\n    resp = conn.urlopen(\n  File \"/usr/lib/python3/d

ist-packages/urllib3/connectionpool.py\", line 719, in urlopen\n    retries = retries.increment(\n  File \"/usr/lib/python3/dist-packages/urllib3/util/retry.py\", line 436, in increment\n    raise MaxRetr

yError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Max retries exceeded with url: /exchange/account/json-rpc/v1 (Ca

used by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f7c3bde0c10&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))\n\nDuring handli

ng of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/betfairlightweight/endpoints/baseendpoint.py\", line 

36, in request\n    response = [http://session.post|session.post](http://session.post|session.post)(\n  File \"/usr/lib/python3/dist-packages/requests/api.py\", line 116, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File \"/usr/l

ib/python3/dist-packages/requests/api.py\", line 60, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/usr/lib/python3/dist-packages/requests/sessions.py\", line 533, in 

request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/lib/python3/dist-packages/requests/sessions.py\", line 646, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/lib/python3/di

st-packages/requests/adapters.py\", line 516, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Max retrie

s exceeded with url: /exchange/account/json-rpc/v1 (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f7c3bde0c10&gt;: Failed to establish a new connection: [Errno -3] Te

mporary failure in name resolution'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages

/flumine/clients/betfairclient.py\", line 50, in _get_account_details\n    return self.betting_client.account.get_account_details()\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/betfairlightwe

ight/endpoints/account.py\", line 54, in get_account_details\n    (response, response_json, elapsed_time) = self.request(method, params, session)\n  File \"/home/ubuntu/.local/lib/python3.8/site-packages/

betfairlightweight/endpoints/baseendpoint.py\", line 43, in request\n    raise APIError(None, method, params, e)\nbetfairlightweight.exceptions.APIError: AccountAPING/v1.0/getAccountDetails \nParams: {} \

nException: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Max retries exceeded with url: /exchange/account/json-rpc/v1 (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection o

bject at 0x7f7c3bde0c10&gt;: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution'))", "error": "AccountAPING/v1.0/getAccountDetails \nParams: {} \nException: HTTPSConnection

Pool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Max retries exceeded with url: /exchange/account/json-rpc/v1 (Caused by NewConnectionError('&lt;urllib3.connection.VerifiedHTTPSConnection object at 0x7f7c3bde0c10&gt;: F

ailed to establish a new connection: [Errno -3] Temporary failure in name resolution'))"}             

{"asctime": "2021-06-02 12:58:46,360", "levelname": "WARNING", "message": "[FlumineStream: 2001]: Latency high: 801.6888477802277"}

Killed```

Does anyone have any ideas about the cause of the problem and how I might resolve it. Much appreciated.

---

## 2021-06-02 14:14:18 - general channel

**Mo**

I actually didn't know what Lightsail was, I thought it was a different cloud services provider. I see it's actually an AWS product. My immediate reaction is that the server types might be too restrictive for my use cases but it could well be worth me looking into

---

## 2021-06-02 13:24:43 - general channel

**liam**

you can run an EC2 micro for free for a year as well :slightly_smiling_face:

---

## 2021-06-02 13:19:43 - general channel

**John A**

[@UBS7QANF3](@UBS7QANF3) is AWS located in London or Ireland?

---

## 2021-06-02 12:36:18 - general channel

**Mo**

I use AWS

---

## 2021-05-28 16:15:59 - issues channel

**Phil Anderson**

Hi. Can anyone help me with the MARKET_ON_CLOSE function? I did post before but never got a reply.

I have code set up to place bets, but if I switch persistence type from LAPSE or PERSIST to MARKET_ON_CLOSE, I get FAILURE. I thought it could be because i was using delayed key, but am using live key now and still same.



Code below:



```limit_order = filters.limit_order(

    price=12.5, size=2, persistence_type="MARKET_ON_CLOSE"

)

instruction = filters.place_instruction(

    order_type="LIMIT",

    selection_id=runner.selection_id,

    side="BACK",

    limit_order=limit_order,

)

place_orders = trading.betting.place_orders(

    market_id=j.market_id, instructions=[instruction], # list

)```



---

## 2021-05-28 10:38:40 - general channel

**Beeblebrox**

I have a strategy that in backtesting works slightly better with best price execution turned off.  Is there a way to turn it off when running the strategy live, so that it only applies to that strategy?  I take it there isn't as it's an account-wide setting.  Is the only option to run it in a sub-account with the option turned off?  If so, how do I go about setting one up?

---

## 2021-05-27 19:21:57 - general channel

**liam**

Fairly sure betfair don’t have the ability to do this (unless things have changed) Are you sure it wasn’t a network/server disconnect and then a subscription error on reconnect?

---

## 2021-05-25 12:50:00 - random channel

**Mo**

My point is that bookmakers have a tool they can deploy in this situation which is that they can choose to remove the match and void bets placed on it. As exchange users, we cannot do this even if we strongly believe match fixing is taking place. I think Betfair should be more proactive in the most egregious cases to protect their customers in the same way that bookmakers are protected - and to protect themselves!!

---

## 2021-05-25 12:34:34 - random channel

**liam**

Recent betfair QA, crap answers and crap questions [https://community.betfair.com/service/go/thread/view/94166/31708679/betfair-forum-open-evening-get-your-questions-in-live|https://community.betfair.com/service/go/thread/view/94166/31708679/betfair-forum-open-evening-get-your-questions-in-live](https://community.betfair.com/service/go/thread/view/94166/31708679/betfair-forum-open-evening-get-your-questions-in-live|https://community.betfair.com/service/go/thread/view/94166/31708679/betfair-forum-open-evening-get-your-questions-in-live)

---

## 2021-05-25 11:52:36 - general channel

**John**

Hi [@U4H19D1D2](@U4H19D1D2), looks this AWS/S3 object is quite beyond my level of coding skills/experience, sorry. I wonder if you have an example of saving the market catalogue to a local folder. Thanks again.

---

## 2021-05-24 16:37:59 - random channel

**Mo**

Yes strategies can be doing similar things but I think their fingerprints are a lot more unique than you might think. Your predictions might be similar but maybe your staking is totally different. Maybe you run your strategy on AWS, someone else runs their strategy on Azure. You use bflw, someone else uses C# (this will show up in the HTTP user-agent strings). Maybe you make keepAlive requests for your session tokens at different frequencies. Maybe you scrape settled orders once an hour, someone else does it every 15 minutes etc. etc.

---

## 2021-05-24 06:59:45 - random channel

**liam**

Theoretically as long as my aws bill is paid, no breaking api changes / someone keeps bflw up to date my code would run indefinitely 

---

## 2021-05-23 12:25:49 - general channel

**thambie1**

[@UBS7QANF3](@UBS7QANF3) probably understands this stuff better, but here's how I would figure it out. If subscribing to the order stream, you can get the publish time of any orders you place. Thus you can figure out the matching time on betfair servers. The time between the matching time and when you got the return response, is the second leg. Though for the first leg, it includes both latency, bet delay, plus some part of the 50ms delay between matching cycles (which in theory on average should be 25ms).

---

## 2021-05-23 12:18:59 - general channel

**PeterLe**

Folks quick question please. If you were to calculate the round trip time from server to the betfair api and then the return response to be say 5ms plus the bet delay of say 1000ms ie a combined 1005ms, what would be the first leg to betfair (as it goes via the DDOS provider.  I think i remember [@UBS7QANF3](@UBS7QANF3) saying the first leg is longer than the return but just wondered if anyone had an approx idea please? ie does the first leg make up 60% of the total time etc. Thanks

---

## 2021-05-22 13:25:35 - general channel

**PeterLe**

Just a general question please about running the market recorded in the examples folder.

If I were to run it it on a live sub account (by that I mean an account that is already running a strategy not using Bflw/Flumine). This other program is submitting and matching thousands of bets a day.

the combined amount of markets subscribed to by both my own program and Bflw/Flumine would be less than 200.



Would this be seen in a bad light by Betfair, the fact that Im connecting to a single account by more than one program?

Thanks in advance

---

## 2021-05-21 20:42:54 - issues channel

**Newbie99**

```import time

from datetime import timedelta, timezone, datetime

import logging

import betfairlightweight

import argparse

from betfairlightweight.filters import streaming_market_filter

from pythonjsonlogger import jsonlogger

import racing_functions as rf

import non_streaming_functions as nsf

from flumine import Flumine, clients, BaseStrategy, markets

import account_info as ai

from simple_book import simple_book

from flumine.worker import BackgroundWorker

from marketrecorder import MarketRecorder

from flumine.streams.datastream import DataStream

# from race_card import get_racecard

from price_comparison import get_price_comparison

from terminate import terminate

from cross_market import get_markets

from orders_worker import get_live_orders

from flumine.controls.loggingcontrols import LoggingControl

global snapshot



logger = logging.getLogger()



custom_format = "%(asctime) %(levelname) %(message)"

log_handler = [logging.StreamHandler(), logging.FileHandler(ai.log_folder_path + '/' + datetime.now().strftime("%m-%d-%Y_%H_%M_%S") + '.log')]

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime



for l in log_handler:

    l.setFormatter(formatter)

    logger.addHandler(l)

# logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



rc = rf.open_json(ai.racing_config)



parser = argparse.ArgumentParser()

parser.add_argument("--c", default=None, type=int, help="Conflate messages in ms")

parser.add_argument("--s", "--strategy-list", nargs='+', default=[])

parser.add_argument("--p", default=False, type=bool, help="Paper or real world trading")



args = parser.parse_args()



paper = args.p

# paper = True

if paper == True:

    print('Paper Trading Enabled')

else:

    print('Live Trading Enabled')



market_recorder = False

if market_recorder is True:

    print('Market Recorder Enabled')



strategy_names = args.s

strategy_names = ['politics_test']

conflate = args.c



start_time = datetime.now(timezone.utc)



settings = [setting for setting in rc if setting['on'] == 'True' and setting['strategy_name'] in strategy_names]



trading = betfairlightweight.APIClient(ai.accname, ai.accpass, ai.acckey, certs=ai.path)

# min bet validation temporarily added to stop the min size replace issue

client = clients.BetfairClient(trading, paper_trade=paper) #,  min_bet_validation=False)



framework = Flumine(client=client)



for s in settings:

    country_codes = s['country_codes'] if nsf.true_or_false(s['country_codes']) != False else None

    market_types = s['market_types'] if nsf.true_or_false(s['market_types']) != False else None

    race_types = s['race_types'] if nsf.true_or_false(s['race_types']) != False else None

    venues_to_include = s['venues_to_include'] if nsf.true_or_false(s['venues_to_include']) != False else None

    print({'strategy_name': s['strategy_name'], 'strategy_type': s['strategy_type'], 'event_type_ids': s['event_type_ids'],

           'country_codes': country_codes, 'market_types': market_types, 'race_types': race_types, 'venues': venues_to_include})

    if s['strategy_type'] == 'simple':

        strategy = simple_book(start_time=start_time, settings=s,

            market_filter=streaming_market_filter(

                event_type_ids=s['event_type_ids'],

                country_codes=country_codes,

                market_types=market_types,

                # market_types=['WIN', 'OTHER_PLACE'],

                # country_codes=['AU'],

                # market_types=['WIN','PLACE', 'EACH_WAY'],

                race_types=race_types,

                venues=venues_to_include



            ),

            streaming_timeout=2,

            max_selection_exposure=s['max_selection_exposure'],

            max_order_exposure=s['max_order_exposure'],

            conflate_ms=conflate,

            name=s['strategy_name'],



        )



        framework.add_strategy(strategy)



worker_settings_list = [

    {'function_name': 'get_markets', 'function': get_markets, 'func_kwargs': {"stake": 2, "margin": 0, "exposure_limit": 60}, 'interval': 0.25, 'start_delay': 30},

    {'function_name': 'terminate', 'function': terminate, 'func_kwargs': {"today_only": True, "seconds_closed": 600}, 'interval': 60, 'start_delay': 60},

    {'function_name': 'get_price_comparison', 'function': get_price_comparison, 'func_kwargs': None, 'interval': 60 - rf.randomiser(15), 'start_delay': 30},

    # {'function_name': 'get_live_orders', 'function': get_live_orders, 'func_kwargs': {'snapshot':snapshot}, 'interval': 5, 'start_delay': 30},

    # {'function_name': 'get_racecard', 'function_name': get_racecard,'func_kwargs': {"event_type_ids": 7, 'market_types': 'WIN', 'source': 'Racecard'}, 'interval': 30, 'start_delay': 30}

]



for f in rf.get_worker_functions(settings):

    function_setting = [ws for ws in worker_settings_list if str(ws['function_name']) == f['function_name']]

    if len(function_setting) &gt; 0:

        function_setting = function_setting[0]

        print(f, function_setting)

        framework.add_worker(

            BackgroundWorker(

                framework,

                function_setting['function'],

                func_kwargs=function_setting['func_kwargs'],

                interval=function_setting['interval'],

                start_delay=function_setting['start_delay']

            ))



if market_recorder is True:

    recorder = MarketRecorder(

        name="WIN",

        market_filter=streaming_market_filter(

                event_type_ids=[7],

                country_codes=['GB','IE','US','CA'],

                market_types=['WIN','PLACE'],

            ),

        stream_class=DataStream,

        context={

            "local_dir": ai.unprocessed_data,

            "bucket": "fluminetest",

            "force_update": False,

            "remove_file": True,

        },

    )



    framework.add_strategy(recorder)



framework.run()```



---

## 2021-05-21 18:21:24 - issues channel

**Newbie99**

`{"id": "bf189146-ba58-11eb-91bf-2016d8944b85", "strategy": "politics_test20216", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138409103890878791"], "offset_orders": [], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 1.86, "size": 17.44, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 17.44, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Pending", "status_log": "Pending", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}, "notes": "", "market_notes": null}`

INFO:flumine.order.order:Order status update: Pending

{"asctime": "2021-05-21 17:19:49,088", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.160683973", "selection_id": 25181943, "handicap": 0, "id": "138409103890878791", "customer_order_ref": "e6e7ee3f26f23-138409103890878791", "bet_id": null, "date_time_created": "2021-05-21 17:19:49.087879", "publish_time": "2021-05-21 17:19:47.488000", "trade": {"id": "bf189146-ba58-11eb-b280-2016d8944b85", "strategy": "politics_test10376

---

## 2021-05-21 17:47:35 - issues channel

**Newbie99**

(all those order not found in blotter are of course because there are live orders from the previous strategy name on this market)

---

## 2021-05-21 17:46:51 - issues channel

**Unknown**

Ok so:



strategy name changed

only 1 strategy running

only 1 instance running locally

AWS horse racing instance killed before running

---

## 2021-05-21 17:41:01 - issues channel

**Newbie99**

I have a horse racing strategy on AWS running, that only looks at event_type_id = 7 though, I can kill it to see, but surely with these filters it couldn't possibly interfere could it?



```market_filter=streaming_market_filter(

    event_type_ids=s['7'],

    country_codes=['GB','IE','US'],

    market_types=['WIN','PLACE', 'EACH_WAY'],```



---

## 2021-05-21 10:37:57 - issues channel

**Newbie99**

The whole purpose of this is for long running markets, at some point flumine will have to shut down (rather than running 24/7 for days/weeks/months on end), so this aims to grab existing orders, roll up to calc live exposure and then grab any existing live orders (created previously in a flumine session) and pull them into the current blotter.

---

## 2021-05-21 10:36:30 - issues channel

**Newbie99**

```def populate_existing_positions(trading, market=False, group=True, order_status='ALL', flumine=True):



    ''' current order BFLW object properties '''

    '''

        betId: str,

        averagePriceMatched: float,

        bspLiability: float,

        handicap: float,

        marketId: str,

        orderType: str,

        persistenceType: str,

        placedDate: str,

        selectionId: int,

        side: str,

        sizeCancelled: float,

        sizeLapsed: float,

        sizeMatched: float,

        sizeRemaining: float,

        sizeVoided: float,

        status: str,

        priceSize: dict,

        regulatorCode: str = None,

        matchedDate: str = None,

        customerStrategyRef: str = None,

        customerOrderRef: str = None,

        regulatorAuthCode: str = None,

        lapsedDate: str = None,

        lapseStatusReasonCode: str = None,

        cancelledDate: str = None,

    '''

    '''

    Betfair Order type structure

    

        t = [{'market_id': '1.182015190', 'selection_id': 38378512, 'handicap': 0, 'id': '138378879025142090',

          'customer_order_ref': '0e6bc4bda0c6d-138378879025142090', 'bet_id': '230289893583',

          'date_time_created': '2021-04-16 17:45:02.514251', 'publish_time': '2021-04-16 17:45:02.504000',

          'trade': {'id': '78b5d6de-9edb-11eb-b72a-02f9e1e2dd3a', 'strategy': 'each_way_arb',

                    'place_reset_seconds': 0.0, 'reset_seconds': 0.0,

                    'orders': ['138378879025142090'], 'offset_orders': [],

                    'notes': '', 'market_notes': '4.9,5,4.9', 'status': 'Complete',

                    'status_log': 'Pending, Live, Complete'},

          'order_type': {'order_type': 'Limit', 'price': 4.3, 'size': 2,

                         'persistence_type': 'PERSIST', 'time_in_force': None,

                         'min_fill_size': None, 'bet_target_type': None, 'bet_target_size': None},

          'info': {'side': 'BACK', 'size_matched': 0.0, 'size_remaining': 0.0, 'size_cancelled': 0.0, 'size_lapsed': 2,

                   'size_voided': 0.0, 'average_price_matched': 0.0},

          'responses': {'date_time_placed': '2021-04-16 17:45:02.776941', 'elapsed_seconds_executable': None},

          'runner_status': None, 'status': 'Lapsed',

          'status_log': 'Pending, Lapsed', 'violation_msg': None,

          'simulated': {'profit': 0.0, 'piq': 0.0, 'matched': []}}]

    

    '''



    if flumine is True:

        completed_order_list = get_current_orders_flumine(self=trading, order_status=order_status, market=market)

    else:

        completed_order_list = get_current_orders(trading, order_status)



    order_list = []

    back_amount = 0

    lay_amount = 0

    for current_order in completed_order_list:

        if current_order.size_matched &gt; 0:

            if group is True:

                # order_bet_id = str(current_order.side) + '_' + str(current_order.market_id) + '_' + str(current_order.handicap) + '_' + str(current_order.selection_id)

                order_bet_id = str(current_order.side)[0] + str(current_order.market_id)[2:] + str(current_order.selection_id)

                # print(order_bet_id)

                customer_order_ref = config.hostname

                order_type = "LIMIT"

                persistence_type = "PERSIST"

            else:

                order_bet_id = current_order.bet_id

                customer_order_ref = current_order.customer_order_ref

                order_type = current_order.order_type

                persistence_type = current_order.persistence_type

            if order_bet_id not in [t.bet_id for t in order_list]:

                # print(order_bet_id)

                s = CurrentOrder(

                    order_bet_id,

                    current_order.average_price_matched,

                    0,

                    current_order.handicap,

                    current_order.market_id,

                    order_type,

                    persistence_type,

                    current_order.placed_date,

                    current_order.selection_id,

                    current_order.side,

                    current_order.size_cancelled,

                    current_order.size_lapsed,

                    current_order.size_matched,

                    current_order.size_remaining,

                    current_order.size_voided,

                    current_order.status,

                    {'price': current_order.price_size.price, 'size': current_order.price_size.size},

                    current_order.regulator_code,

                    current_order.matched_date,

                    None,

                    customer_order_ref,

                    current_order.regulator_auth_code,

                    None)

                order_list.append(s)

            else:

                for t in order_list:

                    # seems to keep pulling in same order on repeat, makes no sense

                    if order_bet_id == t.bet_id:

                        t.average_price_matched = ((t.size_matched * t.average_price_matched) - t.size_matched) + \

                                                  ((current_order.average_price_matched * current_order.size_matched) - current_order.size_matched) \

                            if t.average_price_matched is not None and current_order.size_matched &gt; 0 else None

                        t.size_cancelled += current_order.size_cancelled

                        t.size_lapsed += current_order.size_lapsed

                        t.size_matched += current_order.size_matched

                        t.size_remaining += current_order.size_remaining

                        t.size_voided += current_order.size_voided

                        t.matched_date = current_order.matched_date

                        t.average_price_matched = 1 + (t.average_price_matched / t.size_matched) if t.size_matched &gt; 0 and t.average_price_matched is not None else None

                        t.price_size.price = t.average_price_matched # Given the order has been executed, the price is set to the average price matched



    return order_list```



---

## 2021-05-20 17:38:54 - issues channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2) should this scenario be able to occur (as I've replicated it a few times, but didn't think it could):



1. Order placed via Flumine on runner x for market y (market.place_order)

2. Order cancelled manually in Betfair GUI

3. Identical replacement order placed via Flumine  on runner x for market y (market.place_order)

4. Duplicate identical replacement order placed via Flumine on runner x for market y (market.place_order)

Ignoring whether my logic is dodgy or not, surely the in-built Flumine logic shouldn't allow that scenario as it only allows 1 live order per runner per strategy?



Obviously if that is by design I can add some logic to stop it when not desired, but I thought that scenario should not be able to occur due to the 1 order per runner per strategy limit?

---

## 2021-05-19 23:22:26 - general channel

**John**

Hi all, I was using flumine for AU horse racings, and was trying to differentiate thorough bred and pace/trot races (by checking if "pace" or "trot" appears in the race name). When backtesting with the PRO data, you can see race name (e.g. 'Redc (AUS) 1st Apr') from _market_book.market_definition.name_, but when in live/stream API, this name is a None. Guess I am missing something. Please could anyone point me to the right place in flumine to check this race name? Thanks!

---

## 2021-05-17 15:00:14 - issues channel

**liam**

Yeah so its caused by the order stream starting before the market stream has loaded/processed all markets which means you get the warning regarding the missing local market. Easy fix would be to delay the start of the order stream but this would still cause issues because you would then potentially have the same issue during the start delay. I think better fix is for me to figure out this [https://github.com/liampauling/flumine/blob/05182a42c654f505b12f176bcbe7d37edbf099c5/flumine/streams/orderstream.py#L65|todo](https://github.com/liampauling/flumine/blob/05182a42c654f505b12f176bcbe7d37edbf099c5/flumine/streams/orderstream.py#L65|todo) so that it effectively retries to add the live orders

---

## 2021-05-17 14:23:05 - issues channel

**Newbie99**

[@ULDAVFDRP](@ULDAVFDRP) I have made progress here:



```    def process_market_book(self, market, market_book):

        # process marketBook object



        ''' Adds existing market orders (from a previous session) into the current strategy  '''

        for order in self.existing_orders:

            if order.customer_order_ref is not None and order.market_id == market.market_id:

                if order.bet_id not in [o.bet_id for o in market.blotter._live_orders]:

                    print('Adding Bet_id: ', order.bet_id, ' to blotter for market: ', market.market_id)

                    trade = Trade(order.market_id, order.selection_id, order.handicap, self)

                    order_id = order.customer_order_ref[STRATEGY_NAME_HASH_LENGTH + 1:]

                    order = trade.create_order_from_current(order, order_id)

                    order.status = OrderStatus.EXECUTABLE

                    market.blotter.__setitem__(customer_order_ref=order.customer_order_ref, order=order)



        print(market.blotter._live_orders)```

So by trying that approach it adds the order that was created in a previous session into the blotter...however it also then adds a duplicate into the blotter....but importantly does not send a request to try and place this on the exchange (at least from what I can see...it definitely hasn't placed any and I'm not seeing any errors to suggest its trying to).



So it feels like this approach is close, by chance [@U4H19D1D2](@U4H19D1D2) can you see what I might need to tweak here to pick up the previous session order and not create a duplicate in the current live blotter?

---

## 2021-05-17 06:28:02 - general channel

**Greg**

Thanks [@ULDAVFDRP](@ULDAVFDRP) . I have done BA automation for years (bots are running for weeks as we speak) and can see what is delivered in the amount_available_to_back in terms of price and size(when I print the output) but for some reason I can't extract it so instead of price=20 I should have something like price=ex.available_to_back[0].price (in my head) but the ex has an error so I have something wrong somewhere. learning python as one goes is very hard for the 50 yr old brain

---

## 2021-05-16 16:35:02 - general channel

**Peter**

+1 for final integration. I find it useful as the markets arrive as they will for live trading, i.e. in a giant heap rather than neatly ordered in sequence. This can be especially significant when acting on signals from another market as backtesting's event_processing groups related markets nicely, but live data doesn't always exhibit the same helpful structure, especially when you can't rely on event_id as your group index.

---

## 2021-05-16 16:21:57 - issues channel

**Newbie99**

What do you mean by the patch file? I've gone through the flumine Github (well that's just a permanent browser window these days :slightly_smiling_face:  and I have some ideas, but I presume its something along the line of process_current_orders isn't triggered unless it detects a new live order and once this has been run it then processes other orders that it couldn't have known about as they were placed in a prior session. So if I can work out the correct trigger, then potentially I can create a dummy order or something and do it that way (I could presumably create an order that will definitely fail, but that feels too 'hacky')!

---

## 2021-05-16 16:01:41 - issues channel

**Newbie99**

[@ULDAVFDRP](@ULDAVFDRP) yeah that isn't a problem, I know they won't be included, so my plan for already fully matched orders is to call list_current_orders on startup, then I create a single CurrentOrder object (which is just a rolled up total of all historic orders, as there is no roll up for current_orders unlike cleared). That gives me starting exposure, so that part is fine.



The problem is with these live unexecuted orders that do get picked up....but not until an order has been placed.

---

## 2021-05-16 15:31:30 - general channel

**liam**

The same as it uses the same simulation logic, only difference will be the use of live data, can be a good final integration test 

---

## 2021-05-16 15:03:45 - general channel

**birchy**

Can anyone comment on how Flumine paper trading compares to backtesting? I'm just wondering whether to go from backtesting straight to live or run paper trading for a short while first?

---

## 2021-05-15 06:21:56 - general channel

**Greg**

yes thanks Oliver. Probably but I am still fumbling around in the dark and trying simple things first.  :grin:Related : I couldn't figure out how to target a payout using a limit order . I am using something similar to this order_type=LimitOrder(price=2, size=5) Payout would be more useful but tried a variations but has me stumped

---

## 2021-05-15 06:04:15 - general channel

**Greg**

Hi all. ..some more noob questions : what's the best way of ensuring you only have 1 matched bet on a runner? I have tried runner.total_matched == 0 and runner_context.live_trade_count == 0 but neither seem to work i.e.: I get multiple bets on each runner.Thanks

---

## 2021-05-13 09:01:55 - random channel

**Misha**

I have stuffed up one rollout. System for processing cheques and applications for the first Telstra float/IPO (like British Telecom) in 1997 (I think). We (me and one other guy) were developing the software to link all the document imaging, cheque scanning, application entry to process $20B of applications in 4 weeks. Naturally, the client requested changes while we were live. One change that I implemented managed to zero out all application amounts for batches post my rollout. Imagine my surprise/horror when one of the data entry operators showed me a screen that had applications worth $0. And then another. And another. Managed to rollback, find the batches affected, and fix it. Not the most relaxing day of my career :joy:

---

## 2021-05-12 20:54:40 - strategies channel

**birchy**

[@U01PJ5YMFBJ](@U01PJ5YMFBJ) definitely get Flumine up and running with [@U4H19D1D2](@U4H19D1D2)'s marketrecorder example (it pretty much works straight out of the box). That will give you ALL of the market data for each race. Also setup a basic strategy to avoid account suspension for leeching. Again, [@U4H19D1D2](@U4H19D1D2)'s lowestlayer example will suffice, but I'd suggest setting the price trigger to something like 1.80. You won't lose much laying odds on runners.

You can also run backtests in Flumine once you have a month or 3 of recorded data. What I would suggest is to use this ability to parse the data files to gather statistics rather than run full backtests. From there, you may find some useful patterns, upon which you could run a few backtests to prove they work as a strategy. And then.... assuming that you've done all of the above and backtests show that your ideas are sensible, you can port the Flumine strategy to live betting, which will give you "real world" data to analyse.

---

## 2021-05-12 20:13:44 - strategies channel

**ShaunW**

There might only be a finite of set of parameters but each can be viewed in many ways. Eg time, is that the time frame you're active, time between refreshing bets or time to measure price moves or acceleration. And price, is that %implied chance or relative to others, and is that touch price or one of the many fair pricing models (vwap vpoc MA WoM type things). There's more than enough permutations to last a lifetime. And more you're involved the more variations you find but like a lot of complex problems look for the elegant simple solutions.    Don't waste money using trial and error in live, you can be pretty sure before you commit cash.

---

## 2021-05-07 23:14:01 - general channel

**Alex**

Hi everyone, I have a couple of questions and was hoping someone could help me or point me in the right direction. My goal is to simply have an interactive, live representation of the orderbooks for the (live) matches in different sports markets (to begin with, only MATCH ODDS). I am using the bflw library for that as follows: I subscribe to different sports markets (tennis, football, ...) as a whole in my ‚streaming class‘, as there is a limit of how many streams are allowed to be running at a time. This class has an attribute called marketbooks, which is a dictionary (market_ids as keys) that is updated with the most recent streaming updates – therefore, older updates are simply overwritten – in an infinite while loop. I pass this dynamic marketbooks attribute to another class, which is basically dynamically starting processes that take in the marketbooks and filter it for the specific market_id assigned to the thread, thus only processing the orderbook for this specific market_id. In this sub-class I then log the changes in the orderbook in yet another while loop and potentially feed the data into a tkinter gui. While this is no problem with a single market_id and I get live data in my console, I am struggling with a concurrent implementation using asyncio that would start all processes concurrently, so that there is no delay (I tried it with threading first, but ran into issues).

My questions are: Does this implementation make sense? Is it efficient or am I overcomplicating things? How can I easily see – in my setup or in the optimal setup – if a game has ended? How would the asyncio logic look like so that I don’t wait on any of the infinite while loops to finish?

 If this is not appropriate to discuss here, I would appreciate if someone willing to help could send me a private message. Thanks!

---

## 2021-05-07 12:37:17 - general channel

**liam**

its all live order/trade data so constantly updating and my problem is only going to get worse, using Aurora MySQL

---

## 2021-05-05 22:59:58 - strategies channel

**Mo**

I'll give you my personal opinion. I think that the ability to come up with ideas that are likely to lead to profitable strategies is a rare skill. It's hard in the same way that being creative is hard. It's not really something you can force yourself to do. I don't think I personally possess this skill, hence why I believe networking is so important. Taking a ground up approach of building a model with the idea that it will give you a better price than the market is very very tough, whether that is the latest machine learning model or some technical indicator borrowed from finance. You should be thinking about why does the market think the price is X and what are the situations where it may have gotten it wrong. For example, favourite-longshot bias, early markets, obscure sports/leagues, punters wanting to bet on goals, not wanting to bet on draws, etc. etc.  Work out what parameters are implied by the market eg goal scoring rates. When you think the market is correct then plug these into models. Find situations where there are discrepancies (=statistical arbitrage). Think about things in terms of a system you're trying to exploit. How can you probe it? How does the Betfair matcher work? How can you be first in the queue? How does crossmatching work? What are the rules for the markets you're betting on? First understand then attack

---

## 2021-05-05 10:40:30 - general channel

**Jono**

network latency specifically is the problem here, the database is hosted on a cloud server which regardless of how im keeping track of the bets will always be where i have to send updates to. I dont need the strategy to use any info from this db endpoint i just need to send it updates. the resp time is holding things up. If threading is what sounds best for this issue i will definitely look into it.

---

## 2021-05-02 23:49:24 - random channel

**PeterLe**

Over recent months there has been a number of people who have their accounts suspended due to them extracting data and not putting enough bets through.

My question please is; What constitutes ‘enough’

If you had a single account and the choice of two systems to deploy whilst collecting data, the systems would be as follows:-



Sys A, places say 500 bets an hour £2 bets (liability) ie £1000

Sys B places 50 bets an hour, £20 (liability) ie £1000



Are Betfair interested in the amount of bets or the value of the combined bets?

Anyone have a view?



Thanks in advance

---

## 2021-04-30 23:38:57 - strategies channel

**AndyL**

i've had a lot of advice, but to be honest it doesn't always help for whatever reason... maybe advice just out of context or different to what I think I want to do.. At the moment i've collected 2months of data on UK horses, and back fitting strategies, unfortunately "fitting" is probably the theme! I've sorted of ended up with a trigger when some technical event happens, eg.selection far from vwap, then have added loads of extra conditions, like the style of race "Hcap"(theory should revert more), front favs are competitive, relative volume of selection is great than others,... you get the drift. Then "hedge" at the off. So basically going round this sort of circle, attempt after attempt, find something that fits, then live test forward, and repeat... Sadly, the current bot strategy I think is half decent, I tweaked one parameter by 1% and it then became an instant losing strategy.... hm not happy sign

---

## 2021-04-30 16:06:03 - general channel

**Jorge**

Hey guys! Can anyone recommend any data provider for European Soccer? I'm looking for historical point in time live data of Scores

---

## 2021-04-30 08:51:12 - general channel

**Mo**

There is no difference between streaming and copying the file. Either way you are transferring it, it’s just a question of whether you are saving a local copy. But there is no transfer cost between S3 and EC2

---

## 2021-04-30 08:45:05 - general channel

**Taking Value**

Yea I have noticed these charges, I thought that I would get the same fees for shuttling files between S3 and EC2 though and wondered if streaming was cheaper to shuttling the file over.



 It also seems most people use S3 as backup storage but have a lot of data stored locally. I had assumed that as everybody uses S3 for storage they would be using EC2 for modelling but actually it seems most model locally so I will do the same.

---

## 2021-04-30 08:37:33 - general channel

**Taking Value**

Cheers, I have two years worth of horse racing pro data. I was going to build models with one and backtest on the other. I had assumed everyone was just doing backtesting via their EC2 environment. Perhaps I will just download the data locally and see if my laptop can cope with modelling it.

---

## 2021-04-29 17:09:26 - general channel

**MacrcoPolo**

Not sure if there's an equivalent for AWS but I use GCS Fuse on google and keep data in a bucket, produce outputs locally then post back to the bucket when I'm done

---

## 2021-04-29 09:16:19 - general channel

**liam**

It will be quicker from EC2 vs local, I do all my backtesting locally with a local repo/copy of my s3 data, i.e. it attempts to use the file locally if not it downloads. This suits my use case where I tend to use the same files/months over and over

---

## 2021-04-29 09:14:21 - general channel

**Mo**

You can stream from S3 but unless you're streaming to an EC2 instance you should watch out for fees for transferring data out of AWS

---

## 2021-04-29 09:11:35 - general channel

**Taking Value**

If I want to run Flumine over a large amount of data (say a couple of 100 gigs) stored in an S3 bucket is there a more efficient way to do it that copying the files to an EC2 environment? Can they be streamed from S3. Or is this process just a very time consuming process.

---

## 2021-04-28 17:51:30 - general channel

**Newbie99**

If I want to access all orders coming through the order stream (i.e. that includes those not placed via Flumine so they won't necessarily form part of a strategy), where should I be looking?



I have setup a background worker and then was heading down this path:



```def get_live_orders(context: dict, flumine) -&gt; None:



    for stream in flumine.streams._streams:

        # Find the order stream and do whatever```

But is there a preferred way to go about this (i.e. something more obvious I've overlooked)?

---

## 2021-04-26 17:42:49 - random channel

**Newbie99**

Possibly a bit of a longshot, but is anyone else running Nord VPN (locally) on Windows 10?



I'm running Nord VPN on my laptop and wanted to write a script to start an EC2 instance using boto3 etc, that all works fine. However when I try to create an SSH tunnel it seems to be blocked (which is logical), but I can't work out how to bypass this.



In the Nord VPN for windows client I have SSH setup under split tunnelling, so it works fine if I just try to SSH manually.



Additionally if I turn off the Nord VPN client this script works fine.



Here is my script, is anyone aware of a smart way to get something like this working without having to manually switch off the Nord VPN client (the fetch public IP function grabs the public IP of the EC instance)?



```def ssh_tunnel():



    EC2_URL = fetch_public_ip()

    username = "ec2-user"

    pem_file = '~/betfair.pem'



    # Create the tunnel

    server = SSHTunnelForwarder(

        (EC2_URL, 22),

        ssh_username=username,

        ssh_pkey=pem_file,

        remote_bind_address=(EC2_URL, 27017),

        local_bind_address=('127.0.0.1', 27017)

    )

    # Starts the tunnel

    server.start()



    # Prints the local bind port

    print(server.local_bind_port)  

    

    # Closes the tunnel

    server.stop()```

---

## 2021-04-22 13:18:19 - general channel

**D C**

@Dave given the average Joe uses Betfair live streaming that can be significantly delayed it could be a help. Problem is with the accuracy though if you ask me. Sometimes the GPS is not accurate at all. Probably need to be using video AND this to try and manually trade

---

## 2021-04-21 08:56:35 - issues channel

**Peter C**

Hi All, I have an issue when backtesting using marketOnClose orders. The order is placed, but stays 'executable' - it is never matched at SP. This happens regularly in backtesting, but not always. I never have this problem in live markets - the bet at SP is always placed/matched successfully. Is this a known problem with backtesting these order types, or am I doing something wrong? I have replicated the problem in a simple file if necessary. Cheers

---

## 2021-04-20 15:32:35 - issues channel

**Robert**

anyone know an easy way to check betfair who won and placed for a race? I am assuming this is not possible for historical data without using their historical API. How about with the live API

---

## 2021-04-19 08:39:51 - general channel

**Taking Value**

Is the pro data from BF not the same calibre as the live data recorded from the stream?



What are the difference? I had assumed pro would be of the same level of granularity.

---

## 2021-04-18 07:11:07 - general channel

**Peter**

Back when I was displaying visualisations on web pages, I built an app API to deliver the data I needed in json, which was easy to  feed into one of the javascript charting frameworks. I think it was ChartJS, though there were other options and I'm sure they've moved on since then. I can't talk to web applications in R. Although I did learn some R a while back, BFLW and Flumine persuaded me to focus on Python. Not sure what your concerns with displaying data on a desktop would be, but accepting them, if you're the only person looking at it, you might want to look at Google colab, which is Jupyter Notebooks in the cloud and for free. You can import data and logs from your cloud server for analysis.

---

## 2021-04-16 16:45:33 - issues channel

**Jono**

are there any steps in the certs production steps laid out in:



[https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Certificate+Generation+With+XCA)



which are outdated? I keep recieving an error that im fairly confident relates to the private key being incorrect specifically this error: "`HTTPSConnectionPool(host='[http://identitysso-cert.betfair.com|identitysso-cert.betfair.com](http://identitysso-cert.betfair.com|identitysso-cert.betfair.com)', port=443): Max retries exceeded with url: /api/certlogin (Caused by SSLError(SSLError(9, '[SSL] PEM lib (_ssl.c:3991)')))`"



the ploading of the relevant crt file to bf was successful and the trading client is able to find the corresponding .pem file but unfortunately the above problem is stopping me from getting further. Is there a preferred way of setting up certs that i should be following rather than this or what is listed in the bf docs? Alternatively is there a way to run flumine with an interactive login so i can forego the certs?

---

## 2021-04-14 11:43:14 - general channel

**liam**

I use aws eu-west-1, quick enough for me :wink:

---

## 2021-04-14 11:39:08 - general channel

**RicHep**

Liam, do you have your server set up near Betfairs to get that latency or is that just from the UK?

---

## 2021-04-14 11:11:53 - general channel

**liam**

My median round trip is around 130-140ms depending on which way the wind is blowing, exchange cycles are 50ms, so in reality it should be around 80ms ish however:



a) You can change this by overriding in execution

b) Backtesting is never going to be 100% so this latency is trying to be worst case and if you are finding big differences in matching/profit based on this latency I recommend you stop and go live

---

## 2021-04-14 01:32:57 - general channel

**KG**

in case you guys are interested, in Aus next week we're running a workshop using bflw to back test [https://www.betfair.com.au/hub/horse-racing-tips/|ratings from the Hub](https://www.betfair.com.au/hub/horse-racing-tips/|ratings from the Hub) against historic pricing data - might be up your alley. you're welcome to join live or I can post the YouTube link afterwards if you're keen :ok_hand: [https://www.eventbrite.com.au/e/betfairs-education-workshop-back-testing-hub-ratings-with-tom-bishop-tickets-148531815659](https://www.eventbrite.com.au/e/betfairs-education-workshop-back-testing-hub-ratings-with-tom-bishop-tickets-148531815659)

---

## 2021-04-13 13:36:25 - general channel

**Jono**

hi there im trying to compile a short list of all the retrievable details of a game from the api and the docs of github have helped clarify pretty much everything i was wondering. Opta seem to do all live updates like you said but i was curious if the team lists are available through the betfairlightweight package?

---

## 2021-04-12 15:46:16 - issues channel

**Newbie99**

When trying to place an order using:



```trade = Trade(

           market_id=market.market_id,

           selection_id=selection_id,

           handicap=handicap,

           strategy=self,

       )

   order = trade.create_order(

       side=direction, order_type=LimitOrder(price=price, size=stake, persistence_type=persistence_type)

   )



order_placed = market.place_order(order)```

I am trying to catch errors as follows:



```if order_placed is False:

	# Do stuff```

I just noticed an order (correctly) failing due to BPE, so this is expected behaviour and from logs I can see the following:



```{"asctime": "2021-04-12 13:39:15,049", "levelname": "INFO", "message": "Order Place: FAILURE", "bet_id": null, "order_id": "138375275548404227", "status": "FAILURE", "error_code": "BET_LAPSED_PRICE_IMPROVEMENT_TOO_LARGE"}```

However, from place_order in execution.transaction thought the above would return True or False and I could catch the error that way and do whatever I need, but it doesn’t appear to be catching for failures in live (I could have sworn this worked in paper trading, but haven’t been able to replicate yet), it does return True for successfully placed orders, but doesn’t return False.



 So, what is the correct way to catch these errors, as I’m clearly mis-reading the code?

---

## 2021-04-12 11:57:00 - strategies channel

**Michael**

Yeah - there's a sliver of value just in the fact that you're offering and willing to wait to match. If you're not sure how that works imagine doing it constantly on both sides of the book. The problem is adverse selection, which is where your bad bets match and your good ones don't. If you've got a model that's good enough to reduce that just a little bit and then offer into the market that's enough to get you into the black. That's why so many people get off to what looks like a flying start but then stall when they scale. They imagine it's the amazing performance of their model that's generating their profits at small stakes but they're really just partial market makers. When they scale beyond the capacity of those forces they get a reality check. You can watch this play out live on these forums.

---

## 2021-04-12 11:08:04 - random channel

**Aaron Smith**

[@U01TWECSFLH](@U01TWECSFLH)  probably plenty of diffrent ways to get there. My qualifications match yours pretty well and i just played around with liams library and some ideas i had and i m at least making enough to live off of it by now. Havent read any book yet, however i m sure that can help aswell. I ll probs end up checking some books out eventually, but for starters i felt like it may limit my horizon to the stuff discussed in the books. Not sure if that makes any sense, but i can always catch up on reading books :smile:

---

## 2021-04-12 10:43:19 - general channel

**Taking Value**

Cheers Liam. If I pull the latest version of the relevant py file from the repository is there a way to direct it to add to the races files in the same sub directory on my ec2 environment rather than have it setup a new sub directory for the new stream?



My thought process is that this way it will simply add to the race files that have already been created and contain data rather than create a new sub-directory for the new stream and record the latest data to new files that I then have to merge with the old ones.

---

## 2021-04-08 13:18:05 - random channel

**Unknown**

That's certainly something that I've found. This is a plot of cumulative profit v time to get filled for a live strategy I'm testing, that's bet on ~500 markets so far.

---

## 2021-04-07 12:03:18 - general channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2), I was just trying to play around with that new linked events feature, but am being a bit dense, how do I format for live streaming?



Lets say this is the streaming filter, its not a dict, so where does the event processing key go?



```market_filter=streaming_market_filter(

                event_type_ids=[7'],

                country_codes=['GB'],

                market_types=['WIN','PLACE', 'EACH_WAY']

            )```



---

## 2021-04-07 08:02:02 - general channel

**liam**

flumine [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1180-2021-04-07|v1.18.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#1180-2021-04-07|v1.18.0) now released, a few depreciations as warned in v1.17, cleared order now available (thanks [@U01MPC0GUK1](@U01MPC0GUK1)), a pretty major backtesting bug and now the ability to process event data as per live, big thanks to [@U01B8031PM1](@U01B8031PM1) on sponsoring this development and allowing it to be open source :smile:



Small change required to allow event processing:

```strategy = ExampleStrategy(

    market_filter={"markets": [..], "event_processing": True},

)```

The `Market` object has a helper method for accessing other linked event markets (live and simulated):

```place_market = market.event["PLACE"]```

This allows you for example to backtest win/place in racing or all football markets as per live, this also only seems to add around 5% in processing time. I don't think there is anything else out there on the market which allows backtesting like this :sunglasses:

---

## 2021-04-06 22:15:32 - issues channel

**Jonjonjon**

Should [https://github.com/liampauling/flumine/blob/62b40ad11988ecab039b3b822bc70779aed64bca/flumine/strategy/strategy.py#L203-L206|this code](https://github.com/liampauling/flumine/blob/62b40ad11988ecab039b3b822bc70779aed64bca/flumine/strategy/strategy.py#L203-L206|this code):



        `if runner_context.trade_count &gt;= self.max_trade_count:`

            `order.violation_msg = "strategy.validate_order failed: trade_count ({0}) &gt;= max_trade_count ({1})".format(`

                `runner_context.trade_count, self.max_live_trade_count`

            `)`



Become



        `if runner_context.trade_count &gt;= self.max_trade_count:`

            `order.violation_msg = "strategy.validate_order failed: trade_count ({0}) &gt;= max_trade_count ({1})".format(`

                `runner_context.trade_count, self._*max_trade_count*_`

            `)`

---

## 2021-04-04 21:14:08 - random channel

**MacrcoPolo**

"Under some applications WSL was delivering slightly better performance over WSL2, but in general the WSL/WSL2 performance was very close to that of bare metal Ubuntu 20.04."

---

## 2021-04-04 11:55:07 - strategies channel

**Oliver Varney**

I may be missing something but everything you posted today suggest you dont understand what data mining is. This statement has serious flaws:  "If the match is at 6m and the score is 1-0, 18.84% of the games have ended 2-1. If this represents odds of 5.31 and the BF odds for 2-1 are 7.2 for example, is this value is what I am trying to work out. I do not have the BF odds to see how they compare to what I have tried to calculate ie 5.31."

---

## 2021-04-01 18:45:50 - general channel

**PeterLe**

By the way, I noticed a post by Oliver earlier about cutting back on some strategies as the exchange was a bit flaky...obviously depends on the strategy but some of my biggest wins have been on days such as these (safety in mind takes precedence of course)

---

## 2021-03-31 11:09:36 - general channel

**charlie_v**

Right who has a hammer spare, where are the betfair servers, I'm going to hit them with a very large hammer to see if that helps and type a few kill -9 instructions .

---

## 2021-03-30 09:27:16 - general channel

**Mo**

If you do it from AWS they can complain to AWS and have your instance terminated

---

## 2021-03-29 12:57:03 - general channel

**Rob (NZ)**

Previously I got the markets using the code on betfair aussie examples but that was awhile ago so just not sure if there is better approaches now... I've spent the last year working on my model and now need to feed it live info rather than just my back testing which I've been doing manually

---

## 2021-03-28 18:31:05 - random channel

**Mo**

If what you care about is receiving the streaming data as quickly as possible you probably want to be in Dublin AWS

---

## 2021-03-26 14:31:59 - general channel

**Misha**

I use C# and the Microsoft.ACE.OLEDB provider to copy the whole Access database into an SQL Server database each time for data manipulation (takes a couple of minutes)

---

## 2021-03-26 10:54:07 - general channel

**Misha**

[@U0160E9HS2G](@U0160E9HS2G) - OnCourt is a historical database, not a live one

---

## 2021-03-26 00:20:50 - general channel

**Misha**

The odds and points are stored as text files on his server. The data is way too big for an Access database (and too complicated for maintenance)

---

## 2021-03-25 22:34:16 - general channel

**Misha**

I have everything fully automated but I probably spent a couple of months of development to get there (to have it run pretty much perfectly). I copy the full Access database into an SQL Server database, run a bunch of stored procedures on that database, and then run a lot of code performing various steps (processing tournaments, players, matches, seedings, etc) that updates my main database

---

## 2021-03-25 20:20:18 - general channel

**Beeblebrox**

I've got a back testing logging control that records orders when `_process_cleared_orders_meta()` is called (using the example in the repository) and that all works fine.  I'm now trying to write a logging control to record orders from a live strategy, but I'm not sure how to replicate what I've done for the back testing control because  `event.event` in `_process_cleared_orders_meta()` is a list of `Order` objects, but for `_process_cleared_orders()` it's a `ClearedOrders` object.



I want to be able to write `order.trade.market_notes` and `order.trade.nodes_str` for each order, which is easy for the back test control, but for the live control how do I access them from the `ClearedOrder`?



I thought instead I could write the orders when `_process_closed_market()` is called and then loop over the orders in the market blotter, but I also want to write the profit for the order and that doesn't exist as a property for real orders (you can get it when back testing from `order.simulated.profit`).



There must be a way to do it, but I'm probably missing something obvious.  Any clues as to how I can do it? Thanks

---

## 2021-03-24 18:22:40 - general channel

**vorace**

hey guys, just want to confirm i didn't miss anything obvious regarding the order stream..



from the docs, with order stream subscribed, betfair will send order change message which contains 1+ order changes, but from what i can tell, there is no timestamp associated to the order changes itself.



so the question now is "how does one keep track of the match time(s) of one's orders?"



- happy path is obviously when the subscription doesn't go down (be it client or server issue) and you could use the publish time of the order change message itself to best represent the time of individual order changes..

- but in the not so happy path, if the subscription is disrupted because of client or server issues, and you then resubscribe with a clk/initialClk combo, the publish time of the order change message is no longer a good approximation of the time of the individual order changes

---

## 2021-03-23 19:02:52 - strategies channel

**birchy**

[@ULDAVFDRP](@ULDAVFDRP) Yeah, I've always used train/test data but this was for a new sport. Was a bit premature going live TBH, but then again I keep reading that live betting is far more valuable than backtesting...

[@UGV299K6H](@UGV299K6H) sometimes shit turns to gold. I'm not overly bothered about losing a few hundred quid and see that as a necessary expense in the search for the golden eggs. I lost ~£200 on the above strategy, so not a major issue. I find the problem with asking for too much value often ends up with only the "wrong" bets getting matched more frequently, so seems counter intuitive. There is always the option to hedge bets, but again that's counter intuitive. Surely _some_ loss is to be expected when testing strategies?

---

## 2021-03-23 17:48:40 - strategies channel

**birchy**

With my strategy that I inadvertently over-fitted, I'd basically been using the same training data over and over (because I had no other data at the time), using various triggers and ideas until I ended up with a simulated P&amp;L that was satisfactory. I've never actually looked at the training data manually but unintentionally found the sets of triggers and values that hit the sweet spot _for that dataset._ Meanwhile, I was gathering more data for use as a testing sample and that was when I realised what I'd done. An expensive lesson was learnt. :grinning:

The other problem was that this was straight laying only and I'd calculated that a maximum drawdown of &gt; £1000 was to be expected, so even at -£700, I left it playing live. It went up, it went down, but the overall trend was slowly down. Luckily, I have some other strategies that earn small amounts at lower risks, but truth be told, I managed to undo 6 months of profit in a matter of weeks.

---

## 2021-03-22 20:46:13 - strategies channel

**birchy**

Just to forewarn you, I had a very similar result from Flumine backtesting which also went in the opposite direction when live. Mine was for ~1700 markets and ~9000 matched bets out of ~17k bets placed in backtest and double those quantities for live bets. Monte Carlo suggested that the backtest P&amp;Ls were &lt; 0.1% chance of being random...i.e. this was a strong edge rather than a lucky streak.

Have since run the backtest on some new data and had the same result as live betting.

Conclusion: strategy was inadvertently over-fitted.

---

## 2021-03-22 19:06:34 - strategies channel

**mandelbot**

I'll work on doing a Monte Carlo, never done one before. I do have a few profitable live strategies though I have to say my approach is pretty basic compared to many on here. I've only just moved to working with python and bflw in the last few months having come from automating strategies on betangel. Still learning the ropes as it were.



You're right 3k does seem a bit thin, unfortunately that's the extent of my historical database for now. I am also betting on bigger odds but &lt;100 so probably that makes 3k seem even thinner. I figured though since I was beating BSP then I might be doing something right.

---

## 2021-03-22 18:24:32 - strategies channel

**Michael**

Just lick of a finger but I'd say that's not that many for quite a thin edge, but it also depends on your development methodology. Doing a MC simulation won't do any harm but you should be aware that analysing multiple bets in the same market with MC is problematic. Where are you at with your betting more generally - do you have strategies betting live and making money?

---

## 2021-03-20 17:28:02 - general channel

**Aaron Smith**

One more question: My respone from betfair that my order has been played immediatly shows up as an EventType.CURRENT_ORDERS i guess? so i can just process my live order in process_current_orders [@UBS7QANF3](@UBS7QANF3) ?

---

## 2021-03-19 21:56:19 - strategies channel

**Dave**

Should be fairly straightforward to eliminate the latency warnings, it's not something I'd put up with tbh! Is your internet connection a bottleneck maybe? Perhaps try unning it on AWS and see if you get the same warnings.

---

## 2021-03-19 20:05:37 - random channel

**Paul**

As a general point: the AWS business model is built around shared resources and contention of those resources. If your application is particularly sensitive to contention, perhaps it’s not the optimal environment for that application.

---

## 2021-03-19 19:57:53 - random channel

**Paul**

You can’t remove them, but some AWS customers detect them and ditch instances when they think they have them

---

## 2021-03-19 19:56:32 - random channel

**Paul**

This is a good tool for exploration if you’re completely new to it all: [https://aws.amazon.com/ec2/instance-explorer/|https://aws.amazon.com/ec2/instance-explorer/](https://aws.amazon.com/ec2/instance-explorer/|https://aws.amazon.com/ec2/instance-explorer/)

---

## 2021-03-19 19:51:20 - random channel

**Paul**

Disclosure: I work for Amazon and therefore it can be argued I have a vested interest - literally - in everyone spending more money on AWS, even if I’m in Retail and nothing to do with AWS. I still would give this advice even if I didn’t though, I am not shilling.

---

## 2021-03-19 18:50:24 - random channel

**Peter**

A while back Liam posted how much processing he was able to push through a tiny AWS instance and some of us, i.e. me, were stunned. Streaming + BFLW/Flumine is unbelievably efficient.

---

## 2021-03-19 16:37:32 - random channel

**Peter C**

Does anyone here run their flumine stuff on digitalocean? I'm using them simply because it's what I'm used to - thinking about resizing my droplet/moving to aws and I'd be curious to know what size droplet anyone else who uses digitalocean gets by on.

---

## 2021-03-18 09:09:02 - random channel

**Oliver Varney**

but the only way I can see this working maybe im missing something is to have a seperate tracked folder with all the scripts basically replicating the current database objects, apply changes to those files, then run into to sql server but that seems very messy to me and I can see it being very easy for the files to go out of sync with the database objects

---

## 2021-03-17 08:59:14 - strategies channel

**Alessio**

Aren't they getting investigated heavily with a risk of shutting down soon? (e.g.: [https://siliconangle.com/2021/02/23/ifinex-agrees-pay-18-5m-settle-new-york-tether-lawsuit/](https://siliconangle.com/2021/02/23/ifinex-agrees-pay-18-5m-settle-new-york-tether-lawsuit/))

---

## 2021-03-16 12:32:04 - strategies channel

**birchy**

The AWS thing was new to me when I started, but S3 is definitely worth setting up. I run a modified marketrecorder that saves the data into it's corresponding month based on projected start date/time. It just makes it easier to pull out specific months for backtesting, i.e. summer flat racing is &gt; winter racing for one of my strategies.

---

## 2021-03-16 05:42:25 - issues channel

**IndikaE**

Of course. Thanks! Now I feel quite embarrassed for even having asked such a question. I might not have been getting enough sleep lately, working so hard on getting my strategy to production :flushed: 

---

## 2021-03-14 08:39:16 - random channel

**IndikaE**

This is valuable information to me. I will consider starting scraping the IPS. I really only need a rough estimate of second half time stamp, so I think I will will rely on a combination of the proposed heuristics- including the bet delay. Already doing point 1. , btw! My strategy does not need very exact timing - for backtesting I just need to know that I am not using future events to predict the past (of course I don’t want preds to be wildly dated - but when backtesting I prefer to err on the side of caution ). And thanks for pointing me in the direction of IPS - I might try it for better syncing in production. You guys are great :pray:This was one of the last unsolved problems blocking me from deploying my first strategy - now I am pretty excited!

---

## 2021-03-14 08:13:24 - random channel

**Misha**

As a veteran of scraping live data, it's always a good idea to come in on the URL's used by a manual user

---

## 2021-03-13 17:06:49 - strategies channel

**birchy**

That's exactly how I've been operating. It's always a bit of a twitchy-bumhole moment when you go live with a strategy and it turns out that you started on a down trend. I'll man it out and see how it goes. :joy:

---

## 2021-03-13 12:54:23 - strategies channel

**James T**

Taking prices with 3% margin against BSP is decent. Especially pre-off. It will be limited by how scalable it is. If it’s trading an hour before off time, then your backtests might be over optimistic because of market impact.  



You said the returns are bumpy. Is that because you are betting outright rather than closing? Why don’t you close at BSP (or at the off) for now and verify this trigger work in reality? Treat your open bets an hour before the race separately (give them their own strategy ref) to your closing bets, so that it’s easier to analyse them afterwards. 



If you’ve got 3% margin against BSP you’ll still make a healthy profit by closing (if the trigger pans out in live betting). 

---

## 2021-03-13 12:07:51 - strategies channel

**birchy**

To add to the above, I've gone live with the strategy after extensive backtesting and am seeing much greater volatility than backtesting suggested.

---

## 2021-03-11 12:13:33 - random channel

**Beeblebrox**

I'd second that. I was using my own library/bot for running strategies, but bflw/flumine make developing/testing/deploying strats way simpler and quicker, so thanks [@U4H19D1D2](@U4H19D1D2)

---

## 2021-03-11 11:59:50 - random channel

**liam**

Rapid, how have you been able to develop/deploy so quick? (ignoring flumine)

---

## 2021-03-11 09:28:56 - general channel

**Steve Roach**

Can anyone point me to some documentation that will give me a quick-and-dirty start to getting a live feed. Over time, I will be going through the documentation and refining my knowledge, but I'd like a preview if that's possible.

---

## 2021-03-09 19:29:23 - strategies channel

**IndikaE**

Yes - I will take your advice. I have probably been overthinking it a bit. I am really only seeking confirmation that my edge is there. I have no interest in calculating VAR or anything. But my back tests do not look random at all(either I have a pretty decent edge, or I am unwittingly leaking information from train to validation). So I guess I should just use one of the ‘naive’ approaches and get on with it. Looking forward to being able to use the “check £ in account”-approach as soon as I get this strategy deployed(first time for me).

---

## 2021-03-09 15:00:51 - random channel

**Newbie99**

That makes sense, however the reason I wanted to go down this approach is to then try to work out after x games of the series, what is the probability then?



So I thought trying to understand this might help me get my way around the next step, as say we're 3 games in then, the probability will have changed (potentially dramatically) depending on the first 3 results.



In essence, my thought process was as follows:



1. Get starting price (modelled or market doesn't matter for now as its hypothetical)

2. Derive probability of winning each game from starting price (assumes scoreline is 0-0 at the start). I was going to attempt o reverse engineer this somehow once I understood the concept a bit better.

3. Once probability per game is known, then attempt to create live probability based on games won so far / remaining.

Is that approach logical (I appreciate its overly simplistic, its more conceptual than anything else at the mo)?

---

## 2021-03-08 15:37:02 - random channel

**Newbie99**

Sorry, was sidetracked.



Yes, thats why I was ignoring cricket for now, I want to determine the probability of A to win, bearing in mind that:



The series ends once A reaches x out of n (e.g. 4/7)

There can be no draws

It is know prior to the start the probability each team has of winning an individual game



So what I'm struggling to get my head round is why is it not important to discount the unplayed games and factor in that A has to win the final game (or B when in reverse)?

---

## 2021-03-08 14:25:33 - random channel

**Aaron Smith**

i dont know exactly how it works in cricket, but draws should be no problem really

---

## 2021-03-08 14:24:35 - random channel

**Newbie99**

I am making the assumption for say cricket where a series carries on regardless (and draws have to be factored in) the approach would require more work, so was ignoring that for now

---

## 2021-03-08 14:03:13 - random channel

**Newbie99**

This is possibly more of a maths question, than a Python one and hopefully quite a simple one, but my brain appears to be malfunctioning.



All I am trying to do, is create a function that returns the win probability for a and b (be they runners or teams) for a series n length (for now not allowing draws) on the assumption the series ends after one a or b reaches the target number of games (e.g. 3 out of 5, 4 out of 7 etc) and therefore  when calculating for a, a always has to win the final game (and vice versa).



This is what I've come up with, however what's bugging me is that the summed probabilities keeps increasing the higher number of games in the series, so I'm trying to get my head round whether I've made a coding mistake or if there is a mathematical reason for this that I've missed?



```from math import ceil, pow, comb



def calc_win_probability(n, p_a, p_b=False):



    if p_b is False:

        p_b = 1-p_a



    g = ceil(n/2)

    prob_list = []

    for i in range(g,n+1):

        a = (comb(i,g-1) * pow(p_a,g-1) * pow(p_b,(i - (g-1)))) * p_a

        b = (comb(i,g-1) * pow(p_b,g-1) * pow(p_a,(i - (g-1)))) * p_b

        prob_list.append({'games': i, 'prob_a_win': a, 'prob_b_win': b, 'combined_prob': a+b})



    return {'overall_prob': sum([p['combined_prob'] for p in prob_list]) ,'prob_list': prob_list}



t = calc_win_probability(7,2/3,1/3)



for x in t['prob_list']:

    print(x,'\n','----------------------------------------------------------')

print({'total_prob': t['overall_prob']})```

---

## 2021-03-07 10:26:28 - strategies channel

**Dave**

QQ to racing specialists: Have you noticed any impact to your existing strategy performance when deploying another strategy in a seperate market on the same event? I'm deploying some stuff on the place markets and worried I will impact price action on win markets. Hopefully starting off with minimal stakes should show negligible impact...

---

## 2021-03-07 05:07:35 - strategies channel

**Misha**

-) I don't use live prices in my model, only the "starting" price of the match. Obviously I compare with live prices at bet execution time to know whether to bet

---

## 2021-03-06 19:05:38 - issues channel

**liam**

Ideally no, tricky bug as we want the blotter function to be called if there are orders I.e real strategy live on that market 

---

## 2021-03-06 18:51:53 - issues channel

**Alessio**

```strategy = sure_strategy.TakeTheSureStrategy(

      name = 'Take The Sure',

      market_filter=uo25_filter,

      max_live_trade_count=4,

      max_trade_count=4,

      max_order_exposure=40.0,

      max_selection_exposure=40.0

)

framework.add_strategy(strategy)```

---

## 2021-03-05 09:48:27 - issues channel

**birchy**

3.8.8 on dev machine

3.6.9 on production machine

First time I've actually checked that.

---

## 2021-03-04 15:41:52 - issues channel

**birchy**

[@U4H19D1D2](@U4H19D1D2), yeah, have just seen that in the source code. So I have no idea why I'm getting double-bets. Only happens in live strategy. Backtest doesn't do it and it's the same copy/pasted strategy. :thinking_face:

---

## 2021-03-03 09:34:37 - strategies channel

**birchy**

Always remember that the market efficiency increases as more money is traded (aka "wisdom of the crowd"), so in _theory,_ a £500K market will be more efficient than a £100K market. The probabilities for each runner will conclude at either 100% (winner) or 0% (loser). From market opening to market closure, the prices are constantly moving, which by it's very nature indicates that there is uncertainty and inefficiency in the market and _that_ is what you need to exploit. Breaking it down into micro markets triggered by price moves, money flow, etc helps. Think about _why_ the price moved and _how_ the market reports it. You don't have to know that the horse was playing up or the trainer made some comment on TV, all you need to know is if it has a positive or negative effect on the prices and volumes.



You _can_ try to predict the winners or losers using fundamental or statistical analysis but it's a highly competitive approach and would likely require some extremely complicated modelling to beat the market makers.



Simple (and obvious) mathematical approaches DO make a profit but can be difficult to execute optimally, e.g. lay the field at &gt; 100% book, lay 4 runners at &lt; 4.0, back the drifters, lay the steamers, etc. Don't be afraid to oppose what everyone else seems to doing. The horses don't know how much money is being traded and I can tell you with 100% certainty that "insider" knowledge doesn't make a horse win. I live in an agricultural area and know a handful of people that work in the racing industry. If I had £1 for every time I've been told "our horse will win today" (and it didn't), I'd be a lot richer than if I was backing them. Hell, I regularly see these horses get backed from 14/1 into 5/1 and they still don't win, so even the "insiders" get it wrong more often than they get it right.

---

## 2021-03-03 09:29:12 - general channel

**captainonionhead**

Hmm, thanks, that fits with what I was expecting but I must be doing something wrong as I'm only ever getting one Trade of orders placed (because I check runner_context.live_trade_count in process_market_book() as per the example but it doesn't reset back to zero when the orders have matched).

More debugging to do - thanks for your help!

---

## 2021-03-03 08:40:08 - general channel

**captainonionhead**

Morning, newbie question I'm afraid and apologies if this is covered somewhere in the docs, I couldn't find anything specific to this...  I'm trying to implement a simple scalping algorithm (as an exercise to get going with flumine) and am confused by the life-cycle of a Trade and the Orders within it, partly because there seems to be some conflation of the terms "trade" and "order" even though these are different objects within the framework.

I had assumed that a "live" trade was one that had unmatched orders, but (at least during backtest) it appears to be live until the market is closed?

Am I expected to manually call `complete_trade()` once the orders within it have been either matched or cancelled?

---

## 2021-03-02 19:59:58 - general channel

**klozovin**

historical data has market and selection names (in the "mcm" message) and the live streaming data does not? right? there's no way to get the names from the stream as well...

---

## 2021-03-02 17:23:28 - strategies channel

**birchy**

[@U01PQ9SR9MJ](@U01PQ9SR9MJ) It can be quite difficult to understand what "value" is and there is no binary answer to the question. The basic idea of making a profit is to place bets at prices greater than the true probability of an event happening. That could be horse winning a race or a football team losing, etc. The fact is that we never _truly_ know if the bet we are placing is good value or not and all we can do is make our best approximation. If our approximation is correct often enough, we make a profit. Sometimes we get it right, sometimes we get it wrong, but as long as we're right more often than the market, we make a profit. Unlike tossing coins, rolling dice or spinning roulette wheels, sports betting does NOT have probabilities that can be calculated exactly, so at best, we're using educated guesses. Whether intentional or not, strategies like "lay the favourite", "back the outsider", etc will only show a profit IF they are finding value.



Regarding backtesting, as others have mentioned, you are trying to predict _future_ patterns, so building a strategy using _all_ of your data only tells you what happened in the past. The problem is overfitting, where, through multiple filter adjustments and fine tuning, you end up picking all the sweet spots from events that have already happened. It's no different to shopping...if the item you want to buy is £10 today but was £8 yesterday, you will have to pay £10 for it because yesterday's price has now gone. Your strategy may tell you to buy at £8, but that's no longer available, so it's not a true value. You _can_ build a strategy using _all_ of your data but then you would have to place real bets and wait a long time to see if it's actually profitable. OR you can split the data into a "training" set and a "validation" set, the validation set representing the "future" data. It's critical to ensure that while developing the strategy that you _never_ use the validation data as it's for...you guessed it...validation only.



After many years at this game (I was botting [http://Flutter.com|Flutter.com](http://Flutter.com|Flutter.com) via browser emulation before Betfair bought them out in ~2002) I had also become very stuck in my thought process. I have a handful of bots that have earnt small amounts for many years but never enough to actually live off. Joining this Slack group was the best thing I've done in the last 20 years. The quality of knowledge collected here is second to none and the members are more than happy to answer stupid questions (and I've asked a lot of them!). I've recently had an epiphany after reading [https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723|The Logic Of Sports Betting](https://www.amazon.co.uk/Logic-Sports-Betting-Ed-Miller/dp/1096805723|The Logic Of Sports Betting) and listening to the [https://player.fm/series/business-of-betting-podcast|Business Of Betting](https://player.fm/series/business-of-betting-podcast|Business Of Betting) podcasts, both of which were recommended on this group. Don't be put off by negative comments, they're actually a very truthful lot on here and, believe it or not ARE trying to help.

---

## 2021-03-02 12:57:38 - issues channel

**Newbie99**

yeah, that test script is non-sensical, its just to demo this issue :slightly_smiling_face:



I now get this error:



`{"asctime": "2021-03-02 12:56:07,189", "levelname": "WARNING", "message": "validate_order failed: live_trade_count (1) &gt;= max_live_trade_count (1)"}`

---

## 2021-03-02 11:15:17 - strategies channel

**Unknown**

I do something similar to goatiwish. It might be based on technical analysis, but it's definitely got legs. Though it definitely fits into the "Dregs" category of strategies. Below is the live equity curve. The recent bumpiness is because I started trading a second parameterisation of the strategy.

---

## 2021-03-02 10:48:35 - issues channel

**liam**

ah i think i see the problem, the replace is causing a += 1 to the number of live trades when in reality it is still just 1

---

## 2021-03-01 21:15:00 - strategies channel

**Oliver Varney**

I can see many faws focusing on ranking of prices rather than the actual prices but maybe your onto something

---

## 2021-03-01 20:46:05 - general channel

**Lee**

Hi,

`market.context` is fine to use, I use it too



I think you can put a streaming timeout on the strategy when instantiating to make it trigger? Not sure what the default is.



I’d do both, `listener_kwargs` just filters out to speed up backtesting but you’ll need to do it in `check_market_book` for live trading / testing logic

---

## 2021-03-01 19:01:33 - strategies channel

**birchy**

Indeed, I have found some more dregs the last few days and am more than happy to take them. Regarding strategy tuning, let's say you have a fairly simple strategy that uses a handful of `if` statements to check volume traded, last traded, current price, etc and then places bets at N ticks offset. You started off with a rough idea of what these numbers should be, ran a backtest and then went live with it. The strategy shows a decent profit on a reasonable sample size but you don't know if the trigger or execution setpoints are optimal. Given that each of your triggers has a range of possible values that equates to &gt;1000 combinations, how would you approach finding the optimal ranges?

---

## 2021-03-01 17:56:34 - issues channel

**Newbie99**

Also using 1.17.5, once I get a violation for an order below min size then this warning appears and can't place a subsequent order, is this intended behaviour (I would have assumed once you try to place a correctly sized order, the message should go and the order should flow upstream)?



This is just paper trading (in case that makes a difference).



Correctly failed order:

```{"asctime": "2021-03-01 16:18:18,238", "levelname": "WARNING", "message": "Order has violated: ORDER_VALIDATION Error: Order size is less than min bet size (2) or payout (10) for currency", "control": "ORDER_VALIDATION", "error": "Order size is less than min bet size (2) or payout (10) for currency", "order": {"market_id": "1.179865560", "selection_id": 38527720, "handicap": 0, "id": "138339079234466185", "customer_order_ref": "869aeb021cad4-138339079234466185", "bet_id": "100000000162", "date_time_created": "2021-03-01 16:12:03.446618", "publish_time": "2021-03-01 16:12:03.626000", "trade": {"id": "933fb664-7aa8-11eb-a42d-2016d8944b85", "strategy": "simple_back_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138339078008452709", "138339079234466185"], "notes": "", "market_notes": "190,330,340", "status": "Live", "status_log": "Pending, Live, Pending, Live"}, "order_type": {"order_type": "Limit", "price": 180.0, "size": 0.05, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.05, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": "2021-03-01 16:12:03.447618", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Pending, Executable, Violation", "violation_msg": "Order has violated: ORDER_VALIDATION Error: Order size is less than min bet size (2) or payout (10) for currency", "simulated": {"profit": 0.0, "piq": 3.65, "matched": []}}}```

Correctly sized order that fails to place, due to previous violation:

```{"asctime": "2021-03-01 16:25:39,365", "levelname": "WARNING", "message": "Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failure", "control": "STRATEGY_EXPOSURE", "error": "strategy.validate_order failure", "order": {"market_id": "1.179865560", "selection_id": 38527720, "handicap": 0, "id": "138339087393647136", "customer_order_ref": "869aeb021cad4-138339087393647136", "bet_id": null, "date_time_created": "2021-03-01 16:25:39.364713", "publish_time": null, "trade": {"id": "c2a66a1f-7aaa-11eb-a8f0-2016d8944b85", "strategy": "simple_back_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138339087393647136"], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 290.0, "size": 0.06, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.06, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": "Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failure", "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}}}```

---

## 2021-03-01 17:06:36 - issues channel

**Peter C**

I have 'if runner_context.live_trade_count &lt;= 4:' because I have max_live_trade_count = 4 when I add my strategy to the framework - is this wrong?

---

## 2021-03-01 16:50:11 - issues channel

**Peter C**

Today I've updated to the latest version of flumine from 1.16.3. I've added a runner_context.live_trade_count

---

## 2021-03-01 15:07:53 - issues channel

**birchy**

Yeah...My hardware is not even 50% of that. :joy:

Just as an FYI, I've not tried my "million markets" strategy since this update. I implemented it in my own polling library as the strategy plods along steadily on a 10 second poll rate. Only took just over an hour from project to production, so a no brainer really. I think streaming is better suited to the faster aim-&gt;fire type strategies such as inplay geegees (which I had never got involved in previously).

---

## 2021-03-01 14:42:38 - issues channel

**Unknown**

Yeah she really does fly now, a lot of the improvement came from stripping out as much as possible in the listener/stream code and then preventing duplicate resource creation by caching if there hasn't been an update but all of this was in bflw so makes live streaming quicker as well

---

## 2021-03-01 10:51:05 - issues channel

**IndikaE**

Hi. First post here - been lurking for a while. I absolutely love flumine, and I am preparing to deploy my first strategy. I just noticed, that after upgrading to flumine 1.17.4 that I started to hit the transaction limit(5000) when backtesting on around 1000 markets. My strategy usually places only 1-10 orders per market. I initially thought that I had a problem with the monkey patching of datetime, so that the control would use actual system time(this would explain the limit kicking in). However, after debugging a bit, now I don’t believe that to be the case(patched time seems to be ok). My markets are passed and processed in random chronological order, could this be causing trouble in the max transaction limit control? Also the first error occurs with a transaction_count of 6152, not 5001, as I would have expected. As I saw that some changes to this control were made with the latest version, I thought it might be an issue, but I absolutely cannot rule out that I am doing something wrong here. 

---

## 2021-03-01 09:50:32 - random channel

**Dave**

Oddly when I breakdown by API, it groups my CloudWatch costs as MetricStorage:AWS/S3. So perhaps CW is storing results of the monitoring in S3 indefinitely or something

---

## 2021-02-26 19:22:25 - issues channel

**birchy**

In a Flumine strategy, what are the (dis)advantages of `process_orders()` vs `market.blotter.live_orders` ? I presume the actual orders are updated at the same time and both of these contain the _same_ orders?

---

## 2021-02-26 17:15:05 - issues channel

**Newbie99**

```{"asctime": "2021-02-26 15:42:09,137", "levelname": "INFO", "message": "Order status update: Execution complete", "market_id": "1.179756075", "selection_id": 28040651, "handicap": 0, "id": "138336469255121051", "customer_order_ref": "6012fe3b915f7-138336469255121051", "bet_id": "100000000018", "date_time_created": "2021-02-26 15:42:05.512104", "publish_time": "2021-02-26 15:42:05.763000", "trade": {"id": "2d6ee89a-7849-11eb-841c-2016d8944b85", "strategy": "simple_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138336469255121051"], "notes": "", "market_notes": "11,11.5,11.5", "status": "Pending", "status_log": "Pending, Live, Pending"}, "order_type": {"order_type": "Limit", "price": 10.5, "size": 1.57, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 1.57, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": "2021-02-26 15:42:05.716102", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Execution complete", "status_log": "Pending, Executable, Replacing, Execution complete", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 139.7, "matched": []}}

INFO:flumine.execution.baseexecution:Order Cancel: SUCCESS

{"asctime": "2021-02-26 15:42:09,138", "levelname": "INFO", "message": "Order Cancel: SUCCESS", "bet_id": "100000000018", "order_id": "138336469255121051", "status": "SUCCESS", "error_code": null}

INFO:flumine.execution.baseexecution:Order Replace: SUCCESS

{"asctime": "2021-02-26 15:42:09,139", "levelname": "INFO", "message": "Order Replace: SUCCESS", "bet_id": null, "order_id": "138336469291386029", "status": "SUCCESS", "error_code": null}

INFO:flumine.order.order:Order status update: Violation

{"asctime": "2021-02-26 15:42:09,140", "levelname": "INFO", "message": "Order status update: Violation", "market_id": "1.179756075", "selection_id": 28040651, "handicap": 0, "id": "138336469291386029", "customer_order_ref": "6012fe3b915f7-138336469291386029", "bet_id": "100000000057", "date_time_created": "2021-02-26 15:42:09.138602", "publish_time": null, "trade": {"id": "2d6ee89a-7849-11eb-841c-2016d8944b85", "strategy": "simple_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138336469255121051", "138336469291386029"], "notes": "", "market_notes": "11,11.5,11.5", "status": "Pending", "status_log": "Pending, Live, Pending"}, "order_type": {"order_type": "Limit", "price": 11.0, "size": 1.57, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 1.57, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": "2021-02-26 15:42:09.139609", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 254.41, "matched": []}}

WARNING:flumine.controls:Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failure

{"asctime": "2021-02-26 15:42:09,141", "levelname": "WARNING", "message": "Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failure", "control": "STRATEGY_EXPOSURE", "error": "strategy.validate_order failure", "order": {"market_id": "1.179756075", "selection_id": 28040651, "handicap": 0, "id": "138336469291386029", "customer_order_ref": "6012fe3b915f7-138336469291386029", "bet_id": "100000000057", "date_time_created": "2021-02-26 15:42:09.138602", "publish_time": null, "trade": {"id": "2d6ee89a-7849-11eb-841c-2016d8944b85", "strategy": "simple_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138336469255121051", "138336469291386029"], "notes": "", "market_notes": "11,11.5,11.5", "status": "Pending", "status_log": "Pending, Live, Pending"}, "order_type": {"order_type": "Limit", "price": 11.0, "size": 1.57, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 1.57, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": "2021-02-26 15:42:09.139609", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": "Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failure", "simulated": {"profit": 0.0, "piq": 254.41, "matched": []}}} ```

---

## 2021-02-26 10:32:03 - issues channel

**liam**

its because of the change in 1.17.0, you see the logs states `strategy.validate_order failure` this means this [https://github.com/liampauling/flumine/blob/806b5e1bd3f76886818da08dfa1d8db1a78e62fd/flumine/strategy/strategy.py#L179|function](https://github.com/liampauling/flumine/blob/806b5e1bd3f76886818da08dfa1d8db1a78e62fd/flumine/strategy/strategy.py#L179|function) is returning false which is likely because you already have a live trade. I am basically trying to force the user to check the `RunnerContext` first, see [https://github.com/liampauling/flumine/blob/806b5e1bd3f76886818da08dfa1d8db1a78e62fd/examples/strategies/lowestlayer.py#L37|here](https://github.com/liampauling/flumine/blob/806b5e1bd3f76886818da08dfa1d8db1a78e62fd/examples/strategies/lowestlayer.py#L37|here) for example

---

## 2021-02-26 10:29:39 - issues channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2) in flumine 1.17.2 I saw there's a mention of a bug fix around potential exposure, I'm seeing a lot of these:



```INFO:flumine.order.order:Order status update: Violation

{"asctime": "2021-02-26 10:17:21,505", "levelname": "INFO", "message": "Order status update: Violation", "market_id": "1.179742967", "selection_id": 28253114, "handicap": 0, "id": "138336274415049218", "customer_order_ref": "6012fe3b915f7-138336274415049218", "bet_id": null, "date_time_created": "2021-02-26 10:17:21.504921", "publish_time": null, "trade": {"id": "d00f5a01-781b-11eb-8ae9-2016d8944b85", "strategy": "simple_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138336274415049218"], "notes": "", "market_notes": null, "status": "Live", "status_log": ""}, "order_type": {"order_type": "Limit", "price": 16.0, "size": 1.0, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 1.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": null, "elapsed_seconds_executable": null}, "runner_status": null, "status": "Violation", "status_log": "Violation", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}}

WARNING:flumine.controls:Order has violated: STRATEGY_EXPOSURE Error: strategy.validate_order failure```

When by my calcs I shouldn't be anywhere near a violation. I'm placing orders using market.place(order) as opposed to strategy.place and this is paper trading if it makes a difference.



Is it possible a bug remains, or perhaps more likely is there another change I need to make (I read through the change logs but I appear to have missed something)?

---

## 2021-02-25 17:03:06 - general channel

**William Martin**

I am looking to get the live data of outright markets for different football competitions, premier league, champions league, Euros etc.



Its not for a trading bot but for a prototype of a new interface I designed for a gambling product

---

## 2021-02-25 10:43:35 - strategies channel

**liam**

[@UUCD6P13J](@UUCD6P13J) bringing your question back up have you looked at testing market capacity / roi / profit by pure back testing? I have done a few tests where I bump stakes up to silly sizes to see how the profit varies and it can make some interesting graphs that do match up with live betting more than you would think 

---

## 2021-02-24 15:10:44 - issues channel

**Aaron Smith**

Hey guys, i m still struggeling with my Latency issue, i have little clue how to "debug" this. I thought i d just put out all my current warning logs, maybe someone can make any sense of it, i d be grateful :smile: first: the latency issue seemingly isnt the result of to much processing per market update, even when i m not processing anything, i still eventually get the same problem. The framework is running on an ec2 instance, which i thought should be rather stable. Its a linux instance, so OS time should be in sync with an NTP server.

Problem: _"[MarketStream: 2001]: Latency high: 1.0108046531677246"_

Latency can go up to 6s.



That said, my suspicous logs:

• sometimes i get _"Market &lt;market_id&gt; not present when closing"_, i dont know why it happens, timewise it doesnt seem to correlate with the _Latency high_ warning.

• And now the probably bigger problem:

```{"asctime": "2021-02-23 21:19:59,560", "levelname": "ERROR", "message": "[MarketStream: 4001]: SUBSCRIPTION_LIMIT_EXCEEDED: trying to subscribe to 12930 markets whereas max allowed number was: 200"}

{"asctime": "2021-02-23 21:20:10,572", "levelname": "ERROR", "message": "MarketStream run error", "exc_info": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py\", line 216, in _receive_all\n    part = self._socket.recv(self.buffer_size)\n  File \"/usr/lib64/python3.7/ssl.py\", line 1056, in recv\n    return self.read(buflen)\n  File \"/usr/lib64/python3.7/ssl.py\", line 931, in read\n    return self._sslobj.read(len)\nsocket.timeout: The read operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/site-packages/flumine/streams/marketstream.py\", line 32, in run\n    self._stream.start()\n  File \"/usr/local/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py\", line 59, in start\n    self._read_loop()\n  File \"/usr/local/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py\", line 198, in _read_loop\n    received_data_raw = self._receive_all()\n  File \"/usr/local/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py\", line 220, in _receive_all\n    raise SocketError(\"[Connect: %s]: Socket %s\" % (self._unique_id, e))\nbetfairlightweight.exceptions.SocketError: [Connect: 4002]: Socket The read operation timed out"}

{"asctime": "2021-02-23 21:20:12,575", "levelname": "WARNING", "message": "[Listener: 4002]: stream already registered, replacing data"}```

These 3 fellas are getting logged right behind each other. The first few hours everything seems fine, until eventually these show up. I m not sure why it tries to subscribe to 12k (its always roughly 12k) markets, thats at least not what i m telling it to do. The fact it doesnt give this error the first few hours makes me feel like something bad is stacking up in the background (allthough if this was to log as soon as i hit &gt;200 markets, the jump from 200 to 12k seems kinda big). I know its not you guys job to solve my probs, but maybe some smart being immediatly has an idea of whats up - i d surely appreciate it :slightly_smiling_face:

---

## 2021-02-24 13:28:19 - general channel

**river_shah**

instance_id is just my internal model reference id so I can do live A/ B tests. this remains persistent through time

---

## 2021-02-24 09:51:52 - general channel

**river_shah**

How do I get the output from the accountstatememt all transactions using bflw or flumine please? Sorry struggling to figure out how to do this. Basically the output that lives here in the web gui: [https://myaccount.betfair.com/summary/accountstatement](https://myaccount.betfair.com/summary/accountstatement)

---

## 2021-02-21 10:53:50 - issues channel

**birchy**

Just to add to the latency issue, it's important to ensure that your OS time is synchronised with an NTP server. Linux does this by default. Windows tries, but it's a bit fluffy.

---

## 2021-02-20 16:33:06 - issues channel

**Newbie99**

I definitely get random high latency warnings from an ec2 instance. Not to often, but some days it does seem to randomly happen

---

## 2021-02-20 16:18:31 - issues channel

**Aaron Smith**

flumine just threw a bunch of high latency warnings. I m curious what caused this high latency. I suppose one way to get high latency is by to much calculations in your strat so it cant keep up? Highest cpu-utilization was 15% though. Is it possible for an ec2 instance to just "randomly" have a bad connection sometimes?

---

## 2021-02-19 19:25:26 - general channel

**Oliver Varney**

[@U01N8P7UFPB](@U01N8P7UFPB) flumine is a great place to start I think (thanks to [@U4H19D1D2](@U4H19D1D2) ). it provides both live trading and backtesting functionality and creating strategies is easy. Id play around with it look at the code and ask questions in here. As far as I know theres a mix of sports in here but mostly horses, tennis and football (soccer)

---

## 2021-02-19 08:04:13 - general channel

**captainonionhead**

Morning!  I guess I'm coming to the betfair game a little late but better late than never :slightly_smiling_face:  I'm having trouble keeping up with markets using the delayed key though and have a slightly noob question.

I've seen it suggested that I can get my key for the live feed activated as long as I'll be placing a "reasonable" number of bets?  What sort of magnitude is considered "reasonable"?  Until I have more experience, I don't want to be risking huge sums and I have no innate sense of what people consider to be large or small bets and volumes?

Many thanks!  - Jon

---

## 2021-02-18 11:42:13 - random channel

**Aaron Smith**

my last OS update made a big chaos aswell. My VS is no longer showing syntax-mistakes live (have to safe for it to show any errors) and even then, it seems like it misses 90% of errors. Also no more auto-complete. My terminal lost all its colours and no longer tells me which git branch i m in. I read some stack-overflow, but couldnt find the root cause and basically decided to postpone fixing that (has been a month now :smile: ) and now i m just very careful while writing code :smile:

---

## 2021-02-18 11:22:05 - issues channel

**qwerty.nat**

Just confirming, does bflw hit the api keep alive for us or we need to hit it ourselves once every 24 hours to keep our session token valid?

---

## 2021-02-18 10:45:48 - general channel

**Alessio**

no, everybody has their own, and that's why you need to make sure your live data and historical data are from the same provider :stuck_out_tongue:

---

## 2021-02-18 10:36:48 - general channel

**Alessio**

[https://insidersport.com/2021/02/09/genius-sports-launches-lawsuit-against-sportradar/|I think they had a dispute with Sportradar even ... they cannot even ](https://insidersport.com/2021/02/09/genius-sports-launches-lawsuit-against-sportradar/|I think they had a dispute with Sportradar even ... they cannot even )

---

## 2021-02-18 10:31:29 - general channel

**Alessio**

Opta, Sportradar and Statsbomb are the only 3 i know of that do live data delivery (wyscout and instat don't)

---

## 2021-02-17 10:55:02 - general channel

**Robert**

For anyone interested in why the historical data for runner total_matched is not correct whereas the live API is, here is my response from betfair:

```The total cumulative traded volume per runner (at all prices) isn't included in the PRO data set, unlike in the live Stream API.



For a single price point, 'tv' is cumulative, so each "trd" update contains the price and cumulative traded volume traded so far at that price.  The final sum of the 'tv' at each price point for each selection will be equal to the market traded volume.



"op":"mcm","clk":"9702272675","pt":1563905629308,"mc":[{"id":"1.160659053","rc":[{"trd":[[4.1,11.11]],"ltp":4.1,"tv":11.11,"id":9938916},{"atb":[[4.1,9.93]],"id":9938916}],"con":true,"img":false}]}

{"op":"mcm","clk":"9702305064","pt":1563905813250,"mc":[{"id":"1.160659053","rc":[{"trd":[[4.1,31.31]],"ltp":4.1,"tv":31.31,"id":9938916},{"atb":[[4.1,0]],"id":9938916},{"atl":[[4.1,4.9]],"id":9938916}],"con":true,"img":false}]}

{"op":"mcm","clk":"9703398965","pt":1563917405197,"mc":[{"id":"1.160659053","rc":[{"trd":[[4.1,37.64]],"ltp":4.1,"tv":37.64,"id":9938916},{"atl":[[4.1,1.74]],"id":9938916},{"spn":6.6,"spf":1.0,"id":11110173}],"con":true,"img":false}]}

The runner tv ("tv") represents the cumulative traded amount at the last price traded (ltp) or cumulative amount of all 'trd' amounts in the update if multiple prices are included.```

---

## 2021-02-15 08:43:34 - issues channel

**birchy**

Oh. I've got individual certs for each account and handle them via an AWS style credentials file.

---

## 2021-02-10 15:00:36 - general channel

**thambie1**

I'm thinking of hiring people on a more ongoing basis  where you really have to be careful of how you classify them (freelancer vs employee) or you risk breaking laws/regulations. I think hiring a freelancer under an umbrella company would be fine, but I want to get advice to make sure.

---

## 2021-02-09 16:53:15 - issues channel

**ricky**

As i mentioned, i have two instance of fluime running, one only monitor, recording and send me signal notification, another one is check and place bet.

I only start live bet via fluinme last two weeks, i am still tesing my strategies, in particular i need solve my latency issue, i feel more confidence when i can see bet actual place in betfair website.

---

## 2021-02-09 10:16:33 - issues channel

**ricky**

First flumine instance only monitor signal and send me email notification plus recording live data.

Secod flumine instance has nothing to do with first fluime instance, it calcuate the back/lay signal, and trigger to bet.

---

## 2021-02-08 23:44:42 - issues channel

**ricky**

WARNING:flumine.baseflumine:High latency between current time and MarketBook publish time

WARNING:betfairlightweight.streaming.stream:[MarketStream: 2001]: Latency high: 1.961205244064331

WARNING:betfairlightweight.streaming.stream:[MarketStream: 2001]: Latency high: 1.808332920074463

WARNING:betfairlightweight.streaming.stream:[MarketStream: 2001]: Latency high: 1.6995792388916016

WARNING:betfairlightweight.streaming.stream:[MarketStream: 2001]: Latency high: 1.0950758457183838

WARNING:betfairlightweight.streaming.stream:[MarketStream: 2001]: Latency high: 0.7914028167724609

WARNING:betfairlightweight.streaming.stream:[MarketStream: 2001]: Latency high: 0.5924580097198486

WARNING:betfairlightweight.streaming.stream:[MarketStream: 2001]: Latency high: 0.507713794708252



bflw: 2.12.0

flumine: 1.16.0



I run flumine via ubtuntu in an virtual machine. When i place live bet, i somethime receive "Latency high" warning. (less than 300 markets running)



1) Do you think my virtual machine can cost network delay? ( My latptop 4 cores, 8GB RAMs, 120Mbps boardband)

2) "Latency high: 1.961205244064331" means 1.96s delay? should i set both conflate_ms = 2000, and streaming_timeout = 2 to reduce warning?

3) I was running inplay strategies, betfair might push more udpate from one market to other, is there a way i can meaure my code execution time so that i can improve my code?

---

## 2021-02-08 14:28:19 - general channel

**Aaron Smith**

so for 2 diffrent instances on the same ec2 instance wouldnt work (without mass warnings) without changing flumine-code i guess?

---

## 2021-02-06 21:07:19 - strategies channel

**Jonjonjon**

The y scale is pounds sterling, without commission taken off. x axis is either the date (at the top), or 1 tick per market (bottom). It's my first live bot with Flumine.

---

## 2021-02-05 15:03:29 - strategies channel

**liam**

My production stuff is closely coupled to my order recording api but the logging control base class / example is pretty much all there you just need to decide what to record

---

## 2021-02-04 23:50:14 - random channel

**V**

Interesting read for those into this kind of stuff: [https://www.eurex.com/resource/blob/48918/74e70365580415639ea4628ec25a6b72/data/presentation_insights-into-trading-system-dynamics_en.pdf|https://www.eurex.com/resource/blob/48918/74e70365580415639ea4628ec25a6b72/data/presentation_insights-into-trading-system-dynamics_en.pdf](https://www.eurex.com/resource/blob/48918/74e70365580415639ea4628ec25a6b72/data/presentation_insights-into-trading-system-dynamics_en.pdf|https://www.eurex.com/resource/blob/48918/74e70365580415639ea4628ec25a6b72/data/presentation_insights-into-trading-system-dynamics_en.pdf)

---

## 2021-02-04 13:20:59 - issues channel

**mandelbot**

but was also occurring on a windows server ec2

---

## 2021-02-04 13:20:47 - issues channel

**mandelbot**

using them on an ubuntu ec2

---

## 2021-02-03 20:40:15 - issues channel

**Peter C**

I'm trying to access live data for the first time - using the quickstart example from the docs. I import the example strategy, and start(self) runs, but then check_market_book doesn't appear to be called, but the program continues to run with no error. Has anyone experienced this before?

---

## 2021-02-03 17:47:02 - issues channel

**Newbie99**

for reference, this is the message from a cancel I requested (as opposed to BPE):



```INFO:flumine.order.order:Order status update: Execution complete

{"asctime": "2021-02-03 17:44:49,317", "levelname": "INFO", "message": "Order status update: Execution complete", "market_id": "1.178739219", "selection_id": 20867982, "handicap": 0, "id": "138316670786111761", "customer_order_ref": "81ec14d3e0056-138316670786111761", "bet_id": "223440411072", "trade": {"id": "7cb88910-6647-11eb-a5d2-2016d8944b85", "strategy": "betting_market_back", "status": "TradeStatus.LIVE", "orders": ["138316670786111761"], "notes": "", "market_notes": "13.5,15,15.5"}, "order_type": {"order_type": "Limit", "price": 15.0, "size": 1.08, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 1.08, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Execution complete", "status_log": "Pending, Executable, Cancelling, Executable, Execution complete"}

INFO:flumine.order.trade:Trade status update: Complete```

---

## 2021-02-03 12:27:49 - strategies channel

**Mo**

I think that's an important distinction and it should be much easier to say whether this shows you have a systematic edge than try to use 7 year old back test data to justify rolling your strategy out into production

---

## 2021-02-01 09:37:23 - random channel

**liam**

i use it due to the scaling, because the load on my API varies a lot! I would either have to have an oversized EC2 or mess around with ASG instead I just let the gateway and lambda handle it

---

## 2021-01-29 14:31:54 - general channel

**birchy**

Can anyone confirm if [mailto:info@betfair.com|info@betfair.com](mailto:info@betfair.com|info@betfair.com) is still valid? Sent them an email but didn't get the usual automated response. This was for a follow-up to a CS question via their live chat/*mess*aging facility 5 days ago, on which they have not acted yet (I requested a lifetime P&amp;L report). Did speak to Neil a few days ago but it's not his department, although he did "escalate" it, but still no response or confirmation...

---

## 2021-01-28 10:21:17 - general channel

**liam**

I have been pushing them for more clarification but they only give [https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Application+Keys#ApplicationKeys-Delay&amp;LiveApplicationKeysOverviewPBA|this](https://docs.developer.betfair.com/display/1smk3cen4v3lu3yomq5qye0ni/Application+Keys#ApplicationKeys-Delay&amp;LiveApplicationKeysOverviewPBA|this), you should only by using your live key when placing orders, the problem is the delayed key is useless..

---

## 2021-01-28 08:30:59 - general channel

**Jono**

Morning everyone. Im about to deploy an odds scraper and i would like to have an instance of the scraper running for each of the main markets of a given sport. Does logging into the api in separate instances cause problems? For example if i am recording the match_odds market and then start up another program that will login and begin recording the handicap market will one of the sessions terminate or expire etc?

---

## 2021-01-25 08:25:28 - general channel

**Unknown**

bflw [https://github.com/liampauling/betfair/blob/master/HISTORY.rst#2120-2021-01-25|2.12.0](https://github.com/liampauling/betfair/blob/master/HISTORY.rst#2120-2021-01-25|2.12.0) now released, recommend testing first as I have refactored the streaming logic, everything is cached and reused if possible so there is the potential for side effects if you do not treat the data from streaming as read only (as you should). I and a few others have been using it with no issues and have backtested a couple thousand markets and everything is as expected.



As mentioned a few weeks back it is now rapid when backtesting (quicker than my naive golang backtester) also looking at about a 40-50% reduction in CPU when using live.

---

## 2021-01-24 16:50:23 - general channel

**river_shah**

bflw handles all the critical features for market and order streams and all other helper functions to communicate with betfair servers for market catalogues, runner meta data. flumine, builds on top of this core to provide a feature rich strategy design, backtesting and productionizing framework.

---

## 2021-01-24 15:40:26 - random channel

**Ruben**

good evening all, system design question: let's say I'm pulling data from an API, and I would like several script running in a server to be able to access that data, without each of them having to re-query the API. What's the best way to query the API once, and republish data so different scripts can access it?

---

## 2021-01-21 00:25:52 - random channel

**Misha**

OnCourt is our base for stats. Over 1.4 million matches. I automate updates (every 12 hours) from the .mdb file into my own database structures in SQL Server

---

## 2021-01-15 18:30:54 - random channel

**Newbie99**

I'm sure this has been asked before, but I couldn't see anything in the search...



Does  `order.elapsed_seconds` start when an order is placed in Flumine, or when an order is executable (in short I'm just trying to clarify whether when order.elapsed = 1 for in-play does that mean the order has been placed, post delay and has been live for 1 second, or does that mean it has been pending and is now turning active)?

---

## 2021-01-15 12:04:48 - general channel

**JC**

If you have recorded data and are wanting to generate some CSVs, is there a way to conflate the market books so that you can have a row every 0.25 secs for example? Can see conflate_ms for live streaming but not sure if there is a way for historical. Cheers

---

## 2021-01-15 11:58:04 - random channel

**bb**

One potential thing to be aware of for modelling with RPRs is they are not set in stone and are sometimes adjusted retrospectively. This is not a frequent thing but is enough to be a potential source of data leakage and throw off a model in production

---

## 2021-01-14 09:34:30 - issues channel

**Newbie99**

```{"asctime": "2021-01-14 09:12:30,117", "levelname": "INFO", "message": "Order status update: Cancelling", "market_id": "1.177891689", "selection_id": 16786966, "handicap": 0, "id": "138299083469138870", "customer_order_ref": "6012fe3b915f7-138299083469138870", "bet_id": "221690469430", "date_time_created": "2021-01-14 09:12:26.913912", "publish_time": "2021-01-14 09:12:26.886000", "trade": {"id": "9ef0aa3a-5648-11eb-a0bb-02f9e1e2dd3a", "strategy": "simple_lay_mm", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138299083469138870"], "notes": "", "market_notes": "8.4,8.8,8.6", "status": "Live", "status_log": "Pending, Live"}, "order_type": {"order_type": "Limit", "price": 8.2, "size": 2.08, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 2.08, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": "2021-01-14 09:12:27.117261", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Cancelling", "status_log": "Pending, Executable, Cancelling", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 0.0, "matched": []}}

INFO:flumine.streams.orderstream:Stopped OrderStream 1001

{"asctime": "2021-01-14 09:12:30,117", "levelname": "INFO", "message": "Stopped OrderStream 1001"}

INFO:flumine.streams.marketstream:Stopped MarketStream 2001

{"asctime": "2021-01-14 09:12:30,118", "levelname": "INFO", "message": "Stopped MarketStream 2001"}

INFO:flumine.execution.baseexecution:Shutting down Execution (SimulatedExecution)

{"asctime": "2021-01-14 09:12:30,118", "levelname": "INFO", "message": "Shutting down Execution (SimulatedExecution)"}

INFO:flumine.execution.baseexecution:Shutting down Execution (BetfairExecution)

{"asctime": "2021-01-14 09:12:30,119", "levelname": "INFO", "message": "Shutting down Execution (BetfairExecution)"}

INFO:flumine.worker:BackgroundWorker keep_alive shutting down

{"asctime": "2021-01-14 09:12:30,119", "levelname": "INFO", "message": "BackgroundWorker keep_alive shutting down", "worker_name": "keep_alive", "function": "&lt;function keep_alive at 0x7f7644889680&gt;"}

INFO:flumine.streams.orderstream:Stopped output_thread (OrderStream 1001)

{"asctime": "2021-01-14 09:12:30,365", "levelname": "INFO", "message": "Stopped output_thread (OrderStream 1001)"}

INFO:flumine.streams.marketstream:Stopped output_thread (MarketStream 2001)

{"asctime": "2021-01-14 09:12:32,043", "levelname": "INFO", "message": "Stopped output_thread (MarketStream 2001)"}

INFO:flumine.worker:BackgroundWorker poll_account_balance shutting down

{"asctime": "2021-01-14 09:12:34,120", "levelname": "INFO", "message": "BackgroundWorker poll_account_balance shutting down", "worker_name": "poll_account_balance", "function": "&lt;function poll_account_balance at 0x7f76448967a0&gt;"}

INFO:flumine.worker:BackgroundWorker poll_market_catalogue shutting down

{"asctime": "2021-01-14 09:12:38,120", "levelname": "INFO", "message": "BackgroundWorker poll_market_catalogue shutting down", "worker_name": "poll_market_catalogue", "function": "&lt;function poll_market_catalogue at 0x7f7644896710&gt;"}

INFO:flumine.worker:BackgroundWorker poll_cleared_orders shutting down

{"asctime": "2021-01-14 09:12:42,121", "levelname": "INFO", "message": "BackgroundWorker poll_cleared_orders shutting down", "worker_name": "poll_cleared_orders", "function": "&lt;function poll_cleared_orders at 0x7f7644896830&gt;"}

INFO:flumine.baseflumine:Exiting flumine

{"asctime": "2021-01-14 09:12:46,214", "levelname": "INFO", "message": "Exiting flumine", "client": {"id": "74093be2", "exchange": "Betfair", "betting_client": "APIClient", "chargeable_transaction_count": 0, "trading_controls": ["&lt;flumine.controls.clientcontrols.MaxOrderCount object at 0x7f7643506d50&gt;"], "order_stream": true, "best_price_execution": true, "paper_trade": false}, "markets": {"market_count": 262, "open_market_count": 259}, "streams": ["&lt;OrderStream(OrderStream, stopped daemon 140145877423872)&gt;", "&lt;MarketStream(MarketStream, stopped daemon 140145869031168)&gt;"], "logging_controls": [], "threads": ["&lt;_MainThread(MainThread, started 140146403047232)&gt;", "&lt;BackgroundWorker(keep_alive, started daemon 140145910994688)&gt;", "&lt;BackgroundWorker(poll_account_balance, started daemon 140145902601984)&gt;", "&lt;BackgroundWorker(poll_market_catalogue, started daemon 140145894209280)&gt;", "&lt;BackgroundWorker(poll_cleared_orders, started daemon 140145885816576)&gt;"]}

Traceback (most recent call last):

  File "jumps.py", line 187, in &lt;module&gt;

    framework.run()

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/flumine.py", line 32, in run

    self._process_current_orders(event)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/baseflumine.py", line 238, in _process_current_orders

    strategy.process_orders(market, strategy_orders)

  File "/home/ec2-user/trading/env/projects/simple_book.py", line 418, in process_orders

    rf.cancel_and_replace_orders(self, orders, market)

  File "/home/ec2-user/trading/env/projects/racing_functions.py", line 3563, in cancel_and_replace_orders

    back_all_runners_without_back_price

  File "/home/ec2-user/trading/env/projects/racing_functions.py", line 3699, in replace_all_orders

    self.replace_order(market, order, back_price)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/strategy/strategy.py", line 146, in replace_order

    market.replace_order(order, new_price)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/markets/market.py", line 70, in replace_order

    order.replace(new_price)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/order/order.py", line 332, in replace

    raise OrderUpdateError("Current status: %s" % self.status)

flumine.exceptions.OrderUpdateError: Current status: OrderStatus.CANCELLING

(env) [ec2-user@ip-172-31-8-45 projects]$       ```

---

## 2021-01-14 09:31:57 - issues channel

**Newbie99**

This is confusing me a bit, I've been getting this a lot over the last couple of days (seemingly out of the blue). Whilst the error is self explanatory, what I'm not understanding is how it occurs, as I check the order status before any cancel or replace:



```Traceback (most recent call last):

  File "jumps.py", line 187, in &lt;module&gt;

    framework.run()

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/flumine.py", line 32, in run

    self._process_current_orders(event)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/baseflumine.py", line 238, in _process_current_orders

    strategy.process_orders(market, strategy_orders)

  File "/home/ec2-user/trading/env/projects/simple_book.py", line 418, in process_orders

    rf.cancel_and_replace_orders(self, orders, market)

  File "/home/ec2-user/trading/env/projects/racing_functions.py", line 3563, in cancel_and_replace_orders

    back_all_runners_without_back_price

  File "/home/ec2-user/trading/env/projects/racing_functions.py", line 3699, in replace_all_orders

    self.replace_order(market, order, back_price)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/strategy/strategy.py", line 146, in replace_order

    market.replace_order(order, new_price)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/markets/market.py", line 70, in replace_order

    order.replace(new_price)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/order/order.py", line 332, in replace

    raise OrderUpdateError("Current status: %s" % self.status)

flumine.exceptions.OrderUpdateError: Current status: OrderStatus.CANCELLING```

---

## 2021-01-13 12:46:26 - issues channel

**Mo**

If you are using flumine to do the recording then the following is handled for you but it might be useful for you to be aware that the live streaming data does not contain runner names or competition information and that you need to record the market catalogue as well

---

## 2021-01-12 15:32:05 - general channel

**Matthieu Labour**

Would that take place within Flumine? The idea is to prepare an order (in CSV or via API like Oliver &amp; liam suggested) but, then, where does the reading of the CSV is taking place? In a Strategy? A background worker?

---

## 2021-01-12 12:37:31 - strategies channel

**liam**

Latter, 90% of the time is processing the market so you should take advantage of this and put in as many strategies as you can. If you have issues then you will probably have issues live and you need to fix 

---

## 2021-01-11 22:23:16 - issues channel

**Newbie99**

```{"asctime": "2021-01-11 22:06:46,792", "levelname": "INFO", "message": "Order status update: Violation", "market_id": "1.177838493", "selection_id": 25383704, "handicap": 0, "id": "138296955628176706", "customer_order_ref": "869aeb021cad4-138296955628176706", "bet_id": "221534743900", "trade": {"id": "f3590071-5458-11eb-b622-2016d8944b85", "strategy": "simple_back_mm", "status": "TradeStatus.LIVE", "orders": ["138296954581483634", "138296954616777215", "138296955628176706"], "notes": "", "market_notes": "13.5,14,13.5"}, "order_type": {"order_type": "Limit", "price": 13.5, "size": 0.2, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.01, "size_remaining": 0.19, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 13.5}, "status": "Violation", "status_log": "Pending, Executable, Replacing, Violation"}

INFO:flumine.order.trade:Trade status update: Complete

{"asctime": "2021-01-11 22:06:46,793", "levelname": "INFO", "message": "Trade status update: Complete", "id": "f3590071-5458-11eb-b622-2016d8944b85", "strategy": "simple_back_mm", "status": "TradeStatus.COMPLETE", "orders": ["138296954581483634", "138296954616777215", "138296955628176706"], "notes": "", "market_notes": "13.5,14,13.5"}

WARNING:flumine.controls:Order has violated: ORDER_VALIDATION Error: Order size is less than min bet size (2) or payout (10) for currency

{"asctime": "2021-01-11 22:06:46,794", "levelname": "WARNING", "message": "Order has violated: ORDER_VALIDATION Error: Order size is less than min bet size (2) or payout (10) for currency", "control": "ORDER_VALIDATION", "error": "Order size is less than min bet size (2) or payout (10) for currency", "order": {"market_id": "1.177838493", "selection_id": 25383704, "handicap": 0, "id": "138296955628176706", "customer_order_ref": "869aeb021cad4-138296955628176706", "bet_id": "221534743900", "trade": {"id": "f3590071-5458-11eb-b622-2016d8944b85", "strategy": "simple_back_mm", "status": "TradeStatus.COMPLETE", "orders": ["138296954581483634", "138296954616777215", "138296955628176706"], "notes": "", "market_notes": "13.5,14,13.5"}, "order_type": {"order_type": "Limit", "price": 13.5, "size": 0.2, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.01, "size_remaining": 0.19, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 13.5}, "status": "Violation", "status_log": "Pending, Executable, Replacing, Violation"}}```

With Flumine, the above violates (obviously because its less than GBP 10), however using the REST API it is possible to replace small orders, so should it work from Flumine, or is the above expected behaviour (specifically on a replace)?



If so, other than quickly placing and cancelling, is there a smarter way to submit small orders (I know this comes up every now and again, but I thought the replace approach was the correct one and I'd never really had much of a need when streaming before)?

---

## 2021-01-11 08:14:20 - general channel

**liam**

Just released a beta of [https://github.com/liampauling/betfair/pull/371|2.12.0b0](https://github.com/liampauling/betfair/pull/371|2.12.0b0) which contains the streaming refactor, using [@UBS7QANF3](@UBS7QANF3) benchmark looking at the following improvements:



```# lightweight (2.3x improvement)

#  master: 6.860s (100%|██████████| 10/10 [00:06&lt;00:00,  1.46it/s])

#  2.12.0: 2.998s (100%|██████████| 10/10 [00:02&lt;00:00,  3.34it/s])



# Non lightweight (3.4x improvement)

#  master: 36.005s (100%|██████████| 10/10 [00:35&lt;00:00,  3.60s/it])

#  2.12.0: 10.625s (100%|██████████| 10/10 [00:10&lt;00:00,  1.06s/it])



# Non lightweight with flumine patching/listener/stream (2.8x improvement)

#  master: 13.896s (100%|██████████| 10/10 [00:13&lt;00:00,  1.39s/it])

#  2.12.0: 5.005s (100%|██████████| 10/10 [00:04&lt;00:00,  2.00it/s])```

A quick [https://github.com/liampauling/flumine/pull/361|test](https://github.com/liampauling/flumine/pull/361|test) on flumine and this equates to around a 2.3x speed improvement which is massive and equates to flumine running through a market faster than the current lightweight implementation! CPU has also halved when running live which is a nice bonus.



Going to stay in beta until I can test this properly in production but I have carried out some regression tests on various markets throughout (hasn't just been optimised for this one market) However feel free to test when backtesting and let me know if there any bugs.

---

## 2021-01-10 20:36:31 - general channel

**Dilla**

:wave: Perhaps someone knows how to open bet365 livescore bypassing the main site?

In my country bet365 is blocked, and VPN does not accept this site or only for a short time.

I could only find such options. But they are very slow. They differ in the speed of updating the livescore, which is on the main site. And also there does not show every point in tennis, only games.

[https://livescore.sportradar.com/bet365/en/tennis](https://livescore.sportradar.com/bet365/en/tennis)

[http://ls.betradar.com/ls/livescore/?/betfair/en/page](http://ls.betradar.com/ls/livescore/?/betfair/en/page)

---

## 2021-01-07 14:09:38 - random channel

**Lee**

I should probably rephrase my question. Currently I use dokku on digital ocean but am migrating everything to aws so considering using ECS

---

## 2021-01-07 14:05:05 - random channel

**Lee**

Does anyone see any issues running flumine using ECS with ec2?

---

## 2021-01-07 13:58:55 - general channel

**Taking Value**

Infact I say 20mins but that's how long it takes on my laptop, the EC2 environment is deffinately running slower than that but I haven't had time to investigate why yet. Spent most of yesterday re-doing my code to see if that could speed it up but it hasn't.

---

## 2021-01-07 13:57:35 - general channel

**Taking Value**

I provisioned an EC2 spot instance with no less than 8gb of ram and 2cpus. Currently it takes me 20mins to scrub a file using the python code I wrote (which seems far too long for me). I am processing each file and adding the desired data to a MySQL Amazon RDS database rather than query the files themselves. I don't know what flumine is beyond seeing its name mentioned in here, on my to do list.

---

## 2021-01-05 10:15:33 - issues channel

**Alessio**

(it's logging in correctly, working everything, but i get those keep alive errors)

---

## 2021-01-05 10:03:28 - issues channel

**Alessio**

Hey [@U4H19D1D2](@U4H19D1D2) how did you choose the 1200 seconds for the API keep_alive? I am seeing repeatedly this stuff:

`{"asctime": "2021-01-05 09:56:39,831", "levelname": "ERROR", "message": "keep_alive error", "exc_info": "Traceback (most recent call last):\n File \"/usr/local/lib/python3.7/site-packages/flumine/worker.py\", line 97, in keep_alive\n  resp = client.keep_alive()\n File \"/usr/local/lib/python3.7/site-packages/flumine/clients/betfairclient.py\", line 32, in keep_alive\n  return self.betting_client.keep_alive()\n File \"/usr/local/lib/python3.7/site-packages/betfairlightweight/endpoints/keepalive.py\", line 30, in __call__\n  (response, response_json, elapsed_time) = self.request(session=session)\n File \"/usr/local/lib/python3.7/site-packages/betfairlightweight/endpoints/keepalive.py\", line 56, in request\n  self._error_handler(response_json)\n File \"/usr/local/lib/python3.7/site-packages/betfairlightweight/endpoints/keepalive.py\", line 63, in _error_handler\n  raise self._error(response)\nbetfairlightweight.exceptions.KeepAliveError: API keepAlive FAIL: NO_SESSION", "trading_function": "keep_alive", "response": "API keepAlive FAIL: NO_SESSION"}`

---

## 2021-01-05 08:22:37 - issues channel

**mandelbot**

On a seperate note why would an order not get matched even though it's taking the price available?

`"orders": ["138291273086345874"], "notes": "", "market_notes": "6.6,6.8,6.8", "status": "Pending", "status_log": "Pending"}, "order_type": {"order_type": "Limit", "price": 6.8, "size": 59.64, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 59.64, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "responses": {"date_time_placed": "2020-11-08 04:39:45.242000", "elapsed_seconds_executable": null}, "runner_status": null, "status": "Executable", "status_log": "Pending, Executable", "violation_msg": null, "simulated": {"profit": 0.0, "piq": 33.57, "matched": []}}`

`{"asctime": "2021-01-05 08:15:08,634", "levelname": "INFO", "message": "Trade status update: Live", "id": "1fd92291-4f2e-11eb-bc86-5c969d7d9766", "strategy": "projectedBotAUSHR", "place_reset_seconds": 0.0, "reset_seconds": 0.0, "orders": ["138291273086345874"], "notes": "", "market_notes": "6.6,6.8,6.8", "status": "Live", "status_log": "Pending, Live"}`

---

## 2021-01-03 17:58:11 - strategies channel

**mandelbot**

Funnily enough I had to restart my recorder that day due to my ec2 acting up so it seems like I also have a dodgy version

---

## 2021-01-03 16:03:29 - general channel

**liam**

When live trading market filter is a `streaming_market_filter` but when backtesting you instead pass the `markets` var so in your example above "market_types" is ignored when backtesting

---

## 2021-01-03 12:57:44 - issues channel

**JC**

this is the start of the log

`{"asctime": "2021-01-03 12:45:09,144", "levelname": "ERROR", "message": "Execution error", "trading_function": "place", "response": "SportsAPING/v1.0/placeOrders \nParams: {'marketId': '1.177514173', 'instructions': [{'orderType': 'LIMIT', 'selectionId': 24003006, 'side': 'BACK', 'handicap': 0, 'limitOrder': {'price': 2.02, 'persistenceType': 'LIMIT', 'size': 4}, 'customerOrderRef': 'ec2e3b2b42f12-138289707090911470'}], 'customerRef': '833f4dc64dc111eb97cd0a1ab5524ac8', 'customerStrategyRef': 'ip-172-31-40-12', 'async': False} \nException: None \nError: {'code': -32602, 'message': 'DSC-0018'} \nFull Response: {'jsonrpc': '2.0', 'error': {'code': -32602, 'message': 'DSC-0018'}, 'id': 1}", "order_package": {"id": "833f4dc6-4dc1-11eb-97cd-0a1ab5524ac8", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f59562b9a50&gt;", "market_id": "1.177514173", "orders": ["138289707090911470"], "package_type": "Place", "customer_strategy_ref": "ip-172-31-40-12"}}`

---

## 2021-01-03 11:57:02 - general channel

**steve**

hi guys new to using live odds. im trying to find the near_price and far_price bsp, but keep getting 'None'. Can get the back and lay prices ok. i'm using the following code. anyone know what im doing wrong?



price_filter = bflw.filters.price_projection(

    price_data=['SP_AVAILABLE','SP_TRADED','EX_BEST_OFFERS','EX_ALL_OFFERS','EX_TRADED']

    )

market_books = trading.betting.list_market_book(

    market_ids=[1.177514203],

    price_projection=price_filter

    )

for runner in market_books[0].runners:

    print(runner.sp.near_price)

    print(runner.sp.far_price)

---

## 2021-01-02 21:52:57 - general channel

**nthypes**

Thie betfair data has a lot of flaws. What the change os BSP prices @ 1.01?

---

## 2021-01-02 20:34:32 - strategies channel

**birchy**

I'm sure this has been asked before but I can't find it in the slack history...

Say you have a strategy that you backtest using Flumine. After ~6k matched bets (match rate ~ 15%, so ~40k bets submitted), it shows -0.1% ROI calculated from `total_pnl / total_risk`, where total risk is the sum of each bet risk rather than the net risk per market. No commission is deducted and each matched bet is deducted from the available volume.

Would you push this strategy to real money? I'm thinking no, but have not (yet) compared a Flumine backtest vs live, so have no idea how matching, P&amp;L, etc compares.

---

## 2021-01-02 17:04:04 - general channel

**Alessio**

AWS folks who run their production strategy there, which instance kind do you use? Is something in the free tier powerful enough (esp. in terms of RAM) ?

---

## 2021-01-02 08:05:45 - general channel

**Matthieu Labour**

Thank you. I will try. This will work for live but not for backtesting, correct? The strategy code will be different depending on backtesting or live, correct?

---

## 2021-01-01 22:24:50 - general channel

**Matthieu Labour**

Hello! I am looking for some guidance in Flumine. How to receive a timeout in a strategy. Let’s say I want a strategy to get a notification some time before a market starts. Would you recommend having a background worker send the notification? Ideally the solution should work for both live and backtesting.

---

## 2021-01-01 21:57:25 - issues channel

**Misha**

Yes, all database servers have improved performance and efficiency over the years

---

## 2021-01-01 14:32:44 - general channel

**Misha**

[@UUE6E1LA1](@UUE6E1LA1) - that's a good point. And with enough time and effort I could "follow" in what these big players do (I probably know enough how to detect it). I could live on about 2% of what my client made on UK racing. But using what I have learnt I reckon eventually I might be able to get close in just in-play tennis, so I'm playing the long game :wink:

---

## 2021-01-01 11:37:17 - issues channel

**liam**

yeah you have the trade count limit and the strategy investment limit (default is 1 live order per runner per strategy)

---

## 2021-01-01 00:22:57 - issues channel

**Misha**

I spend a huge part of my time "cleaning" data, and another huge part of my time just looking at it by running query after query. I have several data sets for different purposes (historically sourced data, live collected data, etc) and it's all in databases on SQL server. I have probably run an average of 100 hand-crafted queries per day for the last 3 months on my data. My view is that you really really need to understand your data. The longer I go the more insights I get from the data. Also remember I use absolutely no ML (my partner working on his model does)

---

## 2020-12-31 16:53:22 - general channel

**liam**

High level is you build cache of the alt data (pt: data) and then do a lookup based on the marketBook pt and inject it into the context for your strategy to use (just like live) 

---

## 2020-12-31 12:46:05 - general channel

**river_shah**

Currently I am using cloud service providers for back testing however when it comes to live trading, just launching scripts off my London based laptop (have high bandwidth reliable residential broadband). Could I please be guided on what the best way to run a prod model may be from a colo perspective? Speed is not so important to me but if I can get a few milliseconds for free running from AWS Dublin or some other location, won’t say no to it. Think this topic was discussed earlier but search history has run out. Thanks

---

## 2020-12-30 17:38:58 - general channel

**Artur Gräfenstein**

Does anyone have any idea where to get the exact live time for a soccer event? Many providers are a little imprecise. The time on the Betfair website is about 1-2 minutes delayed. I need the time quite accurately for my model.

---

## 2020-12-30 17:25:32 - strategies channel

**liam**

It’s based on the backtest time, flumine patches datetime so it will work as though it was ‘live’ however you could just remove 

---

## 2020-12-30 11:50:34 - issues channel

**Mo**

I think they recognise it has flaws and have some regrets over its introduction but they need(ed) some way of recovering their cost of acquisition. There are probably better ways of incentivising behaviour that improves the exchange ecosystem but they obviously haven't felt the various trials they've run with different fee structures have achieved that while maintaining or improving their bottom line.

---

## 2020-12-29 10:08:06 - strategies channel

**Oliver Varney**

just thinking as the view of the market will be different in terms of perceived volumes by the model vs live trading. I assume virtual prices will be show much larger volume due to crossmatching?

---

## 2020-12-26 15:10:09 - general channel

**Dave**

I'll check rsync, might be better than the standard sync tool from aws CLI thanks! Although the overhead from mounting it might not be worth it for the way I use S3. Also I must be doing something funky with cloudwatch, my cloudwatch costs are 97% of my total AWS bill. In absolute it's a negligible amount, the breakdown is odd though :sweat_smile:

---

## 2020-12-24 15:24:21 - random channel

**JazzMan**

I run SQL server 2019 on 2 local Windows 10 pc’s and on 2 Windows Server Azure servers without any problems. I don’t see why AWS would make any difference. I should be buying a license I suppose but be

---

## 2020-12-24 14:58:05 - random channel

**Oliver Varney**

okay so am I right in thinking that I can run SQL server dev in AWS like I do locally without any costs? just wondering what the pros and cons of SQL Server, whether I can use it for free and whether I should ditch it now I have an opportunity

---

## 2020-12-24 14:54:14 - random channel

**John A**

I use SQL Server development version which is free. SQL licenses are expensive but if you’re the only user there’s no reason to buy one. Been using the developer version for years with no problems. If you’re running windows server on AWS I can’t see any problems.

---

## 2020-12-24 14:47:42 - random channel

**Oliver Varney**

Hey chaps, techy question, currently I use SQL Server as my database software (locally). I am starting some work that will basically give me a chance to remodel all my database work going forward. I run everything locally atm but there is scope for this side project to be run on AWS / cloud and my stuff also down the line. Am I right in thinking there is high license fees for SQL server on AWS? If so what opensource/free SQL databases would people recommend

---

## 2020-12-20 16:32:15 - random channel

**Lee**

For those who use lambdas, do you have a separate repo for each function to make CI and deployment simpler? I use github actions to deploy

---

## 2020-12-19 10:33:12 - general channel

**Alessio**

Never trust OVH .. 0 support, 0 help. Also, the 'geolocated IPs' they promise you.. well.. maxmind really doesn't care..the server is still in france, hence they keep it located in france :)

---

## 2020-12-19 09:13:18 - general channel

**Jonjonjon**

I had a similar problem running from a godaddy server. And the idiot from godaddy told me I should trust him, and that it wasn't a problem...

---

## 2020-12-19 00:40:08 - general channel

**Will Morrison**

I'm thinking about moving my code to a server, but I'm located in Thailand. Do you guys know if Betfair will let me run stuff from a server in the UK, or do I have to stay in my country of residence? I asked their help, but they seem to be totally swamped right now.

---

## 2020-12-18 11:00:58 - issues channel

**Misha**

Transient errors happen - have had a handful of 520s over the past 24 hours. Perfectly normal as a client (who knows why the server is generating them)

---

## 2020-12-18 10:39:42 - issues channel

**Mick**

I find that trading.keep_alive() usually works fine but occasionally it produces "betfairlightweight.exceptions.StatusCodeError: Status code error: 520" - any ideas?

---

## 2020-12-17 11:04:03 - issues channel

**ricky**

I am recording marketdata via AWS S3 (S3MarketRecorder) , I expect market_catalogue shall be stored in Json format, but i got flat file.

I am new to AWS, i am not sure if it is cause by my setting problem or the source code need change to



self.s3.put_object(

	Body=str(json.dumps(market.market_catalogue.json())),

	Bucket=self._bucket,

	Key=os.path.join(

		"marketdata",

		"marketCatalogue",

		market.market_id,

	),

)

---

## 2020-12-15 17:23:21 - random channel

**Oliver Varney**

I use sql server, and I think they have something similar. I think it came down to performance in the end. I purely only use it for training models, basically pushes the historical files into it

---

## 2020-12-13 17:23:38 - random channel

**AP**

I worked for someone in Australia who used Citibet extensively. I can’t speak for the laws in the UK but in Australia it was illegal for an offshore gambling entity to offer gambling services to Australians, but not illegal for Australians to use them. 

---

## 2020-12-12 17:22:52 - general channel

**AP**

[https://www.serverless.com/plugins/serverless-python-requirements|https://www.serverless.com/plugins/serverless-python-requirements](https://www.serverless.com/plugins/serverless-python-requirements|https://www.serverless.com/plugins/serverless-python-requirements)

---

## 2020-12-12 12:47:57 - general channel

**Alessio**

ah no, this seems even simpler: [https://docs.aws.amazon.com/pinpoint/latest/developerguide/tutorials-importing-data-create-python-package.html](https://docs.aws.amazon.com/pinpoint/latest/developerguide/tutorials-importing-data-create-python-package.html) without having to use PIP and docker

---

## 2020-12-12 12:40:33 - general channel

**Alessio**

something along these lines: [https://www.serverless.com/blog/serverless-python-packaging](https://www.serverless.com/blog/serverless-python-packaging)  but no docker because you don't use compiled stuff like numpy?

---

## 2020-12-07 16:30:00 - random channel

**Oliver Varney**

yes essentially I use sql server as a long term datastore, with python services that request / add data, call further internal procs that populate tables and build out further features. Just wondered if there was something similar to unittest / pytest within sql server. Or is it a case of using python to pull data out then do the test cases that way

---

## 2020-12-06 17:33:29 - strategies channel

**Alessio**

ah sorry, (1) i didn't build the model (2) this is not for live bets, it's for pre-match prediction

---

## 2020-12-06 15:10:57 - random channel

**Alessio**

What you are looking for is an integration test in a sense.. so basically you'll probably want some golden data apart, a second database in sqlserver and some golden expectations?

---

## 2020-12-06 09:51:21 - strategies channel

**liam**

Yes but you need to change the strategy.[https://github.com/liampauling/flumine/blob/dabcaab5853ed71e95eed1c076706a41ded334e9/flumine/strategy/strategy.py#L44|max_live_trade_count](https://github.com/liampauling/flumine/blob/dabcaab5853ed71e95eed1c076706a41ded334e9/flumine/strategy/strategy.py#L44|max_live_trade_count) 

---

## 2020-12-06 09:13:15 - random channel

**Oliver Varney**

so I guess what I want to design or improve is a data quality / checking service. I run models that use data from many sources + I further aggregate and build out more features. also I would like to build some kind of unit test that tests the built out aggregate features in SQL Server. With a large number of sources + features it can be easy to let a inner join slip through the net instead of being a left join and it might not be fully noticeable straight away.

---

## 2020-12-05 01:31:35 - general channel

**Charlie 303**

Follow-up - I need to run some code every time a new market is added. the docs mention an _add_live_market() function is "called when new Market received through streams".  Is that the right spot? are there any examples of implenenting that? (still brushing off the dust on my python skills, not sure how to override the Flumine class)

---

## 2020-12-04 09:50:13 - general channel

**Jorge**

Hi guys, is there any problem with running bots from 2 different Servers at the same time? I am running them locally for testing and in a production server, wondering if Betfair would like that or not...

---

## 2020-12-03 19:43:51 - strategies channel

**birchy**

[@U4H19D1D2](@U4H19D1D2) great features, and will definitely start using them. Probably missing something here, but I presume the MarketRecorder won't pick up any bets I've got placed as it's a different strategy, so what's the recommended way to save data from a live bot?

---

## 2020-12-03 17:21:27 - issues channel

**liam**

```{"asctime": "2020-12-03 17:15:29,340", "levelname": "INFO", "message": "Order status update: Execution complete", "market_id": "1.176254838", "selection_id": 19746489, "handicap": 0, "id": "138263085288320640", "customer_order_ref": "27cfde50caeb3-138263085288320640", "bet_id": "218400243460", "trade": {"id": "2428e4da-358b-11eb-8b34-a0999b054753", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138263085288320640"], "notes": "", "market_notes": "4.4,4.5,4.5"}, "order_type": {"order_type": "Limit", "price": 4.4, "size": 2.0, "persistence_type": "LAPSE", "time_in_force": "FILL_OR_KILL", "min_fill_size": 0.01, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 2, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 4.4}, "status": "Execution complete", "status_log": "Pending, Executable, Execution complete"}```

---

## 2020-12-03 16:08:45 - issues channel

**Aaron Smith**

```{"asctime": "2020-12-03 15:00:04,154", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.176309310", "selection_id": 28615071, "handicap": 0, "id": "138263004041538760", "customer_order_ref": "strat-138263004041538760", "bet_id": null, "trade": {"id": "3979446e-3578-11eb-b83b-0a13a1974215", "strategy": "strat", "status": "TradeStatus.LIVE", "orders": ["138263004041538760"], "notes": "", "market_notes": null}, "order_type": {"order_type": "Limit", "price": 2.22, "size": 6.76, "persistence_type": "LAPSE", "time_in_force": "FILL_OR_KILL", "min_fill_size": 0.01, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 6.76, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Pending", "status_log": "Pending"}



{"asctime": "2020-12-03 15:00:04,155", "levelname": "INFO", "message": "1 order packages created", "order_packages": [{"id": "39799d2e-3578-11eb-b83b-0a13a1974215", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f9ca51b9b10&gt;", "market_id": "1.176309310", "orders": ["138263004041538760"], "package_type": "Place", "customer_strategy_ref": "ip-172-31-34-22", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0}], "bet_delay": 0}



{"asctime": "2020-12-03 15:00:04,156", "levelname": "INFO", "message": "New requests.Session created", "sessions_created": 1, "session": "&lt;requests.sessions.Session object at 0x7f9c9c29ce90&gt;", "session_time_created": 1607007604.1561081, "session_time_returned": 1607007604.1561098}



{"asctime": "2020-12-03 15:00:04,329", "levelname": "INFO", "message": "[OrderStream: 1001]: 1.176309310 added, 3 markets in cache"}



{"asctime": "2020-12-03 15:00:04,330", "levelname": "WARNING", "message": "Order 218390745703 not present in blotter", "bet_id": "218390745703", "market_id": "1.176309310", "customer_strategy_ref": "ip-172-31-34-22", "customer_order_ref": "strat-138263004041538760"}



{"asctime": "2020-12-03 15:00:04,338", "levelname": "INFO", "message": "execute_place", "trading_function": "place", "elapsed_time": 0.18191218376159668, "response": {"customerRef": "39799d2e357811ebb83b0a13a1974215", "status": "SUCCESS", "marketId": "1.176309310", "instructionReports": [{"status": "SUCCESS", "instruction": {"selectionId": 28615071, "handicap": 0.0, "limitOrder": {"size": 6.76, "price": 2.22, "minFillSize": 0.01, "timeInForce": "FILL_OR_KILL"}, "customerOrderRef": "strat-138263004041538760", "orderType": "LIMIT", "side": "BACK"}, "betId": "218390745703", "placedDate": "2020-12-03T15:00:04.000Z", "averagePriceMatched": 2.236449704142012, "sizeMatched": 6.76, "orderStatus": "EXECUTION_COMPLETE"}]}, "order_package": {"id": "39799d2e-3578-11eb-b83b-0a13a1974215", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x7f9ca51b9b10&gt;", "market_id": "1.176309310", "orders": ["138263004041538760"], "package_type": "Place", "customer_strategy_ref": "ip-172-31-34-22", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0}}



{"asctime": "2020-12-03 15:00:04,339", "levelname": "INFO", "message": "Trade status update: Pending", "id": "3979446e-3578-11eb-b83b-0a13a1974215", "strategy": "strat", "status": "TradeStatus.PENDING", "orders": ["138263004041538760"], "notes": "", "market_notes": "2.24,2.26,None"}



{"asctime": "2020-12-03 15:00:04,339", "levelname": "INFO", "message": "Order Place: SUCCESS", "bet_id": null, "order_id": "138263004041538760", "status": "SUCCESS", "error_code": null}



{"asctime": "2020-12-03 15:00:04,339", "levelname": "INFO", "message": "Order status update: Executable", "market_id": "1.176309310", "selection_id": 28615071, "handicap": 0, "id": "138263004041538760", "customer_order_ref": "strat-138263004041538760", "bet_id": "218390745703", "trade": {"id": "3979446e-3578-11eb-b83b-0a13a1974215", "strategy": "strat", "status": "TradeStatus.PENDING", "orders": ["138263004041538760"], "notes": "", "market_notes": "2.24,2.26,None"}, "order_type": {"order_type": "Limit", "price": 2.22, "size": 6.76, "persistence_type": "LAPSE", "time_in_force": "FILL_OR_KILL", "min_fill_size": 0.01, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 6.76, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 2.236449704142012}, "status": "Executable", "status_log": "Pending, Executable"}



{"asctime": "2020-12-03 15:00:04,340", "levelname": "INFO", "message": "Trade status update: Live", "id": "3979446e-3578-11eb-b83b-0a13a1974215", "strategy": "strat", "status": "TradeStatus.LIVE", "orders": ["138263004041538760"], "notes": "", "market_notes": "2.24,2.26,None"}



{"asctime": "2020-12-03 15:00:04,580", "levelname": "WARNING", "message": "Order 218390745703 not present in blotter", "bet_id": "218390745703", "market_id": "1.176309310", "customer_strategy_ref": "ip-172-31-34-22", "customer_order_ref": "strat-138263004041538760"}```

---

## 2020-12-03 12:50:59 - general channel

**liam**

If I understand correctly it would be similar to this 2s [https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/examples/strategies/lowestlayer.py#L55|fillkill](https://github.com/liampauling/flumine/blob/4f08f389a3c5c2722b60252055b143cfdfefb0f7/examples/strategies/lowestlayer.py#L55|fillkill) as process_orders will be called every 0.25s whilst you have live orders, however you can do something similar in process_market_book by looping the orders `market.blotter`

---

## 2020-12-02 16:47:38 - issues channel

**birchy**

The new server is only a temporary setup for testing, but longer term I guess I'll need to upgrade production server to Ubuntu 20+ anyway.

---

## 2020-12-02 16:45:19 - issues channel

**birchy**

Yeah, was just thinking that but am slightly cautious in case the new certs break my production server

---

## 2020-12-02 16:36:36 - issues channel

**birchy**

Obviously, you're correct _again_ [@UBS7QANF3](@UBS7QANF3) :grinning:

```$ uname -v

#30~18.04.1-Ubuntu SMP Tue Oct 20 11:09:25 UTC 2020



$ apt-cache policy openssl

openssl:

  Installed: 1.1.1-1ubuntu2.1~18.04.6

  Candidate: 1.1.1-1ubuntu2.1~18.04.6

  Version table:

 *** 1.1.1-1ubuntu2.1~18.04.6 500

        500 [http://eu-west-1.ec2.archive.ubuntu.com/ubuntu](http://eu-west-1.ec2.archive.ubuntu.com/ubuntu) bionic-updates/main amd64 Packages

        500 [http://security.ubuntu.com/ubuntu](http://security.ubuntu.com/ubuntu) bionic-security/main amd64 Packages

        100 /var/lib/dpkg/status

     1.1.0g-2ubuntu4 500

        500 [http://eu-west-1.ec2.archive.ubuntu.com/ubuntu](http://eu-west-1.ec2.archive.ubuntu.com/ubuntu) bionic/main amd64 Packages

------------------

$ uname -v

#31-Ubuntu SMP Fri Nov 13 11:40:37 UTC 2020



$ apt-cache policy openssl

openssl:

  Installed: 1.1.1f-1ubuntu2

  Candidate: 1.1.1f-1ubuntu2

  Version table:

 *** 1.1.1f-1ubuntu2 500

        500 [http://eu-west-1.ec2.archive.ubuntu.com/ubuntu](http://eu-west-1.ec2.archive.ubuntu.com/ubuntu) focal/main amd64 Packages

        100 /var/lib/dpkg/status```

OpenSSL 1.1.1f is the one that's failing

---

## 2020-12-02 15:26:13 - issues channel

**birchy**

[@U4H19D1D2](@U4H19D1D2) Just had this on a Flumine fresh install on a Lightsail instance:

```File "/home/ubuntu/.local/lib/python3.8/site-packages/betfairlightweight/endpoints/login.py", line 53, in request

    raise APIError(None, exception=e)

betfairlightweight.exceptions.APIError: None

Params: None

Exception: [('SSL routines', 'SSL_CTX_use_certificate', 'ca md too weak')]```

Seems to be OpenSSL related: [https://stackoverflow.com/questions/52218876/how-to-fix-ssl-issue-ssl-ctx-use-certificate-ca-md-too-weak-on-python-zeep|StackOverflow Link](https://stackoverflow.com/questions/52218876/how-to-fix-ssl-issue-ssl-ctx-use-certificate-ca-md-too-weak-on-python-zeep|StackOverflow Link)

OpenSSL version 1.1.1

Flumine version 1.14.12

BetfairLightWeight version 2.10.2



Working fine on my production instance which has exactly the same setup and same certs?



Only difference I can find so far is:

```Linux 5.4.0-1029-aws #30~18.04.1-Ubuntu SMP

Linux 5.4.0-1029-aws #30-Ubuntu SMP```

Ubuntu 18.04 working fine, Ubuntu 20.04 is throwing the error.

---

## 2020-11-30 10:42:32 - strategies channel

**Unknown**

[@U4H19D1D2](@U4H19D1D2) 100% CPU for 5+ minutes. RAM usage was > 80% before the machine started lagging badly until the point where I had to force a restart from AWS admin.

Current live bot is running OK with:

``` market_filter=bflw.filters.streaming_market_filter(

    event_type_ids=['7'],

    country_codes=['GB', 'IE'],

    market_types=['WIN'],

),```



---

## 2020-11-30 10:27:31 - strategies channel

**birchy**

So I have a preplay strategy that I'm currently live testing on UK horse racing WIN markets. Long term, I want to extend this strategy to cover _every_ horse racing market, followed by implementation on other sports and ultimately coverage for ALL betfair markets.

Last week, during initial testing, I attempted to start Flumine with:

```market_filter=bflw.filters.streaming_market_filter(

    event_type_ids=['7'],

),```

...and managed to crash an AWS Lightsail instance when CPU &amp; RAM (2GB) hit 100% during startup. This is only a small % of where I actually want to be. I know Flumine creates a lot of objects, but is it actually THAT resource hungry?

Regarding implementing ALL markets, what's the recommended way to achieve this? Currently thinking that I may have to redesign as a polling bot instead. The strategy is not speed sensitive.

---

## 2020-11-30 09:03:29 - random channel

**Michael**

Assuming you have some idea how you intend to bet your best approach might be to code up some version of your likely strategy (however quick and dirty) and just go live at £2 stakes. That way you're generating useful data and not costing yourself much. Data from real bets placed is far more valuable than back-test data. Rather than trying to bet the absolute minimum without getting banned I would suggest you get a little money down in a lot of markets. If you have no strategy in mind then just put bets on in a good number of market as offers to backers - so you're laying at or slightly shorter than the best available to back price. Maybe have some check that the book is well made up to avoid very low liquidity markets.

---

## 2020-11-30 08:09:53 - random channel

**Artur Gräfenstein**

Hi everyone, does anyone know how tolerant Betfair is with read-only live keys? I can’t really finish my development with a 2 minute delay. So I have to adjust the details with the live key without betting.

---

## 2020-11-29 20:23:51 - issues channel

**Aaron Smith**

I ll let you know, Thanks a lot Oliver and liam for helping me :slightly_smiling_face:

---

## 2020-11-29 17:30:46 - random channel

**Newbie99**

I've been getting this all day and I can't figure it out, as it appears to occur when _process_close_market runs (which I haven't changed from the pip install of flumine)?



```  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/flumine.py", line 44, in run

    self._process_close_market(event)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/baseflumine.py", line 264, in _process_close_market

    market.blotter.process_closed_market(event.event)

  File "/home/ec2-user/trading/env/lib64/python3.7/site-packages/flumine/markets/blotter.py", line 86, in process_closed_market

    for runner in market_book.runners:

AttributeError: 'dict' object has no attribute 'runners'```

---

## 2020-11-28 06:30:31 - issues channel

**mandelbot**

Here's another example, it's not nonrunners...first date is market start time

`1.174235551 2020-10-16 13:53:00 23160590 2020-10-16 13:38:00.219000 OrderStatus.EXECUTION_COMPLETE BACK 30.0 10 0.0`



`{"asctime": "2020-11-28 06:21:33,082", "levelname": "INFO", "message": "Order status update: Execution complete", "market_id": "1.174235551", "selection_id": 23160590, "handicap": 0, "id": "138258372930769969", "customer_order_ref": "c13a2832c6a96-138258372930769969", "bet_id": "100000000003", "trade": {"id": "f5c31430-3141-11eb-8cda-5c969d7d9766", "strategy": "Bot", "status": "TradeStatus.LIVE", "orders": ["138258372930769969"], "notes": "", "market_notes": "30,29,30"}, "order_type": {"order_type": "Limit", "price": 30, "size": 10, "persistence_type": "LAPSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 10, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 30.0}, "status": "Execution complete", "status_log": "Pending, Executable, Execution complete"}`

---

## 2020-11-27 08:42:50 - issues channel

**Jonjonjon**

On an old server, which hasn't been ported to BFLW, I am getting this error message:



`Oops no service available at [https://api.betfair.com/exchange/betting/json-rpc/v1](https://api.betfair.com/exchange/betting/json-rpc/v1) Traceback (most recent call last): File "/home/content/28/xxx/html/xxx/helpers.py", line 66, in callAping_direct ).encode('utf-8')) File "/usr/local/lib/python2.7/urllib2.py", line 126, in urlopen return _opener.open(url, data, timeout) File "/usr/local/lib/python2.7/urllib2.py", line 400, in open response = self._open(req, data) File "/usr/local/lib/python2.7/urllib2.py", line 418, in _open '_open', req) File "/usr/local/lib/python2.7/urllib2.py", line 378, in _call_chain result = func(*args) File "/usr/local/lib/python2.7/urllib2.py", line 1215, in https_open return self.do_open(httplib.HTTPSConnection, req) File "/usr/local/lib/python2.7/urllib2.py", line 1177, in do_open raise URLError(err) URLError: 2020-11-27`

---

## 2020-11-27 08:12:13 - general channel

**liam**

So a market update comes in and processed through all strategies, after this an order package is created which like say can have multiple orders to reduce the number of requests.



The number of orders shouldn’t increase latency, are you monitoring CPU / ram? AWS? And have you tried profiling? I use cprofilev, quickly show you anything that is slow.

---

## 2020-11-25 23:09:27 - general channel

**Newbie99**

If you're streaming you should be getting updates every x milliseconds with a live key (depending on the market, but definitely for UK racing...well during the day...perhaps not at this time)!

---

## 2020-11-25 10:53:16 - issues channel

**Dermot**

Is anyone else having trouble with the api this morning? Can’t login via certlogin (from home or from ec2 ) and the [http://docs.developer.betfair.com|docs.developer.betfair.com](http://docs.developer.betfair.com|docs.developer.betfair.com) seems dead

---

## 2020-11-25 09:42:28 - random channel

**Lee**

I personally wouldn’t use multiprocessing to start multiple live/production instances. I think two separate services would be better.

I use docker so would have two separate docker containers running.

---

## 2020-11-24 16:10:26 - general channel

**birchy**

Not really practical when dedugging/developing a new strategy. The transition from backtest to live has highlighted a couple of issues that I need to rectify, so I'm in a bit of a run-&gt;stop-&gt;edit-&gt;run loop at the moment. Is that not normal procedure when developing/testing?

---

## 2020-11-24 15:28:55 - general channel

**birchy**

Currently testing my first live Flumine strategy. Have noticed that if I stop the bot and restart, bets are being repeated regardless of `max_trade_count` ,`max_live_trade_count` , `max_selection_exposure` , etc. Obviously the new instance handles its own bets but not any other bets that were previously placed. Same applies to bets that are manually cancelled - Flumine does not attempt to bet again until the instance is restarted. The latter is not a big issue, but obviously doubling exposure on restart is. What's the recommended procedure to avoid double betting when an instance is restarted? Is there a way to allow the new instance to take ownership of its previous bets and/or avoid re-betting based on the strategy_ref or similar?

---

## 2020-11-22 00:26:20 - strategies channel

**birchy**

Going to persist with the live data for now as I'm not a massive fan of using 3rd party feeds as they can be unreliable and high maintenance. Even the GPS feed is hit and miss with false information. I've always believed that profits can be made from mug punters and will concentrate on that until it makes or breaks.

---

## 2020-11-21 16:03:25 - strategies channel

**Michael**

He also wrote a whole article on his website about how streaming was a terrible idea because it would INCREASE the load on BF's servers.

---

## 2020-11-20 21:47:33 - random channel

**Oliver Varney**

but with most of my model I like a metric for live to understand / identify any issue / how its performing

---

## 2020-11-20 17:48:35 - issues channel

**ricky**

When i was running backtesting, i realised i can not get market name (e.g. " Flat/Hurdle") from market_definition.

Is there a way i can record market_catalogue data via MarketRecorder via my home computer? ( Unfortunately S3MarketRecorder do requed S3 bucket info, i want to test locally before try AWS)

---

## 2020-11-20 09:44:23 - general channel

**Mo**

This was many years ago so I cannot remember all of the details but the model used the most basic easily available tennis data you can find. The value in the strategy was a sophisticated model, at least more sophisticated than the market makers. Testing was largely live testing against the market and growing stakes organically. 

---

## 2020-11-19 16:15:41 - general channel

**mandelbot**

Lots of people do that and there's built in backtesting functionality in flumine to do this. Though there's always merits to live testing as well.

---

## 2020-11-19 16:13:09 - general channel

**Artur Gräfenstein**

I test it before going live. I build 2 Ways for testing. 1. I have a database with all odds and match live data with timestamps. I record this data by myself 7/24. And when I have an idea for a strategie, I build a bot and test it against the Data in the Database. If erverything looks good, I start testing it the second way. Running it “live” but not placing the bet. The Bot only saves and shows me the odd on witch he would place the bet. After the match ended, the bot shows red or green for every simulated bet and show me a sum up if Im positive or not. Sometime I run these bots over many days in sumlation before starting to place money. Saves me a lot of money.

---

## 2020-11-19 13:40:54 - general channel

**Artur Gräfenstein**

Did anyone get problems with betfair because of to many bets placed or something like that. I need for example to place unmatched bets for about 10-20 seconds. When the bet is not matching after this time i need to update the price. And this maybe 100 times in a market during the live soccer match. I have payed the 299£ activation fee and dont want to get banned or something else.

---

## 2020-11-19 11:49:54 - general channel

**Artur Gräfenstein**

ahh cool! Do you think js would be too slow for live betting on betfair? Im actually working on a project with js and want to add Betfair API. But don’t know if it would be better to switch to Python.

---

## 2020-11-18 21:03:13 - issues channel

**dan2002**

Hi, when i run marketrecorder.py (from flumine.exmaples) with my live api key for 24hr, i managed to record some data but only a very small number of files are zipped after markets close. not all with market closed are zipped. anyone had the same issue?

---

## 2020-11-18 19:50:30 - random channel

**Andrey Yunoshev**

&gt;In principle a break even strategy should be very appealing to anyone who pays premium charge. It depends how scalable it is though



on the backtest I checked up to 20 euros per bet, I can check and even more 100 for example -

but in my opinion it will not show anything, such amounts bets are not indicative for backtesting



run live with more bet size = more volatility, not sure what I can and want

even if everything fits together by the results of the month,

---

## 2020-11-18 19:02:14 - random channel

**Andrey Yunoshev**

ok, already a little less than just interested. is the zero strategy worth something? let's say 200 bets on average per day on greyhounds, tested at 2 euro bets on live for ~30 days, approximately the same result at rates up to 20  euro bets (possible more) via backtesting

---

## 2020-11-18 16:44:25 - general channel

**Lee**

I think it's better to have exactly the same logic in a strategy for backtesting/unittests/live

---

## 2020-11-18 13:16:07 - general channel

**birchy**

In Flumine, if you have a strategy running and at some point in the future you decide you're finished with this market, is there a way to unsubscribe before market closure? This is currently for backtesting but will also be useful when live. More specifically, if code in `check_market_book()` identifies the market is no longer required, what's the best way to skip to the next market file? I know we can use `market_filter` in the framework setup, but sometimes the market changes after subscription, i.e. we've hit our exposure limit, too many runners withdrawn, etc.

---

## 2020-11-16 14:00:15 - general channel

**liam**

Yeah, bflw will try and gracefully [https://github.com/liampauling/betfair/blob/1af4115d5f7493a0b0b4dfb674987830b556464f/betfairlightweight/streaming/betfairstream.py#L68|close](https://github.com/liampauling/betfair/blob/1af4115d5f7493a0b0b4dfb674987830b556464f/betfairlightweight/streaming/betfairstream.py#L68|close) but if the server has disconnected you are left waiting 

---

## 2020-11-16 13:38:26 - general channel

**Newbie99**

np, in the end I just stopped the AWS instance and re-started, not the most elegant way, but it worked!

---

## 2020-11-16 11:01:18 - random channel

**liam**

the provider section of the yml, one of mine looks like this

```provider:

  name: aws

  runtime: python3.6

  stage: prod

  region: eu-west-1

  vpc:

    securityGroupIds:

      - sg-rty

  iamRoleStatements:

    - Effect: "Allow"

      Action:

        - "s3:GetObject"

        - "s3:PutObject"

        - "s3:DeleteObject"

      Resource: "arn:aws:s3:::*"



    - Effect: "Allow"

      Action:

        - "ec2:CreateNetworkInterface"

        - "ec2:DescribeNetworkInterfaces"

        - "ec2:DeleteNetworkInterface"

      Resource: "*"

  environment:

    DATABASE_HOST: ${env:aurora_writer_endpoint}

    DATABASE_RO_HOST: ${env:aurora_reader_endpoint}

    DATABASE_PASSWORD: ${env:aurora_password}```

---

## 2020-11-16 10:59:02 - random channel

**AP**

Are you referring to the AWS credentials file on my computer?

---

## 2020-11-16 06:13:16 - general channel

**liam**

[@U01BBMTK6R3](@U01BBMTK6R3) flumine assumes 100% matching at SP however this is not the case when live as betfair do not guarantee it

---

## 2020-11-16 04:36:44 - random channel

**AP**

Hi Liam, I have managed to deploy my lambda function but there seems to be a permissions error when trying to download my certs from s3. I have fiddled around with adding permissions but yet to have any success, did you have any issues here?

---

## 2020-11-15 20:37:43 - general channel

**dan2002**

Hi is there a queue for SP price matching in flumine backtest and in live? Thanks.

---

## 2020-11-13 19:05:29 - random channel

**AP**

For the AWS users here, what is the most efficient CI/CD pipeline for lambda? I've noticed it doesn't support Docker images

---

## 2020-11-13 18:21:46 - strategies channel

**liam**

Infrastructure / CI / deployment is the bat mobile then

---

## 2020-11-13 18:07:23 - strategies channel

**Alessio**

But if your code doesn't handle rare events and edge cases right, you will end up in crappy situations. Also, faster backseating really gives you the ability to do better and you can't always just throw more AWS credits at the problem

---

## 2020-11-12 12:01:23 - general channel

**Jorge**

Streaming kept working for me fine, but keep_alive NG call failed at 10:28, it succeded at 10:29 though :slightly_smiling_face:

---

## 2020-11-12 10:56:59 - general channel

**liam**

load testing on production??

---

## 2020-11-11 22:05:56 - general channel

**Unknown**

[@U017FQEPV1U](@U017FQEPV1U) Easiest way is to use Flumine MarketRecorder to gather data. Then create a strategy framework and use Flumine backtest. In theory you can change the framework to live without changing the strategy.

[https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py](https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py)

---

## 2020-11-11 15:00:20 - issues channel

**liam**

So the production example handles it by just logging in on run [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

---

## 2020-11-11 14:55:53 - issues channel

**Mo**

As I said, it's correct that streaming does not require the session token to be kept alive

---

## 2020-11-11 14:55:20 - issues channel

**Jorge**

Do you mean bflw handles the MarketStreaming keepAlives or the NG keepAlives? (sorry for too many questions, just migrating my code to bflw at the moment)

---

## 2020-11-11 14:50:01 - issues channel

**Jorge**

So for NG we do need to send keepAlives, but the MarketStreaming itself does not need keepAlives, right?

---

## 2020-11-11 13:42:58 - issues channel

**Jorge**

Is it needed to send keepAlive requests to keep the trading client running while using MarketStreaming?

---

## 2020-11-10 14:33:34 - general channel

**Remi**

I was looking at the streaming [https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py|example](https://github.com/liampauling/betfair/blob/master/examples/examplestreaming.py|example).  Does the  `stream.subscribe_to_markets`  subscribe once to the current live markets that meet the specification, or does it continue to subscribe to new markets that also meet the specification as more markets come online?

---

## 2020-11-08 10:46:47 - random channel

**Mo**

It sounds like you don't understand what the historic data is, which is a reproduction of what you would have received live while streaming. Whether you record the stream yourself (scraping) or download the corresponding historic data, the result should be (essentially) the same

---

## 2020-11-08 02:53:55 - random channel

**stephencornelius**

Ive played around with the Betfair API for sometime, since their previous REST API. Im now looking into developing a full app with strategies etc. using the streaming API and I'm conscious ill need market data in order to backtest / validate them. I recall reading about the REST API that if you requested price data but were not placing bets that you would be disconnected.



With the streaming API is it possible to subscribe to price data without any repercussions? E.g. The first thing I was planning to create was an automated method of subscribing to all prices for all markets for a given event from each market creation until market closure with a result in order to build as much data as quickly as possible while I develop the rest of my app.



Firstly, is this allowed? If so can you use the Live API key or is only the delayed key allowed?

If not allowed can anyone advise the best way to get as much data as possible for backtesting (Would rather not use the historical data as I dont think its either live or complete / rich enough for me). Feel free to point me to answers if this has already been asked but I was unable to locate.



Thanks in advance

---

## 2020-11-06 06:53:14 - strategies channel

**RichL**

It’s possible, but certainly not reliable, to even do it on the historic free data / lagged live data.

---

## 2020-11-05 10:47:28 - general channel

**Jorge**

Is it a good practice to use the delayed AppKey when testing bots and only using the live AppKey when trading considerable amounts? Or do you guys just use the live AppKey all the time and there is no problem about that? I found that for pre-game strategies the delayed key does not have any delay on getting the odds/volume.

---

## 2020-11-05 10:37:08 - random channel

**Alex A**

But with lawsuits and recounts, it could be weeks before this is resolved.

---

## 2020-11-05 09:41:11 - random channel

**Alessio**

AWS will not go into a price war soon

---

## 2020-11-05 08:23:13 - random channel

**Alessio**

because it's a live subscribed API, where you first list events, then subscribe to some, then get updates in a streaming way. but it's in JS, so for now i'm zeromq-ing the updates in, but it's a bit hard to mock into testing without making things super-ugly with IF TEST everywhere

---

## 2020-11-05 08:13:59 - random channel

**Alessio**

and the fact that i want to integrate external souces, which makes deployment on a dfferent machine complicated

---

## 2020-11-05 08:02:24 - random channel

**liam**

I follow gitops, merge to master, test, build, push to ECR and then some lambdas to deploy to servers based on a json config file. The beauty of this is that I can spend 5 minutes fixing a bug and merge to master and that’s it, CI/CD is then automated with a few slack notifications on success / error.



I deploy about 3/4 times a week due to framework updates, strategy adjustments or new strategies for testing. I have about 15 instances running during the day at the moment each running 1/2 instances of my framework. 



RDS for my API and data lake with API Gateway used for a small django app.

---

## 2020-11-04 22:40:18 - random channel

**Misha**

I also run from one server, which has a suite of other applications, a database, runs a web server, etc. I take snapshots of the server as backup and if I need another one

---

## 2020-11-04 22:24:31 - random channel

**birchy**

Once running, bots are left untouched for months, so I'm intrigued at what point automated deployment is necessary?

---

## 2020-11-04 17:00:16 - random channel

**river_shah**

[@U4H19D1D2](@U4H19D1D2) you just run your own private registry in AWS?

---

## 2020-11-04 16:59:31 - random channel

**Jonjonjon**

If I have an image with BFLW, Flumine, and a bit of my own code, how large is it likely to be? And what will it likely cost to host on AWS?

---

## 2020-11-04 16:49:40 - random channel

**Jonjonjon**

Would there be much difference between hosting a docker image on AWS or Dockerhub?

---

## 2020-11-02 16:30:58 - general channel

**Remi**

I don’t think any laws have changed. I was not doing matched betting but they did give me a bunch €50 free bets that I did use.

---

## 2020-10-31 16:41:25 - strategies channel

**Alessio**

i have a question i couldn't easily find an answer and it's a bit difficult to test live ..

---

## 2020-10-29 19:43:59 - general channel

**birchy**

If going down the S3 route, assuming you have `~/.aws/credentials` , you can mount it as a local network drive and use like a normal file system using `s3fs` v1.85+:

```$ mkdir ~/AWS_S3

$ s3fs bucketname ~/AWS_S3```

---

## 2020-10-29 19:43:04 - general channel

**dan2002**

[@U4H19D1D2](@U4H19D1D2) I only run the example strategy on paper trading mode and backtesting mode, I don't see INVALID_PROFIT_RATIO, maybe when i go live it will show in the log. What puzzled me is that it seems flumine has changed the size reduction from 1,51 to 2 as i can see the whole £2 order has been cancelled. Do you know where flumine has make this adjustment?

---

## 2020-10-29 07:56:15 - issues channel

**JonM**

No, going to give that a try now.  I think I've narrowed it down to a problem with just yesterday's account statement.  I moved some funds to my poker wallet (just so they're out of the way as I live test a new strategy).  Is there a known issue at all with this?

---

## 2020-10-28 20:27:02 - general channel

**AP**

Was Watchtower the Python package recommended for logging to AWS cloudwatch?

---

## 2020-10-27 16:55:43 - general channel

**jhaa**

do you just do pip install betfairlightweight -U on the live system or is there anything that will break?

---

## 2020-10-26 11:33:42 - general channel

**Lee**

i only use the logging control for running live. when backtesting i get the orders the same way as this example and the output is consistent. [https://github.com/liampauling/flumine/blob/master/examples/backtest.py#L34](https://github.com/liampauling/flumine/blob/master/examples/backtest.py#L34)

---

## 2020-10-21 22:37:39 - general channel

**ricky**

I am running home servers via Vmware. VM allocated 4 processes and 8G of RAM.

The issue was the process handling data recording via flumine was very high (over 100%).

I will upgrade to latest version of flumine and give a try tomorrow. other possibility may be filesystem issue.

---

## 2020-10-20 18:01:45 - general channel

**dan2002**

i am just wondering if anyone has used flumine for live trading?

---

## 2020-10-20 16:25:13 - general channel

**D**

You're welcome [@U9JHLMZB4](@U9JHLMZB4). Sounds like your project is similar to something I did, whereby a step function schedules a lambda to run at x minutes before market_start_time. Must be the cheapest way to run BFLW on AWS!

---

## 2020-10-20 16:21:04 - general channel

**Peter**

Many thanks [@URMM9463X](@URMM9463X) for the earlier pointer. I have a fully working version now much more quickly than I expected. So big hat tip to you. I have a number of start of day data collection scripts and strategies that run close to an event's start that I suspect will be more efficient run as lambdas than on a server that's mostly idle for the rest of the time. My concern over the python version, was about ongoing maintenance rather than importing within the lambda, which as you say is trivial, but upon reflection, I would expect to create new layers for upgrades to BFL way more frequently than I upgrade the underlying version of Python, so not really an issue.

---

## 2020-10-20 15:22:59 - general channel

**Peter**

Second issue is how to handle the certs and key. Ideally I'd pass them in as strings from the environment variables. But BFL  appears to allow me only to specify a folder or individual files. So I'm thinking maybe my Lambda function needs to be uploaded as a zip file containing the code and a certs folder, but that's extra overhead for each update and I'm not wild about deploying lots of copies of my certs. Alternatively I could try to make the certs folder a layer too.

---

## 2020-10-20 10:51:44 - general channel

**D**

I have used a layer for BetfairLightweight. I didn't use docker, I created it on an EC2 instance. Using this to install locally before creating the zip file for upload: pip3 install betfairlightweight -t ./lambda --upgrade

---

## 2020-10-19 20:32:42 - issues channel

**birchy**

Thanks [@U4H19D1D2](@U4H19D1D2), that's not a big issue. I presume it will be present when running live?

Also, I notice all of the examples use a single filepath for the list `strategy.market_filter['markets']` . I've been feeding it with a full list of 2000+ paths, which seems to be working fine, but I just wanted to confirm that I'm doing that correctly?

---

## 2020-10-17 23:28:09 - issues channel

**AP**

I am deploying a function app (serverless) on Azure and got the following error related to orjson when deploying:



```Collecting orjson==3.4.0; sys_platform == "darwin" or sys_platform == "linux" (from betfairlightweight-&gt;-r requirements.txt (line 14))

  Downloading [https://files.pythonhosted.org/packages/ca/ab/cece004aaae000741d059dcb9b1f6f62a6a5ecb2e11b3ecca09e6180c327/orjson-3.4.0.tar.gz](https://files.pythonhosted.org/packages/ca/ab/cece004aaae000741d059dcb9b1f6f62a6a5ecb2e11b3ecca09e6180c327/orjson-3.4.0.tar.gz) (655kB)

  Installing build dependencies: started

  Installing build dependencies: finished with status 'done'

    Complete output from command python setup.py egg_info:

    Traceback (most recent call last):

      File "&lt;string&gt;", line 1, in &lt;module&gt;

      File "/opt/hostedtoolcache/Python/3.6.12/x64/lib/python3.6/tokenize.py", line 452, in open

        buffer = _builtin_open(filename, 'rb')

    FileNotFoundError: [Errno 2] No such file or directory: '/tmp/pip-install-muey9qtb/orjson/setup.py'```

---

## 2020-10-15 18:43:22 - random channel

**Alessio**

not an aws, rather digitalocean, but yeah, ubuntu

---

## 2020-10-15 18:23:37 - random channel

**birchy**

Taking this chat on a slight detour...what flavour of Linux do you AWS people use? I've always been a Debian/xfce fan on my own machines but have more recently started using Mint as it works out if the box, whereas Debian is a bit more "hardcore" and sometimes requires tinkering to get certain hardware drivers working correctly. As I've only recently started using AWS, I opted for Ubuntu on my one and only instance, but am wondering how Amazon Linux performs? Also, I've read that CentOS runs faster than most other distros?

---

## 2020-10-15 16:09:50 - general channel

**Jonjonjon**

When backtesting with flumine, the `market_book` and `live_market` are received on each update by `process_market_book`.



In that data, how can I tell whether or not:



• the market will go inplay? This is different to the `inplay` flag that defines whether or not it is actually inplay.

• whether or not it the bsp is available?

---

## 2020-10-15 00:03:39 - general channel

**Charlie 303**

BF API n00b here; Getting thing started, I applied and received live app key. Then discovered searching through BF forums that you can increase market subscription limit (200-&gt;1000) by simply asking BF support, which is free of charge.  Didnt see any official documentation on that, but that indeed worked.  Curious if there are other features/limits that can be requested/applied for, either free or premium?

---

## 2020-10-14 20:56:11 - issues channel

**Mo**

Just use your live key and streaming

---

## 2020-10-14 13:29:19 - strategies channel

**ricky**

[@U017A7TRF6E](@U017A7TRF6E) I tried the live chat,  they alway told me "i can not give you an exact time frame as it is conducted by another department". The problem was i dont know where to complaint. i will give a try again today.

---

## 2020-10-14 13:13:40 - strategies channel

**Twatter**

[@U0160KZB6QP](@U0160KZB6QP) I'd first go down the route of getting in touch with LiveChat and threaten an IBAS complaint since there's no reason why a withdrawal should not have completed in that time and anything beyond 10 working days is not on. The only time a really long delay has happened to me is when I had a card expired and I issued a withdraw to expired card and it was stuck in limbo - but was still resolved after contacting LC

---

## 2020-10-13 18:15:46 - general channel

**river_shah**

There is no buffering, you'll lose the updates. Hence record on robust infra (AWS)

---

## 2020-10-13 09:35:49 - issues channel

**birchy**

Yeah, ordered by last modified date in AWS but new files not showing. Looped through my local backups and could see files from yesterday, etc, which are not showing in the AWS view.

---

## 2020-10-12 22:47:30 - issues channel

**birchy**

It's AWS. Have just looped through the files and am still getting new markets. Was going to delete my original message, but thought I'd leave it here in case anyone else has this issue...

---

## 2020-10-12 21:28:23 - issues channel

**Unknown**

Any ideas why MarketRecorder seems to have stopped loading to S3? I've noticed this several times now and restarting the bot fixes it. Still running at present but looks like it's only saved one file since 3rd October. Logging is set to WARNING and there's nothing on there that rings any alarm bells. Haven't checked the actual files as I'm not on the PC right now. Or could it be AWS display being a bit wanky?

---

## 2020-10-12 17:52:43 - strategies channel

**birchy**

I have found a couple of inplay strategies via Flumine backtesting that are showing a positive return over 500 markets/~4000 bets. Going to test against a bigger sample, but am just wondering how backtesting compares to live betting? I've not (yet) run any live betting with Flumine as I'm taking cautious steps due to the sheer speed of streaming, which means a small mistake can quickly become expensive. What would you consider as a "goer"?

---

## 2020-10-12 14:43:20 - general channel

**river_shah**

for historic market catalogue support, I intend to use aws and maintain tables by querying every few minutes and upserting into rds. [@U4H19D1D2](@U4H19D1D2) [@UBS7QANF3](@UBS7QANF3) I believe you are doing this. would it be possible to add any example of this usage into flumine/examples please. still coming to grips with betfair market catalogue fields and want to ensure that i capture all essential catalogue data. thank you

---

## 2020-10-09 21:51:06 - issues channel

**Misha**

Live data is captured by you

---

## 2020-10-09 21:49:44 - issues channel

**Lee**

I'm not sure I agree [@U016535QCJ2](@U016535QCJ2), the historic data a lot of us use we record and is the same as live data. The exposure code run is the same either way (live or historic).

---

## 2020-10-09 21:47:26 - issues channel

**Misha**

It should only happen for historical data - not live data

---

## 2020-10-09 20:46:16 - random channel

**Jonjonjon**

When I run the flumine integration tests for the first time after starting Pycharm, I need to change the working directory from:



`/home/jon/PycharmProjects/flumine/`



to



`/home/jon/PycharmProjects/flumine/tests`



How can I top having to do this serveral times per day?

---

## 2020-10-08 22:40:57 - issues channel

**Newbie99**

```{"asctime": "2020-10-08 21:05:56,243", "levelname": "ERROR", "message": "_get_cleared_markets error", "exc_info": "Traceback (most recent call last):\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 381, in _make_request\n    self._validate_conn(conn)\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 978, in _validate_conn\n    conn.connect()\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connection.py\", line 362, in connect\n    self.sock = ssl_wrap_socket(\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 384, in ssl_wrap_socket\n    return context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"D:\\Python38\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"D:\\Python38\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"D:\\Python38\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nsocket.timeout: _ssl.c:1106: The handshake operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Python38\\lib\\site-packages\\requests\\adapters.py\", line 439, in send\n    resp = conn.urlopen(\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 726, in urlopen\n    retries = retries.increment(\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\util\\retry.py\", line 403, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\packages\\six.py\", line 735, in reraise\n    raise value\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 670, in urlopen\n    httplib_response = self._make_request(\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 384, in _make_request\n    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 335, in _raise_timeout\n    raise ReadTimeoutError(\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=3.05)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Python38\\lib\\site-packages\\betfairlightweight\\endpoints\\baseendpoint.py\", line 36, in request\n    response = [http://session.post|session.post](http://session.post|session.post)(\n  File \"D:\\Python38\\lib\\site-packages\\requests\\api.py\", line 119, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File \"D:\\Python38\\lib\\site-packages\\requests\\api.py\", line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"D:\\Python38\\lib\\site-packages\\requests\\sessions.py\", line 530, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"D:\\Python38\\lib\\site-packages\\requests\\sessions.py\", line 643, in send\n    r = adapter.send(request, **kwargs)\n  File \"D:\\Python38\\lib\\site-packages\\requests\\adapters.py\", line 529, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=3.05)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Python38\\lib\\site-packages\\flumine\\worker.py\", line 200, in _get_cleared_market\n    cleared_markets = betting_client.betting.list_cleared_orders(\n  File \"D:\\Python38\\lib\\site-packages\\betfairlightweight\\endpoints\\betting.py\", line 432, in list_cleared_orders\n    (response, response_json, elapsed_time) = self.request(method, params, session)\n  File \"D:\\Python38\\lib\\site-packages\\betfairlightweight\\endpoints\\baseendpoint.py\", line 45, in request\n    raise APIError(None, method, params, e)\nbetfairlightweight.exceptions.APIError: SportsAPING/v1.0/listClearedOrders \nParams: {'betStatus': 'SETTLED', 'marketIds': ['1.173991824'], 'customerStrategyRefs': ['BlueOcean'], 'settledDateRange': {'from': None, 'to': None}, 'groupBy': 'MARKET'} \nException: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=3.05)", "trading_function": "list_cleared_orders", "response": "SportsAPING/v1.0/listClearedOrders \nParams: {'betStatus': 'SETTLED', 'marketIds': ['1.173991824'], 'customerStrategyRefs': ['BlueOcean'], 'settledDateRange': {'from': None, 'to': None}, 'groupBy': 'MARKET'} \nException: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=3.05)"}



{"asctime": "2020-10-08 21:12:32,721", "levelname": "ERROR", "message": "get_account_funds error", "exc_info": "Traceback (most recent call last):\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 381, in _make_request\n    self._validate_conn(conn)\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 978, in _validate_conn\n    conn.connect()\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connection.py\", line 362, in connect\n    self.sock = ssl_wrap_socket(\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\util\\ssl_.py\", line 384, in ssl_wrap_socket\n    return context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"D:\\Python38\\lib\\ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"D:\\Python38\\lib\\ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"D:\\Python38\\lib\\ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nsocket.timeout: _ssl.c:1106: The handshake operation timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Python38\\lib\\site-packages\\requests\\adapters.py\", line 439, in send\n    resp = conn.urlopen(\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 726, in urlopen\n    retries = retries.increment(\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\util\\retry.py\", line 403, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\packages\\six.py\", line 735, in reraise\n    raise value\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 670, in urlopen\n    httplib_response = self._make_request(\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 384, in _make_request\n    self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n  File \"D:\\Python38\\lib\\site-packages\\urllib3\\connectionpool.py\", line 335, in _raise_timeout\n    raise ReadTimeoutError(\nurllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=6.05)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Python38\\lib\\site-packages\\betfairlightweight\\endpoints\\baseendpoint.py\", line 36, in request\n    response = [http://session.post|session.post](http://session.post|session.post)(\n  File \"D:\\Python38\\lib\\site-packages\\requests\\api.py\", line 119, in post\n    return request('post', url, data=data, json=json, **kwargs)\n  File \"D:\\Python38\\lib\\site-packages\\requests\\api.py\", line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"D:\\Python38\\lib\\site-packages\\requests\\sessions.py\", line 530, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"D:\\Python38\\lib\\site-packages\\requests\\sessions.py\", line 643, in send\n    r = adapter.send(request, **kwargs)\n  File \"D:\\Python38\\lib\\site-packages\\requests\\adapters.py\", line 529, in send\n    raise ReadTimeout(e, request=request)\nrequests.exceptions.ReadTimeout: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=6.05)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"D:\\Python38\\lib\\site-packages\\flumine\\clients\\betfairclient.py\", line 56, in _get_account_funds\n    return self.betting_client.account.get_account_funds()\n  File \"D:\\Python38\\lib\\site-packages\\betfairlightweight\\endpoints\\account.py\", line 35, in get_account_funds\n    (response, response_json, elapsed_time) = self.request(method, params, session)\n  File \"D:\\Python38\\lib\\site-packages\\betfairlightweight\\endpoints\\baseendpoint.py\", line 45, in request\n    raise APIError(None, method, params, e)\nbetfairlightweight.exceptions.APIError: AccountAPING/v1.0/getAccountFunds \nParams: {} \nException: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=6.05)", "error": "AccountAPING/v1.0/getAccountFunds \nParams: {} \nException: HTTPSConnectionPool(host='[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com)', port=443): Read timed out. (read timeout=6.05)"}```

---

## 2020-10-08 22:40:57 - issues channel

**Newbie99**



Have been getting these errors over the last 24 hours (using Flumine live), has anyone else seen similar, or is it just likely to be a timeout caused by a network issue from my end?

---

## 2020-10-08 19:25:31 - issues channel

**Unknown**

I was looking for memory leaks in my bot using tracemalloc, and found (I think) some big ones in bflw. This is a little over my head, and I'm not sure what I'm looking at. But basicaly I made empty bflw bot that only subscribes to all UK/AU/US racing markets, also to order stream from my live bots (~50k bets a week) and counts currenty open markets. Ran this for a 7-8 days, dumping memory snapshots every 10minutes, graphed it. Red line is resident memory, blue currenty subscribed open markets. bflw 2.8.0. Hope this helps

---

## 2020-10-07 14:21:49 - random channel

**Newbie99**

one other idea...perhaps a quick win, maybe introduce a new parameter, so that when flumine (by default) checks to see if an existing order is in place, that can be over ridden easily (I know of course people can tweak the code, but its neater if its done at source), so it allows one back and one lay or even x of each (as opposed to just 1 live order at any given time), that would allow people that search for value on boht sides to leave at least 1 back and 1 lay simultaneously and if anyone was inclined to trade, it would also allow that.



Does that make sense and sound vaguely sensible?

---

## 2020-10-07 08:20:58 - issues channel

**liam**

only within flumine, the cache within bflw lives

---

## 2020-10-05 18:32:32 - general channel

**liam**

cheers, btw 1.13.0 released today, includes a [https://github.com/liampauling/flumine/commit/edd6735de54c46a9c91c76f96dcd6de8ca811b36|change](https://github.com/liampauling/flumine/commit/edd6735de54c46a9c91c76f96dcd6de8ca811b36|change) to backtesting that may have an impact on differences you were seeing between live as well as much improved [https://github.com/liampauling/flumine/pull/283|control](https://github.com/liampauling/flumine/pull/283|control) thanks to [@UPMUFSGCR](@UPMUFSGCR)

---

## 2020-10-05 15:17:29 - general channel

**ricky**

Thanks for your information, i can see MarketRecorder class did save each markets in individual files.

I have two more questions:

1) I use data visualization tools to view the live price data per matches, is it any way to identity which group of market id (files) have the same event name?

2) i can see there is background worker pulling list_market_catalogue every 60s, if i want to get event name from market_catalogues, how can i access market_catalogues data from class PriceRecorder? ( e.g maybe inside process_market_book)

---

## 2020-10-04 16:32:32 - strategies channel

**Mo**

The service you want is the same: EC2. You just want a different instance type. It depends how much you want to pay and how quickly you want the backtests to run. Personally I would not scale with multiple instances but just use a bigger instance type i.e. one with more vCPUs. Look at the M5 family. 

---

## 2020-10-04 16:14:12 - strategies channel

**birchy**

I don't want to use my main workstation for backtesting as I need it for other development work. Under normal use, it runs at around 50c and I've never had any issues with it before. My main aim is to find a way to run multiple backtests on multiple machines. Do AWS offer anything that is designed for high CPU use? They have so many services available, that I'm really struggling to identify what I need. Just read that there's a T2/T3 "unlimited" option but I don't fully understand how the burst credits work.

---

## 2020-10-04 15:14:59 - strategies channel

**birchy**

For backtesting, what are you guys using hardware-wise? I'm currently using my rather old desktop PC, but had to put a sleep(0.001) in the code due to multiprocessing all 4 cores at 100% which pushed the CPU temperature to 105C and caused a hard shutdown. Obviously the cooling fans (4x80mm &amp; 1x120mm) are not sufficient, but I'm a little reluctant to start upgrading cooling on a machine that is mainly a workstation and is otherwise reliable. I'm wondering if AWS offer anything reasonably priced? Not too bothered about 'fast' backtesting, but would like to be able to run multiple backtest strategies concurrently. Would an AWS micro or lightsail instance be sufficient?

---

## 2020-10-02 07:58:37 - random channel

**liam**

I know but 



`If any candidate withdraws for any reason, including death, all bets on the market will stand and be settled as per the defined rules (Updated - 20/06/2020)`

---

## 2020-09-25 17:13:59 - issues channel

**D**

that was my next assumption :sweat_smile:  i’m running this on Mac at the minute, My project is living at `~/Apps/Betfair` so `/` is going for my disk root, which rightfully gives me a read only error. :thinking_face: do i pu them somewhere like `/etc/ssl` and use that as the cert path? I’ve seen elsewhere having these stored in environment files would make sense when time come to push it live

---

## 2020-09-24 18:29:51 - issues channel

**Newbie99**

[@U013R0E7SUV](@U013R0E7SUV) are you using AWS by chance? I'm now getting the same error, with crazy latency times on a t3.mico, on a t2.micro running an older version no issues it seems (also locally no issues).

---

## 2020-09-24 12:59:35 - random channel

**Newbie99**

Haven't been able to replicate yet, so it may be the case there was actually no problem after all...however I came across something else that (I think) is worth a thought.



When an order is replaced, obviously it cancels, then places a new order, sometimes (possibly every time I guess, but I've only noticed it on occasions) this results in 0 live orders being reported, even though in reality (I would argue) it should remain at 1 throughout the process.



I use live orders as a trigger (i.e. when all done, send the next lot down), so its easy enough to workaround, but (just thinking out loud), should the logic be tweaked show that order as live throughout the entire replace process, rather than the current logic?

---

## 2020-09-22 03:30:39 - random channel

**Chris**

Second - for storage I have no idea what to get. AWS seems to define speed in IOPS, which I am researching now but haven't seen before. If I am recording all ticks + basic logging is 1k IOPS enough? (common consumer SSDs have ~50 to 100k). Is the default of 100 enough (similar to what a common HD has)?

---

## 2020-09-18 10:17:46 - issues channel

**liam**

I get a few errors on AWS, I think its betfair tbh and yes it will reconnect

---

## 2020-09-17 14:08:04 - random channel

**agberk**

[@U4H19D1D2](@U4H19D1D2) RE: home server - I've got a pi (not sure what version) that I was planning to use as a home media server and had loftier goals of maybe self hosting email and other services; I forget the exact nature of the problem but it turned out it wasn't powerful enough and the architecture didn't support certain libraries. Haven't done anything since but the conclusion I came to was I would just get a full on PC in a small form factor - maybe something like this [https://www.scan.co.uk/shop/computer-hardware/desktops-home-office/barebone-intel-nucs](https://www.scan.co.uk/shop/computer-hardware/desktops-home-office/barebone-intel-nucs) - of course there's some benefits that cloud brings but no substitution for being fully in control :smile:

---

## 2020-09-16 21:43:45 - issues channel

**Chris**

Hey guys -



I just had an error that originated from there being an order with status EXECUTION_COMPLETE in the market.blotter.live_orders object. My rough expectation was that the only orders I would see in the market.blotter.live_orders object would have a pending status (PENDING, CANCELLING, UPDATING, REPLACING) or the EXECUTABLE status.



Are my expectations wrong here (execution_complete is a valid status for an order in the live_orders object)? Or, was this error a timing / race condition issue that I should be accounting for (i.e. I tried to use the live_orders object before the order could be removed from the live orders object)?

---

## 2020-09-16 20:56:33 - general channel

**Dave**

evening all - looking to set up some higher-freq strategies in the coming months so have deployed the classic flumine market recorder. Qq about restarting on failure - is there any issue with writing the MD to the same file it was writing to prior to any crash/failure? I noticed the example flumine market recorder creates a new directory each time it starts up, so could be a pain when it comes to stitching MD from a previous run with MD recorded from the subsequent run of the recorder (for the same given market of course). I imagine there'll just be extra market definition entries, but other than that no issue?

---

## 2020-09-15 22:41:23 - random channel

**Alex B**

you could just turn off an EC2 instance when you’re not using it, then you’d only be paying for the associated EBS volume(s)

---

## 2020-09-15 20:16:08 - random channel

**liam**

Hmm the data will be from an aws db so maybe I should just use ec2 but just seems such a waste to have it on all be time 

---

## 2020-09-14 13:30:40 - general channel

**agberk**

I've read plenty of criticism with this page and while some might be technically correct, I feel like this is a good guide to explain to someone in your position what the different building blocks are actually for even if there's more nuance once you get acquainted: [https://expeditedsecurity.com/aws-in-plain-english/](https://expeditedsecurity.com/aws-in-plain-english/)

---

## 2020-09-14 07:01:21 - random channel

**Peter**

[@UFTBRB3F1](@UFTBRB3F1) I think you're going to be out of luck. This for the 18:15 works, `race_cards = trading.race_card.get_race_result(market_ids=['1.172974372'])` but stick '1.172950042' in there instead and it's still throwing an http 500 (server error) response. So def. a problem at the Betfair end and one likely to affect everybody.

---

## 2020-09-13 13:34:40 - general channel

**Taking Value**

[https://www.youtube.com/watch?v=WE303yFWfV4](https://www.youtube.com/watch?v=WE303yFWfV4) Was also useful - walk through of creating an ec2 instance and uploading files to it.

---

## 2020-09-13 13:31:40 - general channel

**D C**

Cheers [@UBS7QANF3](@UBS7QANF3) . That is a great concise explanation. So could I effectively just use EC2 as it if were a remote linux server and install whatever I need on it and go from there?

---

## 2020-09-13 13:29:29 - general channel

**Mo**

EC2: Elastic cloud compute. It lets you provision servers on demand. They can be Windows, Linux, with multiple distro options, or your own saved configuration. The hardware resources (CPUs and memory) can be as small or large as you like. There are specialist options for e.g. GPUs, FPGAs etc.



S3: Simple storage service. A way to store files for quick and easy access.



Lambda: lets you run serverless code e.g. a scraping process that runs hourly and saves some data to S3

---

## 2020-09-13 13:29:04 - general channel

**Taking Value**

Yea you are where I was at a week ago (had no idea what EC2 meant - its elastic cloud compute or something like that)

---

## 2020-09-13 13:26:58 - general channel

**D C**

I need an even more basic idea than that though. What actually IS EC2 (OS, fancy name, product tier label etc)? Do you do access like ssh login? Is it linux based environment? Are there OS type choices like VPS? Someone told me I need Lambda to run a bot - no idea what that is or if it is true. I need the real idiot's guide introduction and I guess youtube is best place to start. I just favour written resources but in this instance maybe a video is best.

---

## 2020-09-13 13:23:04 - general channel

**Taking Value**

My first concern was -If I an signing up to this how much does it cost? But actually you can do the whole thing for free and only incur costs when you have stored  data (even then its super cheap) or if you want a better equipped ec2 instance than the free tier (I can't see why you would)

---

## 2020-09-13 13:13:29 - general channel

**Taking Value**

Tonne of good youtube videos on it. I had no idea what AWS was a week ago. I'm not far off running data collection and maintenance on it now.

---

## 2020-09-13 13:08:56 - general channel

**D C**

On the topic of AWS can anyone suggest a really good idiot's guide to it? When I say idiot, I mean someone who is comfortable with develpoment and sysadmin but no experience with cloud whatsoever. Something short and sweet that explains what EC2, S3, Lambda etc are. I know I can pick the bones of it starting with amazon pages but as there are lots here who use this stuff I wondered if anyone can suggest a better starting point? I have put this off for way too long how and its time to roll my sleeves up and get hands dirty.

---

## 2020-09-13 12:45:13 - general channel

**liam**

The speed of being able to develop / deploy in python is more important to me 

---

## 2020-09-13 12:39:02 - general channel

**Mo**

I’m a bit confused - sounds like you can already use awscli with it picking up credentials from the credentials file so you should already be able to use boto3 without setting the credentials in code

---

## 2020-09-13 12:36:19 - general channel

**Taking Value**

[@UBS7QANF3](@UBS7QANF3) the boto3 example code I am considering implementing assigns the public and private keys to variables.Obviously I don't want to do that. I have configured s3 using I am roles so my ec2 instance has access and I can use console commands to get it to send files across. I wanted to write a python script to do it but basically don't know how or where I should be looking to learn.

---

## 2020-09-13 12:29:25 - general channel

**D**

Hard-coding the key pair is quite a crude way of passing them to boto3, there are several other ways that are better. Following on from Mo's comment, if you type *`aws configure`* at the command prompt you can set up the credentials once only.

---

## 2020-09-13 12:19:13 - general channel

**Taking Value**

For any one using AWS  - Do you know is there a way to move a file from EC2 to S3 using python but using IAM roles as opposed to boto3 where the key pair has to be hard-coded. I was going down the boto3 route but read its best practice to use IAM roles (which I can do via console but don't know how to do it via python).

---

## 2020-09-11 21:24:48 - general channel

**Chris**

good point - not super worried but mine is bad now so I definitely have to buy server space somewhere.



I know if you are really trying to squeeze out performance you have to move to a compiled language like C++ and that's what most HFT firms use but do you have any idea how big of a difference it actually makes? For example, do you have stats on your average time difference between receiving a message and trading on that info? Is it &lt;10 ms or is it more in the neighborhood of 50 ms? I was assuming it was &lt; 10 ms but I am genuinely not sure

---

## 2020-09-11 21:10:58 - general channel

**Chris**

Does anyone know where the bf servers are actually located? I found a thread from 2014 that seemed to imply you could get 5 ms ping with some instances of aws

---

## 2020-09-11 21:08:05 - general channel

**Chris**

Alright - thanks I will take a look at both. I am leaning toward AWS as I believe [@U4H19D1D2](@U4H19D1D2) uses some of their auxiliary services as well but would be interested to hear if there are specialty host services specifically for trading on the exchange

---

## 2020-09-11 19:12:27 - general channel

**liam**

Yep AWS here 

---

## 2020-09-11 19:12:11 - general channel

**jhaa**

i usually get like 10ms-20ms. I used to use ovh but they messed up their geo location conf and now I used quickhostuk. most people use aws in here i think

---

## 2020-09-11 17:25:30 - general channel

**jhaa**

I have a newish bot running that uses the streaming. It runs fine for a few days without any flaws and then it goes beserk and trades markets inplay even though it is build to not to do that and it places bets multiple times because it assumes that they were not places yet.



I had the bet placement issue previously and sort of fixed that by removing ujson.



Any idea what might cause this? It sort of looks like something is wrong further down the software stack or I get wrong stuff through the stream

---

## 2020-09-11 15:57:46 - general channel

**Chris**

I see the correct oc / mb message make it in the door and the corresponding market (1.72246112) get added to the OrderStream and Stream. However, I don't see that market get added to flumine.baseflumine until a later mc message that gets processed _after_ the oc message. Is it possible that the implementation depends on the mc message arriving first?



2020-09-11 14:44:08,606 - flumine.baseflumine - INFO - Adding trading control ORDER_VALIDATION

2020-09-11 14:44:08,606 - flumine.baseflumine - INFO - Adding trading control STRATEGY_EXPOSURE

2020-09-11 14:44:08,607 - flumine.baseflumine - INFO - Adding client control MAX_ORDER_COUNT

2020-09-11 14:44:08,607 - flumine.baseflumine - INFO - Adding strategy NaiveMarketMakerStrategy

2020-09-11 14:44:08,607 - flumine.streams.streams - INFO - Creating new &lt;class 'flumine.streams.marketstream.MarketStream'&gt; (2000) for strategy NaiveMarketMakerStrategy

2020-09-11 14:44:08,607 - flumine.baseflumine - INFO - Starting flumine

2020-09-11 14:44:08,609 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): [http://identitysso-cert.betfair.com:443|identitysso-cert.betfair.com:443](http://identitysso-cert.betfair.com:443|identitysso-cert.betfair.com:443)

2020-09-11 14:44:09,372 - urllib3.connectionpool - DEBUG - [https://identitysso-cert.betfair.com:443](https://identitysso-cert.betfair.com:443) "POST /api/certlogin HTTP/1.1" 200 87

2020-09-11 14:44:09,377 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): [http://api.betfair.com:443|api.betfair.com:443](http://api.betfair.com:443|api.betfair.com:443)

2020-09-11 14:44:10,085 - urllib3.connectionpool - DEBUG - [https://api.betfair.com:443](https://api.betfair.com:443) "POST /exchange/account/json-rpc/v1 HTTP/1.1" 200 176

2020-09-11 14:44:10,088 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): [http://api.betfair.com:443|api.betfair.com:443](http://api.betfair.com:443|api.betfair.com:443)

2020-09-11 14:44:10,653 - urllib3.connectionpool - DEBUG - [https://api.betfair.com:443](https://api.betfair.com:443) "POST /exchange/account/json-rpc/v1 HTTP/1.1" 200 158

2020-09-11 14:44:10,656 - flumine.worker - INFO - BackgroundWorker keep_alive starting

2020-09-11 14:44:10,656 - flumine.worker - DEBUG - BackgroundWorker keep_alive executing

2020-09-11 14:44:10,656 - flumine.worker - INFO - BackgroundWorker poll_account_balance starting

2020-09-11 14:44:10,656 - flumine.worker - INFO - BackgroundWorker poll_market_catalogue starting

2020-09-11 14:44:10,657 - flumine.worker - INFO - BackgroundWorker poll_cleared_orders starting

2020-09-11 14:44:10,657 - flumine.streams.streams - INFO - Starting streams..

2020-09-11 14:44:10,657 - flumine.streams.orderstream - INFO - Starting OrderStream

2020-09-11 14:44:10,657 - flumine.streams.orderstream - INFO - Starting output_thread &lt;Thread(OrderStream_output_thread, initial daemon)&gt;

2020-09-11 14:44:10,658 - betfairlightweight.streaming.listener - INFO - Register: orderSubscription 1001

2020-09-11 14:44:10,658 - betfairlightweight.streaming.stream - INFO - [Stream: 1001]: "OrderStream" created

2020-09-11 14:44:10,658 - flumine.streams.marketstream - INFO - Starting MarketStream

2020-09-11 14:44:10,658 - flumine.streams.marketstream - INFO - Starting output_thread &lt;Thread(MarketStream_output_thread, initial daemon)&gt;

2020-09-11 14:44:10,659 - betfairlightweight.streaming.listener - INFO - Register: marketSubscription 2001

2020-09-11 14:44:10,659 - betfairlightweight.streaming.stream - INFO - [Stream: 2001]: "MarketStream" created

starting strategy 'ExampleStrategy'

2020-09-11 14:44:11,333 - betfairlightweight.streaming.betfairstream - DEBUG - [Subscription: 2002] Sending: '{"op": "authentication", "id": 2002, "appKey": "bOP9A5Vmp8n8K48d", "session": "XIlFe2j97G/m11sC7MXvyXllXn+/pQA3q/6nRSlqnf8="}\r\n'

2020-09-11 14:44:11,333 - betfairlightweight.streaming.betfairstream - DEBUG - [Subscription: 2002] Sending: '{"op": "marketSubscription", "id": 2001, "marketFilter": {"marketIds": ["1.172246112"]}, "marketDataFilter": {"fields": ["EX_ALL_OFFERS", "EX_TRADED", "EX_TRADED_VOL", "EX_LTP", "EX_MARKET_DEF", "SP_TRADED", "SP_PROJECTED"]}, "initialClk": null, "clk": null, "conflateMs": null, "heartbeatMs": null, "segmentationEnabled": true}\r\n'

2020-09-11 14:44:11,335 - betfairlightweight.streaming.listener - INFO - [Connect: 2001]: connection_id: 103-110920144408-1944934

2020-09-11 14:44:11,335 - betfairlightweight.streaming.betfairstream - DEBUG - [Subscription: 1002] Sending: '{"op": "authentication", "id": 1002, "appKey": "bOP9A5Vmp8n8K48d", "session": "XIlFe2j97G/m11sC7MXvyXllXn+/pQA3q/6nRSlqnf8="}\r\n'

2020-09-11 14:44:11,335 - betfairlightweight.streaming.betfairstream - DEBUG - [Subscription: 1002] Sending: '{"op": "orderSubscription", "id": 1001, "orderFilter": {"includeOverallPosition": false, "customerStrategyRefs": ["DESKTOP-PUCIJ46"], "partitionMatchedByStrategyRef": true}, "initialClk": null, "clk": null, "conflateMs": null, "heartbeatMs": null, "segmentationEnabled": true}\r\n'

2020-09-11 14:44:11,336 - betfairlightweight.streaming.listener - INFO - [Connect: 1001]: connection_id: 104-110920144408-1957086

2020-09-11 14:44:11,455 - betfairlightweight.streaming.listener - INFO - [Subscription: 1002]: SUCCESS (9 connections available)

2020-09-11 14:44:11,455 - betfairlightweight.streaming.listener - INFO - [Subscription: 1001]: SUCCESS (9 connections available)

2020-09-11 14:44:11,463 - betfairlightweight.streaming.listener - INFO - [Subscription: 2002]: SUCCESS (8 connections available)

*2020-09-11 14:44:11,475 - betfairlightweight.streaming.listener - DEBUG - [Subscription: 1001]: SUB_IMAGE: {'op': 'ocm', 'id': 1001, 'initialClk': 'FI60s4gDFZzFy4EDFcDZ+IUDFMup9YoDFLfurYQD', 'clk': 'AAAAAAAAAAAAAA==', 'conflateMs': 0, 'heartbeatMs': 5000, 'pt': 1599835448156, 'ct': 'SUB_IMAGE', 'oc': [{'id': '1.172246112', 'orc': [{'fullImage': True, 'id': 56323, 'smc': {'DESKTOP-PUCIJ46': {'mb': [[1.31, 9], [1.3, 9]]}}}]}]}*

*2020-09-11 14:44:11,475 - betfairlightweight.streaming.stream - INFO - [OrderStream: 1001] 1.172246112 added, 1 markets in cache*

*2020-09-11 14:44:11,476 - betfairlightweight.streaming.stream - INFO - [Stream: 1001]: 1 oc added*

2020-09-11 14:44:11,479 - betfairlightweight.streaming.listener - INFO - [Subscription: 2001]: SUCCESS (8 connections available)

2020-09-11 14:44:11,503 - betfairlightweight.streaming.listener - DEBUG - [Subscription: 2001]: SUB_IMAGE: {'op': 'mcm', 'id': 2001, 'initialClk': 'whrdibCSB8wa//HplwfFGo7WiZkH', 'clk': 'AAAAAAAA', 'conflateMs': 0, 'heartbeatMs': 5000, 'pt': 1599835448182, 'ct': 'SUB_IMAGE', 'mc': [{'id': '1.172246112', 'marketDefinition': {'bspMarket': False, 'turnInPlayEnabled': True, 'persistenceEnabled': True, 'marketBaseRate': 5, 'eventId': '29969294', 'eventTypeId': '1', 'numberOfWinners': 1, 'bettingType': 'ODDS', 'marketType': 'MATCH_ODDS', 'marketTime': '2020-09-12T16:30:00.000Z', 'suspendTime': '2020-09-12T16:30:00.000Z', 'bspReconciled': False, 'complete': True, 'inPlay': False, 'crossMatching': True, 'runnersVoidable': False, 'numberOfActiveRunners': 3, 'betDelay': 0, 'status': 'OPEN', 'runners': [{'status': 'ACTIVE', 'sortPriority': 1, 'id': 56323}, {'status': 'ACTIVE', 'sortPriority': 2, 'id': 48317}, {'status': 'ACTIVE', 'sortPriority': 3, 'id': 58805}], 'regulators': ['MR_INT'], 'countryCode': 'GB', 'discountAllowed': True, 'timezone': 'GMT', 'openDate': '2020-09-12T16:30:00.000Z', 'version': 3356134545, 'priceLadderDefinition': {'type': 'CLASSIC'}}, 'rc': [{'atb': [[10, 422.04], [10.5, 556.23], [9.8, 141.82], [9.4, 2], [1.01, 73767.48], [9.6, 20], [1.04, 18699.25], [1.02, 62403.5], [1.03, 44624.06], [2, 1.47], [1.1, 10000], [1.05, 14000], [1.09, 10000], [1.08, 10000], [1.07, 14000], [1.06, 14000]], 'atl': [[11, 120.65], [11.5, 603.9], [12, 511.84], [12.5, 568.7], [15.5, 17.5], [13.5, 749.87], [1000, 7.9], [14, 422.21], [13, 490.67], [15, 117.5], [16, 2]], 'trd': [[11, 3916], [10.5, 1267.23], [11.5, 7465.42], [13.5, 60.63], [13, 168.73], [12.5, 5668.28], [12, 9072.82], [10, 22.17], [9.6, 10.32], [9.8, 29.56]], 'ltp': 11, 'tv': 27681.16, 'id': 48317}, {'atb': [[1.32, 8095.3], [1.29, 36848.2], [1.3, 27113.82], [1.31, 9545.35], [1.01, 148763.48], [1.27, 30772.3], [1.28, 34136.61], [1.04, 18699.25], [1.24, 30010.9], [1.22, 25824.06], [1.02, 137403.5], [1.25, 50201.35], [1.23, 20937.03], [1.26, 50513.1], [1.03, 59624.06], [1.21, 25], [1.09, 10100], [1.1, 10000], [1.08, 10000], [1.07, 14000], [1.06, 14000], [1.05, 14000]], 'atl': [[1.33, 1193.27], [1.34, 1675.23], [1.35, 2113.48], [1.4, 20], [1.38, 600], [1.36, 1436.29], [2, 11.85], [1.45, 6.53], [1.8, 3.66], [1000, 5.05], [10, 82]], 'trd': [[1.32, 13620.02], [1.33, 42973.12], [1.34, 1499.87], [1.31, 36034.48], [1.29, 31452.81], [1.3, 64317.62], [1.27, 3095.66], [1.28, 22549.18]], 'ltp': 1.32, 'tv': 215542.76, 'id': 56323}, {'atb': [[6.6, 103.43], [6.4, 449.32], [6.2, 499.94], [5.8, 2], [5.9, 139.33], [6, 464.6], [1.01, 43.45], [1.03, 2], [3.5, 5]], 'atl': [[6.8, 391.26], [7.2, 1193.74], [7, 1406.89], [7.8, 708.38], [1000, 7.9], [8, 677.37], [7.4, 1667.71], [7.6, 892.78], [950, 2.2], [13.5, 2]], 'trd': [[6.6, 4780.12], [6.8, 11467.19], [6.4, 1194.02], [6.2, 26.38], [7, 12286.69], [7.4, 1110.62], [7.2, 3942.68], [7.6, 127.79], [6, 8], [7.8, 0.2]], 'ltp': 6.6, 'tv': 34943.69, 'id': 58805}], 'img': True, 'tv': 278167.61}]}

2020-09-11 14:44:11,503 - betfairlightweight.streaming.stream - INFO - [MarketStream: 2001] 1.172246112 added, 1 markets in cache

2020-09-11 14:44:11,505 - betfairlightweight.streaming.stream - INFO - [Stream: 2001]: 1 mc added

2020-09-11 14:44:11,506 - flumine.baseflumine - INFO - Adding: 1.172246112 to markets

---

## 2020-09-09 14:36:43 - general channel

**liam**

1. Yes an order is live if betfair consider it EXECUTABLE ie. unmatched 

2. All orders will be in the blotter, you can get your strategy orders from that function you have mentioned or the exposure using `selection_exposure` the plan is to add more helpful functions [https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/markets/blotter.py#L96|here](https://github.com/liampauling/flumine/blob/274c002debbaf875e0d954fc366ba8d7b3e2a512/flumine/markets/blotter.py#L96|here) so welcome any ideas

---

## 2020-09-09 01:11:05 - general channel

**Chris**

Hopefully a quick two-parter regarding live orders  (Market --&gt; Blotter --&gt; Live Orders) and market/runner exposure. My goal here is to calculate exposure at the runner level so I can handle inventory at that level (ex. if I already have X exposure on a runner then don't buy/sell any more [I don't know where to get the X number])

1. Is an order only considered live if it has at least some unmatched size in the market?

2. Once an order has been completely filled is it then considered where can I access them? Do I need to use market.blotter.strategy_orders(strategy) and then parse it down to the runner / market level?

---

## 2020-09-06 07:42:51 - general channel

**Dylan**

Yes, I am using that production example. I have it running as a Daemon, and Ideally it would be to prevent multiple threads being spawned unnecessarily if it was invoked more than once

---

## 2020-09-06 07:20:06 - general channel

**liam**

Why? Are you using the production example? [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py|https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py|https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py)

---

## 2020-09-05 23:08:17 - issues channel

**Ruben**

there's something that is keeping me a bit worried....I run my market recorder on a very modest server (1 GB RAM), and yesterday the python process suddenly stopped, without any errors/warnings in the log, nor in stdout. I have no clue what could have caused this, any ideas?

---

## 2020-09-05 19:43:02 - issues channel

**Jonjonjon**

Ah, please ignore that. I fixed it. I have very bad infrastructure and a legacy bot does not use BFLW. It is on an old server that I don't have time to move. And I accidently killed the proxy server I used this morning. My own fault for being disorganised.

---

## 2020-09-02 13:08:36 - general channel

**Misha**

That's not bad (I have customised software that I developed myself), but my main question is: how do you "eyeball" the data generated by the system? So not error data, actual system captured data (like you store on AWS)

---

## 2020-09-02 13:07:26 - general channel

**Taking Value**

Thanks for all the input guys. I will go for AWS probably, depends on the cost.

---

## 2020-09-02 10:16:51 - general channel

**liam**

I store all streaming data in AWS s3 (object storage) and then use AWS rds for db, all managed and easy to setup with terraform

---

## 2020-09-02 09:57:33 - general channel

**Misha**

Spin up a server from any provider (I use Vultr). I just need the machine - I don't need any AWS services, so that might sway your decision. I definitely don't run any servers on location any more

---

## 2020-09-02 09:51:42 - general channel

**Mo**

Combination of AWS and dedicated servers

---

## 2020-09-02 09:49:43 - general channel

**Taking Value**

Can I just check when collecting data, storing it in a database and operating bots would it be advisable to use a cloud based solution or to buy/build some basic servers. What do others here do?

---

## 2020-09-01 09:15:15 - general channel

**liam**

I personally use AWS lambda for all cron type work

---

## 2020-08-30 21:21:40 - general channel

**Mo**

[@U4H19D1D2](@U4H19D1D2) can _definitely_ help with getting your live key activated

---

## 2020-08-30 21:20:46 - general channel

**Ruben**

[@U4H19D1D2](@U4H19D1D2) might be able to help you with getting your live key activated

---

## 2020-08-30 14:52:58 - general channel

**Taking Value**

Yea I realised its missing key data e.g. traded volume. Very few of the market updates come through too. I can still plan my schema for the database using it though and I will get that script written fir the two quid bets before I go live so that I don't piss them off. Thanks for the help guys.

---

## 2020-08-30 14:23:50 - general channel

**Taking Value**

Can I just check, I am not getting very frequent updates via the listeners. I think its because I am only using the demo API key at the moment. Is there a way to captures all updates to market data using the demo key or must you have the live key. I am happy to pay BF for the live key but there is a bit of a questionnaire to fill in first. Is this something to be taken seriously, have there been cases where people have lost access to BF because they didn't bet frequently enough or in a large enough volume?  I was going to do nothing but record the data for half a year first.

---

## 2020-08-29 09:50:17 - random channel

**Misha**

[@UU1URJ8L8](@UU1URJ8L8) - speed isn't really helpful to "beat" others. The randomness of delays from the Betfair exchange server and across the stream (which needs only to be milliseconds) means that trying to be the fastest is useless. Good performance is more about the ability to process all updates in real time. I am moving into in-play tennis where there is a 3 or 5 second delay to actually place orders, and all the money disappears during the point so you need to wait for a few seconds after a point anyway for the volume to "arrive"

---

## 2020-08-28 11:23:53 - general channel

**Jono**

ah right ok so scores isnt useful in my case. For the in_play_service is just dependent on whether betfair have live data being fed for that specific game? Are there some instances where bf dont bother to have a feed of each teams scores?

---

## 2020-08-28 11:16:44 - general channel

**Jono**

# login

trading.login_interactive()

# event_type_ids_list = [1]

event_ids = [29975922]





live_events_with_scores = trading.scores.list_available_events(event_ids=event_ids)

for live_event in live_events_with_scores:



    print(live_event.__dict__)

    # event_ids.append(live_event.event_id)





# score request (provide list / returns list)

scores = [http://trading.in|trading.in](http://trading.in|trading.in)_play_service.get_scores(event_ids=event_ids)

print(scores)

for score in scores:

    print(

        score,

        score.description,

        score.status,

        "%s-%s" % (score.score.home.score, score.score.away.score),

    )  # view resources or debug to see all values available

---

## 2020-08-28 08:07:32 - general channel

**birchy**

[@U4H19D1D2](@U4H19D1D2) that looks pretty nifty! Not using docker, but probably should. Still learning aws, bflw, flumine, and a big list of python modules I've never used before. All a bit overwhelming to be honest. :flushed:

---

## 2020-08-28 07:49:50 - general channel

**Unknown**

[@U016TGY3676](@U016TGY3676) if you run your code in docker you can use the [https://docs.docker.com/config/containers/logging/awslogs/|CloudWatch](https://docs.docker.com/config/containers/logging/awslogs/|CloudWatch) handler on top and everything from stdout gets piped through. There is also a CloudWatch agent to do the same thing via files but thats more work

---

## 2020-08-27 21:25:13 - general channel

**birchy**

I have a noob question regarding the python logging module... I pretty much copy &amp; pasted [https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|market recorder.py](https://github.com/liampauling/flumine/blob/master/examples/marketrecorder.py|market recorder.py) to an AWS Lightsail instance a month ago and launched as a detached process. I've just discovered that despite the process still running, no data has been saved for over a week. I've not previously used the logging module, so where is the logging info going? I assume there's a file somewhere, otherwise it's not actually "logging" anything?!

---

## 2020-08-27 18:38:56 - random channel

**Newbie99**

`{"asctime": "2020-08-27 17:28:52,375", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.172407748", "selection_id": 24110964, "handicap": 0, "id": "138178421323748982", "customer_order_ref": "27cfde50caeb3-138178421323748982", "bet_id": null, "trade": {"id": "c6a05675-e88a-11ea-8728-2016d8944b85", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138178421323748982"], "notes": "1.172407748,2020-08-27 17:30:00,2020-08-27 17:26:42.946000,OPEN,False,24110964,Walk It Talk It,0,7.6,7.4,7.6,7.2,7.8,15228.77,54.539999999997235,7.3562828939581335,7.3548410230981025,7.51219521213161e-05,0.0005672825912416327,-0.485228770809155,-1.6962085742376443,-1.1856819494519342,1.5158875740819253,-1.1856819494519342,402.8352149156427,0.009164996444333584,7.6,8", "market_notes": null}, "order_type": {"order_type": "Limit", "price": 8, "size": 2.14, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Pending", "status_log": "Pending"}`

`{"asctime": "2020-08-27 17:28:52,521", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.172407748", "selection_id": 11562443, "handicap": 0, "id": "138178421325218981", "customer_order_ref": "27cfde50caeb3-138178421325218981", "bet_id": null, "trade": {"id": "c6b6c4a4-e88a-11ea-a5ec-2016d8944b85", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138178421325218981"], "notes": "1.172407748,2020-08-27 17:30:00,2020-08-27 17:26:42.946000,OPEN,False,11562443,Bailarico,0,26,25,26,24,27,5153.51,32.36000000000058,28.535445913411476,28.548128733377446,-0.0001627451517698249,-0.02293902095003263,-0.6511582668772105,10.045226723497908,-0.1199440789187813,-41.791918890313944,-0.1199440789187813,-41.791918890313944,0.0078068834946611,29,32,29", "market_notes": null}, "order_type": {"order_type": "Limit", "price": 32, "size": 0.43, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Pending", "status_log": "Pending"}`

`{"asctime": "2020-08-27 17:28:52,581", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.172407748", "selection_id": 24420530, "handicap": 0, "id": "138178421325818936", "customer_order_ref": "27cfde50caeb3-138178421325818936", "bet_id": null, "trade": {"id": "c6bfec37-e88a-11ea-a189-2016d8944b85", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138178421325818936"], "notes": "1.172407748,2020-08-27 17:30:00,2020-08-27 17:26:42.946000,OPEN,False,24420530,Selsey Sizzler,0,11.5,11,11.5,10.5,12,8648.15,12.989999999999782,11.418560202853765,11.419067823979393,-1.4023312049340216e-05,-8.680345957619678e-05,-0.49187450420717704,-0.9872627534678625,-0.5703850484523233,-1.0597854728689005,-0.5703850484523233,-1.0597854728689005,0.03665404088008629,12.5,13.5,12.5", "market_notes": null}, "order_type": {"order_type": "Limit", "price": 13.5, "size": 1.1, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Pending", "status_log": "Pending"}`

`{"asctime": "2020-08-27 17:28:52,721", "levelname": "INFO", "message": "1 order packages created", "order_packages": [{"id": "c6d54903-e88a-11ea-b947-2016d8944b85", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x0000014904B43670&gt;", "market_id": "1.172407748", "orders": ["138178421323748982", "138178421325218981", "138178421325818936"], "package_type": "Place", "customer_strategy_ref": "BlueOcean", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0}]}`

---

## 2020-08-27 17:22:45 - random channel

**Newbie99**

(and latency could explain why its only happening to me, perhaps a connection / machine issue...although I've tried on AWS and have replicated so I guess its not that).



Okay cool, will try that and report back...

---

## 2020-08-27 15:59:27 - random channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2) I seem to have been able to replicate that issue where a pending order never appears:



```{"asctime": "2020-08-27 14:56:02,323", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.172406553", "selection_id": 22583996, "handicap": 0, "id": "138178329623238566", "customer_order_ref": "27cfde50caeb3-138178329623238566", "bet_id": null, "trade": {"id": "6cd997a5-e875-11ea-8295-2016d8944b85", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138178329623238566"], "notes": "1.172406553,2020-08-27 15:00:00,2020-08-27 14:53:16.392000,OPEN,False,22583996,Go Hard Or Go Home,0,3.7,3.7,3.75,3.65,3.8,24071.260000000002,44.0,3.640561751239103,3.6228700238460068,0.0027530273204042087,0.0028067051365076334,0.5678797216934938,0.502030814968028,-0.7126908839976814,-1.4514553590612573,-0.7126908839976814,-1.4514553590612573,0.013439251525677598,3.65,3.75,3.65", "market_notes": null}, "order_type": {"order_type": "Limit", "price": 3.65, "size": 5.66, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Pending", "status_log": "Pending"}

{"asctime": "2020-08-27 14:56:02,323", "levelname": "INFO", "message": "1 order packages created", "order_packages": [{"id": "6cd997a7-e875-11ea-a46f-2016d8944b85", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x000001E40D033970&gt;", "market_id": "1.172406553", "orders": ["138178329623238566"], "package_type": "Place", "customer_strategy_ref": "BlueOcean", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0}]}```

That was (and still is as I write) the last message to appear

---

## 2020-08-26 18:34:32 - general channel

**birchy**

Refreshing to see that [@U4H19D1D2](@U4H19D1D2) is having success with simple "if this condition" type strategies as that's exactly what I do. I basically have a set of functions that check various market conditions and then combine the return values to make a decision. In the early days I would create a strategy based on my understanding of the markets and then go straight to live testing at minimum stakes. I still do that occasionally but mostly gain some confidence first by backtesting against historical data. The one thing you really have to understand is the power of big numbers... i.e. 1000 bets or markets is a reasonable starting point but even that can sometimes be showing a loss for a strategy that is actually profitable over a much larger sample. Variance is both our enemy and our best friend.



There was a time when I would write off bots if they lost £100 at £2 stakes (which was day one in some cases!), but learnt quite a few years ago that you realistically need a bank of at least 1000 bets just to cover the variance.

---

## 2020-08-26 16:48:58 - issues channel

**ricky**

I am using Live key in streaming, price and volumn got Instant updated, however i did not understand why both "runner.total_matched" and "runner.last_price_traded" alway return None?



My data filter setting:

market_data_filter = streaming_market_data_filter(

    fields=["EX_BEST_OFFERS", "EX_MARKET_DEF", "EX_TRADED"], ladder_levels=10

)

---

## 2020-08-26 15:55:02 - random channel

**Unknown**

This fixes it, the issue is caused by the fact there are no orders live in the market so `_process_current_orders` is never executed and the trade never completed, adding the following fixes it just not sure if that is the way I want to fix it

---

## 2020-08-26 13:56:37 - random channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2), I've created another script to demo the violation issue (i.e. that the trade still counts as a live trade), this strips out everything, so hopefully makes it clearer where (I think) the problem lies (or alternatively it may be clearer where I'm wrong if that's the case)!



All it does is try to lay the lowest price runner (I just adapted the example script) @ 1.2, with a size of 2, but the max_order_exposure is set to 0.1, so it should violate.



This occurs as expected and it states the trade is complete, however it appears to remain in the live trade count (which I assume is not intended behaviour). So nothing to do with replaces as I incorrectly said yesterday, but I assumed when the violated trade is completed, the live trade count should drop to 0.





---

## 2020-08-26 13:56:37 - random channel

**Newbie99**

```from flumine import BaseStrategy, clients, Flumine

from flumine.order.trade import Trade

from flumine.order.order import OrderStatus

from flumine.order.ordertype import LimitOrder

from flumine.utils import get_price

import account_info as ai

import betfairlightweight

from betfairlightweight.filters import streaming_market_filter

import logging

from pythonjsonlogger import jsonlogger

import time



logger = logging.getLogger()



custom_format = "%(asctime) %(levelname) %(message)"

log_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_handler.setFormatter(formatter)

logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))



time_to_open_execution_in_seconds = 900

time_to_close_execution_in_seconds = 0



class test_violation(BaseStrategy):



    def check_market_book(self, market, market_book):

        if market_book.status not in ["CLOSED", "SUSPENDED"]:

            return True



    def process_market_book(self, market, market_book):



        if market.market_catalogue is not None and \

                market.seconds_to_start &lt;= time_to_open_execution_in_seconds and \

                market.seconds_to_start &gt;= time_to_close_execution_in_seconds:

            print('Open for orders')



            prices = [

                (r.selection_id, r.last_price_traded)

                for r in market_book.runners

                if r.status == "ACTIVE"

            ]

            prices.sort(key=lambda tup: tup[1])

            selection_id = prices[0][0]



            for runner in market_book.runners:

                if runner.selection_id == selection_id:

                    # lay at current best lay price

                    lay = 1.2

                    trade = Trade(

                        market_book.market_id, runner.selection_id, runner.handicap, self,

                    )

                    order = trade.create_order(

                        side="LAY", order_type=LimitOrder(lay, 2),

                    )

                    self.place_order(market, order)



                    runner_data = self.get_runner_context(market_id=market_book.market_id,

                                                              selection_id=runner.selection_id,

                                                              handicap=runner.handicap)

                    live_trade_count = runner_data.live_trade_count

                    trade_count = runner_data.trade_count

                    print({'selection_id': runner.selection_id,

                           'live_trade_count': live_trade_count, 'trade_count': trade_count})



    def process_orders(self, market, orders):

        if market.market_book.status not in ['CLOSED', 'SUSPENDED']:

            for order in orders:

                print({'selection_id': order.selection_id, 'customer_order_ref': order.customer_order_ref,

                       'trade_complete': order.trade.complete, 'trade_status': order.status})



trading = betfairlightweight.APIClient(ai.accname, ai.accpass, ai.acckey, certs=ai.path)

client = clients.BetfairClient(trading)



framework = Flumine(client=client)





strategy = test_violation(

    market_filter=streaming_market_filter(

        event_type_ids=[7],

        country_codes=['GB'],

        market_types=['WIN'],

    ),

    streaming_timeout=2,

    # max_selection_exposure=50,

    max_order_exposure=0.1

)



framework.add_strategy(strategy)

framework.run()```



---

## 2020-08-26 07:34:22 - random channel

**Mo**

Sort of... the aim of their historic feed is not to provide an accurate recording of the live data but to provide the most accurate positions possible. This still has value from e.g. more accurate sectional timing. TPD is aware that there is demand for the kind of historic data we'd need for backtesting and have made noises that they are working on it but I'm not expecting it any time soon

---

## 2020-08-26 05:50:24 - random channel

**Mo**

TPD provide historic data BUT it's not a reproduction of what you would have received live as they apply some transformations after the fact to try to make the positions more accurate. As a result I've found it largely useless for backtesting and this seems to be a barrier to entry that protects the guys who have been doing it since the beginning and have collected their own data... on the flip side, they would have had zero historic data having started from the beginning and have done very well for themselves

---

## 2020-08-25 22:53:58 - random channel

**birchy**

All this talk of gps issues makes me wonder if it's actually reliable enough to be used for triggering bets? In particular, this is concerning:



`How accurate is the live data?

Accuracy is recorded as the difference between the official finish time and the GPS derived finish time, 

the BHA defined conversion between time and length varies by ground and code between 4 and 6 

lengths per second, taking speed = 5.5ls-1 and L=2.4m, and a standard deviation of a sample of the 

data measured as 100ms, then 95% of the live data is within 2.59m of the true position.`



...considering that there are often multiple runners within a 2.59m section. I would imagine GPS could easily be reporting positions in the wrong order? And that's only 95% of the time. The other 5% must be a monumental fuckup, particularly where all the runners are tightly bunched?

---

## 2020-08-25 14:50:23 - general channel

**Twatter**

i.e. it's a short lived strategy (well across a few years)

---

## 2020-08-25 10:50:29 - random channel

**Newbie99**

```def process_market_book(self, market, market_book):

                total_live_trade_count = 0

                for runner in runner_list:

                    runner_data = strategy.get_runner_context(market_id=market_book.market_id,

                        selection_id=runner['selection_id'],

                                handicap=runner['handicap'])

                    live_trade_count = runner_data.live_trade_count

                    print({'selection_id': runner['selection_id'], 'runner_name': runner['runner_name'], 'live_trade_count': live_trade_count})

                    total_live_trade_count = total_live_trade_count + live_trade_count



                if total_live_trade_count == 0:

                    for runner in runner_list:

                        if 'price' not in runner.keys():

                            pass



                        else:

                            price = runner['price']

                            if price &gt; absolute_min_price and price &lt; absolute_max_price:

                                stake = 2



                                trade = Trade(

                                    market_id=market_book.market_id,

                                    selection_id=runner['selection_id'],

                                    handicap=runner['handicap'],

                                    strategy=strategy,

                                    notes=runner

                                )

                                order = trade.create_order(

                                    side="LAY", order_type=LimitOrder(price=price, size=stake, persistence_type=persistence_type)

                                )

                                self.place_order(market, order)```

---

## 2020-08-24 20:34:19 - random channel

**Newbie99**

```{"asctime": "2020-08-24 19:29:22,513", "levelname": "WARNING", "message": "Order has violated ORDER_VALIDATION and will not be placed", "control": "ORDER_VALIDATION", "error": "Order size is less than min bet size (2) or payout (10) for currency", "order": {"market_id": "1.172363533", "selection_id": 19763676, "handicap": 0, "id": "138175901625116463", "customer_order_ref": "27cfde50caeb3-138175901625116463", "bet_id": null, "trade": {"id": "1ce2672e-e640-11ea-9229-2016d8944b85", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138175901625116463"], "notes": "1.172363533,2020-08-24 19:30:00,2020-08-24 19:29:22.551000,OPEN,False,19763676,Hush Buffy,0,2.88,2.88,2.9,2.84,2.92,384.1600000000001,39.120000000000005,2.9232699537837674,2.9232699537837674,-0.00016599506431987516,-0.0018108429044144984,0,-0.43546624108134707,-1.0,-1.020314057737078,-1.0,-1.020314057737078,0.012721252976427583,2.82,2.9,2.82", "market_notes": "2.88,2.9,2.88"}, "order_type": {"order_type": "Limit", "price": 2.9, "size": 0.73, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Violation", "status_log": "Pending, Violation"}}

{"asctime": "2020-08-24 19:29:22,515", "levelname": "WARNING", "message": "Empty package, not executing", "id": "1ce28e3d-e640-11ea-905c-2016d8944b85", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x000001BC1E36D100&gt;", "market_id": "1.172363533", "orders": ["138175901625116463"], "package_type": "Place", "customer_strategy_ref": "BlueOcean", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0}```

---

## 2020-08-24 16:25:35 - random channel

**Newbie99**

[@U4H19D1D2](@U4H19D1D2), I haven't been able to replicate the issue with pending orders from the other day, but came across this today and wanted to check the correct way it's intended to be handled:



```{"asctime": "2020-08-24 15:11:49,150", "levelname": "INFO", "message": "Order status update: Violation", "market_id": "1.172319685", "selection_id": 26578374, "handicap": 0, "id": "138175747090349752", "customer_order_ref": "27cfde50caeb3-138175747090349752", "bet_id": null, "trade": {"id": "21e4feb7-e61c-11ea-8071-2016d8944b85", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138175747090349752"], "notes": "1.172319685,2020-08-24 15:15:00,2020-08-24 15:11:48.783000,OPEN,False,26578374,Phoenix Strike,0,32,32,34,30,36,1116.3899999999999,4.0,34.20734773394412,34.20734773394412,-0.00017030046190238602,-0.015229531148119538,0,-0.34410707742395563,-1.0,-0.9994686305661997,-1.0,-0.9994686305661997,0.04510370240413563,32,36,32", "market_notes": "32,34,32"}, "order_type": {"order_type": "Limit", "price": 32, "size": 2, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Violation", "status_log": "Pending, Violation"}

{"asctime": "2020-08-24 15:11:49,151", "levelname": "WARNING", "message": "Order has violated STRATEGY_EXPOSURE and will not be placed", "control": "STRATEGY_EXPOSURE", "error": "Order exposure (62) is greater than strategy.max_order_strategy (20)", "order": {"market_id": "1.172319685", "selection_id": 26578374, "handicap": 0, "id": "138175747090349752", "customer_order_ref": "27cfde50caeb3-138175747090349752", "bet_id": null, "trade": {"id": "21e4feb7-e61c-11ea-8071-2016d8944b85", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138175747090349752"], "notes": "1.172319685,2020-08-24 15:15:00,2020-08-24 15:11:48.783000,OPEN,False,26578374,Phoenix Strike,0,32,32,34,30,36,1116.3899999999999,4.0,34.20734773394412,34.20734773394412,-0.00017030046190238602,-0.015229531148119538,0,-0.34410707742395563,-1.0,-0.9994686305661997,-1.0,-0.9994686305661997,0.04510370240413563,32,36,32", "market_notes": "32,34,32"}, "order_type": {"order_type": "Limit", "price": 32, "size": 2, "persistence_type": "PERSIST", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "LAY", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Violation", "status_log": "Pending, Violation"}}```

So that order failed validation, which makes sense, all looks correct...however,

```runner_data = strategy.get_runner_context(market_id=market_book.market_id,

    selection_id=runner['selection_id'],

            handicap=runner['handicap'])

live_trade_count = runner_data.live_trade_count```

live_trade_count returned 1 until the close.



If an order fails validation, does the live_trade_count need to be manually adjusted and is that intentional?

---

## 2020-08-23 11:57:49 - general channel

**Misha**

For historical data. The number of updates live is limited by how many there are at the time :wink:

---

## 2020-08-23 09:29:02 - general channel

**Unknown**

To anyone that uses FlumineBacktest... I've been experimenting with a slightly modified version of [https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|backtest_multi.py](https://betfairlightweight.slack.com/files/UPMUFSGCR/F013J0H5F3M/backtest_multi.py|backtest_multi.py) using simple strategies and running them on a low spec AWS Lightsail instance (1 CPU, 2gb RAM).



Iterating 100 markets takes about 3.5 minutes - is that normal?

---

## 2020-08-20 10:41:11 - general channel

**D C**

I agree regarding the speed and complexity. Look at something like Hawkeye to capture motion of cricket ball which is a VERY simple problem by comparison. You basically just need to fit an ellipse to the outputs of some edge detection. Cameras positioned accurately to optimise that. Not sure if they do that in real time or just run it after the ball has been delivered. Imagine a foggy February day over the fences trying to use image recognition in a 20 strong field. Really can't see it being feasible.

---

## 2020-08-20 08:32:24 - issues channel

**Unknown**

Hi [@U0198BFR9GR](@U0198BFR9GR), flumine ingests historical files either purchased from betfair or recorded yourself, there are [https://github.com/liampauling/flumine/tree/69d68904b4ae0c8ba3c0d60991c3a6a38066040d/tests/resources|two](https://github.com/liampauling/flumine/tree/69d68904b4ae0c8ba3c0d60991c3a6a38066040d/tests/resources|two) in the package itself used for the integration tests, however attached is the example I have used in that example. Paper trading uses live data but records orders / position locally.

---

## 2020-08-19 19:25:36 - issues channel

**JojoBeans**

The output looks like

```---------------------------------------------------------------------------

SocketError                               Traceback (most recent call last)

&lt;ipython-input-48-10c95a06d625&gt; in &lt;module&gt;

     21 )

     22 

---&gt; 23 betfair_socket.start()  # blocking



~/anaconda3/envs/py36_new/lib/python3.6/site-packages/betfairlightweight/streaming/betfairstream.py in start(self)

     57             self._connect()

     58             self.authenticate()

---&gt; 59         self._read_loop()

     60 

     61     def stop(self) -&gt; None:



~/anaconda3/envs/py36_new/lib/python3.6/site-packages/betfairlightweight/streaming/betfairstream.py in _read_loop(self)

    200         """

    201         while self._running:

--&gt; 202             received_data_raw = self._receive_all()

    203             if self._running:

    204                 self.receive_count += 1



~/anaconda3/envs/py36_new/lib/python3.6/site-packages/betfairlightweight/streaming/betfairstream.py in _receive_all(self)

    232                     raise SocketError(

    233                         "[Connect: %s]: Connection closed by server"

--&gt; 234                         % (self._unique_id,)

    235                     )

    236                 else:



SocketError: [Connect: 2]: Connection closed by server```



---

## 2020-08-19 08:19:08 - random channel

**liam**

```{"asctime": "2020-08-19 07:16:22,242", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.172171556", "selection_id": 8329902, "handicap": 0, "id": "138171141822420430", "customer_order_ref": "27cfde50caeb3-138171141822420430", "bet_id": null, "trade": {"id": "e28912a4-e1eb-11ea-86ee-a0999b054753", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138171141822420430"], "notes": "", "market_notes": null}, "order_type": {"order_type": "Limit", "price": 10, "size": 2, "persistence_type": "MARKET_ON_CLOSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Pending", "status_log": "Pending"}

{"asctime": "2020-08-19 07:16:22,242", "levelname": "INFO", "message": "1 order packages created", "order_packages": [{"id": "e28930c2-e1eb-11ea-86ee-a0999b054753", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x104826b20&gt;", "market_id": "1.172171556", "orders": ["138171141822420430"], "package_type": "Place", "customer_strategy_ref": "Liams-MBP", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0}]}

{"asctime": "2020-08-19 07:16:22,243", "levelname": "INFO", "message": "New requests.Session created", "sessions_created": 1, "session": "&lt;requests.sessions.Session object at 0x10485e850&gt;", "session_time_created": 1597821382.243398, "session_time_returned": 1597821382.2433999}

{"asctime": "2020-08-19 07:16:22,488", "levelname": "INFO", "message": "execute_place", "trading_function": "place", "elapsed_time": 0.24187803268432617, "response": {"customerRef": "e28930c2e1eb11ea86eea0999b054753", "status": "SUCCESS", "marketId": "1.172171556", "instructionReports": [{"status": "SUCCESS", "instruction": {"selectionId": 8329902, "handicap": 0.0, "limitOrder": {"size": 2.0, "price": 10.0, "persistenceType": "MARKET_ON_CLOSE"}, "customerOrderRef": "27cfde50caeb3-138171141822420430", "orderType": "LIMIT", "side": "BACK"}, "betId": "208759879486", "placedDate": "2020-08-19T07:16:22.000Z", "averagePriceMatched": 0.0, "sizeMatched": 0.0, "orderStatus": "EXECUTABLE"}]}, "order_package": {"id": "e28930c2-e1eb-11ea-86ee-a0999b054753", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x104826b20&gt;", "market_id": "1.172171556", "orders": ["138171141822420430"], "package_type": "Place", "customer_strategy_ref": "Liams-MBP", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0}}

{"asctime": "2020-08-19 07:16:22,488", "levelname": "INFO", "message": "Trade status update: Pending", "id": "e28912a4-e1eb-11ea-86ee-a0999b054753", "strategy": "ExampleStrategy", "status": "TradeStatus.PENDING", "orders": ["138171141822420430"], "notes": "", "market_notes": "3.05,3.15,3.1"}

{"asctime": "2020-08-19 07:16:22,488", "levelname": "INFO", "message": "Order Place: SUCCESS", "bet_id": null, "order_id": "138171141822420430", "status": "SUCCESS", "error_code": null}

{"asctime": "2020-08-19 07:16:22,488", "levelname": "INFO", "message": "Order status update: Executable", "market_id": "1.172171556", "selection_id": 8329902, "handicap": 0, "id": "138171141822420430", "customer_order_ref": "27cfde50caeb3-138171141822420430", "bet_id": "208759879486", "trade": {"id": "e28912a4-e1eb-11ea-86ee-a0999b054753", "strategy": "ExampleStrategy", "status": "TradeStatus.PENDING", "orders": ["138171141822420430"], "notes": "", "market_notes": "3.05,3.15,3.1"}, "order_type": {"order_type": "Limit", "price": 10, "size": 2, "persistence_type": "MARKET_ON_CLOSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Executable", "status_log": "Pending, Executable"}

{"asctime": "2020-08-19 07:16:22,488", "levelname": "INFO", "message": "Trade status update: Live", "id": "e28912a4-e1eb-11ea-86ee-a0999b054753", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138171141822420430"], "notes": "", "market_notes": "3.05,3.15,3.1"}```

---

## 2020-08-18 22:58:25 - issues channel

**JojoBeans**

I keep getting the following error trying to run the example code

```SocketError: [Connect: 2]: Connection closed by server```

Any ideas on what could be causing it?

---

## 2020-08-18 20:10:53 - random channel

**Newbie99**

```{"asctime": "2020-08-18 18:54:33,224", "levelname": "INFO", "message": "Order status update: Pending", "market_id": "1.172181073", "selection_id": 26836902, "handicap": 0, "id": "138170696732214253", "customer_order_ref": "27cfde50caeb3-138170696732214253", "bet_id": null, "trade": {"id": "411783ec-e184-11ea-8bfc-2016d8944b85", "strategy": "ExampleStrategy", "status": "TradeStatus.LIVE", "orders": ["138170696732214253"], "notes": "1.172181073,2020-08-18 18:56:00,2020-08-18 18:53:46.472000,OPEN,False,26836902,4. Energize Bingo,0,2.94,2.88,2.94,2.86,2.96,3693.63,355.2099999999996,2.757220771341723,2.7819444254664205,0.008706128457104802,0.012755260522776427,-0.5531056874468805,1.0469958809755968,-1.5485176472436966,-2.423454304332246,0,0,0.03303360044899266,2.54,2.74,True", "market_notes": null}, "order_type": {"order_type": "Limit", "price": 2.88, "size": 5.21, "persistence_type": "MARKET_ON_CLOSE", "time_in_force": null, "min_fill_size": null, "bet_target_type": null, "bet_target_size": null}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Pending", "status_log": "Pending"}

{"asctime": "2020-08-18 18:54:33,230", "levelname": "INFO", "message": "1 order packages created", "order_packages": [{"id": "4118e32b-e184-11ea-aace-2016d8944b85", "client": "&lt;flumine.clients.betfairclient.BetfairClient object at 0x000001B04D3CFDF0&gt;", "market_id": "1.172181073", "orders": ["138170696732214243", "138170696732214245", "138170696732214247", "138170696732214249", "138170696732214251", "138170696732214253"], "package_type": "Place", "customer_strategy_ref": "BlueOcean", "bet_delay": 0, "market_version": null, "retry": true, "retry_count": 0}]}```

---

## 2020-08-18 20:08:14 - random channel

**Newbie99**

Using flumine (live), when an order goes into a pending state and then the "message": "1 order packages created" appears in the next line as expected. Sometimes the orders never actually make it onto the order book. Given the orders weren't in any violation of the controls and there don't appear to be any API issues, does this mean there is some kind of latency logic built in (i.e. if the order book receives the order x seconds after it was sent it automatically fails or something along those lines)?

---

## 2020-08-18 14:05:15 - general channel

**liam**

```There's been a power failure at the Data Centre on which the Gmax systems depend, this means that until their issues are resolved the live GPS stream and other APIs will be unavailable.



[https://status.exn.uk/](https://status.exn.uk/)

[https://status.giga.net.uk/incidents/3zcfz8s8g43h?u=syfy92mhvnh0](https://status.giga.net.uk/incidents/3zcfz8s8g43h?u=syfy92mhvnh0)



Apologies for the inconvenience```

---

## 2020-08-17 13:11:04 - general channel

**birchy**

I'm away from home this week and the only device I have is my android phone. Nevertheless, I thought I'd look at getting this S3 streaming working but am having a hard time. Having converted all the saved files from .zip to .bz2, I'm still not having any joy. As far as I understand from the smart_open docs, it should auto decompress the bz2 files but that doesn't seem to be happening. In the following code, "content" is a byte object and I can't do anything with it. `.decode("utf-8")`, `smart_open.open()`, etc all fail or return the original bytes. Getting really frustrated with this now as it shouldn't be THIS difficult.



`import smart_open`



`bucket_name = 'mybucket'`



`for key, content in smart_open.s3_iter_bucket(bucket_name): workers=1):

    print(content)

    exit()`



Also tried:



`import boto3`



`bucket_name = 'mybucket'`



`s3 = boto3.resource('s3')

bucket = s3.Bucket(bucket_name)

for obj in bucket.objects.all():`

    `body = obj.get()['Body'].read()`

    `print(data)

    exit()`

But this is what `smart_open.s3_iter_bucket()` does in the background, so the results are the same.



I simply want to run my backtest code in the Lightsail instance as I can then leave it to run and check results later. As you know, I am a noob to the whole aws thing, so I'm wondering what I am failing to understand? 



Logically, I am assuming that the workflow is simply a case of:

a) save data using the S3MarketRecorder example (.zip)

b) stream/read each market from S3 using `smart_open`

c) process market



I hate asking for spoon feeding but I am starting to think I am too stupid for this. :flushed:

---

## 2020-08-16 18:14:16 - general channel

**Lee**

Out of interest, what does elapsed_time include and does the following time seem slow?

```{

	"message": "execute_place",

	"trading_function": "place",

	"elapsed_time": 1.1396217346191406

}```

I mainly ask because a few of my offered prices did not match in live and if I increase `PLACE_LATENCY`  in simulatedexecution.py it seems to match live more accurately.

---

## 2020-08-15 08:06:20 - general channel

**Mo**

Off the top of my head, with `awscli` OS package installed and configured



```aws s3 sync s3://&lt;bucket_name&gt;/&lt;path_to_directory_containing_zip_files/ /tmp/betfair/

find -type f /tmp/betfair -exec sh "unzip {} ; rm {}" \;

find -type f /tmp/betfair -exec bzip2 {} \;

aws s3 sync --delete /tmp/betfair/ s3://&lt;bucket_name&gt;/&lt;path_to_directory_containing_zip_files/```

I wouldn't run the last command - which will delete everything in the S3 directory that now doesn't exist in /tmp/betfair - without checking you're happy with what /tmp/betfair contains

---

## 2020-08-14 16:42:34 - random channel

**liam**

By default it will look to see if the runner already has live/pending orders for that strategy aka [https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py#L121|invested](https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py#L121|invested), if you want to prevent this logic just override the `def validate_order` function to return True.



Strongly recommend testing this via the backtester or paper trading as it will fire in a lot of orders very quickly!

---

## 2020-08-14 15:57:38 - random channel

**Newbie99**

Just playing around with Flumine and I was testing placing live orders, but I can't seem to place multiple orders against the same runner, is that by design, or is there a parameter I'm missing?



```import time

import logging

import betfairlightweight

from betfairlightweight.filters import streaming_market_filter

from pythonjsonlogger import jsonlogger



from flumine import Flumine, clients, BaseStrategy

from flumine.order.trade import Trade

from flumine.order.ordertype import LimitOrder

from flumine.order.order import OrderStatus

import account_info as ai



logger = logging.getLogger()



custom_format = "%(asctime) %(levelname) %(message)"

log_handler = logging.StreamHandler()

formatter = jsonlogger.JsonFormatter(custom_format)

formatter.converter = time.gmtime

log_handler.setFormatter(formatter)

logger.addHandler(log_handler)

logger.setLevel([http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))





class ExampleStrategy(BaseStrategy):

    def start(self):

        # subscribe to streams

        print("starting strategy 'ExampleStrategy'")



    def check_market_book(self, market, market_book):

        # process_market_book only executed if this returns True

        if market_book.status != "CLOSED":

            return True



    def process_market_book(self, market, market_book):

        # process marketBook object

        for runner in market_book.runners:

            if (

                runner.status == "ACTIVE"

                #and runner.last_price_traded

                and runner.selection_id == 5512138

            ):

                trade = Trade(

                    market_id=market_book.market_id,

                    selection_id=runner.selection_id,

                    handicap=runner.handicap,

                    strategy=self,

                )

                order = trade.create_order(

                    side="BACK", order_type=LimitOrder(price=1000, size=2.00)

                )

                # self.offset_orders= []

                # self.customer_order_ref='t'

                self.place_order(market, order)

        print(market_book.publish_time)



    def process_orders(self, market, orders):

        for order in orders:

            if order.status == OrderStatus.EXECUTABLE:

                # if order.elapsed_seconds and order.elapsed_seconds &gt; 2:

                #     self.cancel_order(market, order)

                # if order.elapsed_seconds and order.elapsed_seconds &gt; 5:

                #     # print(order.bet_id, order.average_price_matched, order.size_matched)

                #     if order.size_remaining == 2.00:

                #         self.cancel_order(market, order, size_reduction=0.01)

                # self.update_order(market, order, "PERSIST")

                # if order.order_type.price == 1.01 and order.size_remaining == 0.49:

                #     self.replace_order(market, order, 1.02)

                # if order.order_type.price == 1.02:

                #     self.replace_order(market, order, 1.03)

                # if order.order_type.price == 1.03:

                #     self.replace_order(market, order, 1.05)

                pass





trading = betfairlightweight.APIClient(ai.accname, ai.accpass, ai.acckey, certs=ai.path)

client = clients.BetfairClient(trading)



framework = Flumine(client=client)



strategy = ExampleStrategy(

    market_filter=streaming_market_filter(market_ids=["1.172039988"]),

    streaming_timeout=2,

    # max_selection_exposure=5000,

    # max_order_exposure=2000

)

framework.add_strategy(strategy)

framework.run()```

I can place, cancel, update etc successfully, but I assumed the above would try to fire off multiple orders, but it only ever seems to place one (obviously there's not a real world use for this script, I was just trying to play around to place multiple orders). Have I missed something, or is this intended behaviour?

---

## 2020-08-12 12:40:05 - issues channel

**Lee**

I think i might not be understanding the MarketOnCloseOrder correctly. I'm trying to take SP just before going inplay but can't seem to get any money matched (flumine backtesting). I've removed my trigger and try to place an order on all runners and still get zero matched.

I've also set my strategy exposure high and not getting any voilation errors on the orders in the logs.

I'm creating an order using the following

```order = trade.create_order(

    side="BACK", order_type=MarketOnCloseOrder(liability=20)

)```

Any idea where i'm going wrong or if it's just a misunderstanding



here's a log too

```{"asctime": "2020-08-12 11:32:33,047", "levelname": "INFO", "message": "Order status update: Execution complete", "market_id": "1.170576255", "selection_id": 19436243, "handicap": 0, "id": "138165247530344110", "customer_order_ref": "890b98c67ce74-138165247530344110", "bet_id": "100000000007", "trade": {"id": "83599e5e-dc8f-11ea-80ef-d45d6453ef52", "strategy": "xxxxxxx", "status": "TradeStatus.LIVE", "orders": ["138165247530344110"], "notes": "", "market_notes": "15.5,16,15.5"}, "order_type": {"order_type": "Market on close", "liability": 20}, "info": {"side": "BACK", "size_matched": 0.0, "size_remaining": 0.0, "size_cancelled": 0.0, "size_lapsed": 0.0, "size_voided": 0.0, "average_price_matched": 0.0}, "status": "Execution complete", "status_log": "Pending, Executable, Execution complete"}```

---

## 2020-08-11 22:57:09 - general channel

**birchy**

I now have a load of data stored on S3 using Flumine S3MarketRecorder class, so am wondering how best to integrate this data into backtesting? I've got some basic backtesting code from the Flumine examples which works with a downloaded file after manually unzipping it. What I want to do in Flumine is:

a) download data from S3 to AWS Lightsail on the fly

b) automatically unzip the file

c) run backtest

d) goto a)

Are there any examples on how to do this?

---

## 2020-08-11 20:03:42 - issues channel

**liam**

It depends on the market capacity and volatility as you would expect. I find on inplay racing it underplays the match rate compared to live which I prefer as it means I know that if I am breaking even or making profit backtesting then it’s time to get some money in the market.



With flumine you use the exact same strategy code live as you do backtesting so it also acts as a good integration test as well.

---

## 2020-08-08 15:17:26 - issues channel

**Ruben**

anyone ever got a "NO_SESSION" error when calling keep_alive()? after several successful keep_alive() calls

---

## 2020-08-07 12:01:18 - strategies channel

**D C**

I should probably add [@UQL0QDEKA](@UQL0QDEKA) that I am most likey using the least advanced setup and least optimal execution. My stuff is currently running inside a Qt GUI application running on Linux on my home desktop with CPU time wasted on such frivolous things as a real time GPS render tool and other unecessary things. Once I make the switch to headless running on AWS or similar I would be hoping that the profitability increases as there are SO many optimisations to potentially make.

---

## 2020-08-07 11:08:08 - strategies channel

**Mo**

[@UQL0QDEKA](@UQL0QDEKA) it took me 3 months to go from having access to the data to running a strategy at test stakes and you know what my level of experience and time commitment is like.



But I was coming from a cold start with no existing horse racing strategies - either in running or pre-event - so that may help you get going faster. 



A very important point that anyone starting now should consider is that the historically available GPS data undergoes some significant transformations from what you would have received live which seriously limits its usefulness for back testing. Guys like [@U4H19D1D2](@U4H19D1D2) who have been doing this from the start and collected their own data are in a much better position in this regard. 



You definitely have to account for trackers on the wrong horse, horses running without trackers, seriously miscalibrated trackers (horse running outside the track, starts the race 10m behind the stalls, appears to be leading when is actually in second place, etc)

---

## 2020-08-07 09:03:09 - issues channel

**liam**

Ah yes that is incorrect, ignore the strategy part as it should be the same as live, the only change for paper trading is

```client = clients.BetfairClient(trading, paper_trade=True)```

---

## 2020-08-06 23:16:43 - issues channel

**mlpanda**

Hey [@U4H19D1D2](@U4H19D1D2), first of all thanks for creating Flumine! I have so far used the backtesting module (and matched the results close enough to my own simple backtester) and I am now looking to use the paper trading functionality. I am currently looking at the example here: [https://github.com/liampauling/flumine/blob/7b50f7e8672c4c5e7c46ac5684617374f1e58c61/docs/quickstart.md](https://github.com/liampauling/flumine/blob/7b50f7e8672c4c5e7c46ac5684617374f1e58c61/docs/quickstart.md)

I don't really understand why the strategy is passed the path to a file rather than the streaming_market_filter() which is passed during live mode. I would expect  the paper trading to look exactly like the live mode except that the trades wouldn't be executed. Am I misunderstanding something here?

---

## 2020-08-06 12:07:10 - general channel

**Misha**

My advice is to keep it as simple as you can. You only ever learn from experience so it isn't till you actually get going that you start to get an idea of what you need. The syndicates making money have surprisingly simple setups beyond using hosted servers (none of the ones I worked for used AWS services at the time). I know because I ran a few of them. It's the model that makes the money, and the operational system can increase the profits (but can never overcome a poor model)

---

## 2020-08-06 11:27:48 - general channel

**Twatter**

I haven't used docker myself, but wouldn't that just make it easy to deploy a working bot? i.e. if you've already got it running on a server, and it's a bot/strategy that won't benefit from deploying the same code to a 2nd/3rd server etc, then I would have thought that containerising your code and deployment into docker would only help with speed of deployment and speed of recovery if the server fell over?

---

## 2020-08-06 10:17:13 - general channel

**Twatter**

I joined here so I can get an idea of what everyone else is doing for Betfair bots and how they using Liam's BFLW/flumine and to get help whilst developing.... to be honest over the history of what i've been reading i've been completely blown away by how sophisticated all your trading set-ups are. I was expecting a few hobbyists like my self trying to write a bot for a bit of side income - and instead found a bunch of people using bits of AWS that i've never come across, ML, real time GPS feeds from races that I didn't even know existed, and god knows what else going on. Crazy...

---

## 2020-08-06 09:42:32 - general channel

**Misha**

So after 10 years, when my last client exited Australia, I thought why not have a go. So I have teamed up with a modeller and we are about to go live. Have spent the last 7 months updating the system in preparation. I specialise in the execution engine (my code) which I then update for clients and they can use it. When I part ways I just take the generic bits and keep developing

---

## 2020-08-06 08:11:42 - general channel

**Misha**

BTW, in production it does run on a hosted server :wink:

---

## 2020-08-06 08:10:42 - general channel

**Misha**

On version 5 now after being in development for 10 years. Has been used in production for most of that time for huge clients. Every 2-3 years I review and update architecturally

---

## 2020-08-05 20:29:51 - random channel

**birchy**

I believe it's unlimited as it's basically a simplified version of EC2.

---

## 2020-08-04 18:38:40 - general channel

**PeterLe**

Hi Kai, Thanks. Ive been running EC2 for many years, but thought Id give lightsail a go as it was cheaper. I thought Id try the $40 month package, but  it was overkill so now running on $20/Month.  only use it for my bots, so suits me just fine :+1:

---

## 2020-08-04 18:31:55 - general channel

**Kai**

lightsail is basically meant to be a simple cheap alternative for people who don't like the complexity of AWS and just want a virtual server IMO.

An alternative to the simple web based interface of digitalocean.

If you create and destroy your machines often then I wouldnt use it.

---

## 2020-08-03 09:56:46 - general channel

**Misha**

No - so not sure about Python threading. But keep-alives are pretty fast so worthwhile doing them often

---

## 2020-08-03 09:52:41 - general channel

**liam**

your not python though? I don't think you can send a keep-alive using python/requests

---

## 2020-08-02 12:07:42 - strategies channel

**Lee**

When trying to optimise a strategy on low stakes, i've duplicated and deployed both the original side by side with the new variant to have something to compare again. Basically an AB test. Does this seem a reasonable approach?

---

## 2020-08-01 00:17:44 - general channel

**birchy**

True to my word...

I now have GitHub setup with a private repo. As you suggested, doing minor edits in the browser is fine for those occasional quick fixes. Regarding deployment on vps, I would prefer to only get the latest source when a bot is (re)started. The VPS is purely a deployment machine, so all edits will be done externally.

So I created a SSH key and loaded it into GitHub. I then ran a git clone from the VPS to download my source code ready for deployment. Bot deployed, slippers fitted, scrumpy poured. All good so far...

Following a new commit, I decide to restart a bot. Git clone fails due to folder existing already, so I delete the folder, git clone and start the bot.

It's all working as expected, however the git clone over ssh feels a bit hacky, particularly having to delete the local folder in order to allow a clone. Also, having to start ssh-agent service and then adding the key in order to connect to GitHub feels like a workaround given that I normally create a SSH config file and simply do "ssh lightsail" to gain access to the server.



Is there a nicer way of doing this?

---

## 2020-07-29 10:17:45 - general channel

**Robert**

at the moment its to make deployment safer -- betfair can go down temporarily

---

## 2020-07-29 09:50:18 - issues channel

**Newbie99**

```

Replace Error: RELATED_ACTION_FAILED

Exception in thread Thread-1:

Traceback (most recent call last):

  File "/usr/lib64/python3.7/threading.py", line 926, in _bootstrap_inner

    self.run()

  File "/usr/lib64/python3.7/threading.py", line 870, in run

    self._target(*self._args, **self._kwargs)

  File "racing.py", line 568, in combined_queue

    replaced_orders_report = rf.replace_orders(market_id, replace_list, market_info, trading, filters)

  File "/home/ec2-user/trading/env/projects/racing_functions.py", line 1941, in replace_orders

    'selection_id': place_instruction_report.instruction.selection_id,

AttributeError: 'NoneType' object has no attribute 'selection_id'```

---

## 2020-07-28 15:46:57 - issues channel

**Aaron**

uninstall and reinstall requests on your im assuming ubuntu EC2 instance. Few threads on SO indicate this may be the fix

---

## 2020-07-28 15:26:27 - issues channel

**mlpanda**

Any suggestions why this code snippet



```market_catalogues = trading.betting.list_market_catalogue(

    filter=filters.market_filter(

        event_type_ids=["7"],  # filter on just horse racing

        market_countries=["GB"],  # filter on just GB countries

        market_type_codes=["WIN"],  # filter on just WIN market types

    ),

    market_projection=[

        "MARKET_START_TIME",

        "RUNNER_DESCRIPTION",

        "EVENT",

    ],  # runner description required

    max_results=100,

)```

gives me the error



`Exception: Timeout value connect was (3.05, 16), but it must be an int or float.`



It's rather strange as it works on my local mac, but when I run it on an EC2 instance in AWS (location: London) it fails with the above message. I'm also located in London locally, so seems odds.



Any suggestions would be very much appreciated :)

---

## 2020-07-28 10:29:54 - general channel

**PeterLe**

I haven’t tried it yet Liam, I created it last night (to compare against EC2). So will be testing today. If its a problem, I may move a few of the non time dependent strategies to my home PC. it just seems much cheaper and on the face of it seems to do everything I want. Yes it starts at zero credit and resets to zero if the instance is stopped/restarted (I think :grinning:)

---

## 2020-07-28 09:49:15 - general channel

**PeterLe**

Quick question please for those using Lightsail…it seems counterintuitive to me; but am I right in thinking that you are still better keeping the server running even when its not being used to increase the CPU burst capacity? (It appears that it resets if you stop the instance?), do you just leave it running 24x7 even if it's not being used for the full period? Thanks

---

## 2020-07-28 09:24:15 - issues channel

**liam**

[@U016TGY3676](@U016TGY3676) bflw is currently setup to use env vars and up to the user to handle how its used, I guess its personal preference but there is no reason you can't override the client to use a config file. It's probably a bit late in the day to move to using a config file only like aws which means we would have the added complication of handling both but open to thoughts

---

## 2020-07-27 22:44:59 - issues channel

**birchy**

Just a thought...but would it be worth using a credentials file for storing login username, password, app key, certs path, etc? It's common to have ~/.aws/credentials, ~/.ssh/config, etc so why not ~/.betfair/credentials? Could also put the certs folder in .betfair and the relative path in the credentials file. Saves putting sensitive info in the .py files and avoids accidentally publishing account info on github/etc. And before you ask...No...I haven't done that. :joy:

---

## 2020-07-27 11:45:25 - strategies channel

**D C**

I would like to avoid busy loops that is for sure. My background is mostly C++ stuff in comms for hardware (stuff like CAN and serial port connections) so when it comes to socket coding I try to stick to blocking socket stuff within a designated thread and thread safe queues to read/write. I am sure there are far better ways, but for HTTPS stuff and python users I imagine you are using higher level packages rather than some low level stuff using OpenSSL libraries. I was just wondering what kind of analysis you guys had done on latency. I am moving to this headless deployment on designated server (not sure yet probably AWS) but these things take time when you are essentially migrating to a new language.

---

## 2020-07-27 11:33:07 - strategies channel

**liam**

you run production code on your normal computer?

---

## 2020-07-26 22:18:31 - general channel

**Mo**

So I have copies of my GitHub repos checked out to a Dropbox folder and any changes I make to my working copies are automatically synced with Dropbox. 



I push my commits to GitHub about once a day to make sure my collaborators have access in case I get hit by a bus etc. Or I’ll push if the changes need to be deployed to production. 



Managing infrastructure with ansible means that the setup of my servers is described in code that can similarly be version controlled. Deploying trading code to production is a case of running ansible tasks that will pull the relevant code from GitHub. 

---

## 2020-07-26 22:09:46 - general channel

**Mo**

I think you’re conflating three separate issues. 



You 100% should have all of your code under version control, whether that is with GitHub or some other service.



As far as synchronising work in progress across multiple devices, I personally like to use Dropbox.



As far as deployment of production code to your VPC, I like to use ansible but I know that Docker is a popular option. 

---

## 2020-07-26 18:20:11 - general channel

**birchy**

Never used Git, is it secure enough for production code?

---

## 2020-07-26 17:58:50 - general channel

**jfo**

The best solution is one you are comfortable with and can use :smile:. I personally prefer to use docker but that comes with its own overhead. I have also just pushed code to git and pulled it down onto aws lightsail machines before

---

## 2020-07-26 17:33:04 - general channel

**birchy**

Have just setup AWS Lightsail and S3. Just wondering what everyone else does regarding their working environment? I've been using Tagadab and basically network mount a remote folder on my local machines and put all my source code into there. This allows me to pick up the source code from multiple machines/devices. Pycharm is OK with this setup as it's effectively a local drive. Sometimes I do quick edits from my phone via SSH and Nano. Are any of you doing something similar with S3? I'm currently investigating how to mount S3 as a network drive. Are there any known issues? Will also be looking at mounting the S3 in the Ubuntu instance on Lightsail to save having to SCP source code. Will be interesting to see how others setup their environments.

---

## 2020-07-26 17:23:28 - strategies channel

**birchy**

Thanks [@UBS7QANF3](@UBS7QANF3). It's not something I obsess about, more of a "nice to have". I was only testing the water as I'm new to the AWS setup and was wondering how it compared to Tagadab.

---

## 2020-07-26 15:00:10 - strategies channel

**birchy**

Very true. I like to optimise my find bet -&gt; place bet turnaround time, but 12ms is the same as my current Tagadab VPS so I've not lost anything performance-wise moving to AWS.

---

## 2020-07-26 11:37:36 - strategies channel

**birchy**

Regarding MSA, I've been using 2 normal accounts for years. One is my test account which has limited funds to avoid any catastrophic "oops" moments and the other is used for production bots. Now that I'm adopting streaming, I've discovered that my test account is not enabled for streaming but my main one is. Any ideas why?

---

## 2020-07-26 08:37:28 - strategies channel

**SrFabio**

How do you guys manage multiple live strategies running in parallel (in terms of bankroll management)? Are you allowed to create sub-accounts and run a strategy per account or do you create some "virtual balance" and assign a portion to each bot

---

## 2020-07-25 16:09:12 - strategies channel

**Mathias Tejs**

Hi all, thanks for letting me join the group :) I want to get into betting and I'm wondering if there are any resources (e.g. books, blogs, etc.) that you guys can recommend? I'm an "expert" in algorithms, software development, machine learning and probability theory but I know very little about the world of betting (and also very little about trading) - I'm only saying that to say that I'm not too interested in general programming books and so on :)



Another related newbie question: I'm especially interested in football (soccer) betting. My guess is that my biggest chance for betting profitably there would be through live betting (I want to bet 100% programmatically). Are there any live feed sources that you can recommend? I have saved up around 30k euro that I plan to risk on getting started - is that enough, or do you need a higher volume to avoid all of your winnings going to paying monthly live feed APIs? (I can see from the slack history that some of you guys are winning considerably more than that)

---

## 2020-07-25 09:49:22 - strategies channel

**liam**

My problem is that I lock in that profit with a nice live dashboard and then it drops 5 figures 

---

## 2020-07-24 23:27:42 - general channel

**birchy**

Actually, now that I've written that down, I'm questioning my own ability. I've not watched a betfair market for at least 5 years, so as such I'm developing strategies blindly, using nothing but educated guesses. I do test them on historical data before going live, but in reality everything is based on assumptions that I've accumulated over the last 30 years, mixed with a handful of new ideas I've read on various forums and betting blogs.

---

## 2020-07-24 17:35:40 - strategies channel

**Amanda**

Good one Lee, “first strategy live” and your worrying about premium charges, that’s optimism for you, I like it.

---

## 2020-07-24 13:49:24 - strategies channel

**Lee**

Finally got my first strategy live which seems to be going well so far. My betfair rewards is set to Basic (2% commission), is this something i should change to increase my commission / reduce premium charge if/when it comes or not something to worry about yet?

---

## 2020-07-23 12:37:54 - issues channel

**Andrey Yunoshev**

thanks, I dont see any spike, but I will try ec2 instance with 2 vcpu

---

## 2020-07-23 12:31:46 - issues channel

**Andrey Yunoshev**

I always (around each 10 min) have warning "latency high", at home and at ec2 Ireland also. Its is normal?

---

## 2020-07-22 23:27:53 - strategies channel

**L**

lightsail is just an ec2 instance under the hood with less bells and whistles

---

## 2020-07-22 20:17:10 - strategies channel

**liam**

EC2, lambda for backtesting and cron like jobs 

---

## 2020-07-22 20:15:50 - strategies channel

**birchy**

Just to clarify, do you run your bots in lambda or EC2?

---

## 2020-07-22 20:10:58 - strategies channel

**birchy**

I've not used AWS before. Setup S3 last night and that wasn't as straight forward as I was expecting due to the plethora of options available.

---

## 2020-07-22 20:08:28 - strategies channel

**birchy**

oh... I must of been lucky then, as I've not had any noticeable issues other than regular connection errors, but I've always assumed that was a betfair issue rather than Tagadab? What's the latency like on EC2?

---

## 2020-07-22 19:38:49 - strategies channel

**liam**

Yeah I am 100% aws, found tagadab to be useless 

---

## 2020-07-22 19:35:20 - strategies channel

**birchy**

So yes...and no. lol

[@U4H19D1D2](@U4H19D1D2) are you running your bots on EC2? I've been using Tagadab (now SimplyHosting) VPS for years as it had low latency to the bf servers.

---

## 2020-07-22 19:21:44 - strategies channel

**liam**

I download as I process, run everything on ec2 or lambda so latency is low anyway 

---

## 2020-07-22 19:18:52 - strategies channel

**birchy**

Or running the bot/backtest code from AWS?

---

## 2020-07-21 21:57:30 - general channel

**birchy**

Quick question regarding AWS S3...are you guys using the Basic, Developer or Business Plan?

---

## 2020-07-20 18:24:29 - general channel

**liam**

Btw paper trading is now implemented so you can simulate trading against live markets, makes debugging matching a lot easier as you can use the betfair site 

---

## 2020-07-17 14:54:57 - strategies channel

**Jono**

Thats fair man, I did crap my pants when I saw the functionality of the repo straight out of the box. No point delaying the inevitable necessary. I guess one additional thing is: Are there any problems or legal implications for using betfairlightweight for large scale commercial gain? I work for a company that sells predictions to clients drawing traffic into betfair, betfred etc and we thought initially that building our own system from scratch would prevent intellectual ownership problems from arising. Would using betfairlightweight for our bf api uses be violating any rules/laws in this context?

---

## 2020-07-17 14:38:53 - strategies channel

**Jono**

sweet lord ok. the more i get into this task the more i realise that I do need to do that :sweat_smile: Is it such a complicated undertaking that trying to send a "keep live" request using my current method just isnt even worth trying? also apologies for the "Thanks but" above it sounded snappy but im extremely greatful for any help

---

## 2020-07-17 14:33:54 - strategies channel

**Jono**

Thanks but is that not betfairlightweight specific? i delved in (perhaps foolishly) and created my own scripts and process for placing bets and reviewing current orders about three months ago and got quite deep in to my workflow before finding out about betfairlightweight. Is there anyway to ensure a session is kept live using the basic api call above?

---

## 2020-07-17 14:30:03 - strategies channel

**Jono**

Hey everyone got pointed to here after problems with keepAlive functionality. I am wondering the exact request I should have in this function (taken from examples in the bf developer library) in order for my session to never time out:





------------------------------------------

def callApi(jsonrpc_req, appKey, sessionToken, initialHeaders):



url = "[https://api.betfair.com/exchange/betting/json-rpc/v1](https://api.betfair.com/exchange/betting/json-rpc/v1)"



initialHeaders = {'X-Application': appKey, 'X-Authentication': sessionToken, 'content-type': 'application/json'}



request = urllib.request.Request(url, jsonrpc_req. encode('utf-8'), initialHeaders)



response = urllib.request.urlopen(request)

jsonResponse = response.read()



return jsonResponse.decode('utf-8')



------------------------------------------



I have read the keep live documentation but unfortunately am very inexperienced with other programming languages other than python and was therefore wondering if there is a simple change I can make to the code above in order to keep my session live, or if there is a much more straightforward function that is applicable to my situation. Thank you for any help.

---

## 2020-07-16 12:52:08 - general channel

**JC**

Hi guys, pretty new here and really enjoying getting to learn the ropes of working with the API. Thanks to the brilliant libraries of bflw and flumine and previous queries in this chat I have managed so far to implement an S3 market recorder running on an EC2 instance. I am now looking to capture as much other available data about the market and runners as possible i.e. metadata available via `list_market_catalogue` , as well as some summary data such as total amount traded at start and end, so that I can query and filter the files on S3 using this. I would also like to record a stream for in-play scores data for tennis. Seems like I should set up a simple script for the former on a time loop which can update and feed my database. For the scores streaming, is there a way to do this in flumine? Any advice would be very welcome on either of these. Thanks again for these libraries and for the active community here :grinning:

---

## 2020-07-13 17:11:22 - general channel

**Oliver Varney**

[@U4H19D1D2](@U4H19D1D2) am I right in thinking non runner adjustments in flumine are only not implement for simulated markets only? Orders on live markets should update?

---

## 2020-07-08 20:05:48 - general channel

**birchy**

[@UQL0QDEKA](@UQL0QDEKA) was just thinking...you mentioned earlier that you use multiple accounts. I have 2 accounts, but only because I use one for testing and the other for my production bots. I don't have any issues with multiple bots placing bets on the same markets. It used to be beneficial to use one account as it boosted the commission discount, but of course they have the 2% package now. Do you not use the strategyRef parameter to identify bots?

---

## 2020-07-08 19:15:23 - general channel

**birchy**

Ditto the above...a server/cloud is just another pc. The code you run on it is no different than what you run on your pc at home.

On a EC2 Linux instance, launch the script from a command terminal with '&amp;' at the end, so that it detaches from the terminal, allowing you to close the terminal and logout.

---

## 2020-07-08 19:03:06 - general channel

**liam**

Just run an ec2 server 

---

## 2020-07-08 19:01:58 - general channel

**user34**

I have a script that is triggered a certain amount of time before an event starts and then places my bets if the appropriate conditions are met. I currently just collect trigger times in advance and then leave python running on my PC to process the data and place the bets. I guess I would need AWS Lambda for the triggering, something (Elastic Beanstalk?) for the processing, some way to interact with Betfair using my API key, and some way to tie it all together. Sorry that this is such a broad question.

---

## 2020-07-08 18:43:12 - general channel

**user34**

Did anyone find any particularly useful resources for how to get betfairlightweight running on AWS? I've been looking through the documentation provided by Amazon and am a bit lost.

---

## 2020-07-07 20:52:34 - general channel

**liam**

Great, yeah readme can be improved on what it does and doesn’t do, I think the real selling point is being able to move from backtesting to live with no changes to your strategy (betfair are still giving 2020 q1 data away for free if you want to try it out) Classes can certainly help abstract away the low level stuff, I find reading code is the best way to find new ways of doing stuff (and sometimes how not to do it!) 

---

## 2020-07-03 10:11:03 - general channel

**Eswyn**

Hi all, I'm working on a script that will run on a server 24/7. Streaming_market_filter doesn't accept market_start_time, so what is the best way to filter the stream to get only the races starting in the next 24 hours at any given time?

---

## 2020-07-02 18:05:37 - general channel

**liam**

There are some limitations but you can restart and the live order will be pulled into the strategy on restart 

---

## 2020-07-02 14:44:56 - general channel

**Oliver Varney**

Question on flumine, can strategies be added / removed after run has been called on live markets?

---

## 2020-06-30 21:04:53 - general channel

**Dave Simonds**

I have zero problem with this if all the revenue generated goes towards making their infrastructure a bit more stable. Nothing sucks more than their servers/service being up and down like a hookers knickers

---

## 2020-06-30 19:20:03 - general channel

**liam**

Big [https://forum.developer.betfair.com/filedata/fetch?id=32354|changes](https://forum.developer.betfair.com/filedata/fetch?id=32354|changes) in transaction charges 



```Transaction Charge Changes

Over the years, the Exchange has processed an increasing number of transactions at an ever-increasing cost to infrastructure and stability.

Therefore, from 1st September 2020, Betfair will be changing the thresholds of the Transaction Charge. The changes are as follows:

• The new limit of “free” transactions will be set at 5,000 transactions per hour. o A “transaction” shall include all bets placed and all failed transactions.

• Every transaction that exceeds the limit will incur a cost of £0.002 (a fifth of a penny). o This cost will be offset by a user’s commission generated.

• There will be no exemptions from this charge.

We have modelled the new charge to disincentivise wasteful transactional behaviour and promote platform stability on a level playing field. It is possible to trade multiple events and markets without ever incurring any of these charges.

Your account manager will be in touch if you would have incurred these charges in the past, giving you an opportunity to update trading strategies and fix bugs before the charge goes live on 1st September.```

---

## 2020-06-30 13:07:23 - general channel

**Jono**

Inside the racecard example on line 9 &amp; 10 where the trading instance is created it says that "don't need username/password" so was under the impression this was an example of connection to live data without credentials. As for why, i want to share some of my scripts with a friend and it would be convenient if there was a way of me sending him logic that would run without requiring inputs from them

---

## 2020-06-29 18:27:17 - issues channel

**liam**

Looks like a fix was deployed this morning so it is now at least consistent  [@U0131E2H1SP](@U0131E2H1SP) [https://forum.developer.betfair.com/forum/developer-program/announcements/32066-retrospective-api-release-to-prevent-minimum-bet-abuse-19th-june|https://forum.developer.betfair.com/forum/developer-program/announcements/32066-retrospective-api-release-to-prevent-minimum-bet-abuse-19th-june](https://forum.developer.betfair.com/forum/developer-program/announcements/32066-retrospective-api-release-to-prevent-minimum-bet-abuse-19th-june|https://forum.developer.betfair.com/forum/developer-program/announcements/32066-retrospective-api-release-to-prevent-minimum-bet-abuse-19th-june)

---

## 2020-06-29 10:23:29 - general channel

**sartux**

same problem with requests



{"faultcode":"Server","faultstring":"DSC-0002","detail":{}}

---

## 2020-06-29 10:06:37 - general channel

**sartux**

```url = "[https://cashout-service.betfair.it/cashout-service/transactional/v1.0/cashout](https://cashout-service.betfair.it/cashout-service/transactional/v1.0/cashout) "

values = {'appKey': '#######',

          'marketId': marketId,

          'quotePercentage': 100,

          'quoteValue': 0}



data = urllib.parse.urlencode(values)

data = data.encode('ascii')  # data should be bytes

req = urllib.request.Request(url, data)

with urllib.request.urlopen(req) as response:

    the_page = response.read()

    print(the_page)```

  File "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/urllib/request.py", line 649, in http_error_default

    raise HTTPError(req.full_url, code, msg, hdrs, fp)

urllib.error.HTTPError: HTTP Error 500: Server Error

---

## 2020-06-28 19:54:32 - general channel

**Mo**

I don't know what kind of capabilities your host provides but for example on AWS you can have a security policy that only allows access to the box from certain IP addresses

---

## 2020-06-28 19:51:50 - general channel

**Jorge**

Ah so is it a security problem that I am getting people to try to connect to my VPC? I don't use AWS, it's a smaller provider

---

## 2020-06-28 19:50:46 - general channel

**fjt1973**

Thanks for commenting.. I'll try uploading the libraries with my code as package to the function as opposed to using layers for my libraries. AWS is just something I've just starting to look at.

---

## 2020-06-28 19:49:45 - general channel

**Mo**

I mean is it in a cloud service like AWS?

---

## 2020-06-28 19:48:14 - general channel

**Jorge**

Aha, and why would I have 5 processes running *python /usr/bin/fail2ban-server -xf start* in my server?

---

## 2020-06-28 19:45:43 - general channel

**Jorge**

Hey guys, does anyone know what is /usr/bin/fail2ban-server ?? I had problems with Python scripts using a lot of RAM memory and I have found out it is actually because of some executions of this....

---

## 2020-06-28 14:01:59 - issues channel

**Andrey Yunoshev**

```AU

PLACE ORDER BACK

INFO:flumine.order.order:Order status update: Pending

PLACE ORDER LAY

INFO:flumine.order.order:Order status update: Pending

INFO:flumine.markets.blotter:1 order packages created

	BACK/PENDING 0.0 price, 0.0 size

	LAY/PENDING 0.0 price, 0.0 size

INFO:flumine.execution.betfairexecution:execute_place

INFO:flumine.order.trade:Trade status update: Pending

INFO:flumine.execution.baseexecution:Order Place: SUCCESS

INFO:flumine.order.order:Order status update: Executable

INFO:flumine.order.trade:Trade status update: Live

INFO:flumine.order.trade:Trade status update: Pending

INFO:flumine.execution.baseexecution:Order Place: SUCCESS

INFO:flumine.order.order:Order status update: Executable

INFO:flumine.order.trade:Trade status update: Live

INFO:flumine.order.order:Order status update: Execution complete

INFO:flumine.order.trade:Trade status update: Complete

REPLACE PRICE, FROM 4.0 TO 3.7

INFO:flumine.order.order:Order status update: Replacing

	BACK/REPLACING 0.0 price, 0.0 size

	LAY/EXECUTION_COMPLETE 3.7 price, 2.16 size

INFO:flumine.markets.blotter:1 order packages created

INFO:flumine.execution.betfairexecution:execute_replace

INFO:flumine.order.trade:Trade status update: Pending

INFO:flumine.execution.baseexecution:Order Cancel: SUCCESS

INFO:flumine.order.order:Order status update: Execution complete

INFO:flumine.execution.baseexecution:Order Replace: SUCCESS

INFO:flumine.order.order:Order status update: Pending

INFO:flumine.order.order:Order status update: Executable

INFO:flumine.order.trade:Trade status update: Live

INFO:flumine.streams.orderstream:Stopped OrderStream 1001

INFO:flumine.streams.marketstream:Stopped MarketStream 2001

INFO:flumine.streams.marketstream:Stopped output_thread (MarketStream 2001)

INFO:flumine.execution.baseexecution:Shutting down Execution (SimulatedExecution)

INFO:flumine.execution.baseexecution:Shutting down Execution (BetfairExecution)

INFO:flumine.streams.orderstream:Stopped output_thread (OrderStream 1001)

INFO:flumine.baseflumine:Exiting flumine

Traceback (most recent call last):

  File "./pydogs/greyscout2live.py", line 727, in &lt;module&gt;

    framework.run()

  File "/usr/local/lib/python3.7/site-packages/flumine/flumine.py", line 32, in run

    self._process_current_orders(event)

  File "/usr/local/lib/python3.7/site-packages/flumine/baseflumine.py", line 183, in _process_current_orders

    strategy_orders = market.blotter.strategy_orders(strategy)

  File "/usr/local/lib/python3.7/site-packages/flumine/markets/blotter.py", line 27, in strategy_orders

    return [order for order in self if order.trade.strategy == strategy]

  File "/usr/local/lib/python3.7/site-packages/flumine/markets/blotter.py", line 27, in &lt;listcomp&gt;

    return [order for order in self if order.trade.strategy == strategy]

RuntimeError: dictionary changed size during iteration```

---

## 2020-06-28 11:50:24 - general channel

**fjt1973**

[@U4H19D1D2](@U4H19D1D2) is BFL compatible with AWS Lambda? I've uploaded BFL as a package into AWS Layers but I'm getting the following error.

---

## 2020-06-27 16:56:27 - general channel

**Mo**

Assuming you mean the live data

---

## 2020-06-26 15:47:24 - issues channel

**D C**

Afternoon all. Are any of you noticing sticky streaming API in-running today. I have had bad examples at Doncaster 14:25 and 15:25 where there are no price updates for in excess of 4 seconds - right at the closing stage of the race. Anyone else seeing this? I am getting heartbeat so I know I am still connected to the server but no price updates are getting sent.

---

## 2020-06-22 08:03:11 - general channel

**liam**

[https://github.com/liampauling/betfair/blob/master/HISTORY.rst#250-2020-06-22|v2.5.0](https://github.com/liampauling/betfair/blob/master/HISTORY.rst#250-2020-06-22|v2.5.0) released, thanks to [@UUCD6P13J](@UUCD6P13J) for the `file_path` improvement :thumbsup: Strongly recommend the use of github actions for testing/deployment, I don't need to anything other than approve to get things tested / deployed / docs updated

---

## 2020-06-20 14:18:51 - general channel

**Andrey Yunoshev**

```REPLACE PRICE, FROM 9.0 TO 8.6

{'bet_id': '202759245511',

 'customer_order_ref': 'da32eac47a66a-138119515919016440',

 'handicap': 0,

 'id': '138119515919016440',

 'info': {'average_price_matched': 0.0,

          'side': 'BACK',

          'size_cancelled': 0.0,

          'size_lapsed': 0.0,

          'size_matched': 0.0,

          'size_remaining': 2,

          'size_voided': 0.0},

 'market_id': '1.170903586',

 'order_type': {'bet_target_size': None,

                'bet_target_type': None,

                'min_fill_size': None,

                'order_type': 'Limit',

                'persistence_type': 'LAPSE',

                'price': 9.0,

                'size': 2,

                'time_in_force': None},

 'selection_id': 28741430,

 'status': 'Executable',

 'status_log': 'Pending, Executable',

 'trade': {'id': UUID('cae6e176-b2f7-11ea-8807-d0034bcc9d7d'),

           'market_notes': '8.2,8.6,9',

           'notes': '8.6,8.2,9.0,Henlow,2020-06-20 13:17:00,Layton Jim,4',

           'orders': ['138119515919016440'],

           'status': &lt;TradeStatus.LIVE: 'Live'&gt;,

           'strategy': &lt;__main__.LiveStrategy object at 0x10ffbf4d0&gt;}}

INFO:flumine.order.order:Order status update: Replacing```

---

## 2020-06-19 15:01:48 - general channel

**Mo**

No, not sure how you got to that? The runners are listed in the market definitions in the live streaming data but there are no names

---

## 2020-06-18 17:14:21 - strategies channel

**Stefan**

[@U0155J92A7Q](@U0155J92A7Q) Really it does not matter for how many selections you are closing bet position. Have a look at here on app I use for manual trading: [https://bit.ly/3fzrNm6](https://bit.ly/3fzrNm6) and of course you can use dutching as well. You can take it as inspiration, or just reverse engineer app's algorithms. [@U013Z24LD0U](@U013Z24LD0U), yes when you trade in-play where odds moves very rapidly, there is need to recalculate closing bets and update them again, till all bets are matched, or level profit is gained on all selections, therefor we call such short programs bots, they do simple work, and can be reused in more complicated strategy. For instance in football correct score market it makes no sense to close bet position on when opened on already unrealistic scores, so you need live score for the match and feed close market bet position just by selections with possible score lines...

---

## 2020-06-18 05:50:22 - general channel

**Mo**

Yes lots of people here use AWS, myself included

---

## 2020-06-18 05:49:57 - general channel

**gprokisch**

Before I try to move to AWS or Azure I want to know if someone here had success to deploy in API on those services.

---

## 2020-06-18 05:26:02 - general channel

**gprokisch**

Hi guys, I had deployed a script using Google cloud but I'm getting this error [https://stackoverflow.com/questions/25817167/betfair-api-betting-restricted-location-when-logging-in-from-google-app-engine|BETTING_RESTRICTED_LOCATION](https://stackoverflow.com/questions/25817167/betfair-api-betting-restricted-location-when-logging-in-from-google-app-engine|BETTING_RESTRICTED_LOCATION) cause google ip address, in my local computer it's works fine.

---

## 2020-06-17 21:26:11 - random channel

**JonJonJon**

I've had a doodle with it. Hard to say if it improves performance or not directly, as we can't change live orders to a set, and leave it as that. We have to hack this bit too:



```# process current orders

blotter = market.blotter

completed = set()

for order in blotter.live_orders:

    process.process_current_order(order)

    if order.trade.status.value == "Complete":

        #blotter.complete_order(order)

        completed.add(order)

blotter._live_orders = blotter._live_orders - completed```

The above change is obviously not suitable for production as we are modifying the private variable. But I just did it as a quick hack to make things work for testing.



Given variable run times. It might or might not shave 0.2-0.4 seconds off my backtest which, takes around 12.5 seconds to run. But there is variability, so it is impossible to say for sure.



However, using sets expresses the intent of the code clearer, as there should be no duplicates in "live orders", and they should be unordered.



Perhaps we should just leave this for now, but be aware of it anyone reports bottlenecks in this part of the code. I think using sets is more likely to be better if a user has hundreds of live orders at once. I think that is unlikely. But I could be wrong!

---

## 2020-06-16 21:29:01 - random channel

**JonJonJon**

2 questions...



Could this code be moved into blotter, and called something like "process_live_orders"?



```for order in blotter.live_orders:

    process.process_current_order(order)

    if order.trade.status.value == "Complete":

        blotter.complete_order(order)```



---

## 2020-06-16 14:08:16 - issues channel

**Rich**

[https://www.theregister.com/2020/06/16/aws_credit_virtual_summit_incentive/](https://www.theregister.com/2020/06/16/aws_credit_virtual_summit_incentive/)

---

## 2020-06-15 16:46:38 - issues channel

**Peter**

As a newbie to Linux, be aware that there are broadly two flavours: Debian and Centos. Most of the help you'll find on the web is for Ubuntu which is Debian-flavoured. But if you use the AWS Linux its Centos-flavoured. So f you're googling, always include Centos in your search.

---

## 2020-06-15 16:42:30 - issues channel

**Peter**

This might help: a little script that I run on my flumine servers to initialise them. Can probably be optimised a little, but it works for me on AWS Linux 2. The only wrinkle is that sometimes I have to tweak the python version - up or down! Haven't worked out why yet.

---

## 2020-06-15 16:28:44 - issues channel

**liam**

yeah i use aws linux 2

---

## 2020-06-15 16:28:30 - issues channel

**Rich**

AWS linux probably best

---

## 2020-06-15 16:24:39 - issues channel

**Peter**

[@U010GM77S4W](@U010GM77S4W) An extra tip when setting up new instances. At the top of the list that AWS will offer you are the t2 series, but scroll down to the t3s - they're better and cheaper. E.g. Your t2.micro in London is $0.0132 ph, but the t3.micro is only $0.0118 ph. Moreover your t2 has a single cpu, the t3 has two (memory is the same 1GB for both).

---

## 2020-06-15 14:19:54 - issues channel

**liam**

haha, you won't look back, i think a 'how to setup a python/bflw/flumine instance on aws for free' could be a good thing to add to the faq/docs/repo

---

## 2020-06-15 14:12:25 - issues channel

**brightcake**

fwiw i'm recording around 200 markets and it uses up around 0.15% of the CPU on my AWS

---

## 2020-06-15 13:38:51 - issues channel

**liam**

go to EC2 / instances click on the instance and then monitoring

---

## 2020-06-15 13:34:00 - issues channel

**liam**

Can you screenshot the ec2 monitoring graph page? 

---

## 2020-06-15 13:28:43 - issues channel

**liam**

If you go in the AWS console what’s your CPU use like? 

---

## 2020-06-15 13:27:30 - issues channel

**mandelbot**

I keep getting latency warnings when I run the marketrecorder (even though other instances are fine). I am running on an AWS EC2. Any ideas why? Is it because im running a few instances?

`{"asctime": "2020-06-15 12:20:13,784", "levelname": "WARNING", "message": "[Stream: 1001]: Latency high: 2465.7244164943695"}`

that instance gets jammed up and continues only after hitting return

---

## 2020-06-15 09:50:51 - random channel

**D**

[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html|https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html|https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-regions-availability-zones.html)

Glad I could contribute something!

---

## 2020-06-15 09:41:56 - random channel

**D**

Just for info, the AWS availability zones a,b,c are assigned randomly to each account, I.e. your zone 1a could be somebody else’s 1c. This is to stop everyone piling into the same zone. 

---

## 2020-06-13 07:42:57 - random channel

**mandelbot**

is the AWS free tier only available in us regions? No option to choose UK it seems.

---

## 2020-06-13 06:52:59 - issues channel

**liam**

S3 recorder, running on an aws ec2 instance (nano) and I just start and forget 

---

## 2020-06-08 12:00:40 - issues channel

**liam**

`RunnerContext.invested` is your golden source in knowing if you have a live trade in the market, if you override it then you need another way to prevent your strategy from placing trades/orders on every update

---

## 2020-06-08 11:54:39 - issues channel

**liam**

it will block the second because it limits one live trade per selection per strategy, you can override the [https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py#L118|validate](https://github.com/liampauling/flumine/blob/master/flumine/strategy/strategy.py#L118|validate) function, test first!

---

## 2020-06-08 10:30:42 - strategies channel

**Remi**

So I finally got a live key activated a few days ago (thank you [@U4H19D1D2](@U4H19D1D2)!) and the first results look good though this is just a single weekend. Mean is around 7 cents per trade (a single trade can be multiple orders/markets for this strategie), value at risk on that 7ct is low but sometimes I do have small negative returns, and I calculated the probability that the mean is higher than 0 with a one-sided T-test and that came out very close to 1. I feel pretty confident this thing is working (for now). 



I have a few questions though. 



1. Do you optimize order size for rounding+commission? Sometimes you can get one extra cent for free because of rounding. Is this a micro optimization or worth some effort?

2. I don’t mean to be impolite but, do you see markets being manipulated? E.g. in more regulated markets you are not allowed to match your own orders but on Betfair you can (I don’t know if it is allowed on bf). I don’t plan on doing this but I’d like to know better what I am dealing with.

3. What are some strategies to lower the commission? Right now I have 0% commission discount and my Betfair Points isn’t growing.



---

## 2020-06-05 14:28:06 - issues channel

**Dave Simonds**

Theres a bit more to it than just smashing the server… I’ve probably not explained things very well. Sometimes when betfair publish their markets there is no delay, and then its a simple as getting your bet on quick, I’m pleased to say my code-base is quite good at this and I’m first to market more often than not. If all markets were published and in a ready state then I would not need to retry, and like i said if 1 retry is effectively 150 attempts (150 runners across 10 markets published simultaneously) you really rack up the errors quite quickly

---

## 2020-06-04 10:18:15 - general channel

**liam**

I use github actions (test/build) -&gt; AWS ECR -&gt; pulled into ec2 instances (mixture of scripts)

---

## 2020-06-04 09:58:02 - general channel

**AP**

Do the docker images get deployed to an EC2 instance (or similar)?

---

## 2020-06-04 08:04:04 - issues channel

**Ruben**

leaving this here in case someone else comes accross the same problem: I was using a Live key but getting delayed data. Turns out that even if you have an activated live key, if your betfair account is not funded, you will get delayed data

---

## 2020-06-04 08:03:13 - general channel

**Ruben**

I am wondering, what is are the best practices to be able to deploy a python project in any server and avoid it causing too much pain? using conda environments?

---

## 2020-06-03 11:09:31 - random channel

**JonM**

I use Lightsail - as far as I can tell it's just a lighter weight skin on AWS.  Simpler website/interface etc.  I'm only using a single VM and MySQL instance, so it suits me.  If you had a more complex infrastructure the limited functionality would get pretty annoying pretty quickly.

---

## 2020-06-03 10:15:20 - general channel

**liam**

stay well away from dockerhub, the uptime is terrible, the builds takes ages, sometimes they will just stop randomly and you have to wait for it to timeout, after a painful migration I now use github actions + aws ecr

---

## 2020-06-02 23:25:23 - random channel

**PeterLe**

About the same on AWS. I tested by running the same program (and bet criteria) on two of my accounts. One on AWS, one on my home PC.  I then compared the Betids from each account at the end of the day. There was literally nothing in it.

---

## 2020-06-02 23:15:11 - random channel

**Dave**

Yes forgot to mention I run a mysql server on it too. It's 20 USD (DO charge USD so it happens to be under £20). I could probably do with just a dual core machine (rather than quad) which is 10 USD I think, but needed the extra RAM that comes with the quad core to train various models.

---

## 2020-06-02 23:04:08 - random channel

**PeterLe**

I appreciate that all traffic is routed before arriving at betfair, but Just wondered if anyone had a view on which service had the fastest connection overall. I did some simple tests and found that my home PC was as fast as AWS.

---

## 2020-06-02 17:20:43 - random channel

**liam**

[@UBS7QANF3](@UBS7QANF3) not sure if I have asked you this before but are you 100% AWS?

---

## 2020-06-02 16:58:22 - random channel

**Mo**

Roughly speaking if you’re running it 24/7 it’ll be cheaper to get a dedicated server. But AWS gives discounts for paying for eg 12 months in advance which makes the price about the same

---

## 2020-06-02 16:54:21 - random channel

**brightcake**

if you're looking at ec2 you could probably do it with a micro machine

---

## 2020-06-02 16:52:17 - random channel

**PeterLe**

If you havent used AWS before there is a free tier available that should run BFLW

---

## 2020-06-02 16:49:19 - random channel

**jfo**

AWS lightsail isn’t cheap, might even be cheaper than that now

---

## 2020-06-02 16:48:03 - random channel

**JonJonJon**

How much does it cost to run an instance of AWS per month, running a bflw horse trading bot 24x7?

---

## 2020-06-02 14:33:07 - general channel

**Andrey Yunoshev**

```    strategy = LiveStrategy(

        market_filter=streaming_market_filter(

            event_type_ids=["4339"],

            market_types=["WIN"],

            bsp_market=True,

            country_codes=["GB", "UK", "AU"],

        ),

        streaming_timeout=0,

        max_order_exposure=20,

        max_selection_exposure=10,

    )



    client = clients.BetfairClient(trading)

    framework = Flumine(client=client)

    framework.add_strategy(strategy)```

---

## 2020-06-02 14:32:34 - general channel

**liam**

Sorry only live, catalogue isn't available in backtesting but you can get the runners names from the [https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L32|marketDefinition](https://github.com/liampauling/betfair/blob/master/betfairlightweight/resources/streamingresources.py#L32|marketDefinition)

---

## 2020-06-01 12:11:09 - random channel

**Jonatan (skyw)**

[@UBS7QANF3](@UBS7QANF3) I've used vs code for python, I did not like the experience. For me everything is just not as good.

• Visuals, easier to navigate project explorer, notice errors in code

• Easier to refactor code

• Less buggy, Microsoft Language  Server just felt buggy and slow overall

• A lot better auto imports

---

## 2020-06-01 11:29:17 - random channel

**SrFabio**

the big problem is that this type of operation is incredibly hard to test with delay API key, and can be costly to test with live key

---

## 2020-06-01 07:33:20 - general channel

**Will Morrison**

Mo and liam, thanks a ton for the help last week, I super appreciate the package and you guys keeping an active community here! I haven't bought an active key yet, so when I run my modified version of the streaming example, I just get an update from the while loop market_books = output_queue.get() every 180 seconds. For now, I just put some loops underneath that to grab the data that I want from the market books and save it as a csv, but this feels like it can't be the right approach once I have a live key, since I would be copying a bunch of unchanged data constantly. I'm guessing that the way the data stream comes in is that it only sends updates to things that have changed, like the json lines in the historical data? Is there an example that would help me see how to record the stream in a smart way? And thanks in advance for your patience, I suspect that for a more experienced programmer this might be a trivial question to figure out.

---

## 2020-05-27 09:31:03 - general channel

**liam**

yeah that is the idea, I want to clean up the trade/strategy reset logic so its cleaner and easier to work out if you have live trades/pending orders etc. just haven't worked it out in my head yet hence why this hasn't been completed :slightly_smiling_face: I will look to put some warning logs going forward when something is being used which won't do anything

---

## 2020-05-25 12:30:12 - strategies channel

**James T**

I trade order flow on racing, but am actively trying to move to a more fundamental based approach. Order flow techniques tend to be lower margin and less scalable but can be very consistent, but will soon have issues with transaction limits (number of bets you can place and cancel as you move with the price). 



And on fundamentals it’s sort of flipped. You’d aim for bigger margins, fewer bets, but end up with bigger volatility in returns. 



I suppose it’s looking like I might end up somewhere in between that actually. But I won’t know until I finish my project and go live with it. 



Trouble with fundamentals, especially in this current situation, is that it needs historical performance data for the runners. And if there’s been a two month break for the sport, it’ll be hard to know how that translates when it restarts. It’s not something that could be backtested because it’s never happened before. So I’d be cautious on any fundamental approach until there’s at least a couple months of new data. I might even wait until next year now. 

---

## 2020-05-25 07:05:52 - general channel

**Ruben**

on that same topic, to those that have strategies in production, does it happen often that you see a price available that you want, and when you go take it, it is already gone?

---

## 2020-05-24 13:26:50 - strategies channel

**VT**

[@UPMUFSGCR](@UPMUFSGCR) I work as a trader at Betfair for 3 years, full time, I see the market changing and improving every season, correcting the value of the odds according to the time and what happens on the field, without a doubt this is more difficult today than 3 years for those who trade live, I notice repetitive behaviors and this is what I try to automate with bots, but when I start investigating the past data (2 - 4 seasons) the market errors are small and the exorbitant 6% rate of Betfair consumes most of the "errors and gaps" in the market.

---

## 2020-05-24 12:53:22 - general channel

**Julio**

I use Heroku. It is easy to set up, free and you can choose your ip location. You def have less functionalities than AWS but I guess it is the trade off between easy/simple vs complex/bestinclass service.

---

## 2020-05-24 11:56:19 - issues channel

**Steve**

Hi, I'm just started using the streaming API. I consistently get the following message: WARNING:betfairlightweight.streaming.stream:[Stream: 1]: Latency high: 0.5950710773468018. Is this an issue with my connection or the BF server? And what precissely is the 0.5950710773468018 a reference to.

---

## 2020-05-23 14:00:27 - general channel

**liam**

So I have my settings for a strategy which I use for production, I run those exact settings over a set market and assert profit == x on every run 

---

## 2020-05-23 13:59:44 - general channel

**Lee**

backtesting the production image against local backtest results?

---

## 2020-05-23 13:57:47 - general channel

**liam**

Unit testing then integration testing on a handful of markets (every release), once in production I verify backtesting returns the same profit on every run 

---

## 2020-05-23 07:36:02 - general channel

**Mo**

I (and a lot of others) use Amazon Web Services (AWS). You won’t have a problem with IP as you can select the location to run your servers in. For example, London or Dublin

---

## 2020-05-23 00:18:28 - general channel

**VT**

Hi, I'm in the final stage of developing my BOT and now I need to automate and I still don't know how to do that, it's nothing exceptional just cheap json feed for live statistics + previous performance database + a little scraping.



I don't want to leave my computer on 24 hours a day running the system. Any suggestions for (cheap) servers that work and have no problem with the IP? ( my account is from Brazil).

---

## 2020-05-22 11:07:21 - strategies channel

**Jonjonjon**

I did many years ago. But don't bother now as it'll cause problems with my day job (I'm an IT contractor at a bank). Also, it costs a lot more to do algo trading in financials. You need thousands of pounds in margin if you want to just trade single futures contracts. Plus expensive data feeds. It might costs upwards of £2k a month in connectivity and data fees. And one flash crash can wipe out a year or more of profits.



With Betfair, you can realistically start with under £100. You can rent a server to run your bot for under £10 a month. (In my experience) it's easier to find an edge. You get better diversification benefits, due to the hundreds of uncorrelated markets that trade every day. A flash crash wont' wipe out a years worth of profits.



A negative point of doing Betfair vs Financials is that Betfair is probably (in general) less scalable than financials. But that isn't to say that you can't make more than enough to feed your family using Betfair.

---

## 2020-05-22 10:02:22 - issues channel

**Newbie99**

sorry one more question, if I'm reading that correctly minFillSize can only be set in conjunction with FILL_OR_KILL, so there's no way to set a min fill of x but keep the rest of the order live?

---

## 2020-05-21 12:48:57 - general channel

**fjt1973**

Thanks Liam, I've been using Jupyter to develop / test before moving into AWS. I've also noticed you're limited on connections available so am I right in thinking you can only run a max of 10? So being able to end a run would be needed before starting the 11th script?

---

## 2020-05-21 12:39:57 - general channel

**liam**

So flumine is setup for scripting, i.e. you create a script like example.py and then run it on a server (cron/docker etc) It is not really suited for an IDE like jupyter.

However there is certainly some logic missing to end the run when all markets are closed because you have found it it just continues to run, welcome thoughts on this

---

## 2020-05-21 12:36:56 - random channel

**Newbie99**

I have a couple of questions on SP bets (by which I mean bets with a persistence of 'MARKET_ON_CLOSE'), around how Betfair view the £10 minimum liability:



1. If have a lay bet that is partially matched, so say £9.90 gets matched initially (i.e. in live pre-market trading, before the SP is reconciled), is the expected behaviour that this additional 10p gets matched or cancelled?

---

## 2020-05-21 09:31:51 - general channel

**brightcake**

bit of context: i'm trying to deploy a data recorder on aws and want something to be taking the data of all football markets while it runs

---

## 2020-05-19 11:45:41 - strategies channel

**James T**

Backtesting does have its caveats though, as much as I advocate it. You can’t account for market impact or how the market will react to your bets. And that can be enough to kill a live strategy compared to its backtest. 



eg. If you offer, you can’t know if someone will undercut you. If you take, you don’t know how the book will react (potentially further down the line). 



I know [@U4H19D1D2](@U4H19D1D2) had some great ideas on measuring market impact. I’m hoping he’ll include a page on that too when he has time!

---

## 2020-05-19 11:41:00 - strategies channel

**James T**

This is all down to backtesting though. Not live split testing. 



So that’s another good use for a backtesting system like flumine. You can adjust your simulated latency and see what effect that has on returns. 

---

## 2020-05-19 09:39:20 - general channel

**brightcake**

so you keep those market files available on aws?

---

## 2020-05-19 09:34:43 - general channel

**liam**

+1, flumine is built on the premise that you just point it to your data source and the same strategies will work on live and historic data with no changes required from the user. As soon as you move from streaming data things get tricky and large

---

## 2020-05-18 07:52:59 - general channel

**MrBIN**

So do you think that the best is to be hosted in the AWS London Data Center?

---

## 2020-05-18 07:49:52 - general channel

**Will Morrison**

I hadn't looked at the historicalstreaming example because I assumed it was basically the same as the live streaming one but on historical data. Now I see that it has some writing examples which may be exactly what I need to get started. Thanks Liam!

---

## 2020-05-18 07:49:46 - general channel

**MrBIN**

Hi All. Do you know where is Betfair's matching engine server is located? and what is the best AWS datacenter \ availability zone in terms of latency?

---

## 2020-05-17 16:27:02 - strategies channel

**James T**

From what I’ve read, flumine isn’t quite ready yet? Or just about ready for testing? That’s basically what you want to use if you don’t have a backtesting system of your own. Then you just need the data to backtest with. And hope the backtest is a good representation of live betting. Which it mostly is at low stakes and when taking volume. 

---

## 2020-05-17 16:24:04 - strategies channel

**James T**

Well without knowing what the strategy edge is supposed to be (and I guess it might not always be that clear anyway), yes, I’d include everything. 



Are you backtesting or betting live? I would personally backtest so I can try lots of variations in a short space of time, not risk money, etc... But keep a set of out-of-sample data which I can verify the strategy on afterwards. 

---

## 2020-05-17 15:49:19 - strategies channel

**Mo**

This was a good talk at MathSport 2019 "Establishing a performance edge in P2P betting": [https://rstudio-pubs-static.s3.amazonaws.com/509874_7a4aef970d924450bb246cc4b5128d52.html](https://rstudio-pubs-static.s3.amazonaws.com/509874_7a4aef970d924450bb246cc4b5128d52.html)

---

## 2020-05-15 08:58:13 - general channel

**Jonjonjon**

This is my hacked up version of your PriceRecorder:



```from collections import defaultdict



from flumine import BaseStrategy

from flumine.utils import get_price



EPS=1e-6



def diff(a: dict, b: dict):

    """

    Return a minus b



    :param a:

    :param b:

    :return:

    """



    result = {}

    for k, v in a.items():

        difference = v - b.get(k, 0.0)

        if abs(difference)&gt;EPS:

            result[k] = difference



    return result





class PriceRecorder(BaseStrategy):

    """

    Example strategy for recording prices

    from historical or live data.

    """



    def __init__(self, *args, **kwargs):

        BaseStrategy.__init__(self, *args, **kwargs)

        self.market_data = defaultdict(list)

        self.runner_data = defaultdict(lambda: defaultdict(list))

        self.runner_names = defaultdict(dict)

        self._prior_traded_volume = defaultdict(dict)

        self._prior_publish_time = {}

        self.traded_volumes = defaultdict(lambda: defaultdict(list))



    def check_market_book(self, live_market, market_book):

        return market_book.inplay==False



    def process_market_book(self, live_market, market_book):



        market_data = self.market_data[market_book.market_id]

        prior_publish_time = self._prior_publish_time.get(market_book.market_id)

        self._prior_publish_time[market_book.market_id] = market_book.publish_time



        if not market_book.market_id in self.runner_names:

            runners = market_book.market_definition.runners

            d = {runner.selection_id:runner.name for runner in runners}

            self.runner_names = d



        is_currency_update = False



        for runner in market_book.runners:

            runner_data = self.runner_data[market_book.market_id][runner.selection_id]



            k = (market_book.market_id, runner.selection_id)

            prior_traded_volume = self._prior_traded_volume[k]

            traded_volume = {ps['price']:ps['size'] for ps in runner.ex.traded_volume}

            traded_volume_diff = diff(traded_volume, prior_traded_volume)

            self._prior_traded_volume[k] = traded_volume

            if traded_volume_diff:

                min_price = min(traded_volume_diff)

                max_price = max(traded_volume_diff)

                volume = sum(traded_volume_diff.values())



                if prior_publish_time.hour!=market_book.publish_time.hour:

                    if len(traded_volume_diff)&gt;1:

                        is_currency_update = True

                        print('Skipping ', market_book.publish_time)



                if min(traded_volume_diff.values())&lt;0.0:

                    is_currency_update = True



                # if market_book.publish_time&gt;=datetime.datetime(2020, 1, 1):

                #     if max_price==4:

                #         print('stpo')

            else:

                min_price = None

                max_price = None

                volume = 0



            is_currency_update=False



            if not is_currency_update:

                atb = get_price(runner.ex.available_to_back, 0)

                atl = get_price(runner.ex.available_to_lay, 0)



                if max_price and (max_price&gt;atl):

                    print('depth was hit')



                runner_data.append(

                    [

                        market_book.publish_time,

                        runner.last_price_traded,

                        atb,

                        atl,

                        max_price,

                        min_price,

                        volume

                        ]

                )



        market_data.append([

            market_book.market_id,

            market_book.publish_time,

            market_book.status,

            market_book.inplay,

        ])```

---

## 2020-05-14 12:49:26 - general channel

**brightcake**

I'm guessing the S3 is meant for AWS?

---

## 2020-05-14 12:42:00 - general channel

**brightcake**

Does the Flumine example market listener require AWS?

---

## 2020-05-14 10:13:37 - general channel

**Vincent Mele**

Anyone have any tips on how best to continuously record all horse and greyhound races (with a production API key)? I've modified [https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/master/examples/examplestreamingerrhandling.py) with a custom handler to record incoming raw_data (and found flumine only after) with success, but I have a couple questions:

1. Is it correct that new markets are not added to existing streams? e.g., once I set market_filter to "7" and start the thread, it will add all existing horse markets but _not_ add tomorrows' new markets when those markets are published?

2. Because of the 200 market stream subscription limit, what's the best way to manage subscribing to more than 200 automatically? I don't see the logic in flumine and was curious if there was any other source-available methods (otherwise I'll take a stab at it myself of querying for all markets and then building different subscriptions).

---

## 2020-05-13 04:06:26 - random channel

**Tom**

Is anyone on this forum good at scraping data delivered through websockets? There's a stocks app I use that delivers data within a section of their page through this protool. I have scraped every visible node onsite but can't get the websocket part which is delivered through websocket. Using the pythons libs suchs as Scrapy/splash/lua, BS4 and requests-html... Seems the libs don't have the ability. Slightly worried if datasources start to adopt this approach of websocket delivery. Current scraping frameworks are miles behind in terms of documentation for websocket scraping solutions.

---

## 2020-05-12 20:38:56 - general channel

**Lee**

you could do `pip install smart_open[aws]`

---

## 2020-05-12 09:05:30 - general channel

**Lee**

Is there a general rule of thumb how much backtesting I should do before testing a strategy on live markets?

---

## 2020-05-11 17:20:47 - general channel

**Lee**

I'll just be backtesting tonight and will test on live data wed/thursday

---

## 2020-05-04 20:43:37 - general channel

**PeterLe**

How long ago did they start routing via Prolexic mo? Can you recall? I mentioned to Liam the other day; Connection time to betfair for me is currently average 22.98ms (AWS T2 Micro - London), whereas my Home BT Superfast Fibre is 18.83ms. Other than the fact that AWS provides more resilience/fault tolerant etc, is it worth having a VPS these days? (I have screen shot from 2016(?)  and AWS was 12.02ms

---

## 2020-05-03 08:53:05 - random channel

**liam**

With flumine the target is making a simple framework which is going to come at a cost of performance. 

However eventually we can add the ability to use lightweight mode and handle the raw json, this will rapidly speed up backtesting, not worth it for the 99% when live trading unless speed is a priority or you are processing lots of markets. 



Very keen on benchmarking, with my own framework it’s about a minute for each racing market or sub 10s when using lightweight mode. I use a variation of threads, processes and AWS lambda for overnight backtesting. 

---

## 2020-05-01 12:46:10 - general channel

**Keith Davey**

I am wondering though, can this be used in an AWS Lambda?

---

## 2020-04-30 10:17:07 - general channel

**Cagdas Yetkin**

[@UBS7QANF3](@UBS7QANF3) I am trying to learn how to setup the stream and get the live odds. I have the delayed streaming service open now.



If I want to convert this sample tutorial into Belarus soccer live odds streaming then should I change the this way? Let’s say today at 12:00 there is a game between *Smolevichi and Energetik* in Belarus League.



```

market_filter = streaming_market_filter(

    event_type_ids=["1"], country_codes=["BLR"], market_types=["MATCH_ODDS"]

)```

thanks for showing some directions

---

## 2020-04-29 09:45:03 - general channel

**Cagdas Yetkin**

Hello all,



what is the quickest method in `betfairlightweight` to access *in-the-game live odds* for the current live games, for example today Belarus soccer games. I would like to test my application in Belarus soccer games. Any direction could help. Thank you!

---

## 2020-04-28 21:41:01 - random channel

**PeterLe**

In keeping with the above topic; This is an interesting article which draws comparisons between four methods (inc Shin)



[http://article.sciencepublishinggroup.com/pdf/10.11648.j.ajss.20170506.12.pdf](http://article.sciencepublishinggroup.com/pdf/10.11648.j.ajss.20170506.12.pdf)



Taken from : Stephen Clarke, Stephanie Kovalchik, Martin Ingram. Adjusting Bookmaker’s Odds to Allow for Overround. American Journal of Sports Science. Vol. 5, No. 6, 2017, pp. 45-49. doi: 10.11648/j.ajss.20170506.12

---

## 2020-04-21 16:45:32 - general channel

**liam**

yeah its certainly not ready for production, the main parts missing are the controls and order.status, once the latter is implemented simulated execution can start to use it

---

## 2020-04-15 21:11:37 - general channel

**klozovin**

was thinking of spinning up few dozens of AWS instances to see how low does it get... maybe hit a jackpot sometimes :slightly_smiling_face:

---

## 2020-04-15 21:10:54 - general channel

**klozovin**

I vaguely remember Betfair being AWS UK hosted... I get decent pings from AWS and Linode London

---

## 2020-04-15 09:05:31 - general channel

**liam**

place_order will call validate_order first which will check that the market/runner/strategy is not already 'live'

---

## 2020-04-13 20:00:29 - random channel

**James T**

I’ve always started from the Betfair C# samples and gone from there. Never had any issues with the language. Even for ML work I’ve been doing recently I’ll do the R&amp;D in Python and then serialise the model and load it into C# for production use. 

---

## 2020-04-13 19:44:02 - random channel

**Silver Drifter**

Hi team - I'm looking for some general advice: I've been a software developer for 30 years, always using the Microsoft stack (C#, VB, C++, SQL Server, etc).  I'm getting the feeling that Python may be better suited when it comes to the Betfair API specifically.  I'm probably going to teach myself the language.  Does anybody have any pointers as to the best place for me to start, given that I'm an experienced programmer and probably just need to learn the syntax, etc?

---

## 2020-04-11 19:19:08 - issues channel

**Lee**

```{"asctime": "2020-04-11 18:14:24,998", "levelname": "INFO", "message": "Register: marketSubscription 0"}

{"asctime": "2020-04-11 18:14:24,998", "levelname": "INFO", "message": "[Stream: 0]: \"MarketStream\" created"}

{"asctime": "2020-04-11 18:14:24,998", "levelname": "INFO", "message": "Starting historical market 'marketdata/1.169500956'"}

{"asctime": "2020-04-11 18:14:25,002", "levelname": "INFO", "message": "[MarketStream: 0] 1.169500956 added, 1 markets in cache"}

{"asctime": "2020-04-11 18:14:25,005", "levelname": "INFO", "message": "Adding: 1.169500956 to live markets and blotter"}

{"asctime": "2020-04-11 18:15:59,279", "levelname": "INFO", "message": "Completed historical market 'marketdata/1.169500956'"}

{"asctime": "2020-04-11 18:15:59,281", "levelname": "INFO", "message": "Backtesting complete"}

{"asctime": "2020-04-11 18:15:59,281", "levelname": "INFO", "message": "Exiting flumine"}

Sat Apr 11 19:16:51 BST 2020```

---

## 2020-04-11 19:05:41 - issues channel

**Lee**

```(.venv) ➜ git:(master) ✗ python [http://example.py|example.py](http://example.py|example.py) &amp;&amp; date

{"asctime": "2020-04-11 18:03:44,005", "levelname": "INFO", "message": "Creating new &lt;class 'flumine.streams.historicalstream.HistoricalStream'&gt; (1000) for strategy Ex"}

{"asctime": "2020-04-11 18:03:44,006", "levelname": "INFO", "message": "Creating new &lt;class 'flumine.streams.historicalstream.HistoricalStream'&gt; (2000) for strategy Ex"}

{"asctime": "2020-04-11 18:03:44,006", "levelname": "INFO", "message": "Starting flumine"}

{"asctime": "2020-04-11 18:03:44,006", "levelname": "INFO", "message": "Register: marketSubscription 0"}

{"asctime": "2020-04-11 18:03:44,006", "levelname": "INFO", "message": "[Stream: 0]: \"MarketStream\" created"}

{"asctime": "2020-04-11 18:03:44,006", "levelname": "INFO", "message": "Starting historical market 'marketdata/1.169499207'"}

{"asctime": "2020-04-11 18:03:44,009", "levelname": "INFO", "message": "[MarketStream: 0] 1.169499207 added, 1 markets in cache"}

{"asctime": "2020-04-11 18:03:44,012", "levelname": "INFO", "message": "Adding: 1.169499207 to live markets and blotter"}

{"asctime": "2020-04-11 18:04:11,383", "levelname": "INFO", "message": "Completed historical market 'marketdata/1.169499207'"}

{"asctime": "2020-04-11 18:04:11,384", "levelname": "INFO", "message": "Register: marketSubscription 0"}

{"asctime": "2020-04-11 18:04:11,384", "levelname": "INFO", "message": "[Stream: 0]: \"MarketStream\" created"}

{"asctime": "2020-04-11 18:04:11,384", "levelname": "INFO", "message": "Starting historical market 'marketdata/1.169499212'"}

{"asctime": "2020-04-11 18:04:11,387", "levelname": "INFO", "message": "[MarketStream: 0] 1.169499212 added, 1 markets in cache"}

{"asctime": "2020-04-11 18:04:11,389", "levelname": "INFO", "message": "Adding: 1.169499212 to live markets and blotter"}

{"asctime": "2020-04-11 18:04:36,102", "levelname": "INFO", "message": "Completed historical market 'marketdata/1.169499212'"}

{"asctime": "2020-04-11 18:04:36,102", "levelname": "INFO", "message": "Backtesting complete"}

{"asctime": "2020-04-11 18:04:36,102", "levelname": "INFO", "message": "Exiting flumine"}

Sat Apr 11 19:04:45 BST 2020```



---

## 2020-04-11 07:17:16 - general channel

**liam**

[@UUCD6P13J](@UUCD6P13J) what do you mean? I run about 5 strategies each collecting data on one instance in production with no problems 

---

## 2020-04-10 22:14:41 - general channel

**Lee**

Is it okay to keep the market recorder running in flumine as one of the strategies in production or keep them as two separate services?

---

## 2020-04-10 21:59:19 - issues channel

**liam**

You shouldn’t connect direct to a db from production code without an api in front (imho)

---

## 2020-04-10 21:51:27 - issues channel

**Oliver Varney**

Does anyone run a backend sql server database. If so, what library/ module do they use?. I can get down to about 10 milliseconds insert/updates  but I'm not sure if I can go further with python?

---

## 2020-04-10 18:04:21 - strategies channel

**Jonatan (skyw)**

Just read live data... I do not know about the other sorry ..

---

## 2020-04-10 17:59:12 - strategies channel

**Jonatan (skyw)**

Checkout Flumine, then you can collect live data yourself without paying :slightly_smiling_face:

---

## 2020-04-06 16:10:00 - general channel

**liam**

Release on flumine to [https://github.com/liampauling/flumine/blob/master/HISTORY.rst#120-2020-04-06|1.2.0](https://github.com/liampauling/flumine/blob/master/HISTORY.rst#120-2020-04-06|1.2.0), a lot of breaking changes in preparation for execution integration (live and simulated)

---

## 2020-04-02 10:12:28 - general channel

**Remi**

Given that you want to create a GUI it might be a lot easier to create a web app with a python server behind it, even factoring in that you’d have to additionally learn html/css + javascript + a js framework (you could see this as a single skill).

---

## 2020-03-31 10:24:07 - issues channel

**mandelbot**

Ah I didn't realize I need AWS, how do I configure to save locally? (i am using marketrecorder). I do have some basic programming experience (VBA).

---

## 2020-03-31 10:19:13 - issues channel

**Mo**

You need to configure your AWS credentials

---

## 2020-03-30 09:40:08 - random channel

**liam**

Anyone got experience in AWS Aurora? (specifically multi readers)

---

## 2020-03-24 05:29:59 - general channel

**liam**

[@UU6AE43V0](@UU6AE43V0) many purchase historic data but you can record your own using [https://github.com/liampauling/flumine|flumine](https://github.com/liampauling/flumine|flumine) which is now being developed into a trading framework as well. I store the data in aws s3 and have an API/database which allows me to browse / search markets

---

## 2020-03-22 22:14:41 - general channel

**Rory**

just catching up on recent messages ... lots of good questions/discussion around ML ... we've been using it heavily in soccer for the last 2 years .. there is absolutely no doubt that machines are better at spotting patterns in large data sets than humans ... that's just proven beyond doubt by now with most of the publicly available models out there (think AWS image recognition, Google Translate, etc)



but sport by nature is pretty random, so it's a long slog to find the right combination of models to use and the right data to feed them ... we're definitely getting there but it takes a lot of dedication

---

## 2020-03-17 16:49:43 - general channel

**Alex A**

Nice opportunity to do all my TODOs before going into production properly I guess.

---

## 2020-03-16 17:17:50 - issues channel

**Peter**

Couple of Flumine newbie questions ... Using it to stream historic data locally, but this appears to create an unnecessary streaming connection with Betfair, Tried overriding the enter method to remove login in, and then also to remove workers to drop the keep alive, but it continued to log new connection IDs (or if I processed multiple files in a loop, multiple connection IDs). Is there a way to stop this?

---

## 2020-03-09 11:29:14 - issues channel

**liam**

Have you seen the error handling / production example?

---

## 2020-03-05 10:18:57 - issues channel

**liam**

I use something very similar to flumine and handle 500-1000 markets per instance with 20-50 strategies, 3+ order streams, 5+ market streams placing 10's of thousands of bets per hour all off a single server (single cpu / 2gb ram)

---

## 2020-03-05 10:13:10 - issues channel

**liam**

yeah single, streaming is so lightweight there is no need to have multiple servers unless you are subscribing to a lot of markets / like dealing with dist systems, kiss

---

## 2020-03-05 10:12:10 - issues channel

**Peter**

Oh. I think a penny just dropped. Are you using a single application, and therefore one instance of the class subscribing to streams? I run separate processes on different servers triggered by cron, so I have multiple independent processes all starting at the same place. However, it's on my todo list for today to dive deeper into flumine, so that may change how I go about this.

---

## 2020-03-02 09:42:01 - general channel

**liam**

yep and docs for those interested, expect a lot breaking changes so pin to that version when using in production [https://liampauling.github.io/flumine/](https://liampauling.github.io/flumine/)

---

## 2020-03-01 08:45:12 - general channel

**Mo**

Login sessions last for 8 hours. You need to call the keepAlive endpoint to refresh them: [https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py|https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py)

---

## 2020-02-29 22:07:37 - general channel

**jhaa**

I get occasional 'INVALID_SESSION_INFORMATION' in list_market_book which for now kills my bot and restarts it. Why do I likely get that error? Is that a server restart that requires a new login?

---

## 2020-02-29 17:21:04 - strategies channel

**Lee**

I’ve been testing my first (simple) strategy on live data and seem to be able to make ~5pence per a back+lay trade from £2 stakes.

I know I can potentially optimise the strategy and increase these stakes but is ~1% profit on my stake (£2 x 2 trades) a reasonable strategy? I know I should aim for as much as possible but just trying to understand whats realistic vs risk and reward.

---

## 2020-02-26 18:54:28 - issues channel

**Remi**

But just to be clear, are you saying that betfair will not give detailed data about the orderbook without a live key (expected behavior) or are you just saying “that’s your problem, FU” ?

---

## 2020-02-25 16:30:55 - issues channel

**Peter**

[@UPEFBEEL8](@UPEFBEEL8) In case this helps, I develop in Jupyter notebooks before refactoring into python scripts for deployment. This is the skeleton structure that I use:

---

## 2020-02-21 23:06:05 - issues channel

**Jonjonjon**

Does anyone here use Squid as a proxy server to connect to Betfair?



I have set up a proxy server, and am attempting to download some event types using the following example code, but it gives me this error:



URLError: &lt;urlopen error Tunnel connection failed: 503 Service Unavailable&gt;



```headers = {'X-Application': APPKEY, 'X-Authentication': SESSION_TOKEN, 'content-type': 'application/json'}

event_type_req = '{"jsonrpc": "2.0", "method": "SportsAPING/v1.0/listEventTypes", "params": {"filter":{ }}, "id": 1}'



req = urllib2.Request(API_NG_URL, event_type_req, headers)

req.set_proxy(https_proxy, 'https')

req.set_proxy(http_proxy, 'http')

response = urllib2.urlopen(req)

print response.read()```



---

## 2020-02-16 17:24:05 - general channel

**Peter**

[@UPMUFSGCR](@UPMUFSGCR) Might not be relevant, but I had a problem a couple of years ago when 1 in 3 of my stream connections were failing. At the time I was using a server in the Netherlands. Managed to track it down to specific servers at the Betfair end (from the connection IDs) and it turned out to be a failing process at the Betfair end, failing to properly replicate acceptable IP addresses to some new validation servers. Once identified they were very quick to sort it out.

---

## 2020-02-14 22:54:50 - general channel

**Jonatan (skyw)**

Cool, there are some ways to set it up, but someone else might have a solid working production code for it : )

---

## 2020-02-08 12:27:37 - general channel

**Mo**

I believe that a lot of us use AWS, yes

---

## 2020-02-08 12:26:56 - general channel

**JK**

Ah that's interesting, thanks for that. prob saved me a few hours in the future haha. In production how would you run your bot? Like would I just let a script run 24/7 on an ec2 instance for example? Not a dev so pretty new to all this stuff.

---

## 2020-02-08 12:21:03 - general channel

**Mo**

One important "gotcha" regarding order streaming is that when you first subscribe you only receive data on *unmatched* orders, not matched ones. You would have to use the listCurrentOrders API call if you were interested in these or just make sure that the order stream is running before you place any bets. It doesn't sound like this would be a big problem for your strategy as it sounds like your orders are short lived but it's a surprising limitation of the order stream.

---

## 2020-01-29 09:39:00 - general channel

**JonJonJon**

It's not something I'm currently looking at. I've found that live testing on the exchange is better than doing backtesting on historical data.

---

## 2020-01-26 12:34:42 - issues channel

**Jonatan (skyw)**

I wonder if this is the intended behavior,



[https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/streaming/betfairstream.py#L210](https://github.com/liampauling/betfair/blob/f41260e759b27ceb1b91444cb7fd626c8a5acfcc/betfairlightweight/streaming/betfairstream.py#L210)



``` while self._running and part[-2:] != crlf_bytes:

            try:

                part = self._socket.recv(self.buffer_size)```

First could this not result in endless loop (theoretically)?



Assume that we choose our receive read buffer small and that the server pushes messages faster then we can read. Then the kernel buffer will never empty if we are not lucky and find a delimiter.



This would not yield a real world problem but should we not read up until the first delimiter  and then process, or set some limit on how many maximum messages  we should receive?

---

## 2020-01-24 17:15:19 - random channel

**Tom Poole**

To Senior DevOps Engineers, perm or contractors looking for some security, who want a new and exciting opportunity within a sports and data streaming organisation, 3d immersive visualisation and gamification of sporting events is the plan. Tech stack - AWS/GCP, Docker, Jenkins, Terraform, Kubernetes and Ansible.  Location - Central London. Competitive salary, exceptional bonus and benefits. Let me know if you want to learn more, or know of anyone that may want a step in the right direction.

---

## 2020-01-21 16:33:51 - general channel

**Rory**

quick question re streaming prices ... I'm using a Live key and pulling soccer prices, like this:

---

## 2020-01-21 11:44:33 - general channel

**Mo**

Well one way of dealing with it would be to have a separate thread keeping your session token alive so when the connection is lost and it tries to reconnect with that session token then it’s valid. 



But I think a better way would be to handle the error, relogin and resubscribe

---

## 2020-01-21 11:28:31 - general channel

**ash**

Morning, I have been trying out streaming using how it is set up in the new examplestreamingerrhandling.py and I keep getting this error.

everything works fine but if I get a connection issue, for example, I am getting this:

```    raise SocketError('[Connect: %s]: Connection closed by server' % (self._unique_id,))

betfairlightweight.exceptions.SocketError: [Connect: 1002]: Connection closed by server

21/01/2020 11:00,581 GB __main__ INFO Starting MarketStreaming

21/01/2020 11:00,713 GB betfairlightweight.streaming.listener INFO [Connect: 1002]: connection_id: 202-210120110000-541

21/01/2020 11:00,756 GB betfairlightweight.streaming.listener ERROR [Subscription: 1003] INVALID_SESSION_INFORMATION: UnrecognisedCredentials

21/01/2020 11:00,756 GB __main__ ERROR MarketStreaming run error

Traceback (most recent call last):

  File "/Volumes/2/GitHub/OddsTrading/versionTWO.py", line 115, in run

    self.stream.start()

  File "/Users/ash/anaconda3/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 60, in start

    self._read_loop()

  File "/Users/ash/anaconda3/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 198, in _read_loop

    self._data(received_data)

  File "/Users/ash/anaconda3/lib/python3.7/site-packages/betfairlightweight/streaming/betfairstream.py", line 239, in _data

    raise ListenerError(self.listener.connection_id, received_data)

betfairlightweight.exceptions.ListenerError: connection_id: 202-210120110000-541, data: {"op":"status","id":1003,"statusCode":"FAILURE","errorCode":"INVALID_SESSION_INFORMATION","errorMessage":"UnrecognisedCredentials","connectionClosed":true,"connectionId":"202-210120110000-541"}

21/01/2020 11:00,759 GB __main__ INFO Starting MarketStreaming```

This is then stuck in the `@retry` loop raising the same error each time.

---

## 2020-01-20 18:06:49 - general channel

**Fab**

[@URM3TT0UE](@URM3TT0UE) It depends on what you're building. For example, if your aim is to take a snapshot of horse markets 3 minutes before the off and at post time, polling is a good solution. You'll only hit the Betfair API endpoint 2 times per market.



But if you want to gather, process and react to data more frequently (think in-play horse racing), polling is inefficient and the reason lies in the way HTTP works. Every time you hit a Betfair API endpoint:



- Your machine establishes a new connection with the Betfair server.

- It sends an HTTP request and receives and HTTP response.

- HTTP requests/responses carry headers, so more data is sent/received.



Streaming is more efficient because:



- The connection gets established once.

- From that moment, data keeps flowing from Betfair to your machine.

- There are no data overheads as you would have with polling / HTTP.

---

## 2020-01-19 12:35:56 - general channel

**bting91**

i havent signed up for the live data one yet

---

## 2020-01-08 20:28:20 - issues channel

**DonJ80**

thanks Liam, i appreciate it. i will test it and let you know. "_*its a simplification of what I use"  what other/new features it includes without giving details and without sharing code, i am and i think others will be very interested to know general information about production level features that professionals like you use or implement and learn from you*_?

---

## 2020-01-08 17:14:44 - general channel

**AP**

Ok so for trading live the historical data is stored in memory for the most part 

---

## 2020-01-08 08:53:05 - issues channel

**liam**

[@US6CDMD7Z](@US6CDMD7Z) I have added a new example which covers error handling and retrying that you and others might find handy, its a simplification of what I use in production and its rock solid :smile: It is in the v2.0 branch that I am working on [https://github.com/liampauling/betfair/blob/task/version-2.0/examples/examplestreamingerrhandling.py](https://github.com/liampauling/betfair/blob/task/version-2.0/examples/examplestreamingerrhandling.py)

---

## 2019-12-24 17:25:56 - general channel

**Chris**

If you have high latency on a streamer (downloading data with flumine on a server), does this mean it could potentially miss updates? Or is it just saying that it will take longer to process

---

## 2019-12-20 15:14:29 - general channel

**liam**

So I use docker and then the cloud watch driver so the logs get pushed to AWS. I see you are using azure have you used this before? 

---

## 2019-12-19 14:57:24 - general channel

**Rory**

the NoSQL bandwagon is alive and well in many places (speaking from experience) ... like everything else in tech, there are valid use cases for it but people are too quick to shoe horn it as a solution to every problem

---

## 2019-12-18 12:56:34 - general channel

**Newbie99**

A very general question, but what do people tend / prefer to use for storing 'live data', by which I mean price data or similar from currently live markets?



I have been using MySQL for my static stuff, but thats way too slow to use for pricing data, so until now I've just created a previous market state in my Python scripts, however I want to get a bit more sophisticated (and possibly tidy up the code too!), so was thinking of an in-memory database like Reddis (which I have seen mentioned on here previously), but was just curious to see what others are using.

---

## 2019-12-14 23:08:39 - general channel

**fjt1973**

Evening All... this has been racking by brains for days. Creation and Uploading of a SSL Certificate! I have a Ubuntu server running, created / uploaded and verification of the certificate works just fine. However, Windows 10, Windows Server 2012 and Windows Server 2016 with certificated created by OpenSSL I get the following error when running the Curl Command from the directory the certificated are sat in "{"loginStatus":"CERT_AUTH_REQUIRED"}". I've even tried running the Curl command from root and putting the in the full path...... Is there something I'm missing!

---

## 2019-12-02 10:52:13 - random channel

**Tom Poole**

Liam has given me permission to post an exciting new DevOps Lead Engineer opportunity in a high traffic data and streaming platform sports company, based in Central London.



As Lead DevOps Engineer you will report to the VP of Technology and will be tasked with shaping and executing the cloud and DevOps strategy. Given the seniority of the role, you should have a strong analytical mindset, be a natural problem-solver and enjoy working closely with Software Engineering teams to create a best in breed infrastructure ecosystem. You'll be responsible for shaping, building and monitoring a scalable cloud-based infrastructure (hybrid AWS / GCP… you decide!) and live operations environment. Given this is a new greenfield team you’ll have the opportunity to do this speed, scalability and security in mind. You’ll be given the autonomy to choose the best technologies and processes that will allows Engineers to consumer services effortlessly. The company has a great culture with lots of autonomy and empowerment to do things the right way.



As Lead DevOps Engineer you will need the following:

•	DevOps Engineer

•	Cloud experience AWS and/or GCP

•	Containerisation

•	CI / CD Pipelines

•	Strategy and execution

•	Docker and Kubernetes

•	Excellent communication skills

•	Advocate Agile working

•	Sports enthusiast



In return you will receive a generous salary package ranging from £90,000 - £110,000 plus bonus as well as some great benefits including free gym memberships, private medical/dental insurance and a competitive annual holiday plan.



If you are interested in learning more about the role, then please let me know when you’re next available, so I can update my diary. I look forward to hearing from you

---

## 2019-12-01 16:09:05 - general channel

**liam**

It’s setup for one when it comes to historical data, obvs the live stream can handle multiple because multiple markets come from betfair and there is only one market per file 

---

## 2019-11-28 13:26:08 - issues channel

**Evaldas**

but turns out it doesnt log errors, you live and learn

---

## 2019-11-18 17:46:42 - general channel

**Cagdas Yetkin**

Hi! I am new here. I wrote a few small applications already to place bets. I want to bring in the live statistics like number of corner kicks. What method do you prefer for that? I can see some python libraries when i google it. There are also a few paid APIs, like 19 usd/month.

---

## 2019-11-10 03:16:26 - general channel

**keoki**

Hi all, I'm new here so apologies for asking questions that I'm sure have been answered before.

- What charges are involved in obtaining a live app key, and using it to execute transactions thereafter?

- Are there throttling limits?

---

## 2019-11-03 18:08:15 - general channel

**Mo**

I believe they should be AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY

---

## 2019-10-25 09:39:26 - general channel

**stefan**

[@UBS7QANF3](@UBS7QANF3) [@U4H19D1D2](@U4H19D1D2) Just to let you know that the service should be working again. They have made some changes, and it has been live for the past 2 weeks. During that time I was able to download again via the website without any issues: `[https://forum.developer.betfair.com/forum/developer-program/historical-data/26927-fixed-historical-data-data-for-nov-18-to-date](https://forum.developer.betfair.com/forum/developer-program/historical-data/26927-fixed-historical-data-data-for-nov-18-to-date)`

---

## 2019-10-04 12:08:31 - general channel

**Unknown**

so for that problem i have a strategy runner_context which stores the current state, ie. if an order is live, this would stop an order being placed if in a pending state

---

## 2019-10-01 08:32:16 - general channel

**OT**

Has anyone ever had betfair stream subscription requests rejected (connection reset by peer) ? I find it works for 12 hours or so and then starts rejecting the requests. I've tried reducing the volume of requests, incase it is some kind of volume-type issue. My software has some architecture flaws, but I'm almost finished with the replacemnt. Just need a quick fix on the old one at the moment ://

---

## 2019-09-16 07:31:46 - general channel

**Newbie99**

yep, I had just stripped out the market bits, the full code is:



```import threading

import logging

from datetime import datetime

import mysql.connector

import queue

from new_functions import calc_live_order_risk, view_runners, view_orders, database_run_sp, getprofitandloss, check_queue, \

    set_initial_market_book, inital_position, create_initial_market_book, calc_initial_risk, check_positions, live_positions, create_initial_risk, combine_stream_results

import auto_hedge

import betfairlightweight

from betfairlightweight.filters import (

    streaming_market_filter,

    streaming_market_data_filter,

    streaming_order_filter,

    market_filter,

    price_data,

    price_projection,

    ex_best_offers_overrides

)

from account_info import accname, accpass, acckey, path, db_host, db_user, db_passwd, db_database

from betfairlightweight.resources.bettingresources import MarketBook, CurrentOrders

from betfairlightweight.endpoints.account import Account

from betfairlightweight.endpoints.betting import Betting

from place_orders import place_order



logging.basicConfig(level=logging.DEBUG)  # change to DEBUG to see log all updates



filters = betfairlightweight.filters



mysql = mysql.connector



logging.basicConfig(level=[http://logging.info|logging.info](http://logging.info|logging.info))



trading = betfairlightweight.APIClient(accname, accpass, acckey, certs=path)

trading.login()



order_queue = queue.Queue()

order_listener = betfairlightweight.StreamListener(output_queue=order_queue)

order_stream = trading.streaming.create_stream(listener=order_listener)

order_filter = streaming_order_filter(

    include_overall_position=True

    # customer_strategy_refs=[""]

    #    partition_matched_by_strategy_ref=None

)

order_stream.subscribe_to_orders(

    order_filter=order_filter)

order_stream.start(async_=True)



market_ids=['1.148222764','1.132099836','1.130856098']



market_queue = queue.Queue()

market_listener = betfairlightweight.StreamListener(output_queue=market_queue)

market_stream = trading.streaming.create_stream(listener=market_listener)

market_filter = streaming_market_filter(

market_ids=market_ids)



market_data_filter = streaming_market_data_filter(

    fields=['EX_ALL_OFFERS', 'EX_MARKET_DEF', 'EX_TRADED', 'EX_LTP'])



market_stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter)



market_stream.start(async_=True)



mb =[]



initial_risk_list = create_initial_risk(trading, filters, market_ids)



def consumer(queue, initial_risk_list):

    while True:

        message = queue.get()

        output = check_queue(message,'market_list', 'order_list', update_time, initial_risk_list)

        live_data = combine_stream_results(output, mb, queue, update_time)





if __name__ == "__main__":

    format = "%(asctime)s: %(message)s"

    logging.basicConfig(format=format, level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO), datefmt="%H:%M:%S")

    pipeline = queue.Queue(maxsize=50)

    c = threading.Thread(target=consumer, args=(pipeline,initial_risk_list))

    c.start()



while True:

    update_time = datetime.now().strftime("%m/%d/%Y, %H:%M:%S")

    order_books = order_queue.get()

    for order_book in order_books:

        print(order_book.streaming_update)

    order_list = {'order_list': view_orders(order_books)}

    pipeline.put(order_list)

    market_books = market_queue.get()

    for market_book in market_books:

        print(market_book.streaming_update)

    runner_list, mb = view_runners(mb, market_books, mysql, db_host, db_user, db_passwd, db_database)

    runner_list = {'runner_list': runner_list}

    pipeline.put(runner_list)

    market_list = {'market_list': mb}

    pipeline.put(market_list)```

---

## 2019-09-14 23:22:49 - general channel

**Newbie99**

I think I'm still missing something here...



`{'id': '1.132099836', 'orc': [{'id': 13231442, 'uo': [{'id': '178557559012', 'p': 48, 's': 2, 'side': 'B', 'status': 'E', 'pt': 'L', 'ot': 'L', 'pd': 1568499248000, 'sm': 0, 'sr': 2, 'sl': 0, 'sc': 0, 'sv': 0, 'rac': '', 'rc': 'REG_GGC', 'rfo': '', 'rfs': ''}]}]}

{'id': '1.132099836', 'orc': [{'id': 13231442, 'uo': [{'id': '178557559012', 'p': 48, 's': 2, 'side': 'B', 'status': 'EC', 'pt': 'L', 'ot': 'L', 'pd': 1568499248000, 'sm': 0, 'sr': 0, 'sl': 0, 'sc': 2, 'sv': 0, 'rac': '', 'rc': 'REG_GGC', 'rfo': '', 'rfs': ''}]}]}

{'id': '1.132099836', 'rc': [{'batl': [[9, 110, 3], [8, 100, 1.99], [7, 80, 4], [6, 75, 1.99], [5, 70, 2.01], [4, 65, 2.01], [3, 60, 4.01], [2, 55, 1.98], [1, 50, 7.4], [0, 48, 2]], 'id': 13231442}]}`



So  I placed a lay order for £2 @ 48, the first update comes through for the order and then I cancel it, hence the second update comes through, again for the order.



I then receive 1 market book update, showing the updated market with the order still live, having already received both of the order messages.



I can live with the weird ordering as that presumably can't be helped...but the issue is I never received another market book update.



I've now removed any reference to ladder levels completely and (initially) the updates appear to be working. Just to make sure my logic is (hopefully finally) sound, if I have no reference to ladder levels and therefore am intending to receive the entire non-virtual order book as you suggest, is there anything else I could have inadvertently (or just plain stupidly :slightly_smiling_face: set, that might be limiting the market book messages somehow?

---

## 2019-08-16 17:33:30 - general channel

**Rory**

a little bit like Cloud9 ... where you can write a serverless function, debug it, test it and then deploy that same function ... all from within the IDE ... no messing around with different tools

---

## 2019-08-16 12:05:41 - general channel

**liam**

unless you are using a slim/alpine image or something funky I see no benefit in debugging your code in a container. Normally flow is code/debug in an ide -&gt; create container -&gt; run tests using unittest or docker-compose-test -&gt; deploy

---

## 2019-08-16 12:03:51 - general channel

**Rory**

I take that to mean actual dev and not just deployments though ... unit tests obviously help but I'm still pretty keen on using a debugger ... and that's what holds me back from doing proper dev on containers (for Python at least. Eclipse has the notion of remote debuggers for Java).

---

## 2019-08-16 09:27:52 - general channel

**Ian**

I've found dockerising python development to be very liberating - i'm an oldschool desktop dev learning python/docker &amp; bflw/api at the same time. Using azure not aws. don't want to turn this into a docker conversation but I found this guide useful - onlything not working for me is the watchmedo

[https://hackernoon.com/efficient-development-with-docker-and-docker-compose-e354b4d24831](https://hackernoon.com/efficient-development-with-docker-and-docker-compose-e354b4d24831)

---

## 2019-08-15 13:05:55 - general channel

**Mo**

Redis is an in memory database but it has lots of functionality built upon that (you can use it for pubsub messaging for example). MySQL, Postgres etc would be RDBMSs. From your brief description, I suspect Redis might be a better fit for what you’re using MySQL for. Very generally speaking, I think Redis is better suited for live trading and a RDBMS for all of your “offline” data needs like backtesting, bet reconciliation etc

---

## 2019-08-15 12:57:54 - general channel

**Rob (NZ)**

Cheers Mo ... is the AWS Rds a RDBMS? My main experience is with a local server with mysql and then using phpmyadmin over the top of it so just trying to understand how these parts interact

---

## 2019-08-15 12:50:06 - general channel

**liam**

Infrastructure as code, manages the state of what you have deployed. As it’s code you can put it under version control, once you get used to it you never want to go back to using the AWS console 

---

## 2019-08-15 12:49:18 - general channel

**Jonatan (skyw)**

If you never dealt with containers before, I would first try to dockerize your app and have a way to build new images easily as a first step,



I use pycharm which can make use of docker containers as python interpreter which makes it easy to develop with.



When you got your Dockerized app just get it into the cloud, without worrying about deployment cycle :slightly_smiling_face:



Then terraform I would suggest :slightly_smiling_face:

---

## 2019-08-15 12:18:01 - general channel

**liam**

that way you can connect from local and from ec2

---

## 2019-08-15 12:17:40 - general channel

**liam**

(aws mysql)

---

## 2019-08-15 12:16:58 - general channel

**Rob (NZ)**

Awesome so good logic would be to setup a ec2 instance and use docker to deploy on it with the python code and betfairlightweight and other python libraries...  any thoughts on then how I should have my database ... so that the selections can be picked up etc (in my current setup it's just a local mysql setup so just not sure what would be the best in the cloud )

---

## 2019-08-15 12:09:09 - general channel

**liam**

you can spin up an ec2 instance for a few dollars a month

---

## 2019-08-15 12:07:42 - general channel

**Rob (NZ)**

Any advice for taking the leap from running code locally to putting it in AWS or GCP etc . I currently have a database with selections and some code that checks that every 30secs to look for bets to place .. if a bet meets the criteria then it places the bet ..  i use Jupyter notebooks so have an initial step that does the connection to the api so that seems to be persistent when I run other cells like the betting code etc.   Just wondering what type of structure others are using and if anyone knows of any good blogs on it etc. Happy to change my structure to something better but just not sure what that is..   wondering if a virtual machine is the way to go or possibly just use lambdas on AWS etc

---

## 2019-08-05 20:35:41 - general channel

**Conr**

Hi, anyone got any thoughts on how to connect without certs? I am trying to deploy an App to ping the api every 30 mins , but certs are proving troublesome when connecting

---

## 2019-07-28 06:42:05 - general channel

**liam**

Exactly the same as you would with live data but use historical_stream

---

## 2019-07-22 14:47:18 - general channel

**Mo**

Is that some kind of AWS log viewer?

---

## 2019-07-15 23:05:02 - general channel

**Filippo Bovo**

[@U4H19D1D2](@U4H19D1D2), sorry, maybe I did not explain myself correctly. It works well on my local machine. Moreover, it works well on the AWS instance if I log out a few minutes after logging in. However, when I try to schedule the start of the market stream the night before on AWS, the day after I get the error when logging out.

---

## 2019-07-15 09:19:11 - general channel

**liam**

[@UHQV8CW1Y](@UHQV8CW1Y) are you saying it works on AWS but not locally? On the buggy instance what version of openssl are you running?

---

## 2019-07-14 22:52:11 - general channel

**Filippo Bovo**

Hello, everyone. When I log out of Betfair through the APIs after these steps, I get a `ConnectionError`:

1. The day before a match, I set up a scheduler that waits until 30 minutes before the start of a match to begin streaming a market.

2. 30 minutes before the start of the match, the scrip logs in again to Betfair if `trading.session_expired` is true and launches the market stream.

3. As soon as the match turns in-play the stream stops and the scrips logs out of Betfair as `trading.logout()` — at that point I've got the error.



Note that I run the above script in an AWS compute instance and that normally, I don't get errors when I log out.



Does anyone have a hint at why I get the error? Thanks.

---

## 2019-07-06 20:32:32 - general channel

**liam**

Send an email to bfp asking for your key to be authorised for streaming, this is in addition to getting a live key (no cost though)

---

## 2019-07-06 20:24:13 - general channel

**Jonatan (skyw)**

You have to Buy a live key or get a delayed key approved for the streaming markets, I had no success getting delayed key from them

---

## 2019-06-28 15:39:25 - general channel

**liam**

[@U7QLCB7HA](@U7QLCB7HA) why multiple classes? For me I just use the api class for streaming order / market data and my execution class which handles place/cancel/replace, I then have a few threads for keep alive and catalogue requests when required. All handed the trading class at initialisation and then forgotten about 

---

## 2019-06-13 13:55:35 - general channel

**liam**

is this live data?

---

## 2019-06-12 07:22:50 - general channel

**liam**

Containers on ec2 instances

---

## 2019-06-12 06:31:19 - general channel

**Ian**

hi all - small question about running your code in production in the cloud - do you typically use compute in a VM connected to storage or or do you use the next level of abstraction / webapps etc.?

---

## 2019-04-25 11:31:15 - general channel

**Filippo Bovo**

I am a data scientist and have been using Pandas for about three years. Pandas was built with speed in mind using C (and Numpy), and it is good to manipulating static data. However, I have never tried dealing with live data with Pandas DataFrames. The main bottleneck I see in this case is appending and dropping data to an existing DataFrame. If I will test this in the future, I will let you know.

---

## 2019-04-04 10:16:29 - general channel

**LK**

[http://docs.python-requests.org/en/latest/user/advanced/#keep-alive](http://docs.python-requests.org/en/latest/user/advanced/#keep-alive)

---

## 2019-04-03 17:19:20 - general channel

**LK**

these are stats for every 100 messages received from betfair, comparing the publishTime with the time on the server. Latency is 1-way latency, since it is market datastream.

---

## 2019-04-03 17:05:06 - general channel

**Paw**

so I'm an GCP expert but have no time right now to check latency from their servers

---

## 2019-04-03 16:02:28 - general channel

**LK**

amazon ec2 dublin t3.medium

---

## 2019-04-03 14:00:38 - general channel

**liam**

You get what you pay for with lightsail, I have always stuck with ec2 t2/t3

---

## 2019-04-03 09:39:51 - general channel

**LK**

ping times from AWS lightsail instance of 3.50 p/mo ;

---

## 2019-04-03 08:57:26 - general channel

**liam**

Looks to be about double the price of AWS assuming you have it on 24/7 which you don’t need to do 

---

## 2019-04-03 08:34:19 - general channel

**LK**

why would one use AWS instead of VPS like this one? [https://www.tradingservers.co.uk/?language=english](https://www.tradingservers.co.uk/?language=english)

---

## 2019-04-02 09:39:26 - general channel

**Paw**

[@UHD6B7RAQ](@UHD6B7RAQ) I went to the last betfair meetup they run and they mentioned one of the biggest problem they're facing is people stealing they live data

---

## 2019-03-28 04:04:10 - general channel

**Rob (NZ)**

Random question... anyone using betfairlightweight within lambdas on AWS..

---

## 2019-03-24 23:27:35 - issues channel

**Tom**

[@UH9K87HAA](@UH9K87HAA) Yes you need to contact betfair support to activate streaming. You could chance asking CS in the online chat if they can flick the switch and grant you access to streaming. If they can't they'll make a ticket to the dev team to activate it.  [https://responsiblegambling.betfair.com/live-chat/](https://responsiblegambling.betfair.com/live-chat/)

---

## 2019-03-10 11:46:31 - random channel

**LK**

Is anyone aware of a free streaming service for soccer inplay (goals) updates? I'm looking at the websocket from futbol24: [wss://p.futbol24.com/f24/liveHtmlXml/ws/15](wss://p.futbol24.com/f24/liveHtmlXml/ws/15) I could try and reverse engineer it but if anyone has better ideas they are much appreciated. I don't want to make a ton of requests to the betfair inplay service. When this works well I want to switch a to commercial streaming service.

---

## 2019-02-26 14:28:10 - issues channel

**ShahGar**

so streaming will retrieve ordinary sizes/liquid when going to live key, or what do you mean?

---

## 2019-02-20 18:32:47 - issues channel

**Newbie99**

I tried that initially, but for some reason I get an error (I've played around with variants too just to see if I could work it out):



```

ERROR:engineio.server:message handler error

Traceback (most recent call last):

  File "D:/Python37/webpages/betfair_flaskio.py", line 51, in handle_message

    market_books = output_queue.get(False)

  File "D:\Python37\lib\queue.py", line 167, in get

    raise Empty

_queue.Empty



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "D:\Python37\lib\site-packages\engineio\server.py", line 505, in _trigger_event

    return self.handlers[event](*args)

  File "D:\Python37\lib\site-packages\socketio\server.py", line 590, in _handle_eio_message

    self._handle_event(sid, pkt.namespace, pkt.id, pkt.data)

  File "D:\Python37\lib\site-packages\socketio\server.py", line 526, in _handle_event

    self._handle_event_internal(self, sid, data, namespace, id)

  File "D:\Python37\lib\site-packages\socketio\server.py", line 529, in _handle_event_internal

    r = server._trigger_event(data[0], namespace, sid, *data[1:])

  File "D:\Python37\lib\site-packages\socketio\server.py", line 558, in _trigger_event

    return self.handlers[namespace][event](*args)

  File "D:\Python37\lib\site-packages\flask_socketio\__init__.py", line 258, in _handler

    *args)

  File "D:\Python37\lib\site-packages\flask_socketio\__init__.py", line 641, in _handle_event

    ret = handler(*args)

  File "D:/Python37/webpages/betfair_flaskio.py", line 52, in handle_message

    except queue.empty:

AttributeError: module 'queue' has no attribute 'empty'

```

---

## 2019-02-20 16:11:46 - issues channel

**Newbie99**

Ok...I think I've figured out why it hangs now...it is to do with output_queue.get() returning an empty value.



I am trying to structure the below to catch (and ignore) an empty outcome (if there are values the queue works fine, which is why it always works first time), however the below returns the error shown in the logs below (which I think confirms my logic is correct, but the way I try to catch it is incorrect):



```

@socketio.on('ping')

def handle_message(*_args, **_kwargs):

    try:

            market_books = output_queue.get(False)

    except output_queue.empty:

            pass

    else:

        for market_book in market_books:

            emit('my_response2', {'message': 'update', 'mb': market_book.streaming_update})

```



Server logs (which I believe confirm my suspicion):



```

ERROR:engineio.server:message handler error

Traceback (most recent call last):

  File "D:/Python37/webpages/betfair_flaskio.py", line 51, in handle_message

    market_books = output_queue.get(False)

  File "D:\Python37\lib\queue.py", line 167, in get

    raise Empty

_queue.Empty



During handling of the above exception, another exception occurred:



Traceback (most recent call last):

  File "D:\Python37\lib\site-packages\engineio\server.py", line 505, in _trigger_event

    return self.handlers[event](*args)

  File "D:\Python37\lib\site-packages\socketio\server.py", line 590, in _handle_eio_message

    self._handle_event(sid, pkt.namespace, pkt.id, pkt.data)

  File "D:\Python37\lib\site-packages\socketio\server.py", line 526, in _handle_event

    self._handle_event_internal(self, sid, data, namespace, id)

  File "D:\Python37\lib\site-packages\socketio\server.py", line 529, in _handle_event_internal

    r = server._trigger_event(data[0], namespace, sid, *data[1:])

  File "D:\Python37\lib\site-packages\socketio\server.py", line 558, in _trigger_event

    return self.handlers[namespace][event](*args)

  File "D:\Python37\lib\site-packages\flask_socketio\__init__.py", line 258, in _handler

    *args)

  File "D:\Python37\lib\site-packages\flask_socketio\__init__.py", line 641, in _handle_event

    ret = handler(*args)

  File "D:/Python37/webpages/betfair_flaskio.py", line 54, in handle_message

    except output_queue.empty:

TypeError: catching classes that do not inherit from BaseException is not allowed



```

---

## 2019-02-19 14:16:17 - issues channel

**Newbie99**

Ok, I thought I'd test something and its interesting to see where the problem lies.



```

@socketio.on('ping')

def handle_message(*_args, **_kwargs):

    socketio.sleep(1)

    market_books = output_queue.get()

    emit('my_response2', {'message': 'update', 'mb': len(market_books)})

    socketio.sleep(0.5)

    emit('my_response3')

```



That shouldn't clog anything up, yet it still hangs on the last ping (i.e.  the client receives the message with the number of array items, it pings back, it then receives the null response and pings back...then nothing happens).



On the server side, it looks like this:



```

127.0.0.1 - - [19/Feb/2019 14:14:19] "GET /socket.io/?EIO=3&amp;transport=polling&amp;t=1550585641996-6 HTTP/1.1" 200 381 0.000999

INFO:engineio.server:116a710b7ac54ccb89b0c116f4c15675: Received packet MESSAGE data 2["ping"]

INFO:socketio.server:received event "ping" from 116a710b7ac54ccb89b0c116f4c15675 [/]

(5068) accepted ('127.0.0.1', 60202)

INFO:engineio.server:116a710b7ac54ccb89b0c116f4c15675: Received request to upgrade to websocket

127.0.0.1 - - [19/Feb/2019 14:14:20] "GET /socket.io/?EIO=3&amp;transport=polling&amp;t=1550585659960-8&amp;sid=116a710b7ac54ccb89b0c116f4c15675 HTTP/1.1" 200 215 0.291910

INFO:socketio.server:emitting event "my_response3" to b3a33d164b34497f9e9cbafe6df082a3 [/]

INFO:engineio.server:b3a33d164b34497f9e9cbafe6df082a3: Received packet MESSAGE data 2["ping"]

INFO:socketio.server:received event "ping" from b3a33d164b34497f9e9cbafe6df082a3 [/]

INFO:engineio.server:b3a33d164b34497f9e9cbafe6df082a3: Received packet PING data None

INFO:engineio.server:Receive error -- socket is closed

127.0.0.1 - - [19/Feb/2019 14:14:20] "GET /socket.io/?EIO=3&amp;transport=websocket&amp;sid=b3a33d164b34497f9e9cbafe6df082a3 HTTP/1.1" 200 0 178.484812

```



I thought at first perhaps the loop wasn't ending somehow, but that doesn't appear to be the issue...but at least I can see where the problem must lie...just can't figure out the final step, ha!

---

## 2019-02-19 14:10:39 - general channel

**klozovin**

My live key is activated (it seems at least), I can authenticate to the streaming API just fine... do you think I should ask them to activate the delayed key as well? I was planning on using the delayed key for development and testing...

---

## 2019-02-19 13:58:52 - general channel

**klozovin**

It seems that I can't use the delayed API key with streaming services, only the live one... is this how it's supposed to be?

---

## 2019-02-19 12:05:03 - issues channel

**Newbie99**

No luck with different ports or browsers, I even tried a fresh install on my Surface Go (which admittedly is also Windows 10, but doesn't have much on it). No joy, but I did get different errors, which may be more useful to decypher:



HTTP400: BAD REQUEST - The request could not be processed by the server due to invalid syntax.

(XHR)GET - [http://localhost:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=1550577763533-16&amp;sid=a30e5471ac3446018e7290fa00eba7a5](http://localhost:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=1550577763533-16&amp;sid=a30e5471ac3446018e7290fa00eba7a5)



 HTTP400: BAD REQUEST - The request could not be processed by the server due to invalid syntax.

(XHR)POST - [http://localhost:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=1550577763604-17&amp;sid=a30e5471ac3446018e7290fa00eba7a5](http://localhost:3000/socket.io/?EIO=3&amp;transport=polling&amp;t=1550577763604-17&amp;sid=a30e5471ac3446018e7290fa00eba7a5)



Doesn't make a great deal of sense to me, as the message is logically the same as the previous one that worked fine, but perhaps it needs a line break or something in it?

---

## 2019-02-18 19:36:35 - issues channel

**Newbie99**

Hmm, I tried a couple of different browsers, with the same result, however I'm 99% sure your diagnosis is correct here, from playing around with sleep in Python and setInterval on the browser side.



Server:



```

@socketio.on('ping')

def handle_message(*_args, **_kwargs):

    emit('my_response2', 'text')

    socketio.sleep(1)

    stream_send()



def stream_send(*_args, **_kwargs):

    market_books = output_queue.get()

    for market_book in market_books:

       emit('my_response',{'message':market_book.streaming_update})

    socketio.sleep(1)

    emit('my_response2', 'text')

```



In the server logs this pauses here:



```

INFO:engineio.server:3fd83d077e694382a70dc6d73429cc51: Sending packet MESSAGE data 2["my_response",{"message":{"id":"1.155109947","marketDefinition":{"bspMarket":true,"turnInPlayEnabled":false,"persistenceEnabled":false,"marketBaseRate":5,"eventId":"29138495","eventTypeId":"7","numberOfWinners":1,"bettingType":"ODDS","marketType":"WIN","marketTime":"2019-02-19T01:10:00.000Z","suspendTime":"2019-02-19T01:10:00.000Z","bspReconciled":false,"complete":true,"inPlay":false,"crossMatching":false,"runnersVoidable":false,"numberOfActiveRunners":9,"betDelay":0,"status":"OPEN","runners":[{"status":"ACTIVE","sortPriority":1,"id":22937351},{"status":"ACTIVE","sortPriority":2,"id":22937352},{"status":"ACTIVE","sortPriority":3,"id":22937353},{"status":"ACTIVE","sortPriority":4,"id":22937354},{"status":"ACTIVE","sortPriority":5,"id":22937355},{"status":"ACTIVE","sortPriority":6,"id":22937356},{"status":"ACTIVE","sortPriority":7,"id":22937361},{"status":"ACTIVE","sortPriority":8,"id":22937362},{"status":"ACTIVE","sortPriority":9,"id":11502826}],"regulators":["MR_NJ","MR_INT"],"venue":"Dover Downs","countryCode":"US","discountAllowed":true,"timezone":"US/Eastern","openDate":"2019-02-18T21:30:00.000Z","version":2645887285,"priceLadderDefinition":{"type":"CLASSIC"}},"rc":[{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.67]],"id":22937361},{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.63]],"id":22937356},{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.65]],"id":22937354},{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.69]],"id":11502826},{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.73]],"id":22937355},{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.69]],"id":22937352},{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.69]],"id":22937353},{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.73]],"id":22937351},{"batb":[[0,1.01,17.52]],"batl":[[0,1000,1.7]],"id":22937362}],"img":true}}]

INFO:socketio.server:emitting event "my_response2" to 3fd83d077e694382a70dc6d73429cc51 [/]

INFO:engineio.server:3fd83d077e694382a70dc6d73429cc51: Sending packet MESSAGE data 2["my_response2","text"]

127.0.0.1 - - [18/Feb/2019 19:28:27] "POST /socket.io/?EIO=3&amp;transport=polling&amp;t=1550518105709-1&amp;sid=3fd83d077e694382a70dc6d73429cc51 HTTP/1.1" 200 219 2.080826

INFO:engineio.server:3fd83d077e694382a70dc6d73429cc51: Upgrade to websocket successful

INFO:engineio.server:3fd83d077e694382a70dc6d73429cc51: Received packet MESSAGE data 2["ping"]

INFO:socketio.server:received event "ping" from 3fd83d077e694382a70dc6d73429cc51 [/]

INFO:socketio.server:emitting event "my_response2" to 3fd83d077e694382a70dc6d73429cc51 [/]

INFO:engineio.server:3fd83d077e694382a70dc6d73429cc51: Sending packet MESSAGE data 2["my_response2","text"]

```



From the client side it appears to ping pong twice successfully on connection, then sends the market book, then (seemingly) it ping pongs twice before stopping (the above is with no delay on emitting a ping on the browser side, if I delay then the behaviour actually seems counter intuitive in that it appears 2 pings arrive before the last pong).



So as you can see, it does appear your diagnosis is correct, but somewhere along the line maybe the messages go out of sync, thus breaking the socket?

---

## 2019-02-17 15:28:41 - issues channel

**Newbie99**

I have got that sample code up and running, but for me it behaves the same as the previous effort (almost), in that it sends through the initial price cache as expected, however then it just pings away (i.e. the client console.log shows 'pinging').



The server logs, show:



```

INFO:socketio.server:received event "ping" from 41f1cafd420740b3921c67883d9b0bb0 [/]

INFO:socketio.server:emitting event "my_response" to 41f1cafd420740b3921c67883d9b0bb0 [/]

INFO:engineio.server:41f1cafd420740b3921c67883d9b0bb0: Client is gone, closing socket

INFO:engineio.server:41f1cafd420740b3921c67883d9b0bb0: Client is gone, closing socket

```



The client logs are interesting, as there are some errors that show up (which we never had previously):



```

socket.io.min.js:1 POST [http://localhost:5000/socket.io/?EIO=3&amp;transport=polling&amp;t=1550416830901-24&amp;sid=0e8737cace6149e1abff289ba77e7c24](http://localhost:5000/socket.io/?EIO=3&amp;transport=polling&amp;t=1550416830901-24&amp;sid=0e8737cace6149e1abff289ba77e7c24) 500 (Internal Server Error)

socket.io.min.js:1 GET [http://localhost:5000/socket.io/?EIO=3&amp;transport=polling&amp;t=1550416745904-23&amp;sid=0e8737cace6149e1abff289ba77e7c24](http://localhost:5000/socket.io/?EIO=3&amp;transport=polling&amp;t=1550416745904-23&amp;sid=0e8737cace6149e1abff289ba77e7c24) 400 (BAD REQUEST)

```



and also:



```

socket.io.min.js:2 WebSocket connection to '[ws://localhost:5000/socket.io/?EIO=3&amp;transport=websocket&amp;sid=9ae605a3067b40328159b3072a45112c](ws://localhost:5000/socket.io/?EIO=3&amp;transport=websocket&amp;sid=9ae605a3067b40328159b3072a45112c)' failed: WebSocket is closed before the connection is established.

```

---

## 2019-02-16 18:25:55 - issues channel

**Newbie99**

no errors (I took out all the sleep's too as that would have messed things up with the faster pings).



The server log reads fine...but again it only fires once:



```

INFO:engineio.server:a552c499c6ab49508a89e7982010a99c: Received request to upgrade to websocket

INFO:engineio.server:a552c499c6ab49508a89e7982010a99c: Received packet MESSAGE data 2/test,["ping"]

INFO:socketio.server:received event "ping" from a552c499c6ab49508a89e7982010a99c [/test]

INFO:socketio.server:emitting event "my_response" to a552c499c6ab49508a89e7982010a99c [/test]

INFO:engineio.server:a552c499c6ab49508a89e7982010a99c: Sending packet MESSAGE data 2/test,["my_response","received a ping"]

```

---

## 2019-02-16 17:48:44 - issues channel

**Newbie99**

But thats the issue I guess, I'm not understanding what is necessary to resolve that, I added the socketio.sleep(1) at the end as someone suggested that could free up the CPU and would exit the thread, but that a) seems a bit untidy and b) clearly doesn't do exactly what I need.



So in effect, it sticks there as the server is looking for instruction and nothing is telling it what to do in effect as I don't get any errors, because it pauses there (which is logical from what you say).



But having read up on python loop logic I don't see an obvious solution here, I thought maybe the logic needs to involve a background thread, but although I can get that to fire, I still end up in the same place as it just executes through once.

---

## 2019-02-16 17:39:36 - issues channel

**Newbie99**

hello again, I have spent a lot of time stepping through the logic in a bit more of a sensible fashion and tidied things up a bit. From a logical point of view, I'm still missing something, the code appears to make sense and doesn't throw up any errors. I've setup a ping from the client, the server then responds with a pong...but only on the first ping. In the console.log on the client after that it pings away, but the server never receives them, which seems odd. I make the assumption that the error is actually along the lines of my server code is not looking for the ping, rather than any issue on the client side. I'm guessing there is an issue with Threads that I'm not understanding, because I can't see what else it could be.

---

## 2019-02-09 20:33:02 - issues channel

**Newbie99**

appreciate you've already helped out a lot, however even with Visual Studio there are no errors thrown up by the debugger....it appears the code is functioning correctly...except of course its not. The issue is around the ping or emit, one or the other (or both) are not behaving as I would expect, but are not throwing up any errors (the debugger just shows the connections to the betfair streaming data as expected).



If I refresh the browser window (which I assume pings the server), the command line console refreshes, so somehow the emit part of the code is not firing. I'm guessing (but it is a guess), its to do with this section:



```

@socketio.on('ping')

def handle_message(_message):

    market_books = output_queue.get()

    print(market_books)

```



But as no error is thrown, I'm not really sure where to go from here!



Apologies again for the dumb questions, 'pure' scripting languages (e.g. Node.js / Python) are new to me and clearly I'm struggling with the logic a bit it would seem!

---

## 2019-02-07 18:38:51 - issues channel

**Newbie99**

thank you for your code snippet seaders, however maybe I'm being really dense, but it doesn't appear to be working. The client code says io isn't defined, so I tried with the sample code from the Flask page, which doesn't return an error, but still nothing happens. I feel that the problem is probably still with my server code, which is currently as follows. Python is very new to me, so this attempt at editing the streaming example, might be a bit poor, but as I don't get any errors, its a bit tricky to see where the problem is!



[code]

import os

import logging

import queue



import betfairlightweight

from betfairlightweight.filters import (

    streaming_market_filter,

    streaming_market_data_filter,

)



from flask import Flask, render_template

from flask_socketio import SocketIO



# setup logging

logging.basicConfig(level=[http://logging.INFO|logging.INFO](http://logging.INFO|logging.INFO))  # change to DEBUG to see log all updates



# create trading instance (app key must be activated for streaming)

username = os.environ.get('username')





# create trading instance

trading = betfairlightweight.APIClient(user_name,password,appkey,certs=cert_path)



trading.login()



# create queue

output_queue = queue.Queue()



# create stream listener

listener = betfairlightweight.StreamListener(

    output_queue=output_queue,

)



# create stream

stream = trading.streaming.create_stream(

    listener=listener,

)



# create filters (GB WIN racing)

market_filter = streaming_market_filter(

    event_type_ids=['7'],

    country_codes=['GB'],

    market_types=['WIN'],

)

market_data_filter = streaming_market_data_filter(

    fields=['EX_BEST_OFFERS', 'EX_MARKET_DEF'],

    ladder_levels=3,

)



# subscribe

streaming_unique_id = stream.subscribe_to_markets(

    market_filter=market_filter,

    market_data_filter=market_data_filter,

    conflate_ms=1000,  # send update every 1000ms

)



# start stream

stream.start(_async=True)



"""

Data can also be accessed by using the snap function in the listener, e.g:

    market_books = listener.snap(

        market_ids=[1.12345323]

    )

Errors need to be caught at stream.start, resubscribe can then be used to

prevent full image being sent, e.g:

    streaming_unique_id = stream.subscribe_to_markets(

        market_filter=market_filter,

        market_data_filter=market_data_filter,

        conflate_ms=1000,  # send update every 1000ms

        initial_clk=listener.initial_clk,

        clk=listener.clk,

    )

The streaming unique id is returned in the market book which allows multiple

streams to be differentiated if multiple streams feed into the same queue.

"""



# check for updates in output queue

while True:

    market_books = output_queue.get()

    print(market_books)





    for market_book in market_books:

        print(

            market_book,

            market_book.streaming_unique_id,  # unique id of stream (returned from subscribe request)

            market_book.streaming_update,  # json update received

            market_book.market_definition,  # streaming definition, similar to catalogue request

            market_book.publish_time  # betfair publish time of update

        )



app = Flask(__name__)

app.config['SECRET_KEY'] = 'secret!'

socketio = SocketIO(app)



if __name__ == '__main__':

    socketio.run(app)



stream = None

queue = None





def get_queue():

    global queue, stream



    if not queue:

        stream, queue =  trading.streaming.create_stream(

    listener=listener,

)

    return queue



@socketio.on('ping')

def handle_message(_message):

    market_books = get_queue().get()

    print(market_books)



    for market_book in market_books:

        emit(

            'market_book',

            f'{market_book} '

            f'{market_book.streaming_unique_id} '

            f'{market_book.streaming_update}, '

            f'{market_book.market_definition}, '

            f'{market_book.publish_time} '

        )





if __name__ == '__main__':

    socketio.run(app)

    stream.stop()



[/code]

---

## 2018-12-28 18:06:00 - general channel

**Rory**

I understand +1.0, +2.0, etc or +1.5, +2.5 (no draws)

---

## 2018-12-28 18:03:15 - general channel

**seaders**

They include draws, is the thing

---

## 2018-12-11 20:06:34 - random channel

**liam**

[!channel](!channel) Anyone looking for a new job? 



Exciting opportunity to work in Financial Crime Technology at KPMG (with me) Tech stack is mostly python but opportunity to work with AWS, ECS, terraform, vault, JavaScript (reactjs), Jenkins, docker... machine learning.. We are focussed on finding someone who is keen to learn so no requirement to have experience in all of the tech listed or Financial Crime. Feel free to send me a message to learn more!



[https://www.linkedin.com/feed/update/urn:li:activity:6478346553730875392](https://www.linkedin.com/feed/update/urn:li:activity:6478346553730875392)

---

## 2018-12-02 10:55:18 - general channel

**Donkey**

I’m looking for some help please with the .bash_profile content.

Currently set with (fake username/api/pass this is just an example, I use the real ones on my server):

```export username="johnsmith"

export appKey="1234567890"

export password="abcdef"```



In the exampleone.py I have:

```# create trading instance

trading = betfairlightweight.APIClient('username', 'password', app_key='appKey')```



On execution I get however the following error:

```# python exampleone.py

Traceback (most recent call last):

  File "exampleone.py", line 10, in &lt;module&gt;

    trading.login()

  File "/usr/lib/python3.6/site-packages/betfairlightweight/endpoints/login.py", line 30, in __call__

    (response, elapsed_time) = self.request(self.url, session=session)

  File "/usr/lib/python3.6/site-packages/betfairlightweight/endpoints/login.py", line 52, in request

    self._error_handler(response_data)

  File "/usr/lib/python3.6/site-packages/betfairlightweight/endpoints/login.py", line 57, in _error_handler

    raise self._error(response)

betfairlightweight.exceptions.LoginError: API login: ACCOUNT_PENDING_PASSWORD_CHANGE```



But my actual password logs me in betfair account without any issues.



Thanks for the help.

---

## 2018-11-16 20:09:57 - general channel

**LK**

what do you mean by "hacked out of the betfair website"? Do you think it's problematic to hammer their servers with this request? Would you recommend a different service?

---

## 2018-11-12 11:53:49 - general channel

**liam**

aws eu-west

---

## 2018-11-12 11:33:28 - general channel

**George**

still interested in what people think the internal latency (receiving order at their server through to order being placed) is, roughly

---

## 2018-11-12 11:16:19 - general channel

**George**

does anyone know about latency within Betfair's systems - as in, if my order reaches their servers at time X, roughly how long does it take before the order gets matched?

---

## 2018-10-18 16:47:19 - general channel

**LK**

Do I understand correctly that in live-streaming data you can opt to see virtual (I would like them implied) prices? But when you buy historical data you only get the actuals back and lay priceladders. (Theoretically you could calculate the virtual prices yourself)

---

## 2018-08-16 21:42:28 - general channel

**Rory**

just about to start using live streaming ... so will find out soon :grin:

---

## 2018-07-26 16:15:36 - random channel

**Anatoliy**

Hello; I have a question. I am looking for a way to get live data for tennis matches to be able to place live bets.

Currently I am using the following method:

betfairlightweight.APIcleint.in_play_service.get_scores

I am wondering; 

- is it possible to get live scores using a websocket?

- if a websocket is not an option, at what rate should I use "get_scores"? Is every 3 seconds too often?

---

## 2018-07-09 16:26:54 - general channel

**klozovin**

[@U93H3483E](@U93H3483E) yeah, AWS and Linode are okay for that, that's what I'm planning. I was just wondering about Sporting Servers because they promise low latency, maybe lower than one can achieve with just a VPS in the UK

---

## 2018-06-02 10:14:07 - issues channel

**liam**

1.6.3 now pushed to pypi, minor fix to handle ciso8601 update and setting specific versions for third party libs to prevent it happening again. I used in production last night with no issues (streaming). Thanks [@U9CP5N36K](@U9CP5N36K) as this would have screwed me over.

---

## 2018-05-06 07:37:47 - general channel

**liam**

Production but using an account I also trade on 

---

## 2018-05-06 07:37:07 - general channel

**favetelinguis**

on the test API i guess betfair would not allow that on the production endpoints?

---

## 2018-05-06 07:29:32 - general channel

**liam**

I can give you an AWS session key with access to just that bucket and you can download, probably tomorrow though (when his headache has gone)

---

## 2018-04-03 19:56:02 - general channel

**liam**

its unbelievable isn't it, testing something but it ended up in production and they didn't even realise!

---

## 2018-03-26 19:10:33 - general channel

**marco**

I was trying to run the code regarding streaming in the github README. I get "betfairlightweight.exceptions.SocketError: Connection closed by server" on the last line. I wonder if this is supposed to work with a test app key which is what I have...

---

## 2018-02-26 07:11:27 - general channel

**liam**

Hi,

 

Following the recent developer survey we’d like to provide some feedback on some of the points raised by customers both generally and in terms of additional feature requests.

 

We’ve grouped these into distinct categories:

 

Market Data &amp; Feeds

 

We are constantly exploring ways of incorporating new data feeds into the API.    Unfortunately, there are restrictions relating to the onward supply of specific data via the API (specifically football related data) but we are investigating the incorporation of other data feeds at the moment, including improved feeds for horse racing.

 

The inclusion of raceType is on the current API roadmap and we are looking to integrate this data as soon as possible but don’t have a specific ETA.

 

We are working to make improvements to the consistency of market names &amp; abbreviations across the platform. 

 

We don’t have any plans to add specific results data into the API.  Customer who require results data following market settlement can use the historical data service (via [https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home)).  This data includes the result for each runner by name within the free BASIC data files.

 

We don’t have any plans to remove the time delay associated with in-play betting.  This is in place to protect customers when betting in-play and watching transmission described as “live” that may be actually delayed.

 

Stream API

 

We have an existing backlog item to add additional filters to the Stream API (including competitionId) but no plans to make any changes to the existing filter name.

 

We don’t have any plans to change the way we conflate data via the Stream API but historical data is available via [https://historicdata.betfair.com/#/home](https://historicdata.betfair.com/#/home) in the same format as provided via the Stream API if required for testing &amp; analysis purposes.

 

Pricing &amp; Charges

 

We don’t have any current plans to increase/reduce the £299 fee for Live Application Keys.

 

We received some requests to include an indicator within the API relation to transaction charging.   For transaction charging, we recommend that customers count unique the number of betId’s created in a single hour (0000-0059, 0100-0159, 0200-0259).  Any transaction fees are offset by the following (Commission + Implied Commission) ÷ 2

where Implied Commission = market losses x 3% which is calculated on a daily basis. Full details can be found via [http://www.betfair.com/aboutUs/Betfair.Charges/](http://www.betfair.com/aboutUs/Betfair.Charges/) &gt; Transaction Charges.

 

If there any specific questions/queries you’d like us to follow up with you directly please get in touch via Developer Support ([https://developer.betfair.com/support/](https://developer.betfair.com/support/))

 

Kind Regards

 

Neil

Betfair Developer Program

---

## 2018-02-23 07:30:06 - general channel

**liam**

host var, defaults to production 

---

## 2018-02-22 23:38:33 - general channel

**Henry**

Isn't it only paid for live data? Or Steam API is included as well

---

## 2018-02-05 18:10:29 - general channel

**liam**

I use terraform for infra and the a mix of portainer (for monitoring, its great) and then my own deployment library using the docker python wrapper (also great) I have found that Sports trading programs don't really suit ECS / container services

---

## 2018-02-05 18:08:16 - general channel

**jfo**

are you manually setting up the ec2 instances with docker? or are you using the container service stuff?

---

## 2018-02-05 18:01:28 - general channel

**jfo**

[@U4H19D1D2](@U4H19D1D2) you still runner docker in ec2?

---

## 2017-12-23 18:24:48 - general channel

**Lennart**

Hey Guys - only tangentially related to the API but I think you might be able to help - Are there any good options for getting live (or as close as possible to live) updated in-game data? I'm thinking game score, team on the pitch/court, etc. .

---

## 2017-12-22 08:53:01 - general channel

**Alexander**

Hello, I'm new here. Alexander from Sweden :)

This question isn't really about the betfairlightweight library, but I guess you know the answer. 

I have tried to understand if delayed appkey can place bets on Betfair but with delayed time, or is it only live appkey that can do that?when I read om Betfair I think that delayed appkey can place bets and then I read again and I think it can't.



And Happy Christmas to you all :smile:

---

## 2017-12-20 21:41:27 - issues channel

**OT**

live data, initial image.

---

## 2017-12-15 00:38:33 - general channel

**Unknown**

I'm going to save the headache and use the lightweight api to get the data. I am however having trouble generating an app key. I'm getting a http 404 error from the following link

404 error means server is unable to preform request? Is this service down at the moment?

---

## 2017-11-12 17:22:47 - general channel

**magiclevinho**

Hello Mates!

I m trying to get the live scores for a particular match, but I get connection error. What I use is:

trading.in_play_service.get_scores(event_ids=[event.event.id])

I can successfully get the events, but not the scores!

---

## 2017-11-02 18:43:01 - general channel

**liam**

dont think i can see much advantage on using golang for live data

---

## 2017-11-02 17:55:23 - general channel

**seaders**

[https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py#L26](https://github.com/liampauling/betfair/blob/master/betfairlightweight/endpoints/keepalive.py#L26)

---

## 2017-10-18 09:55:47 - general channel

**liam**

Hmm I guess deploying the strategy then executing is the only option 

---

## 2017-10-16 20:11:53 - general channel

**liam**

Anyone got experience with serverless / pickling data via http? Currently have a backtesting library as a serverless function on AWS which I currently send market ids which are then downloaded from s3 and processed. However I want to be able to send ‘strategies’ which would be a list of classes. When I attempt to unpicked I get all sorts of errors about not being able to find the module. 

---

## 2017-09-14 08:14:02 - general channel

**liam**

 0.15.1 released which 'should' fix the 'connection closed by server' / empty recv error sometimes seen when running a socket for a while. [@U5D4ZBEAG](@U5D4ZBEAG) I also noticed that betfair have added more data to racecard responses (currently failing) I am thinking for these non public endpoints (racecard  and inplayservice) we either add kwargs or dynamically create the class to prevent having to update every time they make a change but welcome any thoughts / PR's

---

## 2017-09-10 07:53:50 - general channel

**liam**

[!channel](!channel) 

Welcome any thoughts on v1.5 PR, want to push to master later this morning as I need get_event_timelines in production. Note the breaking changes to place order response and removal of historical endpoint



[https://github.com/liampauling/betfair/releases](https://github.com/liampauling/betfair/releases)

---

## 2017-09-09 04:42:37 - general channel

**gerg**

I need you help in getting Live Odds

---

## 2017-06-03 16:16:16 - general channel

**liam**

Oh dear, not sure he can be blamed for that. At work the first thing we do on production databases is remove the ability to drop tables /delete without being an admin 

---

## 2017-06-03 13:31:14 - general channel

**jfo**

off topic: [https://www.reddit.com/r/cscareerquestions/comments/6ez8ag/accidentally_destroyed_production_database_on/?st=J3H99RCH&amp;sh=3cf68044](https://www.reddit.com/r/cscareerquestions/comments/6ez8ag/accidentally_destroyed_production_database_on/?st=J3H99RCH&amp;sh=3cf68044)

---

## 2017-05-31 16:56:07 - general channel

**agberk**

It's not that you get access to an API which they are streaming some data separate from the live API or whatever

---

## 2017-05-28 14:56:09 - general channel

**jfo**

```[ec2-user@ip-172-26-5-22 ~]$ date

Sun May 28 13:56:03 GMT 2017```

---

## 2017-05-23 12:20:52 - general channel

**richard**

The workshop we followed was on QwikLabs. One of the links he's just finished-up with for something similar is an Amazon project: [https://aws.amazon.com/getting-started/projects/build-log-analytics-solution/](https://aws.amazon.com/getting-started/projects/build-log-analytics-solution/)

---

## 2017-05-23 12:12:46 - general channel

**richard**

We haven't discussed costs at all (apart from "free to upload data to AWS"), so you could be right.

---

## 2017-05-23 12:03:05 - general channel

**richard**

So in the Amazon workshop this morning we built an app to process weblogs. It strikes me that this could easily translate to capturing Betfair data.

For those that are interested here are the components of the AWS universe that we used.

---

## 2017-05-22 16:36:57 - general channel

**richard**

I'm down in London this week for Strata Data. Tomorrow there is a tutorial session on "Building your first AWS Big Data App", so I'll be there ears open.

---

## 2017-05-22 16:35:44 - general channel

**liam**

I went to awsloft last year, a week of interactive sessions / tutorials going through a lot of the key areas and it was all for free! They seemed to have stopped it this year 

---

## 2017-05-22 16:32:30 - general channel

**richard**

Not seen lightsail yet. I went to an AWS-ome Day recently and even the instructors say there's too much for them to remember.

---

## 2017-05-22 16:30:37 - general channel

**jfo**

i've just got my stuff running on aws lightsail, the smallest one, has mysql running on there and well as a couple of python scripts, not very scalable but it simple and cheap

---

## 2017-05-22 16:17:20 - general channel

**liam**

Yeh for databases AWS seems to be a rip off 

---

## 2017-05-22 16:16:09 - general channel

**richard**

Makes me weep that I used to have a dedicated server at nearly £200/month for my betting - this was when AWS was just in its infancy though.

---

## 2017-05-22 16:12:04 - general channel

**liam**

I'll be honest my process changes almost weekly and is more focussed around budget, want to do a blog post on this topic soon. Are you new to AWS? Because you can run a ec2.micro for free for a year which is what I originally did. I now zip and store json in s3 as soon as a market is complete, much cheaper than db storage 

---

## 2017-05-22 16:02:23 - general channel

**richard**

At some point liam I'd love to understand a bit more about how you use AWS. I've just started on it now I've come back to Betfair and already the cost I'm incurring simply capturing price and volume data is looking like it could be optimised. I have the code on EC2 and have tried both MySQL and Aurora for the database. It's this RDS piece that is the highest. I'm thinking automatically dump to S3 on a daily/weekly basis(?), but haven't yet looked at comparative costs of storage.

---

## 2017-05-20 12:33:38 - general channel

**liam**

used to spend £10 on hosting it but using aws lambda it costs me under £1

---

## 2017-05-20 12:31:30 - general channel

**liam**

i have a django api running through zappa/aws lambda so dont communicate directly to the db

---

## 2017-05-20 11:55:42 - general channel

**jfo**

[@U4H19D1D2](@U4H19D1D2) do you run sql in aws aswell?

---

## 2017-05-17 14:54:15 - general channel

**agberk**

Yeah that is an advantage - I've got the AWS app on my phone but you're right that it doesn't give me the fine grained control to restart my apps, components of them etc.

---

## 2017-05-17 14:41:58 - general channel

**liam**

docker speeds things up for me, can go from no ec2 instances available to a program running and i just have to install docker

---

## 2017-05-09 15:31:16 - general channel

**liam**

I run all of my programs off the AWS ami which is based on ubuntu Linux I think 

---

## 2017-05-09 15:30:20 - general channel

**liam**

If your new take advantage of the free tier and boot up an ec2 micro 

---

## 2017-05-09 15:28:45 - general channel

**jfo**

This is for building my own bot with your lib and ditching betangel. I'm not sure if its worth investing the time wrestling aws to be able to scale things up when needed or just get the lightsail box, but then not if that would be enough transfer per month :thinking_face:

---

## 2017-05-09 15:00:34 - general channel

**jfo**

what kind of plan are you on with aws? I had a look at the autoscaling stuff but no sure if its overkill, might just stick with a lightsail box to begin with?

---

## 2017-05-08 16:47:14 - general channel

**liam**

I average around 0.2s for placeOrders on AWS (Ireland region) BA is good but when using automation or the excel integration there is going to be a big delay between your trigger and the order hitting the market. When it comes to In play racing milliseconds can count. So when it comes to getting matched you need to process your trigger quicker or reduce the latency on your orders 

---

## 2017-05-08 16:36:05 - general channel

**jfo**

still in the process of putting together my own order placing strategy so using betangel server with streaming to get up and running before coding it all. so not sure on latency, at what number does latency become bad? obviously the lower the better

---

## 2017-04-20 00:29:14 - general channel

**agberk**

[http://api.betfair.com|api.betfair.com](http://api.betfair.com|api.betfair.com) is served out of Ireland, Dublin (though not sure it's AWS) - but you get routed through a DDoS protection service in London

---

## 2017-04-19 17:38:43 - general channel

**favetelinguis**

btw betfair is deployd in AWS ireland right?

---

## 2017-04-19 17:37:55 - general channel

**favetelinguis**

AWS is free for imcomming data?

---

## 2017-04-19 17:37:13 - general channel

**jfo**

Do you know how much data you are transferring per month [@U4H19D1D2](@U4H19D1D2)? Was looking at vps costs for streaming data, don't have access to live yet so can't do calculations :thinking_face:

---

## 2017-04-19 17:35:01 - general channel

**favetelinguis**

serverless you mean AWS lambdas?

---

## 2017-04-19 17:34:39 - general channel

**liam**

Want to be able to take advantage of serverless and run a strategy through a couple thousand markets in a few seconds 

---

## 2017-04-06 20:18:21 - general channel

**liam**

Interesting, I guess they are now able to delay streaming as well as the live key would be updating every couple of 10ms with Chelmsford still on

---

## 2017-03-11 12:57:41 - general channel

**liam**

out of interest are you using the library in production/live or just testing?

---


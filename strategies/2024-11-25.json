[
    {
        "user": "U012XF5CNPN",
        "type": "message",
        "ts": "1732528889.204219",
        "client_msg_id": "0de05842-a5be-43aa-894f-3ff0897510e0",
        "text": "Some immediate and random thoughts:\n\n1. The problem with optimisation is that you end up going down the route of specialisation, in essence over-fitting a solution to what you need right now. It feels though, like once you step back a bit you're just saying \"we should make backtests faster, and the ability to snapshot state and key metrics like total volume matched, much faster\". Is that right?\n2. There's a lot of thought that has gone into time series data storage from other industries, much of it open source, that might yield great results with minimal work. The downside of those stacks is that they introduce another dependency that might be horrid to deal with practically. Most of the uplift from that work is likely getting you away from dealing with files on disk, and building in-RAM data structures that yield well to the algorithms the creators were interested in. Stealing that idea while keeping your on-disk formats is worth considering (fewer dependencies, easier to iterate, more adaptable down the line as you're not re-writing files, just in-RAM data structures).\n3. If you want Flumine backtest integration (your number 6), my instinct is improving the Rust wrapper and then extending it to serialise in-RAM into different optimal structures is probably easier in the _long_ run, although there is a lot of upfront work with little immediate upside before you're ready to do the interesting bit, so I don't blame anyone for ducking it. \nAs Kent Beck once wrote:\n\n&gt; For each desired change, make the change easy (warning: this may be hard), then make the easy change",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gf0a1c4f12dc",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/f0a1c4f12dc16bc00aa5abf1a75d640e.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0019-72.png",
            "first_name": "Paul",
            "real_name": "Paul Robinson",
            "display_name": "Paul",
            "team": "T4G9NBD2M",
            "name": "paul",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "RP1a+",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Some immediate and random thoughts:\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "The problem with optimisation is that you end up going down the route of specialisation, in essence over-fitting a solution to what you need right now. It feels though, like once you step back a bit you're just saying \"we should make backtests faster, and the ability to snapshot state and key metrics like total volume matched, much faster\". Is that right?"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "There's a lot of thought that has gone into time series data storage from other industries, much of it open source, that might yield great results with minimal work. The downside of those stacks is that they introduce another dependency that might be horrid to deal with practically. Most of the uplift from that work is likely getting you away from dealing with files on disk, and building in-RAM data structures that yield well to the algorithms the creators were interested in. Stealing that idea while keeping your on-disk formats is worth considering (fewer dependencies, easier to iterate, more adaptable down the line as you're not re-writing files, just in-RAM data structures)."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "If you want Flumine backtest integration (your number 6), my instinct is improving the Rust wrapper and then extending it to serialise in-RAM into different optimal structures is probably easier in the "
                                    },
                                    {
                                        "type": "text",
                                        "text": "long",
                                        "style": {
                                            "italic": true
                                        }
                                    },
                                    {
                                        "type": "text",
                                        "text": " run, although there is a lot of upfront work with little immediate upside before you're ready to do the interesting bit, so I don't blame anyone for ducking it. "
                                    }
                                ]
                            }
                        ],
                        "style": "ordered",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nAs Kent Beck once wrote:\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "For each desired change, make the change easy (warning: this may be hard), then make the easy change"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1732529263.662169",
        "client_msg_id": "a89d5372-d9cd-4329-98d8-c49b8c8bb486",
        "text": "&gt; The dirty secret of market impact? It's messier than anyone admits. This article challenges conventional models by looking at how information actually flows through markets. Here's the real problem: impact hits differently when you're getting in versus getting out of positions - a fundamental asymmetry that most models miss entirely. Add in the fact that every major player's algos are scrapping for the same liquidity, and traditional execution models fall apart.\n<https:\/\/x.com\/imotw2\/status\/1860802661832155549>",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732529263.662169",
        "reply_count": 1,
        "reply_users_count": 1,
        "latest_reply": "1732539558.454519",
        "reply_users": [
            "U01D23DDMTQ"
        ],
        "replies": [
            {
                "user": "U01D23DDMTQ",
                "ts": "1732539558.454519"
            }
        ],
        "is_locked": false,
        "subscribed": true,
        "last_read": "1732539558.454519",
        "attachments": [
            {
                "image_url": "https:\/\/pbs.twimg.com\/media\/GdLXupIWoAAEeLz.jpg:large",
                "image_width": 900,
                "image_height": 360,
                "image_bytes": 37740,
                "from_url": "https:\/\/x.com\/imotw2\/status\/1860802661832155549",
                "service_icon": "http:\/\/abs.twimg.com\/favicons\/twitter.3.ico",
                "id": 1,
                "original_url": "https:\/\/x.com\/imotw2\/status\/1860802661832155549",
                "fallback": "X (formerly Twitter): otw2 (@imotw2) on X",
                "text": "Market Impact and Strategic Execution",
                "title": "otw2 (@imotw2) on X",
                "title_link": "https:\/\/x.com\/imotw2\/status\/1860802661832155549",
                "service_name": "X (formerly Twitter)"
            }
        ],
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "wVQWZ",
                "elements": [
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "The dirty secret of market impact? It's messier than anyone admits. This article challenges conventional models by looking at how information actually flows through markets. Here's the real problem: impact hits differently when you're getting in versus getting out of positions - a fundamental asymmetry that most models miss entirely. Add in the fact that every major player's algos are scrapping for the same liquidity, and traditional execution models fall apart."
                            }
                        ]
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\n"
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/x.com\/imotw2\/status\/1860802661832155549"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U06T30DCSDS"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "U05L8PZD2FM",
        "type": "message",
        "ts": "1732533300.189019",
        "edited": {
            "user": "U05L8PZD2FM",
            "ts": "1732537317.000000"
        },
        "client_msg_id": "fb82fd09-6318-4f26-9efa-198d714ec033",
        "text": "<@UBS7QANF3> To answer 1-5 I have an IO front end with a little dsl that can extract data from the compressed json stream files, fast enough like <2min per year of global racing, but this is largely because there is minimal impedance mismatch between IO and compute and memory in my implementation, it is using very fast local storage and lots of cores for parallel decompression, this is the same point that <@US2RWCWKY> was making.\n\nTo back test \/ train \/ ML that same IO front end is used but to fill memory structures whose contents are 'implied' by the strategy being trained and shape\/order optimised for the hardware (e.g. gpu structures will be totally different shape to cpu). In other words it only keeps the answers to the questions it is likely to need during training and puts them in the memory location that will guarantee the lowest latency during training given the expected order those questions will be asked in. This is the data equivalent to dead code removal during compilation followed by compile time optimisation. The training process then only iterates over these memory structures at high frequency. To do this and ensure that the 'logic' being trained will behave identically when run in production on live streams requires everything be written against interfaces that are implemented multiple times. This is similar to what <@U012XF5CNPN> alluded to in point 3. It could also be thought of as a very application specific cache.\n\nThis would be quite a departure from how Flumine works where there is no real change between live and back testing other than inserting the simulated matcher.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gb417ed434bb",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/b417ed434bbea199d58a9b4bd0affeb9.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0003-72.png",
            "first_name": "Joe",
            "real_name": "Joe",
            "display_name": "",
            "team": "T4G9NBD2M",
            "name": "stapleton_joe",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "0ip1c",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UBS7QANF3"
                            },
                            {
                                "type": "text",
                                "text": " To answer 1-5 I have an IO front end with a little dsl that can extract data from the compressed json stream files, fast enough like <2min per year of global racing, but this is largely because there is minimal impedance mismatch between IO and compute and memory in my implementation, it is using very fast local storage and lots of cores for parallel decompression, this is the same point that "
                            },
                            {
                                "type": "user",
                                "user_id": "US2RWCWKY"
                            },
                            {
                                "type": "text",
                                "text": " was making.\n\nTo back test \/ train \/ ML that same IO front end is used but to fill memory structures whose contents are 'implied' by the strategy being trained and shape\/order optimised for the hardware (e.g. gpu structures will be totally different shape to cpu). In other words it only keeps the answers to the questions it is likely to need during training and puts them in the memory location that will guarantee the lowest latency during training given the expected order those questions will be asked in. This is the data equivalent to dead code removal during compilation followed by compile time optimisation. The training process then only iterates over these memory structures at high frequency. To do this and ensure that the 'logic' being trained will behave identically when run in production on live streams requires everything be written against interfaces that are implemented multiple times. This is similar to what "
                            },
                            {
                                "type": "user",
                                "user_id": "U012XF5CNPN"
                            },
                            {
                                "type": "text",
                                "text": " alluded to in point 3. It could also be thought of as a very application specific cache.\n\nThis would be quite a departure from how Flumine works where there is no real change between live and back testing other than inserting the simulated matcher."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U05C31YKZ1C",
        "type": "message",
        "ts": "1732534210.260059",
        "client_msg_id": "3594429E-E4A0-4D48-AF17-4E2A90C880D9",
        "text": "I am playing some MoC and LoC bets around the scheduled off assuming some criteria is met. I keep getting burnt by races starting minutes after the scheduled time. Is there any way to scratch out small bsp bets if it’s been more than x time since the off? The minimum liability lay makes it tough to scratch out small liability back bets. Do I need to increase my bet sizes until I can cancel it out over the minimum? ",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g8823a6686ca",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/8823a6686ca88a27d29ac5c4a2518126.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0023-72.png",
            "first_name": "James",
            "real_name": "James",
            "display_name": "James",
            "team": "T4G9NBD2M",
            "name": "slack1995",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "OWrvv",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I am playing some MoC and LoC bets around the scheduled off assuming some criteria is met. I keep getting burnt by races starting minutes after the scheduled time. Is there any way to scratch out small bsp bets if "
                            },
                            {
                                "type": "text",
                                "text": "it’s"
                            },
                            {
                                "type": "text",
                                "text": " been more than x time since the off? The minimum liability lay makes it tough to scratch out small liability back bets. Do I need to increase my bet sizes until I can cancel it out over the minimum? "
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U01D23DDMTQ",
        "type": "message",
        "ts": "1732539558.454519",
        "client_msg_id": "9F6F1273-0DA1-4FDA-896D-99904C5CDC42",
        "text": "I’ve been put back in my box sometimes either here or perhaps more over on the Australian forum when I seek to compare betting markets and exchanges with financial markets, but financial markets are my background and you can learn a lot about market impact from them. Plenty out there to read.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "5d6a9e5a85c0",
            "image_72": "https:\/\/avatars.slack-edge.com\/2021-06-26\/2219792322468_5d6a9e5a85c01e3abc9e_72.png",
            "first_name": "Andrew",
            "real_name": "Andrew",
            "display_name": "",
            "team": "T4G9NBD2M",
            "name": "agruskin",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732529263.662169",
        "parent_user_id": "U4H19D1D2",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "JdHGS",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I’ve been "
                            },
                            {
                                "type": "text",
                                "text": "put"
                            },
                            {
                                "type": "text",
                                "text": " back in my box sometimes either here or perhaps more over on "
                            },
                            {
                                "type": "text",
                                "text": "the"
                            },
                            {
                                "type": "text",
                                "text": " Australian forum when I seek to compare betting markets and exchange"
                            },
                            {
                                "type": "text",
                                "text": "s"
                            },
                            {
                                "type": "text",
                                "text": " with financial markets, but financial markets are my background and you can learn a lot about market impact from them. Plenty out there to read."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]
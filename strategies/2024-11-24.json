[
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1732446779.760249",
        "edited": {
            "user": "UBS7QANF3",
            "ts": "1732446853.000000"
        },
        "client_msg_id": "40D7A6C4-4C8A-4039-B84D-33E6F8E35BC6",
        "text": "Thanks for the replies guys, has definitely given me something to think about. While I can see the advantages to converting to a tabular format, what I really have in mind is something capable of producing the standard MarketBook dicts\/objects that betfairlightweight, betfairutil, betfairviz, and flumine work with so it can slot into existing workflows with minimal effort",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Tv4ix",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Thanks for the replies guys, has definitely given me something to think about. While I can see the advantages to converting to a tabular format, what I really have in mind is something capable of producing the standard MarketBook dicts\/objects that betfairlightweight"
                            },
                            {
                                "type": "text",
                                "text": ", betfairutil, betfairviz, "
                            },
                            {
                                "type": "text",
                                "text": "and flumine work with so it can slot into existing workflows with minimal effort"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U4H19D1D2",
        "type": "message",
        "ts": "1732453838.948059",
        "client_msg_id": "3b851acd-3407-4110-9b80-be7ed40d2034",
        "text": "Do you think this would solve the problems you are trying to solve vs just improving the speed of processing the raw stream? ie. get the Rust wrapper up to a level it can be used",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "137c5a3ef323",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-15\/6819395173841_137c5a3ef323f1944a1a_72.png",
            "first_name": "liam",
            "real_name": "liam",
            "display_name": "liam",
            "team": "T4G9NBD2M",
            "name": "liam",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "pYyvJ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Do you think this would solve the problems you are trying to solve vs just improving the speed of processing the raw stream? ie. get the Rust wrapper up to a level it can be used"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1732458170.706919",
        "client_msg_id": "3AD0195A-F340-4A79-BF75-47B9795F032F",
        "text": "Some use cases I have:\n\n1. Get the market books for all markets just before they turned in play\n2. Get the market books corresponding to the times that my bets got filled\n3. Extract the winner(s) of every market\n4. Calculate the total volume traded for every market\n5. Get the market books before and after X minutes before the market start time\n6. Speed up flumine backtests",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "cgBXW",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Some use cases I have:\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Get the market books for all markets just before they turned in play"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Get the market books corresponding to the times that my bets got filled"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Extract the winner(s) of every market"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Calculate the total volume traded for every market"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Get the market books before and after X minutes before the market start time"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Speed up flumine backtests"
                                    }
                                ]
                            }
                        ],
                        "style": "ordered",
                        "indent": 0,
                        "offset": 0,
                        "border": 0
                    }
                ]
            }
        ]
    },
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1732458200.880389",
        "client_msg_id": "842F5347-33BE-4D2D-8928-80F95842C715",
        "text": "Given Rob seems to have abandoned the Rust wrapper I donâ€™t see any point trying to use that as a starting point",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "\/oLDf",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Given Rob seems to have abandoned the Rust wrapper I "
                            },
                            {
                                "type": "text",
                                "text": "donâ€™t"
                            },
                            {
                                "type": "text",
                                "text": " see any point trying to use that as a starting point"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1732458222.596949",
        "client_msg_id": "2058C97C-6B53-4F90-8E2C-3789AC0FCD5B",
        "text": "Some of the features I have in mind like lazy deserialisation should make it even faster than the Rust wrapper ",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "mcOqZ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Some of the features I have in mind like lazy deserialisation should make it even faster than the Rust wrapper "
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U03FS7KM2NL",
        "type": "message",
        "ts": "1732458341.796589",
        "client_msg_id": "D0E5F1C5-086E-404B-959E-5407AFB73917",
        "text": "Most of those I would just log directly from live\/backtest",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g160a197a059",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/160a197a059d9b97aabdd95ca66dc341.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "Ralegh",
            "real_name": "Ralegh",
            "display_name": "",
            "team": "T4G9NBD2M",
            "name": "ralegh",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "NFXgr",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Most of those I would just log directly from live\/backtest"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U07SPFFN010",
        "type": "message",
        "ts": "1732458342.199039",
        "client_msg_id": "21f379ca-c168-4738-aa4b-cc6fe9ebfa1c",
        "text": "<@UBS7QANF3> I'm able to do just that (except #6). I really thing a db is the way to go. embedded bitemporal db like xtdb is awesome, but any db should suffice for such simple stuff",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "013148a74023",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-10-20\/7910027379108_013148a74023365902a3_72.png",
            "first_name": "Jhonny",
            "real_name": "Jhonny",
            "display_name": "Jhonny",
            "team": "T4G9NBD2M",
            "name": "olajeremy123",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "w4NIG",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UBS7QANF3"
                            },
                            {
                                "type": "text",
                                "text": " I'm able to do just that (except #6). I really thing a db is the way to go. embedded bitemporal db like xtdb is awesome, but any db should suffice for such simple stuff"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1732458715.699629",
        "client_msg_id": "B9E0CABB-8BFB-444C-859C-D11E86DDBD5C",
        "text": "<@U03FS7KM2NL> sure, easy enough to do these when you know you want them. The problem is when you come up with a new requirement and need to process the market again",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "QrI\/D",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "U03FS7KM2NL"
                            },
                            {
                                "type": "text",
                                "text": " sure, easy enough to do these when you know you want them"
                            },
                            {
                                "type": "text",
                                "text": "."
                            },
                            {
                                "type": "text",
                                "text": " The problem is when you come up with a new requirement and need to process the market again"
                            }
                        ]
                    }
                ]
            }
        ],
        "reactions": [
            {
                "name": "100",
                "users": [
                    "UUE6E1LA1"
                ],
                "count": 1
            }
        ]
    },
    {
        "user": "UBS7QANF3",
        "type": "message",
        "ts": "1732460306.988059",
        "client_msg_id": "e738f9c4-c01d-4f6f-9664-946bbac0347e",
        "text": "<@U07SPFFN010> I don't doubt that your approach is powerful but 6. is a key requirement",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gaaf844a4a90",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/eaaf844a4a905431d83430e563b077aa.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0011-72.png",
            "first_name": "",
            "real_name": "Maurice Berk",
            "display_name": "Mo",
            "team": "T4G9NBD2M",
            "name": "maurice",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "iDZCO",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "U07SPFFN010"
                            },
                            {
                                "type": "text",
                                "text": " I don't doubt that your approach is powerful but 6. is a key requirement"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U04H54Q392N",
        "type": "message",
        "ts": "1732460713.888959",
        "edited": {
            "user": "U04H54Q392N",
            "ts": "1732460855.000000"
        },
        "client_msg_id": "57ab78ae-e33c-4c67-9d88-ff3c1b53b519",
        "text": "a potentially cheap experiment would be to add a caching layer that takes your back testing input (recording JSON) and then pickles it. e.g.\n```USE_PICKLE_CACHE = bool(os.evnrion.get('USE_PICKLE_CACHE'))\n\ndef my_pickle_cache(inner_function):\n    \"\"\"Use a cache if enabled with a non-empty USE_PICKLE_CACHE value in the environment.\"\"\"\n\n    if not USE_PICKLE_CACHE:\n        return inner_function\n    @functools.wraps(inner_function)\n    def cacher(filename):\n        cache_path = f'{filename}.pkl'\n        with open(cache_path, 'rb') as f:\n            return pickle.load(f)\n        result = inner_function(filename)\n        with open(cache_path, 'wb') as f:\n            pickle.dump(result, f, pickle.HIGHEST_PROTOCOL)\n        return result\n     return cacher\n\n@my_pickle_cache\ndef read_normal_json_input(filename) -> list[dict]:\n    ...```\nthen you cache once. If IO rate is the issue (probably a natural follow on to test), you could replace the `with open(...)` stuff with `with gzip.open(...)` and use something like level 4 compression, which I suspect is around the sweet spot for maximising throughput.\n\nThe pickle version is a variable that can have a bit of impact on its performance, so something to experiment with, just like compression level.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "922f15b4653c",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-11-24\/8061990184023_922f15b4653c337eadd3_72.jpg",
            "first_name": "Oliver",
            "real_name": "Oliver",
            "display_name": "Oliver",
            "team": "T4G9NBD2M",
            "name": "evilumbrella",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "cn8wM",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "a potentially cheap experiment would be to add a caching layer that takes your back testing input (recording JSON) and then pickles it. e.g.\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_preformatted",
                        "elements": [
                            {
                                "type": "text",
                                "text": "USE_PICKLE_CACHE = bool(os.evnrion.get('USE_PICKLE_CACHE'))\n\ndef my_pickle_cache(inner_function):\n    \"\"\"Use a cache if enabled with a non-empty USE_PICKLE_CACHE value in the environment.\"\"\"\n\n    if not USE_PICKLE_CACHE:\n        return inner_function\n    @functools.wraps(inner_function)\n    def cacher(filename):\n        cache_path = f'{filename}.pkl'\n        with open(cache_path, 'rb') as f:\n            return pickle.load(f)\n        result = inner_function(filename)\n        with open(cache_path, 'wb') as f:\n            pickle.dump(result, f, pickle.HIGHEST_PROTOCOL)\n        return result\n     return cacher\n\n@my_pickle_cache\ndef read_normal_json_input(filename) -> list[dict]:\n    ..."
                            }
                        ],
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "then you cache once. If IO rate is the issue (probably a natural follow on to test), you could replace the "
                            },
                            {
                                "type": "text",
                                "text": "with open(...)",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " stuff with "
                            },
                            {
                                "type": "text",
                                "text": "with gzip.open(...)",
                                "style": {
                                    "code": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " and use something like level 4 compression, which I suspect is around the sweet spot for maximising throughput.\n\nThe pickle version is a variable that can have a bit of impact on its performance, so something to experiment with, just like compression level."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U07SPFFN010",
        "type": "message",
        "ts": "1732461060.054329",
        "client_msg_id": "4929a7fb-450d-4052-9ae6-0a4689ecf8d6",
        "text": "<@UBS7QANF3> I don't see why #6 isn't possible. I mentioned I don't do it simply because my stack is in clojure, not python. But, yh, I do have backtest speed and flexibility (I'm still working on this part tho). Maybe even have 2 storage formats optimized for different needs if needed.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "013148a74023",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-10-20\/7910027379108_013148a74023365902a3_72.png",
            "first_name": "Jhonny",
            "real_name": "Jhonny",
            "display_name": "Jhonny",
            "team": "T4G9NBD2M",
            "name": "olajeremy123",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "ECJrF",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UBS7QANF3"
                            },
                            {
                                "type": "text",
                                "text": " I don't see why #6 isn't possible. I mentioned I don't do it simply because my stack is in clojure, not python. But, yh, I do have backtest speed and flexibility (I'm still working on this part tho). Maybe even have 2 storage formats optimized for different needs if needed."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "US2RWCWKY",
        "type": "message",
        "ts": "1732491178.807199",
        "edited": {
            "user": "US2RWCWKY",
            "ts": "1732491300.000000"
        },
        "client_msg_id": "d6d74737-361e-4b43-a5d6-704f28c76c62",
        "text": "If youâ€™re familiar with a systems language like Rust or C++, you can solve your requirements 1 through 5 by just having a pretty minimal program in a systems language that processes the streams files into an orderbook, and doesnâ€™t have to integrate with flumine or anything.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gf934fece8db",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/f934fece8db401123296fc3e9f70453a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png",
            "first_name": "",
            "real_name": "Alex A",
            "display_name": "",
            "team": "T4G9NBD2M",
            "name": "alexander.abboud",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "+2lAL",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "If youâ€™re familiar with a systems language like Rust or C++, you can solve your requirements 1 through 5 by just having a pretty minimal program in a systems language that processes the streams files into an orderbook, and doesnâ€™t have to integrate with flumine or anything."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "US2RWCWKY",
        "type": "message",
        "ts": "1732491442.366039",
        "client_msg_id": "9ab14dde-aeff-4c86-a2eb-3095bb8d9b6a",
        "text": "My bot is in C++, and running on my laptop using 16 threads I can process six weeks of horse racing in 90 seconds and spit out the answers to your first five questions, so doing it for a year of data would only take about fifteen minutes.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gf934fece8db",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/f934fece8db401123296fc3e9f70453a.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0009-72.png",
            "first_name": "",
            "real_name": "Alex A",
            "display_name": "",
            "team": "T4G9NBD2M",
            "name": "alexander.abboud",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1732377025.890449",
        "parent_user_id": "UBS7QANF3",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "zXwJD",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "My bot is in C++, and running on my laptop using 16 threads I can process six weeks of horse racing in 90 seconds and spit out the answers to your first five questions, so doing it for a year of data would only take about fifteen minutes."
                            }
                        ]
                    }
                ]
            }
        ]
    }
]
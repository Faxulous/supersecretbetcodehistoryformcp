[
    {
        "user": "U05N9773A23",
        "type": "message",
        "ts": "1718560094.852429",
        "client_msg_id": "c303cf21-eea7-4bd7-bc48-504c2fbb17c9",
        "text": "How is everyone on here handling their data manipulation needs for betfair market data? Even with Polars' LazyFrame API and 64GB of memory, I am struggling to work with my data which includes the full price ladder + volumes traded at every tick for every selection in the market. The resulting dataframe is &gt; 30 million rows and will only get bigger as I collect more data. Is there a smarter approach?",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g9a60fb53f27",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/9a60fb53f27a5eefe4207d5f5b9ce4bc.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0021-72.png",
            "first_name": "Justice",
            "real_name": "Justice",
            "display_name": "Justice",
            "team": "T4G9NBD2M",
            "name": "bkwdev",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "reply_count": 12,
        "reply_users_count": 6,
        "latest_reply": "1718654352.280229",
        "reply_users": [
            "U03FS7KM2NL",
            "U05N9773A23",
            "UEA14GBRR",
            "UPMUFSGCR",
            "UC70576CB",
            "U0128E7BEHW"
        ],
        "replies": [
            {
                "user": "U03FS7KM2NL",
                "ts": "1718562228.806609"
            },
            {
                "user": "U05N9773A23",
                "ts": "1718562271.920679"
            },
            {
                "user": "UEA14GBRR",
                "ts": "1718562309.271369"
            },
            {
                "user": "U03FS7KM2NL",
                "ts": "1718562322.309219"
            },
            {
                "user": "U05N9773A23",
                "ts": "1718562665.658789"
            },
            {
                "user": "UPMUFSGCR",
                "ts": "1718563556.083689"
            },
            {
                "user": "U03FS7KM2NL",
                "ts": "1718566059.152629"
            },
            {
                "user": "UC70576CB",
                "ts": "1718578582.205879"
            },
            {
                "user": "UC70576CB",
                "ts": "1718578718.541829"
            },
            {
                "user": "U0128E7BEHW",
                "ts": "1718654158.767439"
            },
            {
                "user": "U0128E7BEHW",
                "ts": "1718654248.732359"
            },
            {
                "user": "U05N9773A23",
                "ts": "1718654352.280229"
            }
        ],
        "is_locked": false,
        "subscribed": false,
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "\/xTbd",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "How is everyone on here handling their data manipulation needs for betfair market data? Even with Polars' LazyFrame API and 64GB of memory, I am struggling to work with my data which includes the full price ladder + volumes traded at every tick for every selection in the market. The resulting dataframe is > 30 million rows and will only get bigger as I collect more data. Is there a smarter approach?"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U0128E7BEHW",
        "type": "message",
        "ts": "1718560707.801219",
        "client_msg_id": "6967e919-1869-46c9-bca1-2d146a13468d",
        "text": "Outside of flumine I use the historical generator stream from bflw, replay the files and handle it tick by tick. You don't need all ticks going back to when the market was open, so you can cache the bits you need yourself as you process each next tick. The advantage here as well is that this is how you'll process market data in production, so you can run the same logic for things like feature generation for both historical and realtime data.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "gb57a2bdd15a",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/b57a2bdd15acccdb845ce257f38940cc.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0024-72.png",
            "first_name": "Dave",
            "real_name": "Dave R",
            "display_name": "Dave",
            "team": "T4G9NBD2M",
            "name": "d7m",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "jlZb1",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Outside of flumine I use the historical generator stream from bflw, replay the files and handle it tick by tick. You don't need all ticks going back to when the market was open, so you can cache the bits you need yourself as you process each next tick. The advantage here as well is that this is how you'll process market data in production, so you can run the same logic for things like feature generation for both historical and realtime data."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U03FS7KM2NL",
        "type": "message",
        "ts": "1718562228.806609",
        "client_msg_id": "5EF8999F-4F91-41CD-BECF-9C848B55A8E3",
        "text": "Is this live? For research I would just get a big ec2 instance",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g160a197a059",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/160a197a059d9b97aabdd95ca66dc341.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "Ralegh",
            "real_name": "Ralegh",
            "display_name": "",
            "team": "T4G9NBD2M",
            "name": "ralegh",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "6DC9S",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Is this live? For research I would just get a big ec2 instance"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U05N9773A23",
        "type": "message",
        "ts": "1718562271.920679",
        "client_msg_id": "7a5072cb-349f-4221-99c5-415fcb041c7f",
        "text": "<@U03FS7KM2NL> Research, testing and model training",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g9a60fb53f27",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/9a60fb53f27a5eefe4207d5f5b9ce4bc.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0021-72.png",
            "first_name": "Justice",
            "real_name": "Justice",
            "display_name": "Justice",
            "team": "T4G9NBD2M",
            "name": "bkwdev",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "i3A19",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "U03FS7KM2NL"
                            },
                            {
                                "type": "text",
                                "text": " Research, testing and model training"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UEA14GBRR",
        "type": "message",
        "ts": "1718562309.271369",
        "client_msg_id": "218bdbd1-69fe-46dd-89c7-c1e573e64b74",
        "text": "SQL",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "889680f46fe7",
            "image_72": "https:\/\/avatars.slack-edge.com\/2024-03-06\/6774538588752_889680f46fe773047cb9_72.jpg",
            "first_name": "",
            "real_name": "Shaun White",
            "display_name": "ShaunW",
            "team": "T4G9NBD2M",
            "name": "shaunwhite",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "mh+IA",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "SQL"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U03FS7KM2NL",
        "type": "message",
        "ts": "1718562322.309219",
        "client_msg_id": "B8E26DFD-4982-4FDB-AF02-27115A986AF3",
        "text": "You can get a spot instance with 1TB+ ram for a few bucks an hour",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g160a197a059",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/160a197a059d9b97aabdd95ca66dc341.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "Ralegh",
            "real_name": "Ralegh",
            "display_name": "",
            "team": "T4G9NBD2M",
            "name": "ralegh",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "9tOXo",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "You can get a spot instance with 1TB+ ram for a few bucks an hour"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U05N9773A23",
        "type": "message",
        "ts": "1718562665.658789",
        "client_msg_id": "99fca3fe-d5a9-4209-b4fa-81613832673f",
        "text": "<@UEA14GBRR> Yes I could use SQL, with a properly normalized database. But I am going to have to de-normalize and load into memory at some point to train my model\/test\/visualize etc.\n\nI could lazily iterate which would avoid memory issues as <@U0128E7BEHW> suggested but it would be awfully slow and I'm impatient. Perhaps I just need more compute.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g9a60fb53f27",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/9a60fb53f27a5eefe4207d5f5b9ce4bc.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0021-72.png",
            "first_name": "Justice",
            "real_name": "Justice",
            "display_name": "Justice",
            "team": "T4G9NBD2M",
            "name": "bkwdev",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "F183D",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UEA14GBRR"
                            },
                            {
                                "type": "text",
                                "text": " Yes I could use SQL, with a properly normalized database. But I am going to have to de-normalize and load into memory at some point to train my model\/test\/visualize etc.\n\nI could lazily iterate which would avoid memory issues as "
                            },
                            {
                                "type": "user",
                                "user_id": "U0128E7BEHW"
                            },
                            {
                                "type": "text",
                                "text": " suggested but it would be awfully slow and I'm impatient. Perhaps I just need more compute."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UPMUFSGCR",
        "type": "message",
        "ts": "1718563556.083689",
        "client_msg_id": "9cccb71f-cf02-4a31-8cf5-e7d6d59d0b31",
        "text": "I store in feather using Pandas. how you store and organise the data has a big effect. Might be worth looking at how HFT's store and manage their data. It's very valuable knowledge.\n\nAlso,  it's one of the few areas where the nonsense software engineers get tested on in interviews actually might have relevance.",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "31c0bb5a442c",
            "image_72": "https:\/\/avatars.slack-edge.com\/2019-10-28\/812386967189_31c0bb5a442c5b8d2c61_72.png",
            "first_name": "Jon",
            "real_name": "Jon Jon Jon Jon Jon Jon Jon Jon",
            "display_name": "Jonjonjon",
            "team": "T4G9NBD2M",
            "name": "fcmisc",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "IJz6i",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I store in feather using Pandas. how you store and organise the data has a big effect. Might be worth looking at how HFT's store and manage their data. It's very valuable knowledge.\n\nAlso,  it's one of the few areas where the nonsense software engineers get tested on in interviews actually might have relevance."
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "U03FS7KM2NL",
        "type": "message",
        "ts": "1718566059.152629",
        "client_msg_id": "AF9D7905-7053-4680-B697-55305A3F040A",
        "text": "Flat file is generally the best, you don’t stream to parquet so you either stream to (eg) CSV or betfairs json format then batch convert to parquet, or batch convert historical data to parquet. <https:\/\/signalsandthreads.com\/state-machine-replication-and-why-you-should-care\/|https:\/\/signalsandthreads.com\/state-machine-replication-and-why-you-should-care\/> - good reading, generally advice is to have writing be single threaded (ie don’t need a dbms) which reduces complexity. If JS can handle their shittons of data in a single thread (and most financial exchanges) then almost anyone can. My betting stuff is set up roughly similar, all data gets logged to disk and the trading is full deterministic so I can fully replicate a days data in the right order and see exactly why everything happened. Each day gets zipped as one file and put in S3, about 100MB each",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g160a197a059",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/160a197a059d9b97aabdd95ca66dc341.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
            "first_name": "Ralegh",
            "real_name": "Ralegh",
            "display_name": "",
            "team": "T4G9NBD2M",
            "name": "ralegh",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "cU5l0",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Flat file is generally the best, you don’t stream to parquet so you either stream to (eg) CSV or betfairs json format then batch convert to parquet, or batch convert historical data to parquet. "
                            },
                            {
                                "type": "link",
                                "url": "https:\/\/signalsandthreads.com\/state-machine-replication-and-why-you-should-care\/",
                                "text": "https:\/\/signalsandthreads.com\/state-machine-replication-and-why-you-should-care\/"
                            },
                            {
                                "type": "text",
                                "text": " - good reading, generally advice is to have writing be single threaded (ie don’t need a dbms) which reduces complexity. If JS can handle their shittons of data in a single thread (and most financial exchanges) then almost anyone can. My betting stuff is set up roughly similar, all data gets logged to disk and the trading is full deterministic so I can fully replicate a days data in the right order and see exactly why everything happened. Each day gets zipped as one file and put in S3, about 100MB each"
                            }
                        ]
                    }
                ]
            }
        ]
    },
    {
        "user": "UC70576CB",
        "type": "message",
        "ts": "1718578582.205879",
        "edited": {
            "user": "UC70576CB",
            "ts": "1718578617.000000"
        },
        "client_msg_id": "006e32bf-717a-423b-a588-0e3cc55bece1",
        "text": "I tend to work in RAM, would consider:\n\n• don't store every row if you don't need to - does every row change every tick? if not, store each unique once with a from\/to datetime\n•  use the smallest possible data types\n• do feature engineering or get feature importance on a smaller sample, and then build your model on a much dataset of columns that amtter  - obviously YMMV, but I don't build models with the full set of traded volumes at every price\n• sample - lots of the time, you're not going to get a massively different answer with a subset that does fit in RAM\n• buy more RAM (it's relatively cheap compared to the time you're investing - 128Gb of DDR5 isn't crazy)\n• get it working locally on a sample, then use a cloud server (e.g. EC2) to fun your full build",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g3b214cb6303",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/83b214cb63031e2a2ae3958016f7bf23.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Rob",
            "display_name": "Rob",
            "team": "T4G9NBD2M",
            "name": "rnobleeddy",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "7eM2Z",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I tend to work in RAM, would consider:\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "don't store every row if you don't need to - does every row change every tick? if not, store each unique once with a from\/to datetime"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": " use the smallest possible data types"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "do feature engineering or get feature importance on a smaller sample, and then build your model on a much dataset of columns that amtter  - obviously YMMV, but I don't build models with the full set of traded volumes at every price"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "sample - lots of the time, you're not going to get a massively different answer with a subset that does fit in RAM"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "buy more RAM (it's relatively cheap compared to the time you're investing - 128Gb of DDR5 isn't crazy)"
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "get it working locally on a sample, then use a cloud server (e.g. EC2) to fun your full build"
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    }
                ]
            }
        ]
    },
    {
        "user": "UC70576CB",
        "type": "message",
        "ts": "1718578718.541829",
        "client_msg_id": "c0882a7c-9c57-4c7f-9f08-84305f0cea06",
        "text": "As someone else said, I would personally prioritise using the exact same code in production vs development, otherwise, with just me in my spare time,  the chance of making a mistake there is too large (different story if you're a massive business with the resource for lots of testing)",
        "team": "T4G9NBD2M",
        "user_team": "T4G9NBD2M",
        "source_team": "T4G9NBD2M",
        "user_profile": {
            "avatar_hash": "g3b214cb6303",
            "image_72": "https:\/\/secure.gravatar.com\/avatar\/83b214cb63031e2a2ae3958016f7bf23.jpg?s=72&d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0015-72.png",
            "first_name": "",
            "real_name": "Rob",
            "display_name": "Rob",
            "team": "T4G9NBD2M",
            "name": "rnobleeddy",
            "is_restricted": false,
            "is_ultra_restricted": false
        },
        "thread_ts": "1718560094.852429",
        "parent_user_id": "U05N9773A23",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "iI8Gt",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "As someone else said, I would personally prioritise using the exact same code in production vs development, otherwise, with just me in my spare time,  the chance of making a mistake there is too large (different story if you're a massive business with the resource for lots of testing)"
                            }
                        ]
                    }
                ]
            }
        ]
    }
]